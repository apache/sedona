{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Licensed to the Apache Software Foundation (ASF) under one\n",
    "or more contributor license agreements.  See the NOTICE file\n",
    "distributed with this work for additional information\n",
    "regarding copyright ownership.  The ASF licenses this file\n",
    "to you under the Apache License, Version 2.0 (the\n",
    "\"License\"); you may not use this file except in compliance\n",
    "with the License.  You may obtain a copy of the License at\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an\n",
    "\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "KIND, either express or implied.  See the License for the\n",
    "specific language governing permissions and limitations\n",
    "under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jovyan/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3625fe59-a0fc-45df-ae07-108b2b675db5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n",
      "\tfound org.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating in central\n",
      "\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-python-adapter-3.0_2.12/1.1.0-incubating/sedona-python-adapter-3.0_2.12-1.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating!sedona-python-adapter-3.0_2.12.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.1.0-25.2/geotools-wrapper-1.1.0-25.2.jar ...\n",
      "\t[SUCCESSFUL ] org.datasyslab#geotools-wrapper;1.1.0-25.2!geotools-wrapper.jar (687ms)\n",
      "downloading https://repo1.maven.org/maven2/org/locationtech/jts/jts-core/1.18.0/jts-core-1.18.0.jar ...\n",
      "\t[SUCCESSFUL ] org.locationtech.jts#jts-core;1.18.0!jts-core.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wololo/jts2geojson/0.16.1/jts2geojson-0.16.1.jar ...\n",
      "\t[SUCCESSFUL ] org.wololo#jts2geojson;0.16.1!jts2geojson.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-core-3.0_2.12/1.1.0-incubating/sedona-core-3.0_2.12-1.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating!sedona-core-3.0_2.12.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-sql-3.0_2.12/1.1.0-incubating/sedona-sql-3.0_2.12-1.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating!sedona-sql-3.0_2.12.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.12.2/jackson-databind-2.12.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.12.2!jackson-databind.jar(bundle) (43ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.12.2/jackson-annotations-2.12.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.12.2!jackson-annotations.jar(bundle) (11ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.12.2/jackson-core-2.12.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.12.2!jackson-core.jar(bundle) (17ms)\n",
      ":: resolution report :: resolve 9110ms :: artifacts dl 931ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n",
      "\torg.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   9   |   9   |   1   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3625fe59-a0fc-45df-ae07-108b2b675db5\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (35019kB/52ms)\n",
      "22/08/28 04:59:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\") .\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     arealandmark|\n",
      "+-----------------+\n",
      "|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|\n",
      "|POINT (3.1 103.1)|\n",
      "|POINT (4.1 104.1)|\n",
      "|POINT (5.1 105.1)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\")\n",
    "point_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkt_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small.tsv\")\n",
    "\n",
    "polygon_wkt_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromWKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkb_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small_wkb.tsv\")\n",
    "\n",
    "polygon_wkb_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromGeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         countyshape|\n",
      "+--------------------+\n",
      "|POLYGON ((-87.621...|\n",
      "|POLYGON ((-85.719...|\n",
      "|POLYGON ((-86.000...|\n",
      "|POLYGON ((-86.574...|\n",
      "|POLYGON ((-85.382...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_json_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testPolygon.json\")\n",
    "\n",
    "polygon_json_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Distance Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "DistanceJoin pointshape1#259: geometry, pointshape2#283: geometry, 2.0, false\n",
      ":- Project [st_point(cast(_c0#255 as decimal(24,20)), cast(_c1#256 as decimal(24,20))) AS pointshape1#259, abc AS name1#260]\n",
      ":  +- FileScan csv [_c0#255,_c1#256] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/binder/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+- Project [st_point(cast(_c0#279 as decimal(24,20)), cast(_c1#280 as decimal(24,20))) AS pointshape2#283, def AS name2#284]\n",
      "   +- FileScan csv [_c0#279,_c1#280] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/binder/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/28 05:00:01 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+-----+\n",
      "|      pointshape1|name1|      pointshape2|name2|\n",
      "+-----------------+-----+-----------------+-----+\n",
      "|POINT (1.1 101.1)|  abc|POINT (1.1 101.1)|  def|\n",
      "|POINT (2.1 102.1)|  abc|POINT (1.1 101.1)|  def|\n",
      "|POINT (1.1 101.1)|  abc|POINT (2.1 102.1)|  def|\n",
      "|POINT (2.1 102.1)|  abc|POINT (2.1 102.1)|  def|\n",
      "|POINT (3.1 103.1)|  abc|POINT (2.1 102.1)|  def|\n",
      "+-----------------+-----+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df_1 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df_1.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df1 = spark.sql(\"SELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1, \\'abc\\' as name1 from pointtable\")\n",
    "point_df1.createOrReplaceTempView(\"pointdf1\")\n",
    "\n",
    "point_csv_df2 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df2.createOrReplaceTempView(\"pointtable\")\n",
    "point_df2 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2, \\'def\\' as name2 from pointtable\")\n",
    "point_df2.createOrReplaceTempView(\"pointdf2\")\n",
    "\n",
    "distance_join_df = spark.sql(\"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) < 2\")\n",
    "distance_join_df.explain()\n",
    "distance_join_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Range Join and RDD API Join\n",
    "\n",
    "Please refer to the example - airports per country: https://github.com/apache/sedona/blob/master/binder/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting GeoPandas to Apache Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/geopandas/geodataframe.py:35: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  out = from_shapely(data)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "gdf = gpd.read_file(\"data/gis_osm_pois_free_1.shp\")\n",
    "gdf = gdf.replace(pd.NA, '')\n",
    "osm_points = spark.createDataFrame(\n",
    "    gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- osm_id: string (nullable = true)\n",
      " |-- code: long (nullable = true)\n",
      " |-- fclass: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|            geometry|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (15.3393145...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (14.8709625...|\n",
      "|29947493|2402|    motel|          null|POINT (15.0946636...|\n",
      "|29947498|2602|      atm|          null|POINT (15.0732014...|\n",
      "|29947499|2401|    hotel|          null|POINT (15.0696777...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "osm_points.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_points.createOrReplaceTempView(\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT osm_id,\n",
    "               code,\n",
    "               fclass,\n",
    "               name,\n",
    "               ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom \n",
    "        FROM points\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|                geom|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (-3288183.3...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (-3341183.9...|\n",
      "|29947493|2402|    motel|          null|POINT (-3320466.5...|\n",
      "|29947498|2602|      atm|          null|POINT (-3323205.7...|\n",
      "|29947499|2401|    hotel|          null|POINT (-3323655.1...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.createOrReplaceTempView(\"points_2180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_within_1000m = spark.sql(\"\"\"\n",
    "        SELECT a.osm_id AS id_1,\n",
    "               b.osm_id AS id_2,\n",
    "               a.geom \n",
    "        FROM points_2180 AS a, points_2180 AS b \n",
    "        WHERE ST_Distance(a.geom,b.geom) < 50\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/28 05:00:09 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 22:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|      id_1|     id_2|                geom|\n",
      "+----------+---------+--------------------+\n",
      "|  26860294| 26860294|POINT (-3341183.9...|\n",
      "|  29947493| 29947493|POINT (-3320466.5...|\n",
      "|4165181885| 29947498|POINT (-3323204.4...|\n",
      "|5818905324| 29947498|POINT (-3323210.6...|\n",
      "|  29947498| 29947498|POINT (-3323205.7...|\n",
      "|  29947499| 29947499|POINT (-3323655.1...|\n",
      "|  30077461| 29947499|POINT (-3323697.1...|\n",
      "|  29947505| 29947505|POINT (-3330369.2...|\n",
      "|  29947499| 30077461|POINT (-3323655.1...|\n",
      "|  30077461| 30077461|POINT (-3323697.1...|\n",
      "| 197624402|197624402|POINT (-3383818.5...|\n",
      "| 197663196|197663196|POINT (-3383367.1...|\n",
      "| 197953474|197953474|POINT (-3383763.3...|\n",
      "| 262310516|262310516|POINT (-3384257.6...|\n",
      "|1074233123|262310516|POINT (-3384262.1...|\n",
      "| 270281140|270281140|POINT (-3385421.2...|\n",
      "|1074232906|270281140|POINT (-3385408.6...|\n",
      "| 270306609|270306609|POINT (-3383982.8...|\n",
      "| 270306746|270306746|POINT (-3383898.4...|\n",
      "| 273101780|273101780|POINT (-3389705.7...|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "neighbours_within_1000m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Apache Sedona to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/28 05:00:15 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = neighbours_within_1000m.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26860294</td>\n",
       "      <td>26860294</td>\n",
       "      <td>POINT (-3341183.976 4318356.064)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29947493</td>\n",
       "      <td>29947493</td>\n",
       "      <td>POINT (-3320466.547 4265941.760)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4165181885</td>\n",
       "      <td>29947498</td>\n",
       "      <td>POINT (-3323204.491 4266510.379)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5818905324</td>\n",
       "      <td>29947498</td>\n",
       "      <td>POINT (-3323210.654 4266502.772)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29947498</td>\n",
       "      <td>29947498</td>\n",
       "      <td>POINT (-3323205.784 4266548.416)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45314</th>\n",
       "      <td>6815618439</td>\n",
       "      <td>6815618435</td>\n",
       "      <td>POINT (-3285827.820 4250345.966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45315</th>\n",
       "      <td>6815618435</td>\n",
       "      <td>6815618439</td>\n",
       "      <td>POINT (-3285831.862 4250347.684)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>6815618439</td>\n",
       "      <td>6815618439</td>\n",
       "      <td>POINT (-3285827.820 4250345.966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45317</th>\n",
       "      <td>6815883980</td>\n",
       "      <td>6815883980</td>\n",
       "      <td>POINT (-3286165.443 4249818.008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45318</th>\n",
       "      <td>6817416704</td>\n",
       "      <td>6817416704</td>\n",
       "      <td>POINT (-3214549.268 4314872.904)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45319 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_1        id_2                              geom\n",
       "0        26860294    26860294  POINT (-3341183.976 4318356.064)\n",
       "1        29947493    29947493  POINT (-3320466.547 4265941.760)\n",
       "2      4165181885    29947498  POINT (-3323204.491 4266510.379)\n",
       "3      5818905324    29947498  POINT (-3323210.654 4266502.772)\n",
       "4        29947498    29947498  POINT (-3323205.784 4266548.416)\n",
       "...           ...         ...                               ...\n",
       "45314  6815618439  6815618435  POINT (-3285827.820 4250345.966)\n",
       "45315  6815618435  6815618439  POINT (-3285831.862 4250347.684)\n",
       "45316  6815618439  6815618439  POINT (-3285827.820 4250345.966)\n",
       "45317  6815883980  6815883980  POINT (-3286165.443 4249818.008)\n",
       "45318  6817416704  6817416704  POINT (-3214549.268 4314872.904)\n",
       "\n",
       "[45319 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
