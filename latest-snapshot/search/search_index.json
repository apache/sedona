{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#09122025-sedona-180-released-this-is-a-major-release-introducing-geopandas-compatible-api-pyflink-support-java-11-upgrade-spark-40-support-and-many-new-features-including-vectorized-udfs-moran-i-autocorrelation-mysql-geometry-support-and-enhanced-spatial-filter-pushdown","title":"09/12/2025: Sedona 1.8.0 released. This is a major release introducing GeoPandas Compatible API, PyFlink support, Java 11 upgrade, Spark 4.0 support, and many new features including vectorized UDFs, Moran I autocorrelation, MySQL geometry support, and enhanced spatial filter pushdown.","text":""},{"location":"#06072025-sedona-172-released-this-is-a-bug-fix-only-release-and-the-last-release-on-java-8","title":"06/07/2025: Sedona 1.7.2 released. This is a bug fix only release and the last release on Java 8.","text":""},{"location":"#03152025-sedona-171-released-it-includes-performance-improvement-for-knn-join-sql-interface-of-geostats-stac-catalog-reader-osm-pbf-reader-and-better-geoparquet-file-partitioning","title":"03/15/2025: Sedona 1.7.1 released. It includes performance improvement for KNN join, SQL interface of GeoStats, STAC catalog reader, OSM PBF reader, and better GeoParquet file partitioning.","text":""},{"location":"#12022024-sedona-170-released-it-introduces-a-new-join-type-named-knn-join-a-new-statistics-module-called-geostats-dataframe-based-readers-for-shapefiles-and-geopackage-and-many-new-st-functions","title":"12/02/2024: Sedona 1.7.0 released. It introduces a new join type named KNN Join, a new statistics module called GeoStats, DataFrame based readers for Shapefiles and GeoPackage, and many new ST functions.","text":""},{"location":"download/","title":"Download","text":""},{"location":"download/#github-repository","title":"GitHub repository","text":"<p>Latest source code: GitHub repository</p> <p>Old GeoSpark releases: GitHub releases</p> <p>Automatically generated binary JARs (per each Master branch commit): GitHub Action</p>"},{"location":"download/#verify-the-integrity","title":"Verify the integrity","text":"<p>Public keys</p> <p>Instructions</p>"},{"location":"download/#versions","title":"Versions","text":""},{"location":"download/#180","title":"1.8.0","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#172","title":"1.7.2","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#past-releases","title":"Past releases","text":"<p>Past Sedona releases are archived and can be found here: Apache archive (on and after 1.2.1-incubating).</p>"},{"location":"download/#security","title":"Security","text":"<p>For security issues, please refer to https://www.apache.org/security/</p>"},{"location":"api/java-api/","title":"Scala/Java doc","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/python-api/","title":"Python api","text":"<p>Will be available soon.</p>"},{"location":"api/flink/Aggregator/","title":"Aggregator (Flink)","text":""},{"location":"api/flink/Aggregator/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A. All inputs must be polygons.</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Constructor/","title":"Constructor (Flink)","text":""},{"location":"api/flink/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML1 and GML2. GML3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4) AS geom\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. Alias of  ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt: String)</code></p> <p><code>ST_GeomFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre> <p>Format: <code>ST_GeomFromWKB (Wkb: Bytes)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT</p> <p>Format: <code>ST_GeomFromWKT (Wkt: String)</code></p> <p><code>ST_GeomFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a LineString from Text</p> <p>Format: <code>ST_LineFromText (Text: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromText('Linestring(1 2, 3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter (Optional). Alias of  ST_LineFromText</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/flink/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Text and Optional SRID</p> <p>Format:</p> <p><code>ST_MLineFromText (Wkt: String)</code></p> <p><code>ST_MLineFromText (Wkt: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Text and Optional SRID</p> <p>Format:</p> <p><code>ST_MPolyFromText (Wkt: String)</code></p> <p><code>ST_MPolyFromText (Wkt: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/flink/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_makepointm","title":"ST_MakePointM","text":"<p>Introduction: Creates a point with X, Y, and M coordinate. Use ST_MakePoint to make points with XY, XYZ, or XYZM coordinates.</p> <p>Format: <code>ST_MakePointM(x: Double, y: Double, m: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePointM(1, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>Point M(1 2 3)\n</code></pre>"},{"location":"api/flink/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointm","title":"ST_PointM","text":"<p>Introduction: Construct a Point from X, Y and M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointM (X: Double, Y: Double, M: Double)</code></p> <p><code>ST_PointM (X: Double, Y: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointM(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 0 3.4567)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointzm","title":"ST_PointZM","text":"<p>Introduction: Construct a Point from X, Y, Z, M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZM(1.2345, 2.3456, 3.4567, 100))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 3.4567, 100)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/flink/Function/","title":"Function (Flink)","text":""},{"location":"api/flink/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/flink/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\n                     ST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output:</p> <pre><code>1.7320508075688772\n</code></pre>"},{"location":"api/flink/Function/#st_addmeasure","title":"ST_AddMeasure","text":"<p>Introduction: Computes a new geometry with measure (M) values linearly interpolated between start and end points. For geometries lacking M dimensions, M values are added. Existing M values are overwritten by the new values. Applies only to LineString and MultiLineString inputs.</p> <p>Format: <code>ST_AddMeasure(geom: Geometry, measureStart: Double, measureEnd: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_AddMeasure(\n        ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)')\n))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 0 10, 1 0 16, 2 0 22, 3 0 28, 4 0 34, 5 0 40)\n</code></pre>"},{"location":"api/flink/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: Return Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/flink/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Examples:</p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/flink/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Compute and return the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code></p> <p>Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/flink/Function/#st_approximatemedialaxis","title":"ST_ApproximateMedialAxis","text":"<p>Introduction: Computes an approximate medial axis of a polygonal geometry. The medial axis is a representation of the \"centerline\" or \"skeleton\" of the polygon. This function first computes the straight skeleton and then prunes insignificant branches to produce a cleaner result.</p> <p>The pruning removes small branches that represent minor penetrations into corners. A branch is pruned if its penetration depth is less than 20% of the width of the corner it bisects.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_ApproximateMedialAxis(geom: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ApproximateMedialAxis(\n  ST_GeomFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((50 45, 50 5), (50 45, 35 45), (65 45, 50 45), (35 45, 65 45))\n</code></pre>"},{"location":"api/flink/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>10\n</code></pre>"},{"location":"api/flink/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/flink/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry.</p> <p>The type parameter (Since: <code>v1.6.1</code>) takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A: Geometry)</code></p> <p><code>ST_AsGeoJSON (A: Geometry, type: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n    \"type\":\"Feature\",\n    \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n    }\n  }\n  ]\n}\n</code></pre>"},{"location":"api/flink/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/flink/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/flink/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/flink/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON ((1 1, 0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/flink/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>Spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : Defaults to <code>both</code>. Setting <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied when <code>left</code> or <code>right</code> are specified.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/flink/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(ST_Collect(smallDf, bigDf)) AS geom\nFROM smallDf, bigDf\n</code></pre> <p>Input: <code>MULTILINESTRING((0 0, 10 0, 10 10, 0 10, 0 0),(10 10, 20 10, 20 20, 10 20, 10 10))</code></p> <p>Output: <code>MULTIPOLYGON(((0 0,0 10,10 10,10 0,0 0)),((10 10,10 20,20 20,20 10,10 10)))</code></p>"},{"location":"api/flink/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/flink/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/flink/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Collect(\n    ST_GeomFromText('POINT(21.427834 52.042576573)'),\n    ST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Collect(\n    Array(\n        ST_GeomFromText('POINT(21.427834 52.042576573)'),\n        ST_GeomFromText('POINT(45.342524 56.342354355)')\n    )\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/flink/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/flink/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/flink/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Result:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/flink/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/flink/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/flink/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/flink/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/flink/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/flink/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/flink/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/flink/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/flink/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/flink/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a LINESTRING representing the exterior ring (shell) of a POLYGON. Returns NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/flink/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force2D.</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_force2d","title":"ST_Force2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force_2D.</p> <p>Format: <code>ST_Force2D (A: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Force2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/flink/Function/#st_force3dm","title":"ST_Force3DM","text":"<p>Introduction: Forces the geometry into XYM mode. Retains any existing M coordinate, but removes the Z coordinate if present. Assigns a default M value of 0.0 if <code>mValue</code> is not specified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds M for in the WKT.</p> <p>Format: <code>ST_Force3DM(geometry: Geometry, mValue: Double = 0.0)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('POLYGON M((0 0 3,0 5 3,5 0 3,0 0 3),(1 1 3,3 1 3,1 3 3,1 1 3))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON M((0 0 3, 0 5 3, 5 0 3, 0 0 3), (1 1 3, 3 1 3, 1 3 3, 1 1 3))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING Z(0 1 3,1 0 3,2 0 3)'), 5))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 5, 1 0 5, 2 0 5)\n</code></pre>"},{"location":"api/flink/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/flink/Function/#st_force4d","title":"ST_Force4D","text":"<p>Introduction: Converts the input geometry to 4D XYZM representation. Retains original Z and M values if present. Assigning 0.0 defaults if <code>mValue</code> and <code>zValue</code> aren't specified. The output contains X, Y, Z, and M coordinates. For geometries already in 4D form, the function returns the original geometry unmodified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format:</p> <p><code>ST_Force4D(geom: Geometry, zValue: Double, mValue: Double)</code></p> <p><code>ST_Force4D(geom: Geometry</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 5, 10))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ZM((0 0 2 10, 0 5 2 10, 5 0 2 10, 0 0 2 10), (1 1 2 10, 3 1 2 10, 1 3 2 10, 1 1 2 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 3, 1))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING ZM(0 1 3 1, 1 0 3 1, 2 0 3 1)\n</code></pre>"},{"location":"api/flink/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/flink/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/flink/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/flink/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/flink/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/flink/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/flink/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LINESTRING\n</code></pre>"},{"location":"api/flink/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[1])\n</code></pre> <p>Output:</p> <pre><code>+----+----------------------+\n| op |               EXPR$0 |\n+----+----------------------+\n| +I |                   78 |\n+----+----------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/flink/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/flink/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: true)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre> <p>Output:</p> <pre><code>+----+--------------------------------+\n| op |                         EXPR$0 |\n+----+--------------------------------+\n| +I | [605547539457900543, 605547... |\n+----+--------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, false), ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, true)\n</code></pre> <p>Output:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                         EXPR$0 |                         EXPR$1 |\n+----+--------------------------------+--------------------------------+\n| +I | [614552609325318143, 614552... | [614552597293957119, 614552... |\n+----+--------------------------------+--------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: Return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Converts an array of Uber H3 cell indices into an array of Polygon geometries, where each polygon represents a hexagonal H3 cell.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((1.0057629565405093 1.9984665139177547, 1.0037116327309097 2.0018325249140068, 0.999727799357053 2.001163270465665, 0.9977951427833316 1.997128228393235, 0.9998461908217928 1.993762152933182, 1.0038301712104316 1.9944311839965523, 1.0057629565405093 1.9984665139177547))]\n</code></pre>"},{"location":"api/flink/Function/#st_hasm","title":"ST_HasM","text":"<p>Introduction: Checks for the presence of M coordinate values representing measures or linear references. Returns true if the input geometry includes an M coordinate, false otherwise.</p> <p>Format: <code>ST_HasM(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasM(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/flink/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/flink/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/flink/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_interpolatepoint","title":"ST_InterpolatePoint","text":"<p>Introduction: Returns the interpolated measure value of a linear measured LineString at the point closest to the specified point.</p> <p>Note</p> <p>Make sure that both geometries have the same SRID, otherwise the function will throw an IllegalArgumentException.</p> <p>Format: <code>ST_InterpolatePoint(linestringM: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InterpolatePoint(\n    ST_GeomFromWKT(\"LINESTRING M (0 0 0, 2 0 2, 4 0 4)\"),\n    ST_GeomFromWKT(\"POINT (1 1)\")\n    )\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/flink/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection(\n    ST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\n    ST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/flink/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/flink/Function/#st_isvalidtrajectory","title":"ST_IsValidTrajectory","text":"<p>Introduction: This function checks if a geometry is a valid trajectory representation. For a trajectory to be considered valid, it must be a LineString that includes measure (M) values. The key requirement is that the M values increase from one vertex to the next as you move along the line.</p> <p>Format: <code>ST_IsValidTrajectory(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 2)')\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 0)')\n)\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/flink/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/flink/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Length2D(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/flink/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/flink/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/flink/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output:</p> <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre>"},{"location":"api/flink/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/flink/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString or MultiLineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If no merging can be performed, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/flink/Function/#st_linesegments","title":"ST_LineSegments","text":"<p>Introduction: This function transforms a LineString containing multiple coordinates into an array of LineStrings, each with precisely two coordinates. The <code>lenient</code> argument, true by default, prevents an exception from being raised if the input geometry is not a LineString.</p> <p>Format:</p> <p><code>ST_LineSegments(geom: Geometry, lenient: Boolean)</code></p> <p><code>ST_LineSegments(geom: Geometry)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineSegments(\n        ST_GeomFromWKT('LINESTRING(0 0, 10 10, 20 20, 30 30, 40 40, 50 50)'),\n       false\n    )\n</code></pre> <p>Output:</p> <pre><code>[LINESTRING (0 0, 10 10), LINESTRING (10 10, 20 20), LINESTRING (20 20, 30 30), LINESTRING (30 30, 40 40), LINESTRING (40 40, 50 50)]\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LineSegments(\n        ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0))')\n    )\n</code></pre> <p>Output:</p> <pre><code>[]\n</code></pre>"},{"location":"api/flink/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/flink/Function/#st_locatealong","title":"ST_LocateAlong","text":"<p>Introduction: This function computes Point or MultiPoint geometries representing locations along a measured input geometry (LineString or MultiLineString) corresponding to the provided measure value(s). Polygonal geometry inputs are not supported. The output points lie directly on the input line at the specified measure positions.</p> <p>Additionally, an optional <code>offset</code> parameter can shift the resulting points left or right from the input line. A positive offset displaces the points to the left side, while a negative value offsets them to the right side by the given distance.</p> <p>This allows identifying precise locations along a measured linear geometry based on supplied measure values, with the ability to offset the output points if needed.</p> <p>Format:</p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double, offset: Double)</code></p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LocateAlong(\n        ST_GeomFromText('LINESTRING M (10 30 1, 50 50 1, 30 110 2, 70 90 2, 180 140 3, 130 190 3)')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT M((30 110 2), (50 100 2), (70 90 2))\n</code></pre>"},{"location":"api/flink/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre>"},{"location":"api/flink/Function/#st_m","title":"ST_M","text":"<p>Introduction: Returns M Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_M(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_M(ST_MakePoint(1, 2, 3, 4))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_mmax","title":"ST_MMax","text":"<p>Introduction: Returns M maxima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMax(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MMax(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_mmin","title":"ST_MMin","text":"<p>Introduction: Returns M minima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMin(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MMin(\n        ST_GeomFromWKT('LINESTRING ZM(1 1 1 1, 2 2 2 2, 3 3 3 3, -1 -1 -1 -1)')\n)\n</code></pre> <p>Output:</p> <pre><code>-1.0\n</code></pre>"},{"location":"api/flink/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/flink/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/flink/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/flink/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/flink/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/flink/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments is 48.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/flink/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/flink/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format: <code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Returns the number of points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/flink/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/flink/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: Returns number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString.</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(1 2, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/flink/Function/#st_perimeter2d","title":"ST_Perimeter2D","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Info</p> <p>This function is an alias for ST_Perimeter.</p> <p>Format:</p> <p><code>ST_Perimeter2D(geom: Geometry)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/flink/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointN(df.geometry, 2)\nFROM df\n</code></pre> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), 2</code></p> <p>Output: <code>POINT (1 2)</code></p> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), -2</code></p> <p>Output: <code>POINT (2 4)</code></p> <p>Input: <code>CIRCULARSTRING(1 1, 1 2, 2 4, 3 6, 1 2, 1 1), -1</code></p> <p>Output: <code>POINT (1 1)</code></p>"},{"location":"api/flink/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointOnSurface(df.geometry)\nFROM df\n</code></pre> <ol> <li>Input: <code>POINT (0 5)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5, 0 10)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))</code></li> </ol> <p>Output: <code>POINT (2.5 2.5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5 1, 0 0 1, 0 10 2)</code></li> </ol> <p>Output: <code>POINT Z(0 0 1)</code></p>"},{"location":"api/flink/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/flink/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/flink/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/flink/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/flink/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/flink/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: Return Linestring with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/flink/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre>"},{"location":"api/flink/Function/#st_s2togeom","title":"ST_S2ToGeom","text":"<p>Introduction: Returns an array of Polygons for the corresponding S2 cell IDs.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_S2ToGeom(cellIds: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_S2ToGeom(array(11540474045136890))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((-36.609392788630245 -38.169532607255846, -36.609392706252954 -38.169532607255846, -36.609392706252954 -38.169532507473015, -36.609392788630245 -38.169532507473015, -36.609392788630245 -38.169532607255846))]\n</code></pre>"},{"location":"api/flink/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/flink/Function/#st_segmentize","title":"ST_Segmentize","text":"<p>Introduction: Returns a modified geometry having no segment longer than the given max_segment_length.</p> <p>The length calculation is performed in 2D. When a segment is longer than the specified maximum length, it is split into multiple, equal-length subsegments.</p> <p>Format: <code>ST_Segmentize(geom: Geometry, max_segment_length: Double)</code></p> <p>Since: v1.8.0</p> <p>SQL Example Long segments are split evenly into subsegments no longer than the specified length. Shorter segments are not modified.</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('MULTILINESTRING((0 0, 0 1, 0 9),(1 10, 1 18))'), 5));\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING((0 0,0 1,0 5,0 9),(1 10,1 14,1 18))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('POLYGON((0 0, 0 8, 30 0, 0 0))'), 10));\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 8,7.5 6,15 4,22.5 2,30 0,20 0,10 0,0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/flink/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/flink/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.5.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>Example:</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/flink/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n           ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/flink/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/flink/Function/#st_straightskeleton","title":"ST_StraightSkeleton","text":"<p>Introduction: Computes the straight skeleton of a polygonal geometry. The straight skeleton is a method of representing a polygon by a topological skeleton, formed by a continuous shrinking process where each edge moves inward in parallel at a uniform speed.</p> <p>This function uses the weighted straight skeleton algorithm based on Felkel's approach.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_StraightSkeleton(geom: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>Example:</p> <pre><code>SELECT ST_StraightSkeleton(\n  ST_GeomFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((50 5, 50 45), (50 45, 35 45), (50 45, 65 45), (35 45, 30 45), (35 45, 40 40), (65 45, 70 45), (65 45, 60 40), (50 5, 45 5), (50 5, 55 5))\n</code></pre>"},{"location":"api/flink/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>A minimum of 5 vertices is required for maxVertices parameter to form a closed box.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre>"},{"location":"api/flink/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/flink/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since v1.3.1.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will force the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, it will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, [Optional] lenientMode: Boolean)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/flink/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it.</p> <p>If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/flink/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/flink/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/flink/Function/#st_union","title":"ST_Union","text":"<p>Introduction:</p> <p>Variant 1: Return the union of geometry A and B.</p> <p>Variant 2: This function accepts an array of Geometry objects and returns the geometric union of all geometries in the input array. If the polygons within the input array do not share common boundaries, the ST_Union result will be a MultiPolygon geometry.</p> <p>Format:</p> <p><code>ST_Union (A: Geometry, B: Geometry)</code></p> <p><code>ST_Union (geoms: Array(Geometry))</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Union(\n    Array(\n        ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'),\n        ST_GeomFromWKT('POLYGON ((-2 1, 2 1, 2 4, -2 4, -2 1))')\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 3, 3 3, 3 -3, -3 -3, -3 3, -2 3, -2 4, 2 4, 2 3))\n</code></pre>"},{"location":"api/flink/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/flink/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/flink/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/flink/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/flink/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output :</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/flink/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmflag","title":"ST_Zmflag","text":"<p>Introduction: Returns a code indicating the Z and M coordinate dimensions present in the input geometry.</p> <p>Values are: 0 = 2D, 1 = 3D-M, 2 = 3D-Z, 3 = 4D.</p> <p>Format: <code>ST_Zmflag(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('LINESTRING Z(1 2 3, 4 5 6)')\n)\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('POINT ZM(1 2 3 4)')\n)\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/flink/Overview/","title":"Overview (Flink)","text":""},{"location":"api/flink/Overview/#introduction","title":"Introduction","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Please read the programming guide: Sedona with Flink SQL app.</p> <p>Sedona includes SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> </ul> </li> <li>Aggregator: Return a single aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> </ul> </li> </ul>"},{"location":"api/flink/Predicate/","title":"Predicate (Flink)","text":""},{"location":"api/flink/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'.</p> <p>If <code>useSpheroid</code> is passed true, ST_DWithin uses Sedona's ST_DistanceSpheroid to check the spheroid distance between the centroids of two geometries. The unit of the distance in this case is meter.</p> <p>If <code>useSpheroid</code> is passed false, ST_DWithin uses Euclidean distance and the unit of the distance is the same as the CRS of the geometries. To obtain the correct result, please consider using ST_Transform to put data in an appropriate CRS.</p> <p>If useSpheroid is not given, it defaults to false</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double, useSpheroid: Optional(Boolean) = false)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <pre><code>Check for distance between New York and Seattle (&lt; 4000 km)\n</code></pre> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT(-122.335167 47.608013), ST_GeomFromWKT(-73.935242 40.730610), 4000000, true)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is within B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/javadoc/spark/legal/jquery/","title":"Jquery","text":""},{"location":"api/javadoc/spark/legal/jquery/#jquery-v371","title":"jQuery v3.7.1","text":""},{"location":"api/javadoc/spark/legal/jquery/#jquery-license","title":"jQuery License","text":"<pre><code>jQuery v 3.7.1\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"api/javadoc/spark/legal/jqueryUI/","title":"jqueryUI","text":""},{"location":"api/javadoc/spark/legal/jqueryUI/#jquery-ui-v1132","title":"jQuery UI v1.13.2","text":""},{"location":"api/javadoc/spark/legal/jqueryUI/#jquery-ui-license","title":"jQuery UI License","text":"<pre><code>Copyright jQuery Foundation and other contributors, https://jquery.org/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/jquery-ui\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nCopyright and related rights for sample code are waived via CC0. Sample\ncode is defined as all source code contained within the demos directory.\n\nCC0: http://creativecommons.org/publicdomain/zero/1.0/\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n</code></pre>"},{"location":"api/javadoc/spark/legal/jszip/","title":"Jszip","text":""},{"location":"api/javadoc/spark/legal/jszip/#jszip-v371","title":"JSZip v3.7.1","text":"<p>JSZip is dual licensed. You may use it under the MIT license or the GPLv3 license.</p>"},{"location":"api/javadoc/spark/legal/jszip/#the-mit-license","title":"The MIT License","text":"<pre><code>Copyright (c) 2009-2016 Stuart Knightley, David Duponchel, Franz Buchinger, Ant\u00f3nio Afonso\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n</code></pre>"},{"location":"api/javadoc/spark/legal/jszip/#gpl-version-3","title":"GPL version 3","text":"<pre><code>                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n</code></pre>"},{"location":"api/javadoc/spark/legal/pako/","title":"Pako","text":""},{"location":"api/javadoc/spark/legal/pako/#pako-v10","title":"Pako v1.0","text":""},{"location":"api/javadoc/spark/legal/pako/#pako-license","title":"Pako License","text":"<pre>\nCopyright (C) 2014-2017 by Vitaly Puzrin and Andrei Tuputcyn\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n(C) 1995-2013 Jean-loup Gailly and Mark Adler\n(C) 2014-2017 Vitaly Puzrin and Andrey Tupitsin\n\nThis software is provided 'as-is', without any express or implied\nwarranty. In no event will the authors be held liable for any damages\narising from the use of this software.\n\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n\n1. The origin of this software must not be misrepresented; you must not\nclaim that you wrote the original software. If you use this software\nin a product, an acknowledgment in the product documentation would be\nappreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not be\n misrepresented as being the original software.\n3. This notice may not be removed or altered from any source distribution.\n\n</pre>"},{"location":"api/pydocs/_static/color-scheme-info/","title":"Color scheme info","text":""},{"location":"api/pydocs/_static/color-scheme-info/#apache-sedona-python-documentation-color-scheme","title":"Apache Sedona Python Documentation Color Scheme","text":""},{"location":"api/pydocs/_static/color-scheme-info/#updated-color-palette-material-design-deep-orange-black","title":"Updated Color Palette (Material Design Deep Orange + Black)","text":""},{"location":"api/pydocs/_static/color-scheme-info/#primary-colors","title":"Primary Colors","text":"<ul> <li>Deep Orange Primary: <code>#ff5722</code> (used for headers, navigation, primary buttons)</li> <li>Deep Orange Dark: <code>#d84315</code> (used for hover states, headings)</li> </ul>"},{"location":"api/pydocs/_static/color-scheme-info/#accent-colors","title":"Accent Colors","text":"<ul> <li>Black Accent: <code>#000000</code> (used for links, current navigation, highlights)</li> <li>Black Accent Dark: <code>#333333</code> (used for hover states on links)</li> </ul>"},{"location":"api/pydocs/_static/color-scheme-info/#supporting-colors","title":"Supporting Colors","text":"<ul> <li>Dark Gray: <code>#263238</code> (sidebar background)</li> <li>Light Gray: <code>#f5f5f5</code> (code backgrounds)</li> <li>Secondary Gray: <code>#e0e0e0</code> (borders, dividers)</li> <li>Text: <code>#333333</code> (main content text)</li> <li>White: <code>#ffffff</code> (content backgrounds)</li> </ul>"},{"location":"api/pydocs/_static/color-scheme-info/#color-usage","title":"Color Usage","text":""},{"location":"api/pydocs/_static/color-scheme-info/#navigation","title":"Navigation","text":"<ul> <li>Header: Deep Orange (<code>#ff5722</code>)</li> <li>Sidebar: Dark Gray (<code>#263238</code>)</li> <li>Current page: Deep Orange with Black accent border</li> <li>Links: Black accent (<code>#000000</code>)</li> </ul>"},{"location":"api/pydocs/_static/color-scheme-info/#content","title":"Content","text":"<ul> <li>Headings: Deep Orange Dark (<code>#d84315</code>)</li> <li>Links: Black accent (<code>#000000</code>)</li> <li>Code blocks: Light Gray backgrounds</li> <li>Admonitions: Deep Orange borders with light orange backgrounds</li> <li>API documentation: Black accent borders</li> </ul>"},{"location":"api/pydocs/_static/color-scheme-info/#interactive-elements","title":"Interactive Elements","text":"<ul> <li>Buttons: Black accent backgrounds</li> <li>Hover states: Darker variants of respective colors</li> <li>Search focus: Black accent border</li> </ul> <p>This color scheme uses Material Design deep orange primary with black accents for a professional, high-contrast appearance.</p>"},{"location":"api/snowflake/vector-data/AggregateFunction/","title":"Aggregate Function (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(envelope)\nFROM src_tbl,\n     TABLE(sedona.ST_Envelope_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(intersected)\nFROM src_tbl,\n     TABLE(sedona.ST_Intersection_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 1, 1 1, 1 0.5, 0.5 0.5, 0.5 1))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(unioned)\nFROM src_tbl,\n     TABLE(sedona.ST_Union_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 1, 0.5 1, 0.5 1.5, 1.5 1.5, 1.5 0.5, 1 0.5, 1 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/","title":"Constructor (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML1 and GML2. GML3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: string, precision: int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt:string)</code> <code>ST_GeomFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format: <code>ST_GeomFromWKB (Wkb:string)</code> <code>ST_GeomFromWKB (Wkb:binary)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_GeomFromWKT (Wkt:string)</code> <code>ST_GeomFromWKT (Wkt:string, srid:integer)</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MLineFromText (Wkt:string)</code> <code>ST_MLineFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MPolyFromText (Wkt:string)</code> <code>ST_MPolyFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X:decimal, Y:decimal, Z:decimal, M:decimal)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X:decimal, Y:decimal)</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ or ST_PointZM to create 3D points.</p> <p>SQL example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code></p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointz_1","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code></p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format: <code>ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal)</code></p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/","title":"Function (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_3DDistance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry, position: integer)</code></p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText('Linestring(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException.</p> <p>Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_approximatemedialaxis","title":"ST_ApproximateMedialAxis","text":"<p>Introduction: Computes an approximate medial axis of a polygonal geometry. The medial axis is a representation of the \"centerline\" or \"skeleton\" of the polygon. This function first computes the straight skeleton and then prunes insignificant branches to produce a cleaner result.</p> <p>The pruning removes small branches that represent minor penetrations into corners. A branch is pruned if its penetration depth is less than 20% of the width of the corner it bisects.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_ApproximateMedialAxis(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT Sedona.ST_ApproximateMedialAxis(\n  ST_GeometryFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output: <code>MULTILINESTRING ((50 45, 50 5), (50 45, 35 45), (65 45, 50 45), (35 45, 65 45))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Area(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_AreaSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((35 34, 30 28, 34 25, 35 34))'))\n</code></pre> <p>Output: <code>201824850811.76245</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsBinary(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKB (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsEWKB(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKT (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsEWKT(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>The type parameter takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A:geometry)</code></p> <p><code>ST_AsGeoJSON (A:geometry, type: String)</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n      \"type\":\"Feature\",\n      \"geometry\": {\n          \"type\":\"Polygon\",\n          \"coordinates\":[\n            [[1.0,1.0],\n              [8.0,1.0],\n              [8.0,8.0],\n              [1.0,8.0],\n              [1.0,1.0]]\n          ]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsGML(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsKML(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry</p> <p>Format: <code>ST_AsText (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output: <code>3.141592653589793</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output: <code>LINESTRING (1 1, 0 0, -1 1, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>Spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : Defaults to <code>both</code>. Setting <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied when <code>left</code> or <code>right</code> are specified.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/snowflake/vector-data/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(\n    ST_GeomFromText('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Centroid(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction:</p> <p>Build an appropriate <code>Geometry</code>, <code>MultiGeometry</code>, or <code>GeometryCollection</code> to contain the <code>Geometry</code>s in it. For example:</p> <ul> <li>If <code>geomList</code> contains a single <code>Polygon</code>, the <code>Polygon</code> is returned.</li> <li>If <code>geomList</code> contains several <code>Polygon</code>s, a <code>MultiPolygon</code> is returned.</li> <li>If <code>geomList</code> contains some <code>Polygon</code>s and some <code>LineString</code>s, a <code>GeometryCollection</code> is returned.</li> <li>If <code>geomList</code> is empty, an empty <code>GeometryCollection</code> is returned.</li> </ul> <p>Note that this method does not \"flatten\" Geometries in the input, and hence if any MultiGeometries are contained in the input, a GeometryCollection containing them will be returned.</p> <p>Format</p> <p><code>ST_Collect(*geom: geometry)</code></p> <p>Example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POINT (40 10)') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('LINESTRING (0 5, 0 10)') AS geom\n)\nSELECT sedona.ST_AsText(collection)\nFROM src_tbl,\n     TABLE(sedona.ST_Collect(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Result:</p> <pre><code>GEOMETRYCOLLECTION (POINT (40 10), LINESTRING (0 5, 0 10))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format: <code>ST_CollectionExtract (A:geometry)</code></p> <p>Format: <code>ST_CollectionExtract (A:geometry, type:Int)</code></p> <p>Example:</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float)</code></p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float, allowHoles:Boolean)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ConcaveHull(polygondf.countyshape, pctConvex)`\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ConvexHull(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/snowflake/vector-data/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Distance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSphere (A:geometry)</code></p> <p>SQL example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre> <p>Output: <code>543796.9506134904</code></p> <p>SQL example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'), 6378137.0)\n</code></pre> <p>Output: <code>544405.4459192449</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre> <p>Output: <code>544430.9411996207</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: This function takes a GeometryCollection/Multi Geometry object and returns a set of geometries containing the individual geometries that make up the input geometry. The function is useful for breaking down a GeometryCollection/Multi Geometry into its constituent geometries.</p> <p>Format: <code>ST_Dump(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT sedona.ST_AsText(geom)\nFROM table(sedona.ST_Dump(sedona.ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))')));\n</code></pre> <p>Output:</p> <pre><code>POINT (10 40)\nPOINT (40 30)\nPOINT (20 20)\nPOINT (30 10)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns a MultiPoint geometry which consists of individual points that compose the input line string.</p> <p>Format: <code>ST_DumpPoints(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output: <code>MultiPoint ((0 0), (0 1), (1 1), (1 0), (0 0))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output: <code>POINT(160 170)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: <code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_FlipCoordinates(df.geometry)\nFROM df\n</code></pre> <p>Input: <code>POINT (1 2)</code></p> <p>Output: <code>POINT (2 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force2D.</p> <p>Format: <code>ST_Force_2D (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))                   |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_force2d","title":"ST_Force2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force_2D.</p> <p>Format: <code>ST_Force2D (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_Force2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))                   |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry, zValue)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Force3D(geometry) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 0, 1 2 0, 2 1 0)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <pre><code>SELECT ST_Force3D(geometry, 2.3) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 2.3, 1 2 2.3, 2 1 2.3)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: geometry, precision: int)</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geohash                      |\n+-----------------------------+\n|u3r0p                        |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer, failIfNotConverged: boolean)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry)</code></p> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: geometry, n: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output: <code>POINT (3 4)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeometryType(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: geometry, n: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output: <code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Intersection(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsEmpty(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText('LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)'))\n</code></pre> <p>Output: <code>true</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsSimple(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvaliddetail","title":"ST_IsValidDetail","text":"<p>Introduction: Returns a row, containing a boolean <code>valid</code> stating if a geometry is valid, a string <code>reason</code> stating why it is invalid and a geometry <code>location</code> pointing out where it is invalid.</p> <p>This function is a combination of ST_IsValid and ST_IsValidReason.</p> <p>The flags parameter is a bitfield with the following options:</p> <ul> <li>0: Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Format:</p> <pre><code>SELECT valid, reason, Sedonm.ST_AsText(location) AS location\nFROM table(Sedona.ST_IsValidDetail(geom: Geometry, flag: Integer))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT valid, reason, Sedonm.ST_AsText(location) AS location\n     FROM table(Sedona.ST_IsValidDetail(Sedona.ST_GeomFromWKT('POLYGON ((30 10, 40 40, 20 40, 30 10, 10 20, 30 10))'), 0))\n</code></pre> <p>Output:</p> <pre><code>+-----+---------------------------------------------------------+-------------+\n|valid|reason                                                   |location     |\n+-----+---------------------------------------------------------+-------------+\n|false|Ring Self-intersection at or near point (30.0, 10.0, NaN)|POINT (30 10)|\n+-----+---------------------------------------------------------+-------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length (A:geometry)</p> <p>SQL example:</p> <pre><code>SELECT ST_Length(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>SQL example:</p> <pre><code>SELECT ST_Length2D(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (10 40, 40 30, 20 20, 30 10)                        |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: geometry, fraction: Double)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2) as Interpolated\n</code></pre> <p>Output:</p> <pre><code>+-----------------------------------------+\n|Interpolated                             |\n+-----------------------------------------+\n|POINT (51.5974135047432 76.5974135047432)|\n+-----------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString or MultiLineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If no merging can be performed, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A:geometry)</code></p> <pre><code>SELECT ST_LineMerge(geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: geometry, startfraction: Double, endfraction: Double)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666) as Substring\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------------------------------------+\n|Substring                                                                                       |\n+------------------------------------------------------------------------------------------------+\n|LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)|\n+------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: Geometry)</code> This Geometry must be a GeometryCollection of the geometry types listed above.</p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes. The holes must be a MultiLinestring.</p> <p>Format: <code>ST_MakePolygon(geom: geometry, holes: &lt;geometry&gt;)</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT\n    ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ST_GeomFromText('MultiLINESTRING((6 2, 8 2, 8 1, 6 1, 6 2))')\n    ) AS polygon\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------+\n|polygon                                                         |\n+----------------------------------------------------------------+\n|POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))|\n+----------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format: <code>ST_MakeValid (A:geometry)</code></p> <p>Format: <code>ST_MakeValid (A:geometry, keepCollapsed:Boolean)</code></p> <p>SQL example:</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different. Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/snowflake/vector-data/Function/#st_maximuminscribedcircle","title":"ST_MaximumInscribedCircle","text":"<p>Introduction: Finds the largest circle that is contained within a (multi)polygon, or which does not overlap any lines and points. Returns a row with fields:</p> <ul> <li><code>center</code> - center point of the circle</li> <li><code>nearest</code> - nearest point from the center of the circle</li> <li><code>radius</code> - radius of the circle</li> </ul> <p>For polygonal geometries, the function inscribes the circle within the boundary rings, treating internal rings as additional constraints. When processing linear and point inputs, the algorithm inscribes the circle within the convex hull of the input, utilizing the input lines and points as additional boundary constraints.</p> <p>Format: <code>ST_MaximumInscribedCircle(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT Sedona.ST_AsText(center) AS center, Sedona.ST_AsText(nearest) AS nearest, radius  FROM table(\n    SELECT ST_MaximumIncribedCircle(ST_GeomFromWKT('POLYGON ((62.11 19.68, 60.79 17.20, 61.30 15.96, 62.11 16.08, 65.93 16.95, 66.20 20.61, 63.08 21.43, 64.48 18.70, 62.11 19.68))'))\n)\n</code></pre> <p>Output:</p> <pre><code>+---------------------------------------------+-------------------------------------------+------------------+\n|center                                       |nearest                                    |radius            |\n+---------------------------------------------+-------------------------------------------+------------------+\n|POINT (62.794975585937514 17.774780273437496)|POINT (63.36773534817729 19.15992378007859)|1.4988916836219184|\n+---------------------------------------------+-------------------------------------------+------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingCircle(geom: geometry, [Optional] quadrantSegments:int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns two columns containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT sedona.ST_AsText(center), radius\nFROM table(sedona.ST_MinimumBoundingRadius(sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))')))\n</code></pre> <p>Result:</p> <pre><code>POINT (0.5 0.5), 0.7071067811865476\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format</p> <p><code>ST_Multi(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(\n    ST_GeomFromText('POINT(1 1)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT (1 1)                                               |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: geometry)</code></p> <p>SQL example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output: <code>3</code></p> <p>SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format</p> <p><code>ST_Normalize(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))'))) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------------+\n|geom                               |\n+-----------------------------------+\n|POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))|\n+-----------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A:geometry)</code></p> <pre><code>SELECT ST_NPoints(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: geometry)</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A:geometry)</code></p> <pre><code>SELECT ST_NumGeometries(df.geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output: <code>1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Format: <code>ST_NumPoints(geom: geometry)</code></p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown.</p> <p>SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre> <p>Output: <code>3</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_perimeter2d","title":"ST_Perimeter2D","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Info</p> <p>This function is an alias for ST_Perimeter.</p> <p>Format:</p> <p><code>ST_Perimeter2D(geom: Geometry)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: geometry, n: integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PointN(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'), 2) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POINT (1 2)                                                    |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A:geometry)</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format: <code>ST_RemovePoint(geom: geometry, position: integer)</code></p> <p>Format: <code>ST_RemovePoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), 1)\n</code></pre> <p>Output: <code>LINESTRING(0 0, 1 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_Reverse(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (3 6, 2 4, 1 2, 0 0)                                |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: geometry, level: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------------------------------------------------------------------+\n|st_s2cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)                                                                 |\n+------------------------------------------------------------------------------------------------------------------------------+\n|[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]|\n+------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_segmentize","title":"ST_Segmentize","text":"<p>Introduction: Returns a modified geometry having no segment longer than the given max_segment_length.</p> <p>The length calculation is performed in 2D. When a segment is longer than the specified maximum length, it is split into multiple, equal-length subsegments.</p> <p>Format: <code>ST_Segmentize(geom: Geometry, max_segment_length: Double)</code></p> <p>Since: v1.8.0</p> <p>SQL Example Long segments are split evenly into subsegments no longer than the specified length. Shorter segments are not modified.</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('MULTILINESTRING((0 0, 0 1, 0 9),(1 10, 1 18))'), 5));\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING((0 0,0 1,0 5,0 9),(1 10,1 14,1 18))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('POLYGON((0 0, 0 8, 30 0, 0 0))'), 10));\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 8,7.5 6,15 4,22.5 2,30 0,20 0,10 0,0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: geometry, index: integer, point: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)')) AS geom\n</code></pre> <p>Result:</p> <pre><code>+--------------------------+\n|geom                      |\n+--------------------------+\n|LINESTRING (0 0, 0 1, 1 0)|\n+--------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A:geometry, srid: Integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_SetSRID(polygondf.countyshape, 3021)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Format: <code>ST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)</code></p> <pre><code>SELECT ST_SimplifyPreserveTopology(polygondf.countyshape, 10.0)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n            ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Format: <code>ST_Split (input: geometry, blade: geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Split(\n    ST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\n    ST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre> <p>Output: <code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_SRID(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output: <code>POINT(100 150)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_straightskeleton","title":"ST_StraightSkeleton","text":"<p>Introduction: Computes the straight skeleton of a polygonal geometry. The straight skeleton is a method of representing a polygon by a topological skeleton, formed by a continuous shrinking process where each edge moves inward in parallel at a uniform speed.</p> <p>This function uses the weighted straight skeleton algorithm based on Felkel's approach.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_StraightSkeleton(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT Sedona.ST_StraightSkeleton(\n  ST_GeometryFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output: <code>MULTILINESTRING ((50 5, 50 45), (50 45, 35 45), (50 45, 65 45), (35 45, 30 45), (35 45, 40 40), (65 45, 70 45), (65 45, 60 40), (50 5, 45 5), (50 5, 55 5))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns a multi-geometry divided based of given maximum number of vertices.</p> <p>A minimum of 5 vertices is required for maxVertices parameter to form a closed box.</p> <p>Format: <code>ST_SubDivide(geom: geometry, maxVertices: int)</code></p> <p>SQL example:</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_SubDivide(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((0 0, 5 5), (5 5, 10 10), (10 10, 21 21), (21 21, 60 60), (60 60, 85 85), (85 85, 100 100), (100 100, 120 120))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of a multi-geometry.</p> <p>A minimum of 5 vertices is required for maxVertices parameter to form a closed box.</p> <p>Format: <code>SELECT SEDONA.ST_AsText(GEOM) FROM table(SEDONA.ST_SubDivideExplode(geom: geometry, maxVertices: int))</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT Sedona.ST_AsText(GEOM)\nFROM table(Sedona.ST_SubDivideExplode(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Result:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available.</p> <p>Note</p> <p>By default, this function uses lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>Note</p> <p>If ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.</p> <p>Format: <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string ,[Optional] DisableError)</code></p> <p>SQL example (simple):</p> <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857')\nFROM polygondf\n</code></pre> <p>SQL example (with optional parameters):</p> <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857', false)\nFROM polygondf\n</code></pre> <p>Note</p> <p>The detailed EPSG information can be searched on EPSG.io.</p>"},{"location":"api/snowflake/vector-data/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format: <code>ST_Translate(geometry: geometry, deltaX: deltaX, deltaY: deltaY, deltaZ: deltaZ)</code></p> <p>Example:</p> <p>Input: <code>ST_Translate(GEOMETRYCOLLECTION(MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0)), ((1 2, 3 4, 3 5, 1 2))), POINT(1, 1, 1), LINESTRING EMPTY), 2, 2, 3)</code></p> <p>Output: <code>GEOMETRYCOLLECTION(MULTIPOLYGON (((3 2, 3 3, 4 3, 4 2, 3 2)), ((3 4, 5 6, 5 7, 3 4))), POINT(3, 3, 4), LINESTRING EMPTY)</code></p> <p>Input: <code>ST_Translate(POINT(1, 3, 2), 1, 2)</code></p> <p>Output: <code>POINT(2, 5, 2)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_union","title":"ST_Union","text":"<p>Introduction: Return the union of geometry A and B</p> <p>Format: <code>ST_Union (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>SQL Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output: <code>0.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(df.geometry) AS xmax\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(df.geometry) AS xmin\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>-1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output: <code>25.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: 2</p>"},{"location":"api/snowflake/vector-data/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output : 0</p>"},{"location":"api/snowflake/vector-data/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output: <code>11.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: <code>1.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output: <code>4.0</code></p>"},{"location":"api/snowflake/vector-data/Overview/","title":"Overview (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates</li> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> <li>Function: Execute a function on the given column or columns</li> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> <li>Aggregate function: Return the aggregated value on the given column</li> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> <li>Predicate: Execute a logic judgement on the given columns and return true or false</li> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul>"},{"location":"api/snowflake/vector-data/Predicate/","title":"Predicate (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Crosses(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Disjoinnt(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'. This function essentially checks if the shortest distance between the envelope of the two geometries is &lt;= the provided distance.</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Equals(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>SQL example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output: <code>true</code></p> <p>SQL example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Overlaps(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A:geometry, B:geometry)</code></p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Touches(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Covers(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_CoveredBy(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/sql/AggregateFunction/","title":"Aggregate function","text":""},{"location":"api/sql/AggregateFunction/#st_collect_agg","title":"ST_Collect_Agg","text":"<p>Introduction: Collects all geometries in a geometry column into a single multi-geometry (MultiPoint, MultiLineString, MultiPolygon, or GeometryCollection). Unlike <code>ST_Union_Aggr</code>, this function does not dissolve boundaries between geometries - it simply collects them into a multi-geometry.</p> <p>Format: <code>ST_Collect_Agg (A: geometryColumn)</code></p> <p>Since: <code>v1.8.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Collect_Agg(geom) FROM (\n  SELECT ST_GeomFromWKT('POINT(1 2)') AS geom\n  UNION ALL\n  SELECT ST_GeomFromWKT('POINT(3 4)') AS geom\n  UNION ALL\n  SELECT ST_GeomFromWKT('POINT(5 6)') AS geom\n)\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1 2), (3 4), (5 6))\n</code></pre> <p>SQL Example with GROUP BY</p> <pre><code>SELECT category, ST_Collect_Agg(geom) FROM geometries GROUP BY category\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/Constructor/","title":"Constructor","text":""},{"location":"api/sql/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML 1 and GML 2. GML 3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Note</p> <p>This method is not recommended. Please use Sedona GeoJSON data source to read GeoJSON files.</p> <p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Warning</p> <p>The way that SedonaSQL reads GeoJSON is different from that in SparkSQL</p>"},{"location":"api/sql/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeomFromText (Wkt: String)</code></p> <p><code>ST_GeomFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_GeomFromWKT (Wkt: String)</code></p> <p><code>ST_GeomFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/sql/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MLineFromText (Wkt: String)</code></p> <p><code>ST_MLineFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MPolyFromText (Wkt: String)</code></p> <p><code>ST_MPolyFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/sql/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_makepointm","title":"ST_MakePointM","text":"<p>Introduction: Creates a point with X, Y, and M coordinate. Use ST_MakePoint to make points with XY, XYZ, or XYZM coordinates.</p> <p>Format: <code>ST_MakePointM(x: Double, y: Double, m: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePointM(1, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>Point M(1 2 3)\n</code></pre>"},{"location":"api/sql/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ to create 3D points.</p> <p>SQL Example</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointm","title":"ST_PointM","text":"<p>Introduction: Construct a Point from X, Y and M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointM (X: Double, Y: Double, M: Double)</code></p> <p><code>ST_PointM (X: Double, Y: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointM(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 0 3.4567)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointzm","title":"ST_PointZM","text":"<p>Introduction: Construct a Point from X, Y, Z, M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZM(1.2345, 2.3456, 3.4567, 100))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 3.4567, 100)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfrommysql","title":"ST_GeomFromMySQL","text":"<p>Introduction: Construct a Geometry from MySQL Geometry binary.</p> <p>Format: <code>ST_GeomFromMySQL (binary: Binary)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT\n    ST_GeomFromMySQL(geomWKB) AS geom,\n    ST_SRID(ST_GeomFromMySQL(geomWKB)) AS srid\nFROM mysql_table\n</code></pre> <p>Output:</p> <pre><code>+-------------+----+\n|         geom|srid|\n+-------------+----+\n|POINT (20 10)|4326|\n|POINT (40 30)|4326|\n|POINT (60 50)|4326|\n+-------------+----+\n</code></pre>"},{"location":"api/sql/DataFrameAPI/","title":"DataFrame Style functions","text":"<p>Sedona SQL functions can be used in a DataFrame style API similar to Spark functions.</p> <p>The following objects contain the exposed functions: <code>org.apache.spark.sql.sedona_sql.expressions.st_functions</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_constructors</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_predicates</code>, and <code>org.apache.spark.sql.sedona_sql.expressions.st_aggregates</code>.</p> <p>Every function can take all <code>Column</code> arguments. Additionally, overloaded forms can commonly take a mix of <code>String</code> and other Scala types (such as <code>Double</code>) as arguments.</p> <p>In general the following rules apply (although check the documentation of specific functions for any exceptions):</p> ScalaPython <ol> <li>Every function returns a <code>Column</code> so that it can be used interchangeably with Spark functions as well as <code>DataFrame</code> methods such as <code>DataFrame.select</code> or <code>DataFrame.join</code>.</li> <li>Every function has a form that takes all <code>Column</code> arguments. These are the most versatile of the forms.</li> <li>Most functions have a form that takes a mix of <code>String</code> arguments with other Scala types.</li> </ol> <ol> <li><code>Column</code> type arguments are passed straight through and are always accepted.</li> <li><code>str</code> type arguments are always assumed to be names of columns and are wrapped in a <code>Column</code> to support that. If an actual string literal needs to be passed then it will need to be wrapped in a <code>Column</code> using <code>pyspark.sql.functions.lit</code>.</li> <li>Any other types of arguments are checked on a per function basis. Generally, arguments that could reasonably support a python native type are accepted and passed through.   4. Shapely <code>Geometry</code> objects are not currently accepted in any of the functions.</li> </ol> <p>The exact mixture of argument types allowed is function specific. However, in these instances, all <code>String</code> arguments are assumed to be the names of columns and will be wrapped in a <code>Column</code> automatically. Non-<code>String</code> arguments are assumed to be literals that are passed to the sedona function. If you need to pass a <code>String</code> literal then you should use the all <code>Column</code> form of the sedona function and wrap the <code>String</code> literal in a <code>Column</code> with the <code>lit</code> Spark function.</p> <p>A short example of using this API (uses the <code>array_min</code> and <code>array_max</code> Spark functions):</p> ScalaPython <pre><code>val values_df = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\nval min_value = array_min(\"values\")\nval max_value = array_max(\"values\")\nval point_df = values_df.select(ST_Point(min_value, max_value).as(\"point\"))\n</code></pre> <pre><code>from pyspark.sql import functions as f\n\nfrom sedona.spark import *\n\ndf = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\n\nmin_value = f.array_min(\"values\")\nmax_value = f.array_max(\"values\")\n\ndf = df.select(ST_Point(min_value, max_value).alias(\"point\"))\n</code></pre> <p>The above code will generate the following dataframe:</p> <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (0 2)|\n+-----------+\n</code></pre> <p>Some functions will take native python values and infer them as literals. For example:</p> <pre><code>from sedona.spark import *\n\ndf = df.select(ST_Point(1.0, 3.0).alias(\"point\"))\n</code></pre> <p>This will generate a dataframe with a constant point in a column:</p> <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (1 3)|\n+-----------+\n</code></pre>"},{"location":"api/sql/Function/","title":"Function","text":""},{"location":"api/sql/Function/#expandaddress","title":"ExpandAddress","text":"<p>Introduction: Returns an array of expanded forms of the input address string. This is backed by the libpostal library's address expanding functionality.</p> <p>Note</p> <p>Jpostal requires at least 2 GB of free disk space to store the data files used for address parsing and expanding. The data files are downloaded automatically when the function is called for the first time.</p> <p>Note</p> <p>The version of jpostal installed with this package only supports Linux and MacOS. If you are using Windows, you will need to install libjpostal and libpostal manually and ensure that they are available in your <code>java.library.path</code>.</p> <p>Format: <code>ExpandAddress (address: String)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ExpandAddress(\"100 W 1st St, Los Angeles, CA 90012\");\n</code></pre> <p>Output:</p> <pre><code>[100 w 1st saint, 100 w 1st street, 100 west 1st saint, 100 west 1st street, 100 w 1 saint, 100 w 1 street, 100 west 1 saint, 100 west 1 street]\n</code></pre>"},{"location":"api/sql/Function/#parseaddress","title":"ParseAddress","text":"<p>Introduction: Returns an array of the components (e.g. street, postal code) of the input address string. This is backed by the libpostal library's address parsing functionality.</p> <p>Note</p> <p>Jpostal requires at least 2 GB of free disk space to store the data files used for address parsing and expanding. The data files are downloaded automatically when the library is initialized.</p> <p>Note</p> <p>The version of jpostal installed with this package only supports Linux and MacOS. If you are using Windows, you will need to install libjpostal and libpostal manually and ensure that they are available in your <code>java.library.path</code>.</p> <p>Format: <code>ParseAddress (address: String)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ParseAddress(\"100 W 1st St, Los Angeles, CA 90012\");\n</code></pre> <p>Output:</p> <pre><code>[{house_number, 100}, {road, w 1st st}, {city, los angeles}, {state, ca}, {postcode, 90012}]\n</code></pre>"},{"location":"api/sql/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/sql/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\n                     ST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output:</p> <pre><code>1.7320508075688772\n</code></pre>"},{"location":"api/sql/Function/#st_addmeasure","title":"ST_AddMeasure","text":"<p>Introduction: Computes a new geometry with measure (M) values linearly interpolated between start and end points. For geometries lacking M dimensions, M values are added. Existing M values are overwritten by the new values. Applies only to LineString and MultiLineString inputs.</p> <p>Format: <code>ST_AddMeasure(geom: Geometry, measureStart: Double, measureEnd: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_AddMeasure(\n        ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)')\n))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 0 10, 1 0 16, 2 0 22, 3 0 28, 4 0 34, 5 0 40)\n</code></pre>"},{"location":"api/sql/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/sql/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/sql/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/sql/Function/#st_approximatemedialaxis","title":"ST_ApproximateMedialAxis","text":"<p>Introduction: Computes an approximate medial axis of a polygonal geometry. The medial axis is a representation of the \"centerline\" or \"skeleton\" of the polygon. This function first computes the straight skeleton and then prunes insignificant branches to produce a cleaner result.</p> <p>The pruning removes small branches that represent minor penetrations into corners. A branch is pruned if its penetration depth is less than 20% of the width of the corner it bisects.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_ApproximateMedialAxis(geom: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_ApproximateMedialAxis(\n  ST_GeomFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((50 45, 50 5), (50 45, 35 45), (65 45, 50 45), (35 45, 65 45))\n</code></pre> <p></p> <p>SQL Example (L-shape):</p> <pre><code>SELECT ST_ApproximateMedialAxis(\n  ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 5, 5 5, 5 10, 0 10, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((2.5 2.5, 2.5 7.5), (7.5 2.5, 2.5 2.5), (2.5 7.5, 2.5 7.5))\n</code></pre>"},{"location":"api/sql/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>10\n</code></pre>"},{"location":"api/sql/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/sql/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. See ST_SetSRID It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Note</p> <p>This method is not recommended. Please use Sedona GeoJSON data source to write GeoJSON files.</p> <p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>The type parameter (Since: <code>v1.6.1</code>) takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A: Geometry)</code></p> <p><code>ST_AsGeoJSON (A: Geometry, type: String)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n      \"type\":\"Feature\",\n      \"geometry\": {\n          \"type\":\"Polygon\",\n          \"coordinates\":[\n            [[1.0,1.0],\n              [8.0,1.0],\n              [8.0,8.0],\n              [1.0,8.0],\n              [1.0,1.0]]\n          ]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/sql/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/sql/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/sql/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/sql/Function/#st_binarydistancebandcolumn","title":"ST_BinaryDistanceBandColumn","text":"<p>Introduction: Introduction: Returns a <code>weights</code> column containing every record in a dataframe within a specified <code>threshold</code> distance.</p> <p>The <code>weights</code> column is an array of structs containing the <code>attributes</code> from each neighbor and that neighbor's weight. Since this is a binary distance band function, weights of neighbors within the threshold will always be <code>1.0</code>.</p> <p>Format: <code>ST_BinaryDistanceBandColumn(geometry:Geometry, threshold: Double, includeZeroDistanceNeighbors: boolean, includeSelf: boolean, useSpheroid: boolean, attributes: Struct)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example</p> <pre><code>ST_BinaryDistanceBandColumn(geometry, 1.0, true, true, false, struct(id, geometry))\n</code></pre> <p>Output:</p> <pre><code>[{{15, POINT (3 1.9)}, 1.0}, {{16, POINT (3 2)}, 1.0}, {{17, POINT (3 2.1)}, 1.0}, {{18, POINT (3 2.2)}, 1.0}]\n</code></pre>"},{"location":"api/sql/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Boundary(ST_GeomFromWKT('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/sql/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>As of now, spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : Defaults to <code>both</code>. Setting <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied when <code>left</code> or <code>right</code> are specified.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/sql/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_BuildArea(\n    ST_GeomFromWKT('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/sql/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/sql/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Collect(\n    ST_GeomFromText('POINT(21.427834 52.042576573)'),\n    ST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Collect(\n    Array(\n        ST_GeomFromText('POINT(21.427834 52.042576573)'),\n        ST_GeomFromText('POINT(45.342524 56.342354355)')\n    )\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/sql/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/sql/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/sql/Function/#st_dbscan","title":"ST_DBSCAN","text":"<p>Introduction: Performs a DBSCAN clustering across the entire dataframe.</p> <p>Returns a struct containing the cluster ID and a boolean indicating if the record is a core point in the cluster.</p> <ul> <li><code>epsilon</code> is the maximum distance between two points for them to be considered as part of the same cluster.</li> <li><code>minPoints</code> is the minimum number of neighbors a single record must have to form a cluster.</li> <li><code>useSpheroid</code> is whether to use ST_DistanceSpheroid or ST_Distance as the distance metric.</li> </ul> <p>Format: <code>ST_DBSCAN(geom: Geometry, epsilon: Double, minPoints: Integer, useSpheroid: Boolean)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DBSCAN(geom, 1.0, 2, False)\n</code></pre> <p>Output:</p> <pre><code>{true, 85899345920}\n</code></pre>"},{"location":"api/sql/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/sql/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/sql/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/sql/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/sql/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/sql/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/sql/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/sql/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/sql/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/sql/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/sql/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/sql/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force2D.</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_force2d","title":"ST_Force2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates. This function is an alias of ST_Force_2D.</p> <p>Format: <code>ST_Force2D (A: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Force2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/sql/Function/#st_force3dm","title":"ST_Force3DM","text":"<p>Introduction: Forces the geometry into XYM mode. Retains any existing M coordinate, but removes the Z coordinate if present. Assigns a default M value of 0.0 if <code>mValue</code> is not specified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds M for in the WKT.</p> <p>Format: <code>ST_Force3DM(geometry: Geometry, mValue: Double = 0.0)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('POLYGON M((0 0 3,0 5 3,5 0 3,0 0 3),(1 1 3,3 1 3,1 3 3,1 1 3))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON M((0 0 3, 0 5 3, 5 0 3, 0 0 3), (1 1 3, 3 1 3, 1 3 3, 1 1 3))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING Z(0 1 3,1 0 3,2 0 3)'), 5))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 5, 1 0 5, 2 0 5)\n</code></pre>"},{"location":"api/sql/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/sql/Function/#st_force4d","title":"ST_Force4D","text":"<p>Introduction: Converts the input geometry to 4D XYZM representation. Retains original Z and M values if present. Assigning 0.0 defaults if <code>mValue</code> and <code>zValue</code> aren't specified. The output contains X, Y, Z, and M coordinates. For geometries already in 4D form, the function returns the original geometry unmodified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format:</p> <p><code>ST_Force4D(geom: Geometry, zValue: Double, mValue: Double)</code></p> <p><code>ST_Force4D(geom: Geometry</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 5, 10))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ZM((0 0 2 10, 0 5 2 10, 5 0 2 10, 0 0 2 10), (1 1 2 10, 3 1 2 10, 1 3 2 10, 1 1 2 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 3, 1))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING ZM(0 1 3 1, 1 0 3 1, 2 0 3 1)\n</code></pre>"},{"location":"api/sql/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/sql/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/sql/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/sql/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/sql/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/sql/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/sql/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LineString\n</code></pre>"},{"location":"api/sql/Function/#st_glocal","title":"ST_GLocal","text":"<p>Introduction: Runs Getis and Ord's G Local (Gi or Gi*) statistic on the geometry given the <code>weights</code> and <code>level</code>.</p> <p>Getis and Ord's Gi and Gi* statistics are used to identify data points with locally high values (hot spots) and low values (cold spots) in a spatial dataset.</p> <p>The <code>ST_WeightedDistanceBand</code> and <code>ST_BinaryDistanceBand</code> functions can be used to generate the <code>weights</code> column.</p> <p>Format: <code>ST_GLocal(geom: Geometry, weights: Struct, level: Int)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example</p> <pre><code>ST_GLocal(myVariable, ST_BinaryDistanceBandColumn(geometry, 1.0, true, true, false, struct(myVariable, geometry)), true)\n</code></pre> <p>Output:</p> <pre><code>{0.5238095238095238, 0.4444444444444444, 0.001049802637104223, 2.4494897427831814, 0.00715293921771476}\n</code></pre>"},{"location":"api/sql/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[0])\n</code></pre> <p>Output:</p> <pre><code>+-----------------------------------------------------------------------------------------------------------------------------------------+\n|st_h3celldistance(st_h3cellids(st_geomfromwkt(POINT(1 2), 0), 8, true)[0], st_h3cellids(st_geomfromwkt(POINT(1.23 1.59), 0), 8, true)[0])|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                                                                                       78|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/sql/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/sql/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre> <p>Output:</p> <pre><code>+-------------------------------------------------------------+\n|st_h3cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)|\n+-------------------------------------------------------------+\n|                                         [6055475394579005...|\n+-------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true) cells union select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, false) cells\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------------------------------------------------------------------------------------------------+\n|cells                                                                                                                                       |\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n|[614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]                    |\n|[614552609325318143, 614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]|\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: Return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Converts an array of Uber H3 cell indices into an array of Polygon geometries, where each polygon represents a hexagonal H3 cell.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((1.0057629565405093 1.9984665139177547, 1.0037116327309097 2.0018325249140068, 0.999727799357053 2.001163270465665, 0.9977951427833316 1.997128228393235, 0.9998461908217928 1.993762152933182, 1.0038301712104316 1.9944311839965523, 1.0057629565405093 1.9984665139177547))]\n</code></pre>"},{"location":"api/sql/Function/#st_hasm","title":"ST_HasM","text":"<p>Introduction: Checks for the presence of M coordinate values representing measures or linear references. Returns true if the input geometry includes an M coordinate, false otherwise.</p> <p>Format: <code>ST_HasM(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasM(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/sql/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/sql/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_interpolatepoint","title":"ST_InterpolatePoint","text":"<p>Introduction: Returns the interpolated measure value of a linear measured LineString at the point closest to the specified point.</p> <p>Note</p> <p>Make sure that both geometries have the same SRID, otherwise the function will throw an IllegalArgumentException.</p> <p>Format: <code>ST_InterpolatePoint(linestringM: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InterpolatePoint(\n    ST_GeomFromWKT(\"LINESTRING M (0 0 0, 2 0 2, 4 0 4)\"),\n    ST_GeomFromWKT(\"POINT (1 1)\")\n    )\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/sql/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersection(\n    ST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\n    ST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/sql/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isvaliddetail","title":"ST_IsValidDetail","text":"<p>Introduction: Returns a row, containing a boolean <code>valid</code> stating if a geometry is valid, a string <code>reason</code> stating why it is invalid and a geometry <code>location</code> pointing out where it is invalid.</p> <p>This function is a combination of ST_IsValid and ST_IsValidReason.</p> <p>The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidDetail(geom: Geometry)\n</code></pre> <pre><code>ST_IsValidDetail(geom: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidDetail(ST_GeomFromWKT('POLYGON ((30 10, 40 40, 20 40, 30 10, 10 20, 30 10))'))\n</code></pre> <p>Output:</p> <pre><code>+-----+---------------------------------------------------------+-------------+\n|valid|reason                                                   |location     |\n+-----+---------------------------------------------------------+-------------+\n|false|Ring Self-intersection at or near point (30.0, 10.0, NaN)|POINT (30 10)|\n+-----+---------------------------------------------------------+-------------+\n</code></pre>"},{"location":"api/sql/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/sql/Function/#st_isvalidtrajectory","title":"ST_IsValidTrajectory","text":"<p>Introduction: This function checks if a geometry is a valid trajectory representation. For a trajectory to be considered valid, it must be a LineString that includes measure (M) values. The key requirement is that the M values increase from one vertex to the next as you move along the line.</p> <p>Format: <code>ST_IsValidTrajectory(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 2)')\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 0)')\n)\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/sql/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/sql/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Length2D(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/sql/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/sql/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/sql/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output:</p> <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre>"},{"location":"api/sql/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/sql/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString or MultiLineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If no merging can be performed, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/sql/Function/#st_linesegments","title":"ST_LineSegments","text":"<p>Introduction: This function transforms a LineString containing multiple coordinates into an array of LineStrings, each with precisely two coordinates. The <code>lenient</code> argument, true by default, prevents an exception from being raised if the input geometry is not a LineString.</p> <p>Format:</p> <p><code>ST_LineSegments(geom: Geometry, lenient: Boolean)</code></p> <p><code>ST_LineSegments(geom: Geometry)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineSegments(\n        ST_GeomFromWKT('LINESTRING(0 0, 10 10, 20 20, 30 30, 40 40, 50 50)'),\n       false\n    )\n</code></pre> <p>Output:</p> <pre><code>[LINESTRING (0 0, 10 10), LINESTRING (10 10, 20 20), LINESTRING (20 20, 30 30), LINESTRING (30 30, 40 40), LINESTRING (40 40, 50 50)]\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LineSegments(\n        ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0))')\n    )\n</code></pre> <p>Output:</p> <pre><code>[]\n</code></pre>"},{"location":"api/sql/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format:</p> <p><code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/sql/Function/#st_localoutlierfactor","title":"ST_LocalOutlierFactor","text":"<p>Introduction: Computes the Local Outlier Factor (LOF) for each point in the input dataset.</p> <p>Local Outlier Factor is an algorithm for determining the degree to which a single record is an inlier or outlier. It is based on how close a record is to its <code>k</code> nearest neighbors vs how close those neighbors are to their <code>k</code> nearest neighbors. Values substantially less than <code>1</code> imply that the record is an inlier, while values greater than <code>1</code> imply that the record is an outlier.</p> <p>Note</p> <p>ST_LocalOutlierFactor has a useSphere parameter rather than a useSpheroid parameter. This function thus uses a spherical model of the earth rather than an ellipsoidal model when calculating distance.</p> <p>Format: <code>ST_LocalOutlierFactor(geometry: Geometry, k: Int, useSphere: Boolean)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LocalOutlierFactor(geometry, 5, true)\n</code></pre> <p>Output:</p> <pre><code>1.0009256283408587\n</code></pre>"},{"location":"api/sql/Function/#st_locatealong","title":"ST_LocateAlong","text":"<p>Introduction: This function computes Point or MultiPoint geometries representing locations along a measured input geometry (LineString or MultiLineString) corresponding to the provided measure value(s). Polygonal geometry inputs are not supported. The output points lie directly on the input line at the specified measure positions.</p> <p>Additionally, an optional <code>offset</code> parameter can shift the resulting points left or right from the input line. A positive offset displaces the points to the left side, while a negative value offsets them to the right side by the given distance.</p> <p>This allows identifying precise locations along a measured linear geometry based on supplied measure values, with the ability to offset the output points if needed.</p> <p>Format:</p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double, offset: Double)</code></p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LocateAlong(\n        ST_GeomFromText('LINESTRING M (10 30 1, 50 50 1, 30 110 2, 70 90 2, 180 140 3, 130 190 3)')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT M((30 110 2), (50 100 2), (70 90 2))\n</code></pre>"},{"location":"api/sql/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre>"},{"location":"api/sql/Function/#st_m","title":"ST_M","text":"<p>Introduction: Returns M Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_M(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_M(ST_MakePoint(1, 2, 3, 4))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_mmax","title":"ST_MMax","text":"<p>Introduction: Returns M maxima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMax(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MMax(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_mmin","title":"ST_MMin","text":"<p>Introduction: Returns M minima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMin(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MMin(\n        ST_GeomFromWKT('LINESTRING ZM(1 1 1 1, 2 2 2 2, 3 3 3 3, -1 -1 -1 -1)')\n)\n</code></pre> <p>Output:</p> <pre><code>-1.0\n</code></pre>"},{"location":"api/sql/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/sql/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/sql/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=false) or a valid geometry of lower dimension (keepCollapsed=true). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different.</p> <p>Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/sql/Function/#st_maximuminscribedcircle","title":"ST_MaximumInscribedCircle","text":"<p>Introduction: Finds the largest circle that is contained within a (multi)polygon, or which does not overlap any lines and points. Returns a row with fields:</p> <ul> <li><code>center</code> - center point of the circle</li> <li><code>nearest</code> - nearest point from the center of the circle</li> <li><code>radius</code> - radius of the circle</li> </ul> <p>For polygonal geometries, the function inscribes the circle within the boundary rings, treating internal rings as additional constraints. When processing linear and point inputs, the algorithm inscribes the circle within the convex hull of the input, utilizing the input lines and points as additional boundary constraints.</p> <p>Format: <code>ST_MaximumInscribedCircle(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT inscribedCircle.* FROM (\n    SELECT ST_MaximumIncribedCircle(ST_GeomFromWKT('POLYGON ((62.11 19.68, 60.79 17.20, 61.30 15.96, 62.11 16.08, 65.93 16.95, 66.20 20.61, 63.08 21.43, 64.48 18.70, 62.11 19.68))')) AS inscribedCircle\n)\n</code></pre> <p>Output:</p> <pre><code>+---------------------------------------------+-------------------------------------------+------------------+\n|center                                       |nearest                                    |radius            |\n+---------------------------------------------+-------------------------------------------+------------------+\n|POINT (62.794975585937514 17.774780273437496)|POINT (63.36773534817729 19.15992378007859)|1.4988916836219184|\n+---------------------------------------------+-------------------------------------------+------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/sql/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/sql/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/sql/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments has been changed to 48 since v1.5.0.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/sql/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/sql/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format:</p> <p><code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/sql/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/sql/Function/#st_perimeter2d","title":"ST_Perimeter2D","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Info</p> <p>This function is an alias for ST_Perimeter.</p> <p>Format:</p> <p><code>ST_Perimeter2D(geom: Geometry)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter2D(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter2D(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/sql/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointN(ST_GeomFromText(\"LINESTRING(0 0, 1 2, 2 4, 3 6)\"), 2)\n</code></pre> <p>Result:</p> <pre><code>POINT (1 2)\n</code></pre>"},{"location":"api/sql/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/sql/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/sql/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/sql/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/sql/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/sql/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/sql/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/sql/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre>"},{"location":"api/sql/Function/#st_s2togeom","title":"ST_S2ToGeom","text":"<p>Introduction: Returns an array of Polygons for the corresponding S2 cell IDs.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_S2ToGeom(cellIds: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_S2ToGeom(array(11540474045136890))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((-36.609392788630245 -38.169532607255846, -36.609392706252954 -38.169532607255846, -36.609392706252954 -38.169532507473015, -36.609392788630245 -38.169532507473015, -36.609392788630245 -38.169532607255846))]\n</code></pre>"},{"location":"api/sql/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/sql/Function/#st_segmentize","title":"ST_Segmentize","text":"<p>Introduction: Returns a modified geometry having no segment longer than the given max_segment_length.</p> <p>The length calculation is performed in 2D. When a segment is longer than the specified maximum length, it is split into multiple, equal-length subsegments.</p> <p>Format: <code>ST_Segmentize(geom: Geometry, max_segment_length: Double)</code></p> <p>Since: v1.8.0</p> <p>SQL Example Long segments are split evenly into subsegments no longer than the specified length. Shorter segments are not modified.</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('MULTILINESTRING((0 0, 0 1, 0 9),(1 10, 1 18))'), 5));\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING((0 0,0 1,0 5,0 9),(1 10,1 14,1 18))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Segmentize(ST_GeomFromText('POLYGON((0 0, 0 8, 30 0, 0 0))'), 10));\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 8,7.5 6,15 4,22.5 2,30 0,20 0,10 0,0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/sql/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.0.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/sql/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n            ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/sql/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Since: <code>v1.4.0</code></p> <p>Format: <code>ST_Split (input: Geometry, blade: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Split(\n    ST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\n    ST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))\n</code></pre>"},{"location":"api/sql/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/sql/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/sql/Function/#st_straightskeleton","title":"ST_StraightSkeleton","text":"<p>Introduction: Computes the straight skeleton of a polygonal geometry. The straight skeleton is a method of representing a polygon by a topological skeleton, formed by a continuous shrinking process where each edge moves inward in parallel at a uniform speed.</p> <p>This function uses the weighted straight skeleton algorithm based on Felkel's approach.</p> <p>This function may have significant performance limitations when processing polygons with a very large number of vertices. For very large polygons (e.g., 10,000+ vertices), applying vertex reduction or simplification is essential to achieve practical computation times.</p> <p>Format: <code>ST_StraightSkeleton(geom: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_StraightSkeleton(\n  ST_GeomFromWKT('POLYGON ((45 0, 55 0, 55 40, 70 40, 70 50, 30 50, 30 40, 45 40, 45 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((50 5, 50 45), (50 45, 35 45), (50 45, 65 45), (35 45, 30 45), (35 45, 40 40), (65 45, 70 45), (65 45, 60 40), (50 5, 45 5), (50 5, 55 5))\n</code></pre> <p></p> <p>SQL Example (Simple Square):</p> <pre><code>SELECT ST_StraightSkeleton(\n  ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((5 5, 0 5), (5 5, 5 0), (5 5, 10 5), (5 5, 5 10))\n</code></pre>"},{"location":"api/sql/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>A minimum of 5 vertices is required for maxVertices parameter to form a closed box.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre>"},{"location":"api/sql/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of list.</p> <p>A minimum of 5 vertices is required for maxVertices parameter to form a closed box.</p> <p>Format: <code>ST_SubDivideExplode(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <p>Query:</p> <pre><code>SELECT ST_SubDivideExplode(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre> <p>Using Lateral View</p> <p>Table:</p> <pre><code>+-------------------------------------------------------------+\n|geometry                                                     |\n+-------------------------------------------------------------+\n|LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)  |\n+-------------------------------------------------------------+\n</code></pre> <p>Query</p> <pre><code>select geom from geometries LATERAL VIEW ST_SubdivideExplode(geometry, 5) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/sql/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since <code>v1.3.1</code>. Since <code>v1.5.1</code>, if the <code>SourceCRS</code> is not specified, CRS will be fetched from the geometry using ST_SRID.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will make sure the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, these CRS will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, lenientMode: Boolean)\n</code></pre> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String)\n</code></pre> <pre><code>ST_Transform (A: Geometry, TargetCRS: String)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/sql/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/sql/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/sql/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/sql/Function/#st_union","title":"ST_Union","text":"<p>Introduction:</p> <p>Variant 1: Return the union of geometry A and B.</p> <p>Variant 2 : As of version <code>1.6.0</code>, this function accepts an array of Geometry objects and returns the geometric union of all geometries in the input array. If the polygons within the input array do not share common boundaries, the ST_Union result will be a MultiPolygon geometry.</p> <p>Format:</p> <p><code>ST_Union (A: Geometry, B: Geometry)</code></p> <p><code>ST_Union (geoms: Array(Geometry))</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Union(\n    Array(\n        ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'),\n        ST_GeomFromWKT('POLYGON ((-2 1, 2 1, 2 4, -2 4, -2 1))')\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 3, 3 3, 3 -3, -3 -3, -3 3, -2 3, -2 4, 2 4, 2 3))\n</code></pre>"},{"location":"api/sql/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/sql/Function/#st_weighteddistancebandcolumn","title":"ST_WeightedDistanceBandColumn","text":"<p>Introduction: Introduction: Returns a <code>weights</code> column containing every record in a dataframe within a specified <code>threshold</code> distance.</p> <p>The <code>weights</code> column is an array of structs containing the <code>attributes</code> from each neighbor and that neighbor's weight. Since this is a distance weighted distance band, weights will be distance^alpha.</p> <p>Format: <code>ST_WeightedDistanceBandColumn(geometry:Geometry, threshold: Double, alpha: Double, includeZeroDistanceNeighbors: boolean, includeSelf: boolean, selfWeight: Double, useSpheroid: boolean, attributes: Struct)</code></p> <p>Since: <code>v1.7.1</code></p> <p>SQL Example</p> <pre><code>ST_WeightedDistanceBandColumn(geometry, 1.0, -1.0, true, true, 1.0, false, struct(id, geometry))\n</code></pre> <p>Output:</p> <pre><code>[{{15, POINT (3 1.9)}, 1.0}, {{16, POINT (3 2)}, 9.999999999999991}, {{17, POINT (3 2.1)}, 4.999999999999996}, {{18, POINT (3 2.2)}, 3.3333333333333304}]\n</code></pre>"},{"location":"api/sql/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/sql/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/sql/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/sql/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/sql/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmflag","title":"ST_Zmflag","text":"<p>Introduction: Returns a code indicating the Z and M coordinate dimensions present in the input geometry.</p> <p>Values are: 0 = 2D, 1 = 3D-M, 2 = 3D-Z, 3 = 4D.</p> <p>Format: <code>ST_Zmflag(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('LINESTRING Z(1 2 3, 4 5 6)')\n)\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('POINT ZM(1 2 3 4)')\n)\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/","title":"Nearest-Neighbour searching","text":"<p>Sedona supports nearest-neighbour searching on geospatial data by providing a geospatial k-Nearest Neighbors (kNN) join method. This method involves identifying the k-nearest neighbors for a given spatial point or region based on geographic proximity, typically using spatial coordinates and a suitable distance metric like Euclidean or great-circle distance.</p>"},{"location":"api/sql/NearestNeighbourSearching/#st_knn","title":"ST_KNN","text":"<p>Introduction: join operation to find the k-nearest neighbors of a point or region in a spatial dataset.</p> <p>Format: <code>ST_KNN(R: Table, S: Table, k: Integer, use_spheroid: Boolean)</code></p> <p>Where R is the queries side table and S is the object side table, K is the number of neighbors. use_spheroid is a boolean value that determines whether to use the spheroid distance or not.</p> <p>Queries side table contains geometries that are used to find the k-nearest neighbors in the object side table.</p> <p>When either queries or objects data contain non-point data (geometries), we take the centroid of each geometry.</p> <p>In case there are ties in the distance, the result will include all the tied geometries only when the following sedona config is set to true:</p> <p>Note for Inner Join:</p> <ul> <li>The <code>ST_KNN</code> join only supports left inner join.</li> <li>It returns only pairs where there is at least one matching neighbor within the k nearest neighbors.</li> <li>If a query point has no valid neighbor (e.g., because k is too large), it is excluded from the result.</li> </ul> <pre><code>spark.sedona.join.knn.includeTieBreakers=true\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/#filter-pushdown-considerations","title":"Filter Pushdown Considerations:","text":"<p>When using ST_KNN with filters applied to the resulting DataFrame, some of these filters may be pushed down to the object side of the kNN join. This means the filters will be applied to the object side reader before the kNN join is executed. If you want the filters to be applied after the kNN join, ensure that you first materialize the kNN join results and then apply the filters.</p> <p>For example, you can use the following approach:</p> <p>Scala Example:</p> <pre><code>val knnResult = knnJoinDF.cache()\nval filteredResult = knnResult.filter(condition)\n</code></pre> <p>SQL Example:</p> <pre><code>CREATE OR REPLACE TEMP VIEW knnResult AS\nSELECT * FROM (\n  -- Your KNN join SQL here\n) AS knnView;\nCACHE TABLE knnResult;\nSELECT * FROM knnResult WHERE condition;\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/#optimization-barrier","title":"Optimization Barrier","text":"<p>Use the <code>barrier</code> function to prevent filter pushdown and control predicate evaluation order in complex spatial joins. This function creates an optimization barrier by evaluating boolean expressions at runtime.</p> <p>The <code>barrier</code> function takes a boolean expression as a string, followed by pairs of variable names and their values that will be substituted into the expression:</p> <pre><code>barrier(expression, var_name1, var_value1, var_name2, var_value2, ...)\n</code></pre> <p>The placement of filters relative to KNN joins changes the semantic meaning of the query:</p> <ul> <li>Filter before KNN: First filters the data, then finds K nearest neighbors from the filtered subset. This answers \"What are the K nearest high-rated restaurants?\"</li> <li>Filter after KNN: First finds K nearest neighbors from all data, then filters those results. This answers \"Of the K nearest restaurants, which ones are high-rated?\"</li> </ul>"},{"location":"api/sql/NearestNeighbourSearching/#example","title":"Example","text":"<p>Find the 3 nearest high-rated restaurants to luxury hotels, ensuring the KNN join completes before filtering.</p> <pre><code>SELECT\n    h.name AS hotel,\n    r.name AS restaurant,\n    r.rating\nFROM hotels AS h\nINNER JOIN restaurants AS r\nON ST_KNN(h.geometry, r.geometry, 3, false)\nWHERE barrier('rating &gt; 4.0 AND stars &gt;= 4',\n              'rating', r.rating,\n              'stars', h.stars)\n</code></pre> <p>With the barrier function, this query first finds the 3 nearest restaurants to each hotel (regardless of rating), then filters to keep only those pairs where the restaurant has rating &gt; 4.0 and the hotel has stars &gt;= 4. Without the barrier, an optimizer might push the filters down, changing the query to first filter for high-rated restaurants and luxury hotels, then find the 3 nearest among those filtered sets.</p>"},{"location":"api/sql/NearestNeighbourSearching/#handling-sql-defined-tables-in-st_knn-joins","title":"Handling SQL-Defined Tables in ST_KNN Joins:","text":"<p>When creating DataFrames from hard-coded SQL select statements in Sedona, and later using them in <code>ST_KNN</code> joins, Sedona may attempt to optimize the query in a way that bypasses the intended kNN join logic. Specifically, if you create DataFrames with hard-coded SQL, such as:</p> <pre><code>val df1 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom1\")\nval df2 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom2\")\n\nval df = df1.join(df2, expr(\"ST_KNN(geom1, geom2, 1)\"))\n</code></pre> <p>Sedona may optimize the join to a form like this:</p> <pre><code>SELECT ST_KNN(ST_Point(0.0, 0.0), ST_Point(0.0, 0.0), 1)\n</code></pre> <p>As a result, the ST_KNN function is handled as a User-Defined Function (UDF) instead of a proper join operation, preventing Sedona from initiating the kNN join execution path. Unlike typical UDFs, the ST_KNN function operates on multiple rows across DataFrames, not just individual rows. When this occurs, the query fails with an UnsupportedOperationException, indicating that the KNN predicate is not supported.</p> <p>Workaround:</p> <p>To prevent Spark's optimization from bypassing the kNN join logic, the DataFrames created with hard-coded SQL select statements must be materialized before performing the join. By caching the DataFrames, you can instruct Spark to avoid this undesired optimization:</p> <pre><code>val df1 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom1\").cache()\nval df2 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom2\").cache()\n\nval df = df1.join(df2, expr(\"ST_KNN(geom1, geom2, 1)\"))\n</code></pre> <p>Materializing the DataFrames with .cache() ensures that the correct kNN join path is followed in the Spark logical plan and prevents the optimization that would treat ST_KNN as a simple UDF.</p>"},{"location":"api/sql/NearestNeighbourSearching/#sql-example","title":"SQL Example","text":"<p>Suppose we have two tables <code>QUERIES</code> and <code>OBJECTS</code> with the following data:</p> <p>QUERIES table:</p> <pre><code>ID  GEOMETRY            NAME\n1   POINT(1 1)          station1\n2   POINT(10 10)        station2\n3   POINT(-0.5 -0.5)    station3\n</code></pre> <p>OBJECTS table:</p> <pre><code>ID  GEOMETRY            NAME\n1   POINT(11 5)         bank1\n2   POINT(12 1)         bank2\n3   POINT(-1 -1)        bank3\n4   POINT(-3 5)         bank4\n5   POINT(9 8)          bank5\n6   POINT(4 3)          bank6\n7   POINT(-4 -5)        bank7\n8   POINT(4 -2)         bank8\n9   POINT(-3 1)         bank9\n10  POINT(-7 3)         bank10\n11  POINT(11 5)         bank11\n12  POINT(12 1)         bank12\n13  POINT(-1 -1)        bank13\n14  POINT(-3 5)         bank14\n15  POINT(9 8)          bank15\n16  POINT(4 3)          bank16\n17  POINT(-4 -5)        bank17\n18  POINT(4 -2)         bank18\n19  POINT(-3 1)         bank19\n20  POINT(-7 3)         bank20\n</code></pre> <pre><code>SELECT\n    QUERIES.ID AS QUERY_ID,\n    QUERIES.GEOMETRY AS QUERIES_GEOM,\n    OBJECTS.GEOMETRY AS OBJECTS_GEOM\nFROM QUERIES JOIN OBJECTS ON ST_KNN(QUERIES.GEOMETRY, OBJECTS.GEOMETRY, 4, FALSE)\n</code></pre> <p>Output:</p> <pre><code>+--------+-----------------+-------------+\n|QUERY_ID|QUERIES_GEOM     |OBJECTS_GEOM |\n+--------+-----------------+-------------+\n|3       |POINT (-0.5 -0.5)|POINT (-1 -1)|\n|3       |POINT (-0.5 -0.5)|POINT (-1 -1)|\n|3       |POINT (-0.5 -0.5)|POINT (-3 1) |\n|3       |POINT (-0.5 -0.5)|POINT (-3 1) |\n|1       |POINT (1 1)      |POINT (-1 -1)|\n|1       |POINT (1 1)      |POINT (-1 -1)|\n|1       |POINT (1 1)      |POINT (4 3)  |\n|1       |POINT (1 1)      |POINT (4 3)  |\n|2       |POINT (10 10)    |POINT (9 8)  |\n|2       |POINT (10 10)    |POINT (9 8)  |\n|2       |POINT (10 10)    |POINT (11 5) |\n|2       |POINT (10 10)    |POINT (11 5) |\n+--------+-----------------+-------------+\n</code></pre>"},{"location":"api/sql/Optimizer/","title":"Query optimization","text":"<p>Sedona Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:</p> <ul> <li>Automatically optimizes range join query and distance join query.</li> <li>Automatically performs predicate pushdown.</li> </ul> <p>Tip</p> <p>Sedona join performance is heavily affected by the number of partitions. If the join performance is not ideal, please increase the number of partitions by doing <code>df.repartition(XXX)</code> right after you create the original DataFrame.</p>"},{"location":"api/sql/Optimizer/#range-join","title":"Range join","text":"<p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by SedonaSQL can trigger a range join.</p> <p>SQL Example</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_DWithin(pointdf.pointshape, polygondf.polygonshape, 10.0)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre> <p>Note</p> <p>All join queries in SedonaSQL are inner joins</p>"},{"location":"api/sql/Optimizer/#distance-join","title":"Distance join","text":"<p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>ST_Distance</code>, <code>ST_HausdorffDistance</code>, <code>ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for planar Euclidean distance:</p> <p>Only consider fully within a certain distance</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape, 0.3) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt; 2\n</code></pre> <p>Consider intersects within a certain distance</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv\n</code></pre> <p>Warning</p> <p>If you use planar euclidean distance functions like <code>ST_Distance</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for meter-based geodesic distance <code>ST_DistanceSpheroid</code> (works for <code>ST_DistanceSphere</code> too):</p> <p>Less than a certain distance==</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <p>Less than or equal to a certain distance==</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre> <p>Warning</p> <p>If you use <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code> as the predicate, the unit of the distance is meter. Currently, distance join with geodesic distance calculators work best for point data. For non-point data, it only considers their centroids.</p>"},{"location":"api/sql/Optimizer/#broadcast-index-join","title":"Broadcast index join","text":"<p>Introduction: Perform a range join or distance join but broadcast one of the sides of the join. This maintains the partitioning of the non-broadcast side and doesn't require a shuffle.</p> <p>Sedona will create a spatial index on the broadcasted table.</p> <p>Sedona uses broadcast join only if the correct side has a broadcast hint. The supported join type - broadcast side combinations are:</p> <ul> <li>Inner - either side, preferring to broadcast left if both sides have the hint</li> <li>Left semi - broadcast right</li> <li>Left anti - broadcast right</li> <li>Left outer - broadcast right</li> <li>Right outer - broadcast left</li> </ul> <pre><code>pointDf.alias(\"pointDf\").join(broadcast(polygonDf).alias(\"polygonDf\"), expr(\"ST_Contains(polygonDf.polygonshape, pointDf.pointshape)\"))\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildRight, false ST_Contains(polygonshape#30, pointshape#52)\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex polygonshape#30: geometry, QUADTREE, [id=#62]\n   +- Project [st_polygonfromenvelope(cast(_c0#22 as decimal(24,20)), cast(_c1#23 as decimal(24,20)), cast(_c2#24 as decimal(24,20)), cast(_c3#25 as decimal(24,20))) AS polygonshape#30]\n      +- FileScan csv\n</code></pre> <p>This also works for distance joins with <code>ST_Distance</code>, <code>ST_DistanceSpheroid</code>, <code>ST_DistanceSphere</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code>:</p> <pre><code>pointDf1.alias(\"pointDf1\").join(broadcast(pointDf2).alias(\"pointDf2\"), expr(\"ST_Distance(pointDf1.pointshape, pointDf2.pointshape) &lt;= 2\"))\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildLeft, true, 2.0 ST_Distance(pointshape#52, pointshape#415) &lt;= 2.0\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex pointshape#415: geometry, QUADTREE, [id=#1068]\n   +- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#415]\n      +- FileScan csv\n</code></pre> <p>Note: If the distance is an expression, it is only evaluated on the first argument to ST_Distance (<code>pointDf1</code> above).</p>"},{"location":"api/sql/Optimizer/#automatic-broadcast-index-join","title":"Automatic broadcast index join","text":"<p>When one table involved a spatial join query is smaller than a threshold, Sedona will automatically choose broadcast index join instead of Sedona optimized join. The current threshold is controlled by sedona.join.autoBroadcastJoinThreshold and set to the same as <code>spark.sql.autoBroadcastJoinThreshold</code>.</p>"},{"location":"api/sql/Optimizer/#raster-join","title":"Raster join","text":"<p>The optimization for spatial join also works for raster predicates, such as <code>RS_Intersects</code>, <code>RS_Contains</code> and <code>RS_Within</code>.</p> <p>SQL Example:</p> <pre><code>-- Raster-geometry join\nSELECT df1.id, df2.id, RS_Value(df1.rast, df2.geom) FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.geom)\n\n-- Raster-raster join\nSELECT df1.id, df2.id FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.rast)\n</code></pre> <p>These queries could be planned as RangeJoin or BroadcastIndexJoin. Here is an example of the physical plan using RangeJoin:</p> <pre><code>== Physical Plan ==\n*(1) Project [id#14, id#25]\n+- RangeJoin rast#13: raster, geom#24: geometry, INTERSECTS,  **org.apache.spark.sql.sedona_sql.expressions.RS_Intersects**\n   :- LocalTableScan [rast#13, id#14]\n   +- LocalTableScan [geom#24, id#25]\n</code></pre>"},{"location":"api/sql/Optimizer/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>If the performance of Sedona optimized join is not ideal, which is possibly caused by  complicated and overlapping geometries, you can resort to Sedona built-in Google S2-based approximate equi-join. This equi-join leverages Spark's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"api/sql/Optimizer/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM rights\n</code></pre>"},{"location":"api/sql/Optimizer/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"api/sql/Optimizer/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"api/sql/Optimizer/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, first(lcs_geom), first(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>first</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"api/sql/Optimizer/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives. Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"api/sql/Optimizer/#regular-spatial-predicate-pushdown","title":"Regular spatial predicate pushdown","text":"<p>Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query.</p> <p>SQL Example</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nAND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre>"},{"location":"api/sql/Optimizer/#push-spatial-predicates-to-geoparquet","title":"Push spatial predicates to GeoParquet","text":"<p>Sedona supports spatial predicate push-down for GeoParquet files. When spatial filters were applied to dataframes backed by GeoParquet files, Sedona will use the <code>bbox</code> properties in the metadata to determine if all data in the file will be discarded by the spatial predicate. This optimization could reduce the number of files scanned when the queried GeoParquet dataset was partitioned by spatial proximity.</p> <p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre> <p>The following figure is the visualization of a GeoParquet dataset. <code>bbox</code>es of all GeoParquet files were plotted as blue rectangles and the query window was plotted as a red rectangle. Sedona will only scan 1 of the 6 files to answer queries such as <code>SELECT * FROM geoparquet_dataset WHERE ST_Intersects(geom, &lt;query window&gt;)</code>, thus only part of the data covered by the light green rectangle needs to be scanned.</p> <p></p> <p>We can compare the metrics of querying the GeoParquet dataset with or without the spatial predicate and observe that querying with spatial predicate results in fewer number of rows scanned.</p> Without spatial predicate With spatial predicate <p>Spatial predicate push-down to GeoParquet is enabled by default. Users can manually disable it by setting the Spark configuration <code>spark.sedona.geoparquet.spatialFilterPushDown</code> to <code>false</code>.</p>"},{"location":"api/sql/Overview/","title":"Quick start","text":""},{"location":"api/sql/Overview/#introduction","title":"Introduction","text":""},{"location":"api/sql/Overview/#function-list","title":"Function list","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\n</code></pre> <p>Alternatively, <code>expr</code> and <code>selectExpr</code> can be used:</p> <pre><code>myDataFrame.withColumn(\"geometry\", expr(\"ST_*\")).selectExpr(\"ST_*\")\n</code></pre> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> </ul> </li> <li>Aggregate function: Return the aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul> </li> </ul> <p>Sedona also provides an Adapter to convert SpatialRDD &lt;-&gt; DataFrame. Please read Adapter Scaladoc</p> <p>SedonaSQL supports SparkSQL query optimizer, documentation is Here</p>"},{"location":"api/sql/Overview/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here Write a SQL/DataFrame application.</p> <ol> <li>Add Sedona-core and Sedona-SQL into your project pom.xml or build.sbt</li> <li>Create your Sedona config if you want to customize your SparkSession.</li> </ol> <pre><code>import org.apache.sedona.spark.SedonaContext\nval config = SedonaContext.builder().\n    master(\"local[*]\").appName(\"SedonaSQL\")\n    .getOrCreate()\n</code></pre> <ol> <li>Add the following line after your Sedona context declaration:</li> </ol> <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(config)\n</code></pre>"},{"location":"api/sql/Parameter/","title":"Parameter","text":""},{"location":"api/sql/Parameter/#usage","title":"Usage","text":"<p>SedonaSQL supports many parameters. To change their values,</p> <ol> <li>Set it through SparkConf:</li> </ol> <pre><code>sparkSession = SparkSession.builder().\n      config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\n      config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\").\n      config(\"sedona.global.index\",\"true\")\n      master(\"local[*]\").appName(\"mySedonaSQLdemo\").getOrCreate()\n</code></pre> <ol> <li>Check your current SedonaSQL configuration:</li> </ol> <pre><code>val sedonaConf = new SedonaConf(sparkSession.conf)\nprintln(sedonaConf)\n</code></pre> <ol> <li>Sedona parameters can be changed at runtime:</li> </ol> <pre><code>sparkSession.conf.set(\"sedona.global.index\",\"false\")\n</code></pre> <p>In addition, you can also add <code>spark</code> prefix to the parameter name, for example:</p> <pre><code>sparkSession.conf.set(\"spark.sedona.global.index\",\"false\")\n</code></pre> <p>However, any parameter set through <code>spark</code> prefix will be honored by Spark, which means you can set these parameters before hand via <code>spark-defaults.conf</code> or Spark on Kubernetes configuration.</p> <p>If you set the same parameter through both <code>sedona</code> and <code>spark.sedona</code> prefixes, the parameter set through <code>sedona</code> prefix will override the parameter set through <code>spark.sedona</code> prefix.</p>"},{"location":"api/sql/Parameter/#explanation","title":"Explanation","text":"<ul> <li>sedona.global.index<ul> <li>Use spatial index (currently, only supports in SQL range join and SQL distance join)</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.global.indextype<ul> <li>Spatial index type, only valid when \"sedona.global.index\" is true</li> <li>Default: rtree</li> <li>Possible values: rtree, quadtree</li> </ul> </li> <li>sedona.join.autoBroadcastJoinThreshold<ul> <li>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.   By setting this value to -1 automatic broadcasting can be disabled.</li> <li>Default: The default value is the same as spark.sql.autoBroadcastJoinThreshold</li> <li>Possible values: any integer with a byte suffix i.e. 10MB or 512KB</li> </ul> </li> <li>sedona.join.gridtype<ul> <li>Spatial partitioning grid type for join query</li> <li>Default: kdbtree</li> <li>Possible values: quadtree, kdbtree</li> </ul> </li> <li>spark.sedona.join.knn.includeTieBreakers<ul> <li>KNN join will include all ties in the result, possibly returning more than k results</li> <li>Default: false</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.join.indexbuildside (Advanced users only!)<ul> <li>The side which Sedona builds spatial indices on</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.numpartition (Advanced users only!)<ul> <li>Number of partitions for both sides in a join query</li> <li>Default: -1, which means use the existing partitions</li> <li>Possible values: any integers</li> </ul> </li> <li>sedona.join.spatitionside (Advanced users only!)<ul> <li>The dominant side in spatial partitioning stage</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.optimizationmode (Advanced users only!)<ul> <li>When should Sedona optimize spatial join SQL queries</li> <li>Default: nonequi</li> <li>Possible values:<ul> <li>all: Always optimize spatial join queries, even for equi-joins.</li> <li>none: Disable optimization for spatial joins.</li> <li>nonequi: Optimize spatial join queries that are not equi-joins.</li> </ul> </li> </ul> </li> <li>spark.sedona.enableParserExtension<ul> <li>Enable the parser extension to parse GEOMETRY data type in SQL DDL statements</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> </ul>"},{"location":"api/sql/Predicate/","title":"Predicate","text":""},{"location":"api/sql/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'.</p> <p>If <code>useSpheroid</code> is passed true, ST_DWithin uses Sedona's ST_DistanceSpheroid to check the spheroid distance between the centroids of two geometries. The unit of the distance in this case is meter.</p> <p>If <code>useSpheroid</code> is passed false, ST_DWithin uses Euclidean distance and the unit of the distance is the same as the CRS of the geometries. To obtain the correct result, please consider using ST_Transform to put data in an appropriate CRS.</p> <p>If useSpheroid is not given, it defaults to false</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double, useSpheroid: Optional(Boolean) = false)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <pre><code>Check for distance between New York and Seattle (&lt; 4000 km)\n</code></pre> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT(-122.335167 47.608013), ST_GeomFromWKT(-73.935242 40.730610), 4000000, true)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Raster-affine-transformation/","title":"Raster affine transformation","text":"<p>This page explains the basic concepts of Affine Transformation in Sedona Raster.</p>"},{"location":"api/sql/Raster-affine-transformation/#affine-transformations","title":"Affine Transformations","text":"<p>Affine transformations are a fundamental concept in computer graphics, geometry, and image processing that involve manipulating an object in a way that preserves lines and parallelism (but not necessarily distances and angles). These transformations are linear transformations followed by a translation, which means they can translate, scale, rotate, and shear objects without altering the relative arrangement of points, lines, or planes.</p>"},{"location":"api/sql/Raster-affine-transformation/#components-of-affine-transformations","title":"Components of Affine Transformations","text":"<p>Affine transformations can be represented as a matrix operation. In two-dimensional space, a typical affine transformation matrix is a 3x3 matrix as follows:</p> <pre><code>| ScaleX  SkewX   TranslationX |\n| SkewY   ScaleY  TranslationY |\n| 0       0       1            |\n</code></pre> <p>Here, <code>ScaleX, ScaleY, SkewX, SkewY, TranslationX,</code> and <code>TranslationY</code> are parameters that define the transformation:</p> <ul> <li><code>ScaleX</code> and <code>ScaleY</code> are scaling factors for the x and y axes, respectively.</li> <li><code>SkewX</code> and <code>SkewY</code> introduce shearing and are responsible for \"skewing\" the shape.</li> <li><code>TranslationX</code> and <code>TranslationY</code> are translation parameters that move the shape in the x and y directions, respectively.</li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#types-of-affine-transformations","title":"Types of Affine Transformations","text":"<ol> <li> <p>Translation: Moves every point of a figure or space by the same distance in a given direction. This primarily affects the <code>TranslationX</code> and <code>TranslationY</code> components.</p> </li> <li> <p>Scaling: Multiplies the coordinates of each point by a constant (ScaleX for x-axis and ScaleY for y-axis), enlarging or reducing its size. Scaling can be uniform (the same factor for both axes) or non-uniform (different factors for each axis).</p> </li> <li> <p>Rotation: Rotates the object about a point (usually the origin or a specified point). This can be expressed through combinations of <code>ScaleX, ScaleY, SkewX,</code> and <code>SkewY</code> where these parameters are derived from the cosine and sine of the rotation angle.</p> </li> <li> <p>Shearing: Transforms parallel lines to still be parallel but moves them so that they are no longer perpendicular to their original orientations. This affects the <code>SkewX</code> and <code>SkewY</code> components.</p> </li> <li> <p>Reflection: Flips the object over a specified axis, which can be achieved by combining scaling and rotation.</p> </li> </ol>"},{"location":"api/sql/Raster-affine-transformation/#mathematical-properties","title":"Mathematical Properties","text":"<ul> <li>Collinearity and Concurrency: Affine transformations preserve points on a line (collinearity) and the intersection of lines (concurrency).</li> <li>Ratios of Segments: They also preserve the ratios of distances between points lying on a straight line.</li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#components-of-affine-transformations_1","title":"Components of Affine Transformations","text":"<p>In affine transformations, which are integral to manipulating graphics, images, and geometric data, the terms ScaleX, ScaleY, SkewX, and SkewY refer to specific types of transformations that alter the shape and position of objects:</p>"},{"location":"api/sql/Raster-affine-transformation/#scalex-and-scaley","title":"ScaleX and ScaleY","text":"<ul> <li> <p>ScaleX: This parameter represents the scaling factor along the x-axis. It modifies the width of an image or object. Values greater than 1 increase the width, values less than 1 decrease it, and negative values reflect the object along the x-axis while scaling.</p> </li> <li> <p>ScaleY: This parameter represents the scaling factor along the y-axis. It affects the height of the object. Similarly to ScaleX, values greater than 1 enlarge the object vertically, values less than 1 reduce it, and negative values invert it along the y-axis.</p> </li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#skewx-and-skewy","title":"SkewX and SkewY","text":"<ul> <li> <p>SkewX: This parameter is used to skew or shear the object along the x-axis. It shifts each point's x-coordinate in proportion to its y-coordinate, creating a slanting effect. This transformation is useful for creating the illusion of depth or perspective in 2D representations.</p> </li> <li> <p>SkewY: Corresponding to SkewX, SkewY skews the object along the y-axis. It alters each point's y-coordinate relative to its x-coordinate, which also creates a slanting effect, but in the vertical direction.</p> </li> </ul> <p>These transformations are typically used together in a transformation matrix, which allows them to be applied to objects in a combined and coherent way. Here's a typical representation of such a matrix:</p> <pre><code>| ScaleX  SkewX   TranslationX |\n| SkewY   ScaleY  TranslationY |\n| 0       0       1            |\n</code></pre> <p>These parameters can be combined in various ways to perform complex transformations such as rotations, translations, scaling, and shearing of images or shapes in both 2D and 3D graphics applications.</p>"},{"location":"api/sql/Raster-aggregate-function/","title":"Raster aggregates","text":""},{"location":"api/sql/Raster-aggregate-function/#rs_union_aggr","title":"RS_Union_Aggr","text":"<p>Introduction: This function combines multiple rasters into a single multiband raster by stacking the bands of each input raster sequentially. The function arranges the bands in the output raster according to the order specified by the index column in the input. It is typically used in scenarios where rasters are grouped by certain criteria (e.g., time and/or location) and an aggregated raster output is desired.</p> <p>Note</p> <p>RS_Union_Aggr expects the following input, if not satisfied then will throw an IllegalArgumentException:</p> <ul> <li>Indexes to be in an arithmetic sequence without any gaps.</li> <li>Indexes to be unique and not repeated.</li> <li>Rasters should be of the same shape.</li> </ul> <p>Format: <code>RS_Union_Aggr(A: rasterColumn, B: indexColumn)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <p>First, we enrich the dataset with time-based grouping columns and index the rasters based on time intervals:</p> <pre><code>// Add yearly and quarterly time interval columns for grouping\ndf = df\n .withColumn(\"year\", year($\"timestamp\"))\n .withColumn(\"quarter\", quarter($\"timestamp\"))\n\n// Define window specs for quarterly indexing within each geometry-year group\nwindowSpecQuarter = Window.partitionBy(\"geometry\", \"year\", \"quarter\").orderBy(\"timestamp\")\n\nindexedDf = df.withColumn(\"index\", row_number().over(windowSpecQuarter))\n\nindexedDf.show()\n</code></pre> <p>The indexed rasters will appear as follows, showing that each raster is tagged with a sequential index (ordered by timestamp) within its group (grouped by geometry, year and quarter).</p> <pre><code>+-------------------+-----------------------------+--------------+----+-------+-----+\n|timestamp          |raster                       |geometry      |year|quarter|index|\n+-------------------+-----------------------------+--------------+----+-------+-----+\n|2021-01-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |1    |\n|2021-01-25 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |2    |\n|2021-02-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |3    |\n|2021-03-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |4    |\n|2021-03-25 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |5    |\n|2021-04-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |1    |\n|2021-04-22 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |2    |\n|2021-05-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |3    |\n|2021-05-20 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |4    |\n|2021-05-29 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |5    |\n|2021-06-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |6    |\n+-------------------+-----------------------------+------------- +----+-------+-----+\n</code></pre> <p>To create a stacked raster by grouping on geometry.</p> <pre><code>indexedDf.createOrReplaceTempView(\"indexedDf\")\n\nsedona.sql('''\n    SELECT geometry, year, quarter, RS_Union_Aggr(raster, index) AS aggregated_raster\n    FROM indexedDf\n    WHERE index &lt;= 4\n    GROUP BY geometry, year, quarter\n''').show()\n</code></pre> <p>Output:</p> <p>The query yields rasters grouped by geometry, year and quarter, each containing the first four time steps combined into a single multiband raster, where each band represents one time step.</p> <pre><code>+--------------+----+-------+--------------------+---------+\n|      geometry|year|quarter|              raster|Num_Bands|\n+--------------+----+-------+--------------------+---------+\n|POINT (72 120)|2021|1      |GridCoverage2D[\"g...|        4|\n|POINT (84 132)|2021|2      |GridCoverage2D[\"g...|        4|\n+--------------+----+-------+--------------------+---------+\n</code></pre>"},{"location":"api/sql/Raster-loader/","title":"Raster loader","text":"<p>Note</p> <p>Sedona loader are available in Scala, Java and Python and have the same APIs.</p> <p>The raster loader of Sedona leverages Spark built-in binary data source and works with several RS constructors to produce Raster type. Each raster is a row in the resulting DataFrame and stored in a <code>Raster</code> format.</p> <p>By default, these functions uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p>"},{"location":"api/sql/Raster-loader/#step-1-load-raster-to-a-binary-dataframe","title":"Step 1: Load raster to a binary DataFrame","text":"<p>You can load any type of raster data using the code below. Then use the RS constructors below to create a Raster DataFrame.</p> <pre><code>sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\n</code></pre>"},{"location":"api/sql/Raster-loader/#step-2-create-a-raster-type-column","title":"Step 2: Create a raster type column","text":""},{"location":"api/sql/Raster-loader/#rs_fromarcinfoasciigrid","title":"RS_FromArcInfoAsciiGrid","text":"<p>Introduction: Returns a raster geometry from an Arc Info Ascii Grid file.</p> <p>Format: <code>RS_FromArcInfoAsciiGrid(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromArcInfoAsciiGrid(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromgeotiff","title":"RS_FromGeoTiff","text":"<p>Introduction: Returns a raster geometry from a GeoTiff file.</p> <p>Format: <code>RS_FromGeoTiff(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromGeoTiff(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeemptyraster","title":"RS_MakeEmptyRaster","text":"<p>Introduction: Returns an empty raster geometry. Every band in the raster is initialized to <code>0.0</code>.</p> <p>Since: <code>v1.5.0</code></p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, cellSize: Double)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits unsigned Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>Cell Size (pixel size): The size of the cells in the raster, in terms of the CRS units.</li> </ul> <p>It uses the default Cartesian coordinate system.</p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double, srid: Integer)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>ScaleX: The scaling factor of the cells on the X axis</li> <li>ScaleY: The scaling factor of the cells on the Y axis</li> <li>SkewX: The skew of the raster on the X axis, effectively tilting them in the horizontal direction</li> <li>SkewY: The skew of the raster on the Y axis, effectively tilting them in the vertical direction</li> <li>SRID: The SRID of the raster. Use 0 if you want to use the default Cartesian coordinate system. Use 4326 if you want to use WGS84.</li> </ul> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Note</p> <p>If any other value than the accepted values for the bandDataType is provided, RS_MakeEmptyRaster defaults to double as the data type for the raster.</p> <p>Spark SQL example 1 (with 2 bands):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 2 (with 2 bands and dataType):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'I', 10, 10, 0.0, 0.0, 1.0) - Create a raster with integer datatype\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 3 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre> <p>Spark SQL example 4 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'F', 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326) - Create a raster with float datatype\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeraster","title":"RS_MakeRaster","text":"<p>Introduction: Creates a raster from the given array of pixel values. The width, height, geo-reference information, and the CRS will be taken from the given reference raster. The data type of the resulting raster will be DOUBLE and the number of bands of the resulting raster will be <code>data.length / (refRaster.width * refRaster.height)</code>.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>RS_MakeRaster(refRaster: Raster, bandDataType: String, data: ARRAY[Double])</code></p> <ul> <li>refRaster: The reference raster from which the width, height, geo-reference information, and the CRS will be taken.</li> <li>bandDataType: The data type of the bands in the resulting raster. Please refer to the <code>RS_MakeEmptyRaster</code> function for the accepted values.</li> <li>data: The array of pixel values. The size of the array cannot be 0, and should be multiple of width * height of the reference raster.</li> </ul> <p>SQL example:</p> <pre><code>WITH r AS (SELECT RS_MakeEmptyRaster(2, 3, 2, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326) AS rast)\nSELECT RS_AsMatrix(RS_MakeRaster(rast, 'D', ARRAY(1, 2, 3, 4, 5, 6))) FROM r\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------+\n|rs_asmatrix(rs_makeraster(rast, D, array(1, 2, 3, 4, 5, 6)))|\n+------------------------------------------------------------+\n||1.0  2.0  3.0|\\n|4.0  5.0  6.0|\\n                          |\n+------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromnetcdf","title":"RS_FromNetCDF","text":"<p>Introduction: Returns a raster geometry representing the given record variable short name from a NetCDF file. This API reads the array data of the record variable in memory along with all its dimensions Since the netCDF format has many variants, the reader might not work for your test case, if that is so, please report this using the public forums.</p> <p>This API has been tested for netCDF classic (NetCDF 1, 2, 5) and netCDF4/HDF5 files.</p> <p>This API requires the name of the record variable. It is assumed that a variable of the given name exists, and its last 2 dimensions are 'lat' and 'lon' dimensions respectively.</p> <p>If this assumption does not hold true for your case, you can choose to pass the lonDimensionName and latDimensionName explicitly.</p> <p>You can use RS_NetCDFInfo to get the details of the passed netCDF file (variables and its dimensions).</p> <p>Format 1: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String)</code></p> <p>Format 2: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String, lonDimensionName: String, latDimensionName: String)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3')\"))\n</code></pre> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3', 'lon', 'lat')\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_netcdfinfo","title":"RS_NetCDFInfo","text":"<p>Introduction: Returns a string containing names of the variables in a given netCDF file along with its dimensions.</p> <p>Format: <code>RS_NetCDFInfo(netCDF: ARRAY[Byte])</code></p> <p>Since: <code>1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\nrecordInfo = df.selectExpr(\"RS_NetCDFInfo(content) as record_info\").first().getString(0)\nprint(recordInfo)\n</code></pre> <p>Output:</p> <pre><code>O3(time=2, z=2, lat=48, lon=80)\n\nNO2(time=2, z=2, lat=48, lon=80)\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/","title":"Raster map algebra","text":""},{"location":"api/sql/Raster-map-algebra/#map-algebra","title":"Map Algebra","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations. The expression can be applied to a single raster band or multiple raster bands. The result of the expression is a new raster.</p> <p>Apache Sedona provides two ways to perform map algebra operations:</p> <ol> <li>Using the <code>RS_MapAlgebra</code> function.</li> <li>Using <code>RS_BandAsArray</code> and array based map algebra functions, such as <code>RS_Add</code>, <code>RS_Multiply</code>, etc.</li> </ol> <p>Generally, the <code>RS_MapAlgebra</code> function is more flexible and can be used to perform more complex operations. The function takes three to four arguments:</p> <pre><code>RS_MapAlgebra(rast: Raster, pixelType: String, script: String, [noDataValue: Double])\n</code></pre> <ul> <li><code>rast</code>: The raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Optional) The nodata value of the output raster.</li> </ul> <p>As of version <code>v1.5.1</code>, the <code>RS_MapAlgebra</code> function allows two raster column inputs, with multi-band rasters supported. The function accepts 5 parameters:</p> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <ul> <li><code>rast0</code>: The first raster to apply the map algebra expression to.</li> <li><code>rast1</code>: The second raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Not optional) The nodata value of the output raster, <code>null</code> is allowed.</li> </ul> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p><code>RS_MapAlgebra</code> also has good performance, since it is backed by Jiffle and can be compiled to Java bytecode for execution. We'll demonstrate both approaches to implementing commonly used map algebra operations.</p> <p>Note</p> <p>The <code>RS_MapAlgebra</code> function can cast the output raster to a different data type specified by <code>pixelType</code>:</p> <ul> <li> <p>If <code>pixelType</code> is smaller than the input raster data type, narrowing casts will be performed, which may result in loss of data.</p> </li> <li> <p>If <code>pixelType</code> is larger, widening casts will retain data accuracy.</p> </li> <li> <p>If <code>pixelType</code> matches the input raster data type, no casting occurs.</p> </li> </ul> <p>This allows controlling the output pixel data type. Users should consider potential precision impacts when coercing to a smaller type.</p>"},{"location":"api/sql/Raster-map-algebra/#ndvi","title":"NDVI","text":"<p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements, typically, but not necessarily, from a space platform, and assess whether the target being observed contains live green vegetation or not. NDVI has become a de facto standard index used to determine whether a given area contains live green vegetation or not. The NDVI is calculated from these individual measurements as follows:</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <p>Assume that we have a bunch of rasters with 4 bands: red, green, blue, and near-infrared. We want to calculate the NDVI for each raster. We can use the <code>RS_MapAlgebra</code> function to do this:</p> <pre><code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>The Jiffle script is <code>out = (rast[3] - rast[0]) / (rast[3] + rast[0]);</code>. The <code>rast</code> variable is always bound to the input raster, and the <code>out</code> variable is bound to the output raster. Jiffle iterates over all the pixels in the input raster and executes the script for each pixel. the <code>rast[3]</code> and <code>rast[0]</code> refers to the current pixel values of the near-infrared and red bands, respectively. The <code>out</code> variable is the current output pixel value.</p> <p>The result of the <code>RS_MapAlgebra</code> function is a raster with a single band. The band is of type double, since we specified <code>D</code> as the <code>pixelType</code> argument.</p> <p>We can implement the same NDVI calculation using the array based map algebra functions:</p> <pre><code>SELECT RS_Divide(\n        RS_Subtract(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)),\n        RS_Add(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4))) as ndvi FROM raster_table\n</code></pre> <p>The <code>RS_BandAsArray</code> function extracts the specified band of the input raster to an array of double, and the <code>RS_Add</code>, <code>RS_Subtract</code>, and <code>RS_Divide</code> functions perform the arithmetic operations on the arrays. The code using the array based map algebra functions is more verbose. However, there is a <code>RS_NormalizedDifference</code> function that can be used to calculate the NDVI more concisely:</p> <pre><code>SELECT RS_NormalizedDifference(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)) as ndvi FROM raster_table\n</code></pre> <p>The result of array based map algebra functions is an array of double. User can use <code>RS_AddBandFromArray</code> to add the array to a raster as a new band.</p>"},{"location":"api/sql/Raster-map-algebra/#awei","title":"AWEI","text":"<p>The Automated Water Extraction Index (AWEI) is a spectral index that can be used to extract water bodies from remote sensing imagery. The AWEI is calculated from these individual measurements as follows:</p> <pre><code>AWEI = 4 * (Green - SWIR2) - (0.25 * NIR + 2.75 * SWIR1)\n</code></pre> <p>AWEI can be implemented easily using <code>RS_MapAlgebra</code>:</p> <pre><code>-- Assume that the raster includes all 13 Sentinel-2 bands\nSELECT RS_MapAlgebra(rast, 'D', 'out = 4 * (rast[2] - rast[11]) - (0.25 * rast[7] + 2.75 * rast[12]);') as awei FROM raster_table\n</code></pre> <p>We can also implement the same AWEI calculation using array based map algebra functions. The code looks more verbose:</p> <pre><code>SELECT RS_Subtract(\n    RS_Add(RS_MultiplyFactor(band_nir, 0.25), RS_MultiplyFactor(band_swir1, 2.75)),\n    RS_MultiplyFactor(RS_Subtract(band_swir2, band_green), 4)) as awei\nFROM (\nSELECT RS_BandAsArray(rast, 3) AS band_green,\n       RS_BandAsArray(rast, 12) AS band_swir2,\n       RS_BandAsArray(rast, 13) AS band_swir1,\n       RS_BandAsArray(rast, 8) AS band_nir\nFROM raster_table) t\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/#further-reading","title":"Further Reading","text":"<ul> <li>Jiffle language summary</li> <li>Raster operators</li> </ul>"},{"location":"api/sql/Raster-operators/","title":"Raster operators","text":""},{"location":"api/sql/Raster-operators/#pixel-functions","title":"Pixel Functions","text":""},{"location":"api/sql/Raster-operators/#rs_pixelascentroid","title":"RS_PixelAsCentroid","text":"<p>Introduction: Returns the centroid (point geometry) of the specified pixel's area. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsCentroid(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsCentroid(RS_MakeEmptyRaster(1, 12, 13, 134, -53, 9), 3, 3))\n</code></pre> <p>Output:</p> <pre><code>POINT (156.5 -75.5)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelascentroids","title":"RS_PixelAsCentroids","text":"<p>Introduction: Returns a list of the centroid point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band. Each centroid represents the geometric center of the corresponding pixel's area.</p> <p>Format: <code>RS_PixelAsCentroids(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsCentroids(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POINT (-13065222 4021263.75),148.0,0,0], [POINT (-13065151 4021263.75),123.0,0,1], [POINT (-13065077 4021263.75),99.0,1,0], [POINT (-13065007 4021261.75),140.0,1,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsCentroids(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------------+-----+---+---+\n|geom                                          |value|x  |y  |\n+----------------------------------------------+-----+---+---+\n|POINT (-13095781.835693639 4021226.5856936392)|0.0  |1  |1  |\n|POINT (-13095709.507080918 4021226.5856936392)|0.0  |2  |1  |\n|POINT (-13095637.178468198 4021226.5856936392)|0.0  |3  |1  |\n+----------------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoint","title":"RS_PixelAsPoint","text":"<p>Introduction: Returns a point geometry of the specified pixel's upper-left corner. The pixel coordinates specified are 1-indexed.</p> <p>Note</p> <p>If the pixel coordinates specified do not exist in the raster (out of bounds), RS_PixelAsPoint throws an IndexOutOfBoundsException.</p> <p>Format: <code>RS_PixelAsPoint(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 2, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>POINT (123.19, -12)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 6, 2)) from rasters\n</code></pre> <p>Output:</p> <pre><code>IndexOutOfBoundsException: Specified pixel coordinates (6, 2) do not lie in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoints","title":"RS_PixelAsPoints","text":"<p>Introduction: Returns a list of the pixel's upper-left corner point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPoints(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoints(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POINT (-13065223 4021262.75),148.0,0,0], [POINT (-13065150 4021262.75),123.0,0,1], [POINT (-13065078 4021262.75),99.0,1,0], [POINT (-13065006 4021262.75),140.0,1,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsPoints(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------+-----+---+---+\n|geom                                  |value|x  |y  |\n+--------------------------------------+-----+---+---+\n|POINT (-13095818 4021262.75)          |0.0  |1  |1  |\n|POINT (-13095745.67138728 4021262.75) |0.0  |2  |1  |\n|POINT (-13095673.342774557 4021262.75)|0.0  |3  |1  |\n+--------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygon","title":"RS_PixelAsPolygon","text":"<p>Introduction: Returns a polygon geometry that bounds the specified pixel. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsPolygon(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPolygon(RS_MakeEmptyRaster(1, 5, 10, 123, -230, 8), 2, 3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((131 -246, 139 -246, 139 -254, 131 -254, 131 -246))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygons","title":"RS_PixelAsPolygons","text":"<p>Introduction: Returns a list of the polygon geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPolygons(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPolygons(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POLYGON ((123.19000244140625 -12, 127.19000244140625 -12, 127.19000244140625 -16, 123.19000244140625 -16, 123.19000244140625 -12)),0.0,1,1],\n[POLYGON ((127.19000244140625 -12, 131.19000244140625 -12, 131.19000244140625 -16, 127.19000244140625 -16, 127.19000244140625 -12)),0.0,2,1],\n[POLYGON ((131.19000244140625 -12, 135.19000244140625 -12, 135.19000244140625 -16, 131.19000244140625 -16, 131.19000244140625 -12)),0.0,3,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsPolygons(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------+-----+---+---+\n|                geom|value|  x|  y|\n+--------------------+-----+---+---+\n|POLYGON ((-130958...|  0.0|  1|  1|\n|POLYGON ((-130957...|  0.0|  2|  1|\n|POLYGON ((-130956...|  0.0|  3|  1|\n+--------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#geometry-functions","title":"Geometry Functions","text":""},{"location":"api/sql/Raster-operators/#rs_envelope","title":"RS_Envelope","text":"<p>Introduction: Returns the envelope of the raster as a Geometry.</p> <p>Format: <code>RS_Envelope (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Envelope(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0,20 0,20 60,0 60,0 0))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_convexhull","title":"RS_ConvexHull","text":"<p>Introduction: Return the convex hull geometry of the raster including the NoDataBandValue band pixels. For regular shaped and non-skewed rasters, this gives more or less the same result as RS_Envelope and hence is only useful for irregularly shaped or skewed rasters.</p> <p>Format: <code>RS_ConvexHull(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ConvexHull(RS_MakeEmptyRaster(1, 5, 10, 156, -132, 5, 10, 3, 5, 0));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((156 -132, 181 -107, 211 -7, 186 -32, 156 -132))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_minconvexhull","title":"RS_MinConvexHull","text":"<p>Introduction: Returns the min convex hull geometry of the raster excluding the NoDataBandValue band pixels, in the given band. If no band is specified, all the bands are considered when creating the min convex hull of the raster. The created geometry representing the min convex hull has world coordinates of the raster in its CRS as the corner coordinates.</p> <p>Note</p> <p>If the specified band does not exist in the raster, RS_MinConvexHull throws an IllegalArgumentException</p> <p>Format:</p> <p><code>RS_MinConvexHull(raster: Raster)</code></p> <p><code>RS_MinConvexHull(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n        Seq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n        \"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0))) as minConvexHullAll\").show()\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------+\n|minConvexHullAll                        |\n+----------------------------------------+\n|POLYGON ((0 -1, 4 -1, 4 -5, 0 -5, 0 -1))|\n+----------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n        Seq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n  \"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0), 1)) as minConvexHull1\").show()\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------+\n|minConvexHull1                          |\n+----------------------------------------+\n|POLYGON ((1 -1, 4 -1, 4 -5, 1 -5, 1 -1))|\n+----------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_MinConvexHull(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>Provided band index 3 does not lie in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-accessors","title":"Raster Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_georeference","title":"RS_GeoReference","text":"<p>Introduction: Returns the georeference metadata of raster as a string in GDAL or ESRI format. Default is GDAL if not specified.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format: <code>RS_GeoReference(raster: Raster, format: String = \"GDAL\")</code></p> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX\nUpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX + ScaleX * 0.5\nUpperLeftY + ScaleY * 0.5\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 100, 100, -53, 51, 2, -2, 4, 5, 4326))\n</code></pre> <p>Output:</p> <pre><code>2.000000\n5.000000\n4.000000\n-2.000000\n-53.000000\n51.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"GDAL\")\n</code></pre> <p>Output:</p> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n100.000000\n200.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"ERSI\")\n</code></pre> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n101.000000\n198.500000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_geotransform","title":"RS_GeoTransform","text":"<p>Introduction: Returns a struct of parameters that represent the GeoTransformation of the raster. The struct has the following schema:</p> <ul> <li>magnitudeI: size of a pixel along the transformed i axis</li> <li>magnitudeJ: size of a pixel along the transformed j axis</li> <li>thetaI: angle by which the raster is rotated (Radians positive clockwise)</li> <li>thetaIJ: angle from transformed i axis to transformed j axis (Radians positive counter-clockwise)</li> <li>offsetX: X ordinate of the upper-left corner of the upper-left pixel</li> <li>offsetY: Y ordinate of the upper-left corner of the upper-left pixel</li> </ul> <p>Note</p> <p>Refer to this image for a clear understanding between i &amp; j axis and x &amp; y-axis.</p> <p>Format: <code>RS_GeoTransform(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_GeoTransform(\n        RS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n       )\n</code></pre> <p>Output:</p> <pre><code>{2.23606797749979, 2.23606797749979, -1.1071487177940904, -2.214297435588181, 1.0, 2.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_height","title":"RS_Height","text":"<p>Introduction: Returns the height of the raster.</p> <p>Format: <code>RS_Height(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Height(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>512\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordx","title":"RS_RasterToWorldCoordX","text":"<p>Introduction: Returns the upper left X coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the X coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordX(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoordX(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>-123\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordy","title":"RS_RasterToWorldCoordY","text":"<p>Introduction: Returns the upper left Y coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the Y coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordY(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoordY(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>54\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoord","title":"RS_RasterToWorldCoord","text":"<p>Introduction: Returns the upper left X and Y coordinates of the given row and column of the given raster geometric units of the geo-referenced raster as a Point geometry. If any out of bounds values are given, the X and Y coordinates of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoord(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoord(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>POINT (-123 54)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rotation","title":"RS_Rotation","text":"<p>Introduction: Returns the uniform rotation of the raster in radian.</p> <p>Format: <code>RS_Rotation(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Rotation(\n        RS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n        )\n</code></pre> <p>Output:</p> <pre><code>-1.1071487177940904\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scalex","title":"RS_ScaleX","text":"<p>Introduction: Returns the pixel width of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleX attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleX will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format: <code>RS_ScaleX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ScaleX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scaley","title":"RS_ScaleY","text":"<p>Introduction: Returns the pixel height of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleY attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleY will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format: <code>RS_ScaleY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ScaleY(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>-2\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_skewx","title":"RS_SkewX","text":"<p>Introduction: Returns the X skew or rotation parameter.</p> <p>Format: <code>RS_SkewX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SkewX(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_skewy","title":"RS_SkewY","text":"<p>Introduction: Returns the Y skew or rotation parameter.</p> <p>Format: <code>RS_SkewY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SkewY(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.2\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperleftx","title":"RS_UpperLeftX","text":"<p>Introduction: Returns the X coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_UpperLeftX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>5\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperlefty","title":"RS_UpperLeftY","text":"<p>Introduction: Returns the Y coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_UpperLeftY(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_width","title":"RS_Width","text":"<p>Introduction: Returns the width of the raster.</p> <p>Format: <code>RS_Width(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Width(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>517\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoord","title":"RS_WorldToRasterCoord","text":"<p>Introduction: Returns the grid coordinate of the given world coordinates as a Point.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Point)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), ST_GeomFromText('POINT (-52 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre> <p>Note</p> <p>If the given geometry point is not in the same CRS as the given raster, the given geometry will be transformed to the given raster's CRS. You can use ST_Transform to transform the geometry beforehand.</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordx","title":"RS_WorldToRasterCoordX","text":"<p>Introduction: Returns the X coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-53 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for latitude and the intended value of world longitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordy","title":"RS_WorldToRasterCoordY","text":"<p>Introduction: Returns the Y coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoordY(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoordY(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-50 50)'));\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -50, 49);\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for longitude and the intended value of world latitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#raster-band-accessors","title":"Raster Band Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_band","title":"RS_Band","text":"<p>Introduction: Returns a new raster consisting 1 or more bands of an existing raster. It can build new rasters from existing ones, export only selected bands from a multiband raster, or rearrange the order of bands in a raster dataset.</p> <p>Format:</p> <p><code>RS_Band(raster: Raster, bands: ARRAY[Integer])</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NumBands(\n        RS_Band(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(2, 5, 5, 3, -215, 2, -2, 2, 2, 0),\n                Array(16, 0, 24, 33, 43, 49, 64, 0, 76, 77, 79, 89, 0, 116, 118, 125, 135, 0, 157, 190, 215, 229, 241, 248, 249),\n                1, 0d\n            ), Array(1,1,1)\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandnodatavalue","title":"RS_BandNoDataValue","text":"<p>Introduction: Returns the no data value of the given band of the given raster. If no band is given, band 1 is assumed. The band parameter is 1-indexed. If there is no data value associated with the given band, RS_BandNoDataValue returns null.</p> <p>Note</p> <p>If the given band does not lie in the raster, RS_BandNoDataValue throws an IllegalArgumentException</p> <p>Format: <code>RS_BandNoDataValue (raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster, 1) from rasters;\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster) from rasters_without_nodata;\n</code></pre> <p>Output:</p> <pre><code>null\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster.\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandisnodata","title":"RS_BandIsNoData","text":"<p>Returns true if the band is filled with only nodata values. Band 1 is assumed if not specified.</p> <p>Format: <code>RS_BandIsNoData(raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>WITH rast_table AS (SELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 2, 2, 0, 0, 1), ARRAY(10d, 10d, 10d, 10d), 1, 10d) as rast)\nSELECT RS_BandIsNoData(rast) from rast_table\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandpixeltype","title":"RS_BandPixelType","text":"<p>Introduction: Returns the datatype of each pixel in the given band of the given raster in string format. The band parameter is 1-indexed. If no band is specified, band 1 is assumed.</p> <p>Note</p> <p>If the given band index does not exist in the given raster, RS_BandPixelType throws an IllegalArgumentException.</p> <p>Following are the possible values returned by RS_BandPixelType:</p> <ol> <li><code>REAL_64BITS</code> - For Double values</li> <li><code>REAL_32BITS</code> - For Float values</li> <li><code>SIGNED_32BITS</code> - For Integer values</li> <li><code>SIGNED_16BITS</code> - For Short values</li> <li><code>UNSIGNED_16BITS</code> - For unsigned Short values</li> <li><code>UNSIGNED_8BITS</code> - For Byte values</li> </ol> <p>Format: <code>RS_BandPixelType(rast: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"D\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 2);\n</code></pre> <p>Output:</p> <pre><code>REAL_64BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0));\n</code></pre> <p>Output:</p> <pre><code>SIGNED_32BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 3);\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_count","title":"RS_Count","text":"<p>Introduction: Returns the number of pixels in a given band. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Format:</p> <p><code>RS_Count(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_Count(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_Count(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>25\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_summarystats","title":"RS_SummaryStats","text":"<p>Introduction: Returns summary statistic for a particular band based on the <code>statType</code> parameter. The function defaults to band index of <code>1</code> when <code>band</code> is not specified and excludes noDataValue if <code>excludeNoDataValue</code> is not specified.</p> <p><code>statType</code> parameter takes the following strings:</p> <ul> <li><code>count</code>: Total count of all pixels in the specified band</li> <li><code>sum</code>: Sum of all pixel values in the specified band</li> <li><code>mean</code>: Mean value of all pixel values in the specified band</li> <li><code>stddev</code>: Standard deviation of all pixels in the specified band</li> <li><code>min</code>: Minimum pixel value in the specified band</li> <li><code>max</code>: Maximum pixel value in the specified band</li> </ul> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Formats:</p> <p><code>RS_SummaryStats(raster: Raster, statType: String, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_SummaryStats(raster: Raster, statType: String, band: Integer = 1)</code></p> <p><code>RS_SummaryStats(raster: Raster, statType: String)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStats(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), \"stddev\", 1, false)\n</code></pre> <p>Output:</p> <pre><code>9.4678403028357\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_summarystatsall","title":"RS_SummaryStatsAll","text":"<p>Introduction: Returns summary stats struct consisting of count, sum, mean, stddev, min, max for a given band in raster. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Formats:</p> <p><code>RS_SummaryStatsAll(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_SummaryStatsAll(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_SummaryStatsAll(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStatsAll(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>{25.0, 204.0, 8.16, 9.4678403028357, 0.0, 25.0}\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStatsAll(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>{14.0, 204.0, 14.571428571428571, 11.509091348732502, 1.0, 25.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstats","title":"RS_ZonalStats","text":"<p>Introduction: This returns a statistic value specified by <code>statType</code> over the region of interest defined by <code>zone</code>. It computes the statistic from the pixel values within the ROI geometry and returns the result. If the <code>excludeNoData</code> parameter is not specified, it will default to <code>true</code>. This excludes NoData values from the statistic calculation. Additionally, if the <code>band</code> parameter is not provided, band 1 will be used by default for the statistic computation. The valid options for <code>statType</code> are:</p> <p>The <code>allTouched</code> parameter (Since <code>v1.7.1</code>) determines how pixels are selected:</p> <ul> <li>When true, any pixel touched by the geometry will be included.</li> <li> <p>When false (default), only pixels whose centroid intersects with the geometry will be included.</p> </li> <li> <p><code>count</code>: Number of pixels in the region.</p> </li> <li><code>sum</code>: Sum of pixel values.</li> <li><code>mean|average|avg</code>: Arithmetic mean.</li> <li><code>median</code>: Middle value in the region.</li> <li><code>mode</code>: Most occurring value, if there are multiple values with same occurrence then will return the largest number.</li> <li><code>stddev|sd</code>: Standard deviation.</li> <li><code>variance</code>: Variance.</li> <li><code>min</code>: Minimum value in the region.</li> <li><code>max</code>: Maximum value in the region.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect when <code>lenient</code> parameter is set to <code>false</code>.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p><code>lenient</code> parameter is set to <code>true</code> by default. The function will return <code>null</code> if the <code>raster</code> and <code>zone</code> geometry do not intersect.</p> <p>Format:</p> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, allTouched: Boolean, excludeNoData: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, allTouched: Boolean, excludeNoData: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, allTouched: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, statType: String, allTouched: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, statType: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>RS_ZonalStats(rast1, geom1, 1, 'sum', true, false)\n</code></pre> <p>Output:</p> <pre><code>10690406\n</code></pre> <p>SQL Example</p> <pre><code>RS_ZonalStats(rast2, geom2, 1, 'mean', false, true)\n</code></pre> <p>Output:</p> <pre><code>226.55992667794473\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstatsall","title":"RS_ZonalStatsAll","text":"<p>Introduction: Returns a struct of statistic values, where each statistic is computed over a region defined by the <code>zone</code> geometry. The struct has the following schema:</p> <p>The <code>allTouched</code> parameter (Since <code>v1.7.1</code>) determines how pixels are selected:</p> <ul> <li>When true, any pixel touched by the geometry will be included.</li> <li> <p>When false (default), only pixels whose centroid intersects with the geometry will be included.</p> </li> <li> <p>count: Count of the pixels.</p> </li> <li>sum: Sum of the pixel values.</li> <li>mean: Arithmetic mean.</li> <li>median: Median.</li> <li>mode: Mode.</li> <li>stddev: Standard deviation.</li> <li>variance: Variance.</li> <li>min: Minimum value of the zone.</li> <li>max: Maximum value of the zone.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect when <code>lenient</code> parameter is set to <code>false</code>.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p><code>lenient</code> parameter is set to <code>true</code> by default. The function will return <code>null</code> if the <code>raster</code> and <code>zone</code> geometry do not intersect.</p> <p>Format:</p> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, allTouched: Boolean, excludeNodata: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, allTouched: Boolean, excludeNodata: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, allTouched: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>RS_ZonalStatsAll(rast1, geom1, 1, true, false)\n</code></pre> <p>Output:</p> <pre><code>{184792.0, 1.0690406E7, 57.851021689230684, 0.0, 0.0, 92.13277429243035, 8488.448098819916, 0.0, 255.0}\n</code></pre> <p>SQL Example</p> <pre><code>RS_ZonalStatsAll(rast2, geom2, 1, false, true)\n</code></pre> <p>Output:</p> <pre><code>{14184.0, 3213526.0, 226.55992667794473, 255.0, 255.0, 74.87605357255357, 5606.423398599913, 1.0, 255.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-predicates","title":"Raster Predicates","text":""},{"location":"api/sql/Raster-operators/#rs_contains","title":"RS_Contains","text":"<p>Introduction: Returns true if the geometry or raster on the left side contains the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format:</p> <p><code>RS_Contains(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Contains(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Contains(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((5 5, 5 10, 10 10, 10 5, 5 5))')) rast_geom,\n    RS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 2, 22, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_intersects","title":"RS_Intersects","text":"<p>Introduction: Returns true if raster or geometry on the left side intersects with the raster or geometry on the right side. The convex hull of the raster is considered in the test.</p> <p>Rules for testing spatial relationship:</p> <ul> <li>If the raster or geometry does not have a defined SRID, it is assumed to be in WGS84.</li> <li>If both sides are in the same CRS, then perform the relationship test directly.</li> <li>Otherwise, both sides will be transformed to WGS84 before the relationship test.</li> </ul> <p>Format:</p> <p><code>RS_Intersects(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Intersects(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Intersects(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_SetSRID(ST_PolygonFromEnvelope(0, 0, 10, 10), 4326)) rast_geom,\n    RS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 1, 11, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_within","title":"RS_Within","text":"<p>Introduction: Returns true if the geometry or raster on the left side is within the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format: <code>RS_Within(raster: Raster, geom: Geometry)</code></p> <p>Format: <code>RS_Within(geom: Geometry, raster: Raster)</code></p> <p>Format: <code>RS_Within(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((0 0, 0 50, 100 50, 100 0, 0 0))')) rast_geom,\n    RS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 30, 30, 2, 22, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-based-operators","title":"Raster Based Operators","text":""},{"location":"api/sql/Raster-operators/#rs_addband","title":"RS_AddBand","text":"<p>Introduction: Adds a new band to a raster <code>toRaster</code> at a specified index <code>toRasterIndex</code>. The new band's values are copied from <code>fromRaster</code> at a specified band index <code>fromBand</code>. If no <code>toRasterIndex</code> is provided, the new band is appended to the end of <code>toRaster</code>. If no <code>fromBand</code> is specified, band <code>1</code> from <code>fromRaster</code> is copied by default.</p> <p>Note</p> <p>IllegalArgumentException will be thrown in these cases:</p> <ul> <li>The provided Rasters, <code>toRaster</code> &amp; <code>fromRaster</code> don't have same shape.</li> <li>The provided <code>fromBand</code> is not in <code>fromRaster</code>.</li> <li>The provided <code>toRasterIndex</code> is not in or at end of <code>toRaster</code>.</li> </ul> <p>Format:</p> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1, toRasterIndex: Integer = at_end)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 2, 1) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_clip","title":"RS_Clip","text":"<p>Introduction: Returns a raster that is clipped by the given geometry.</p> <p>If <code>crop</code> is not specified then it will default to <code>true</code>, meaning it will make the resulting raster shrink to the geometry's extent and if <code>noDataValue</code> is not specified then the resulting raster will have the minimum possible value for the band pixel data type.</p> <p>The <code>allTouched</code> parameter (Since <code>v1.7.1</code>) determines how pixels are selected:</p> <ul> <li>When true, any pixel touched by the geometry will be included.</li> <li>When false (default), only pixels whose centroid intersects with the geometry will be included.</li> </ul> <p>Note</p> <ul> <li>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</li> <li>Since <code>v1.7.0</code>, <code>RS_Clip</code> function will return <code>null</code> if the <code>raster</code> and <code>geometry</code> geometry do not intersect. If you want to throw an exception in this case, you can set the <code>lenient</code> parameter to <code>false</code>.</li> </ul> <p>Format:</p> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, allTouched: Boolean, noDataValue: Double, crop: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, allTouched: Boolean, noDataValue: Double, crop: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, allTouched: Boolean, noDataValue: Double)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, allTouched: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Original Raster:</p> <p></p> <p>SQL Example</p> <pre><code>SELECT RS_Clip(\n        RS_FromGeoTiff(content), 1,\n        ST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n        false, 200, true\n    )\n</code></pre> <p>Output:</p> <p></p> <p>SQL Example</p> <pre><code>SELECT RS_Clip(\n        RS_FromGeoTiff(content), 1,\n        ST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n        false, 200, false\n    )\n</code></pre> <p>Output:</p> <p></p>"},{"location":"api/sql/Raster-operators/#rs_interpolate","title":"RS_Interpolate","text":"<p>Introduction: This function performs interpolation on a raster using the Inverse Distance Weighted (IDW) method. This method estimates cell values by averaging the values of sample data points in the vicinity of each processing cell. The influence of a sample point on the interpolated value is inversely proportional to the distance from the cell being estimated, with nearer points having more influence or weight in the averaging process.</p> <p>This technique is effective in scenarios where continuity of spatial data is important, and it is essential to estimate values for locations that do not have direct measurements, often represented by NaN or noDataValue in raster data.</p> <p>Note</p> <p>This method assumes that the spatial influence of a variable diminishes with distance. In geospatial analysis, this means features or phenomena closer to a point of interest are given more weight than those further away. For example, in environmental data analysis, measurements from nearby locations have a greater impact on interpolated values than distant ones, reflecting the natural gradation and spatial continuity.</p> <p>Formats:</p> <pre><code>RS_Interpolate(raster: Raster)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double, maxRadiusOrMinPoints: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double, maxRadiusOrMinPoints: Double, band: Integer)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>Parameters:</p> <ul> <li><code>raster</code>: The raster to be interpolated.</li> <li><code>band</code>: The band of the raster to be used for interpolation. If <code>band</code> is not provided, interpolation is performed across all bands.</li> <li><code>power</code>: A positive real number defining the exponent of distance in the IDW calculation. This parameter controls the influence of distant points on the interpolated values, default being set to 2.</li> <li><code>mode</code>: Specifies the interpolation mode - either <code>\"Variable\"</code> or <code>\"Fixed\"</code>.</li> <li>In <code>\"Variable\"</code> mode:<ul> <li><code>numPointsOrRadius</code>: Specifies the number of nearest input points to be used for interpolation. Defaults to 12 if not provided.</li> <li><code>maxRadiusOrMinPoints</code>: Sets the maximum search radius, with the default being the diagonal length of the raster.</li> </ul> </li> <li>In <code>\"Fixed\"</code> mode:<ul> <li><code>numPointsOrRadius</code>: Defines the radius within which input sample points are considered. Defaults to the diagonal length of the raster if not specified.</li> <li><code>maxRadiusOrMinPoints</code>: Represents the minimum number of points required within the radius. Defaults to 0 if not provided.</li> </ul> </li> </ul> <p>SQL Example:</p> <pre><code>SELECT RS_Interpolate(raster, 1, 2.0, 'Variable', 12, 1000)\n</code></pre> <p>Output (Shown as heatmap):</p> <p> </p>"},{"location":"api/sql/Raster-operators/#rs_metadata","title":"RS_MetaData","text":"<p>Introduction: Returns the metadata of the raster as a struct. The struct has the following schema:</p> <ul> <li>upperLeftX: upper left x coordinate of the raster, in terms of CRS units</li> <li>upperLeftX: upper left y coordinate of the raster, in terms of CRS units</li> <li>gridWidth: width of the raster, in terms of pixels</li> <li>gridHeight: height of the raster, in terms of pixels</li> <li>scaleX: ScaleX: the scaling factor in the x direction</li> <li>scaleY: ScaleY: the scaling factor in the y direction</li> <li>skewX: skew in x direction (rotation x)</li> <li>skewY: skew in y direction (rotation y)</li> <li>srid: srid of the raster</li> <li>numSampleDimensions: number of bands</li> <li>tileWidth: (Since <code>v1.6.1</code>) width of tiles in the raster</li> <li>tileHeight: (Since <code>v1.6.1</code>) height of tiles in the raster</li> </ul> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p><code>tileWidth</code> and <code>tileHeight</code> are available since <code>v1.6.1</code>, they are the dimensions of the tiles in the raster. For example, rasters written by <code>RS_FromGeoTiff</code> uses the tiling scheme of the loaded GeoTIFF file. For rasters that has only 1 tile, <code>tileWidth</code> and <code>tileHeight</code> will be equal to <code>gridWidth</code> and <code>gridHeight</code> respectively.</p> <p>Format: <code>RS_MetaData (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_MetaData(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>{-1.3095817809482181E7, 4021262.7487925636, 512, 517, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857, 1, 256, 256}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_normalizeall","title":"RS_NormalizeAll","text":"<p>Introduction: Normalizes values in all bands of a raster between a given normalization range. The function maintains the data type of the raster values by ensuring that the normalized values are cast back to the original data type of each band in the raster. By default, the values are normalized to range [0, 255]. RS_NormalizeAll can take upto 7 of the following arguments.</p> <ul> <li><code>raster</code>: The raster to be normalized.</li> <li><code>minLim</code> and <code>maxLim</code> (Optional): The lower and upper limits of the normalization range. By default, normalization range is set to [0, 255].</li> <li><code>normalizeAcrossBands</code> (Optional): A boolean flag to determine the normalization method. If set to true (default), normalization is performed across all bands based on global min and max values. If false, each band is normalized individually based on its own min and max values.</li> <li><code>noDataValue</code> (Optional): Defines the value to be used for missing or invalid data in raster bands. By default, noDataValue is set to <code>maxLim</code> and Safety mode is triggered.</li> <li><code>minValue</code> and <code>maxValue</code> (Optional): Optionally, specific minimum and maximum values of the input raster can be provided. If not provided, these values are computed from the raster data.</li> </ul> <p>A Safety mode is triggered when <code>noDataValue</code> is not given. This sets <code>noDataValue</code> to <code>maxLim</code> and normalizes valid data values to the range [minLim, maxLim-1]. This is to avoid replacing valid data that might coincide with the new <code>noDataValue</code>.</p> <p>Warning</p> <p>Using a noDataValue that falls within the normalization range can lead to loss of valid data. If any data value within a raster band matches the specified noDataValue, it will be replaced and cannot be distinguished or recovered later. Exercise caution in selecting a noDataValue to avoid unintentional data alteration.</p> <p>Formats:</p> <pre><code>RS_NormalizeAll (raster: Raster)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean, noDataValue: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, noDataValue: Double, minValue: Double, maxValue: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean, noDataValue: Double, minValue: Double, maxValue: Double )\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NormalizeAll(raster, 0, 1)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_numbands","title":"RS_NumBands","text":"<p>Introduction: Returns the number of the bands in the raster.</p> <p>Format: <code>RS_NumBands (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NumBands(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_reprojectmatch","title":"RS_ReprojectMatch","text":"<p>Introduction: Reproject a raster to match the geo-reference, CRS, and envelope of a reference raster. The output raster always have the same extent and resolution as the reference raster. For pixels not covered by the input raster, nodata value is assigned, or 0 is assigned if the input raster does not have nodata value.</p> <p>The default resampling algorithm is <code>NearestNeighbor</code>. The following resampling algorithms are supported (case-insensitive):</p> <ol> <li>NearestNeighbor</li> <li>Bilinear</li> <li>Bicubic</li> </ol> <p>This function serves the same purpose as the <code>RasterArray.reproject_match</code> function in rioxarray.</p> <p>Format:</p> <p><code>RS_ReprojectMatch (raster: Raster, reference: Raster, algorithm: String)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>WITH t AS (\n    SELECT RS_MapAlgebra(RS_MakeEmptyRaster(1, 500, 500, 308736,4091167, 1000, -1000, 0, 0, 32611), 'D', 'out = sin(x() * 0.2);') rast1,\n        RS_MapAlgebra(RS_MakeEmptyRaster(1, 500, 500, 16536,4185970, 1000, -1000, 0, 0, 32612), 'D', 'out = sin(y() * 0.2);') rast2\n) SELECT t.rast1, t.rast2, RS_ReprojectMatch(rast1, rast2) rast12, RS_ReprojectMatch(rast2, rast1) rast21 FROM t\n</code></pre> <p>Output:</p> <p></p>"},{"location":"api/sql/Raster-operators/#rs_resample","title":"RS_Resample","text":"<p>Introduction: Resamples a raster using a given resampling algorithm and new dimensions (width and height), a new grid corner to pivot the raster at (gridX and gridY) and a set of georeferencing attributes (scaleX and scaleY).</p> <p>RS_Resample also provides an option to pass a reference raster to draw the georeferencing attributes out of. However, the SRIDs of the input and reference raster must be same, otherwise RS_Resample throws an IllegalArgumentException.</p> <p>For the purpose of resampling, width-height pair and scaleX-scaleY pair are mutually exclusive, meaning any one of them can be used at a time.</p> <p>The <code>useScale</code> parameter controls whether to use width-height or scaleX-scaleY. If <code>useScale</code> is false, the provided <code>widthOrScale</code> and <code>heightOrScale</code> values will be floored to integers and considered as width and height respectively (floating point width and height are not allowed). Otherwise, they are considered as scaleX and scaleY respectively.</p> <p>Currently, RS_Resample does not support skewed rasters, and hence even if a skewed reference raster is provided, its skew values are ignored. If the input raster is skewed, the output raster geometry and interpolation may be incorrect.</p> <p>The default algorithm used for resampling is <code>NearestNeighbor</code>, and hence if a null, empty or invalid value of algorithm is provided, RS_Resample defaults to using <code>NearestNeighbor</code>. However, the algorithm parameter is non-optional.</p> <p>Following are valid values for the algorithm parameter (Case-insensitive):</p> <ol> <li>NearestNeighbor</li> <li>Bilinear</li> <li>Bicubic</li> </ol> <p>Tip</p> <p>If you just want to resize or rescale an input raster, you can use RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format:</p> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, gridX: Double, gridY: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, referenceRaster: Raster, useScale: Boolean, algorithm: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>WITH INPUT_RASTER AS (\n SELECT RS_AddBandFromArray(\n    RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\n    ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nRESAMPLED_RASTER AS (\n SELECT RS_Resample(rast, 6, 5, 1, -1, false, null) as resample_rast from INPUT_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n\n(-0.33333333333333326,0.19999999999999996,6,5,1.388888888888889,-1.24,0,0,0,1)\n</code></pre> <p>SQL Example</p> <pre><code> WITH INPUT_RASTER AS (\n   SELECT RS_AddBandFromArray(\n    RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\n    ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n   ),\n   RESAMPLED_RASTER AS (\n    SELECT RS_Resample(rast, 1.2, -1.4, true, null) as resample_rast from INPUT_RASTER\n   )\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>|       NaN         NaN         NaN         NaN         NaN         NaN         NaN|\n|       NaN    3.050000    3.650000    4.250000    5.160000    6.690000    7.200000|\n|       NaN    5.150000    5.750000    6.350000    7.250000    8.750000    9.250000|\n|       NaN    7.250000    7.850000    8.450000    9.070000    9.730000    9.950000|\n|       NaN    7.400000    8.000000    8.600000    9.200000    9.800000   10.000000|\n\n(0.0, 0.0, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre> <p>SQL Example</p> <pre><code>WITH INPUT_RASTER AS (\n    SELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0), ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nREF_RASTER AS (\n    SELECT RS_MakeEmptyRaster(2, 'd', 6, 5, 1, -1, 1.2, -1.4, 0, 0, 0) as ref_rast\n),\nRESAMPLED_RASTER AS (\n    SELECT RS_Resample(rast, ref_rast, true, null) as resample_rast from INPUT_RASTER, REF_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n\n(-0.20000000298023224, 0.4000000059604645, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setbandnodatavalue","title":"RS_SetBandNoDataValue","text":"<p>Introduction: This sets the no data value for a specified band in the raster. If the band index is not provided, band 1 is assumed by default. Passing a <code>null</code> value for <code>noDataValue</code> will remove the no data value and that will ensure all pixels are included in functions rather than excluded as no data.</p> <p>Since <code>v1.5.1</code>, this function supports the ability to replace the current no-data value with the new <code>noDataValue</code>.</p> <p>Note</p> <p>When <code>replace</code> is true, any pixels matching the provided <code>noDataValue</code> will be considered as no-data in the output raster.</p> <p>An <code>IllegalArgumentException</code> will be thrown if the input raster does not already have a no-data value defined. Replacing existing values with <code>noDataValue</code> requires a defined no-data baseline to evaluate against.</p> <p>To use this for no-data replacement, the input raster must first set its no-data value, which can then be selectively replaced via this function.</p> <p>Format:</p> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer, noDataValue: Double, replace: Boolean)\n</code></pre> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer = 1, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(\n        RS_SetBandNoDataValue(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            -999\n            )\n        )\n</code></pre> <p>Output:</p> <pre><code>-999\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setgeoreference","title":"RS_SetGeoReference","text":"<p>Introduction: Sets the Georeference information of an object in a single call. Accepts inputs in <code>GDAL</code> and <code>ESRI</code> format. Default format is <code>GDAL</code>. If all 6 parameters are not provided then will return null.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format:</p> <pre><code>RS_SetGeoReference(raster: Raster, geoRefCoord: String, format: String = \"GDAL\")\n</code></pre> <pre><code>RS_SetGeoReference(raster: Raster, upperLeftX: Double, upperLeftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX SkewY SkewX ScaleY UpperLeftX UpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX SkewY SkewX ScaleY (UpperLeftX + ScaleX * 0.5) (UpperLeftY + ScaleY * 0.5)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            '3 1.5 1.5 2 22 3'\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n22.000000\n3.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            '3 1.5 1.5 2 22 3', 'ESRI'\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n20.500000\n2.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n            8, -3, 4, 5, 0.2, 0.2\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>4.000000\n0.200000\n0.200000\n5.000000\n8.000000\n-3.000000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setpixeltype","title":"RS_SetPixelType","text":"<p>Introduction: Returns a modified raster with the desired pixel data type.</p> <p>The <code>dataType</code> parameter accepts one of the following strings.</p> <ul> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits unsigned Byte</li> </ul> <p>Note</p> <p>If the specified <code>dataType</code> is narrower than the original data type, the function will truncate the pixel values to fit the new data type range.</p> <p>Format:</p> <pre><code>RS_SetPixelType(raster: Raster, dataType: String)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>RS_SetPixelType(raster, \"I\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalue","title":"RS_SetValue","text":"<p>Introduction: Returns a raster by replacing the value of pixel specified by <code>colX</code> and <code>rowY</code>.</p> <p>Format:</p> <pre><code>RS_SetValue(raster: Raster, bandIndex: Integer = 1, colX: Integer, rowY: Integer, newValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n               RS_SetValue(\n                       RS_AddBandFromArray(\n                               RS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n                           [1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0], 1, 0d\n                           ),\n                       1, 2, 2, 255\n                   )\n           )\n</code></pre> <p>Output:</p> <pre><code>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 255.0, 2.0, 3.0, 3.0, 5.0, 6.0, 7.0, 0.0, 0.0, 3.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalues","title":"RS_SetValues","text":"<p>Introduction: Returns a raster by replacing the values of pixels in a specified rectangular region. The top left corner of the region is defined by the <code>colX</code> and <code>rowY</code> coordinates. The <code>width</code> and <code>height</code> parameters specify the dimensions of the rectangular region. The new values to be assigned to the pixels in this region can be specified as an array passed to this function.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format without ROI <code>geom</code>:</p> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, colX: Integer, rowY: Integer, width: Integer, height: Integer, newValues: ARRAY[Double], keepNoData: Boolean = false)\n</code></pre> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, colX: Integer, rowY: Integer, width: Integer, height: Integer, newValues: ARRAY[Double])\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>The <code>colX</code>, <code>rowY</code>, and <code>bandIndex</code> are 1-indexed. If <code>keepNoData</code> is <code>true</code>, the pixels with NoData value will not be set to the corresponding value in <code>newValues</code>. The <code>newValues</code> should be provided in rows.</p> <p>The geometry variant of this function accepts all types of Geometries, and it sets the <code>newValue</code> in the specified region under the <code>geom</code>.</p> <p>The <code>allTouched</code> parameter (Since <code>v1.7.1</code>) determines how pixels are selected:</p> <ul> <li>When true, any pixel touched by the geometry will be included.</li> <li>When false (default), only pixels whose centroid intersects with the geometry will be included.</li> </ul> <p>Note</p> <p>If the shape of <code>newValues</code> doesn't match with provided <code>width</code> and <code>height</code>, <code>IllegalArgumentException</code> is thrown.</p> <p>Note</p> <p>If the mentioned <code>bandIndex</code> doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Format with ROI <code>geom</code>:</p> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, geom: Geometry, newValue: Double, allTouched: Boolean = false, keepNoData: Boolean = false)\n</code></pre> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, geom: Geometry, newValue: Double, allTouched: Boolean = false)\n</code></pre> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, geom: Geometry, newValue: Double)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n        RS_SetValues(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n                Array(1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0), 1, 0d\n                ),\n            1, 2, 2, 3, 3, [11,12,13,14,15,16,17,18,19]\n            )\n        )\n</code></pre> <p>Output:</p> <pre><code>Array(1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 11.0, 12.0, 13.0, 3.0, 5.0, 14.0, 15.0, 16.0, 0.0, 3.0, 17.0, 18.0, 19.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n        RS_SetValues(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(1, 5, 5, 1, -1, 1, -1, 0, 0, 0),\n                Array(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), 1\n                ),\n            1, ST_GeomFromWKT('POLYGON((1 -1, 3 -3, 6 -6, 4 -1, 1 -1))'), 255, false, false\n            )\n           )\n</code></pre> <p>Output:</p> <pre><code>Array(255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 0.0, 0.0, 0.0, 0.0, 255.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setsrid","title":"RS_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SetSRID (raster: Raster, srid: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SetSRID(raster, 4326)\nFROM raster_table\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_srid","title":"RS_SRID","text":"<p>Introduction: Returns the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SRID (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SRID(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>3857\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_union","title":"RS_Union","text":"<p>Introduction: Returns a combined multi-band raster from 2 or more input Rasters. The order of bands in the resultant raster will be in the order of the input rasters. For example if <code>RS_Union</code> is called on two 2-banded raster, raster1 and raster2, the first 2 bands of the resultant 4-banded raster will be from raster1 and the last 2 from raster 2.</p> <p>Note</p> <p>If the provided input Rasters don't have same shape an IllegalArgumentException will be thrown.</p> <p>Format:</p> <pre><code>RS_Union (raster1: Raster, raster2: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster, raster6: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster, raster6: Raster, raster7: Raster)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Union(raster1, raster2, raster3, raster4) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_value","title":"RS_Value","text":"<p>Introduction: Returns the value at the given point in the raster. If no band number is specified it defaults to 1.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>point</code> geometry differs from that of the <code>raster</code>, then <code>point</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>point</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Value (raster: Raster, point: Geometry)</code></p> <p><code>RS_Value (raster: Raster, point: Geometry, band: Integer)</code></p> <p><code>RS_Value (raster: Raster, colX: Integer, colY: Integer, band: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Examples:</p> <ul> <li>For Point Geometry:</li> </ul> <pre><code>SELECT RS_Value(raster, ST_Point(-13077301.685, 4002565.802)) FROM raster_table\n</code></pre> <ul> <li>For Grid Coordinates:</li> </ul> <pre><code>SELECT RS_Value(raster, 3, 4, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_values","title":"RS_Values","text":"<p>Introduction: Returns the values at the given points or grid coordinates in the raster. If no band number is specified it defaults to 1.</p> <p>RS_Values is similar to RS_Value but operates on an array of points or grid coordinates. RS_Values can be significantly faster since a raster only has to be loaded once for several points.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>points</code> geometries differs from that of the <code>raster</code>, then <code>points</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>points</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry])</code></p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry], band: Integer)</code></p> <pre><code>RS_Values (raster: Raster, xCoordinates: ARRAY[Integer], yCoordinates: ARRAY[Integer], band: Integer)\n</code></pre> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <ul> <li>For Array of Point geometries:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(ST_Point(-1307.5, 400.8), ST_Point(-1403.3, 399.1)))\nFROM raster_table\n</code></pre> <ul> <li>For Arrays of grid coordinates:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(4, 5), Array(3, 2), 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>Array(5.0, 3.0)\n</code></pre> <p>Spark SQL example for joining a point dataset with a raster dataset:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\n  .withColumn(\"raster\", expr(\"RS_FromGeoTiff(content)\"))\n  .withColumn(\"envelope\", expr(\"RS_Envelope(raster)\"))\n\n// Join the points with the raster extent and aggregate points to arrays.\n// We only use the path and envelope of the raster to keep the shuffle as small as possible.\nval df = pointDf.join(rasterDf.select(\"path\", \"envelope\"), expr(\"ST_Within(point_geom, envelope)\"))\n  .groupBy(\"path\")\n  .agg(collect_list(\"point_geom\").alias(\"point\"), collect_list(\"point_id\").alias(\"id\"))\n\ndf.join(rasterDf, \"path\")\n  .selectExpr(\"explode(arrays_zip(id, point, RS_Values(raster, point))) as result\")\n  .selectExpr(\"result.*\")\n  .show()\n</code></pre> <p>Output:</p> <pre><code>+----+------------+-------+\n| id | point      | value |\n+----+------------+-------+\n|  4 | POINT(1 1) |   3.0 |\n|  5 | POINT(2 2) |   7.0 |\n+----+------------+-------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-tiles","title":"Raster Tiles","text":""},{"location":"api/sql/Raster-operators/#rs_tile","title":"RS_Tile","text":"<p>Introduction: Returns an array of rasters resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_Tile(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_Tile(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.1</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_Tile(rast, 2, 2) AS tiles FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|               tiles|\n+--------------------+\n|[GridCoverage2D[\"...|\n+--------------------+\n</code></pre> <p>User can use <code>EXPLODE</code> function to expand the array of tiles into a table of tiles.</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT EXPLODE(RS_Tile(rast, 2, 2)) AS tile FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                tile|\n+--------------------+\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_tileexplode","title":"RS_TileExplode","text":"<p>Introduction: Generates records containing raster tiles resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_TileExplode(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndex: Int, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.0</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. <code>bandIndex</code> can be specified if there is only one selected band, which is equivalent to specifying <code>bandIndices</code> as <code>ARRAY(bandIndex)</code>.Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>The returned records have the following schema:</p> <ul> <li><code>x</code>: The index of the tile along X axis (0-based).</li> <li><code>y</code>: The index of the tile along Y axis (0-based).</li> <li><code>tile</code>: The tile.</li> </ul> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_TileExplode(rast, 2, 2) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+---+---+--------------------+\n|  x|  y|                tile|\n+---+---+--------------------+\n|  0|  0|GridCoverage2D[\"g...|\n|  1|  0|GridCoverage2D[\"g...|\n|  2|  0|GridCoverage2D[\"g...|\n|  0|  1|GridCoverage2D[\"g...|\n|  1|  1|GridCoverage2D[\"g...|\n|  2|  1|GridCoverage2D[\"g...|\n|  0|  2|GridCoverage2D[\"g...|\n|  1|  2|GridCoverage2D[\"g...|\n|  2|  2|GridCoverage2D[\"g...|\n+---+---+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-to-map-algebra-operators","title":"Raster to Map Algebra Operators","text":"<p>To bridge the gap between the raster and map algebra worlds, the following operators are provided. These operators convert a raster to a map algebra object. The map algebra object can then be used with the map algebra operators described in the next section.</p>"},{"location":"api/sql/Raster-operators/#rs_bandasarray","title":"RS_BandAsArray","text":"<p>Introduction: Extract a band from a raster as an array of doubles.</p> <p>Format: <code>RS_BandAsArray (raster: Raster, bandIndex: Integer)</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>BandIndex is 1-based and must be between 1 and RS_NumBands(raster). It returns null if the bandIndex is out of range or the raster is null.</p> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(raster, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                band|\n+--------------------+\n|[0.0, 0.0, 0.0, 0...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_addbandfromarray","title":"RS_AddBandFromArray","text":"<p>Introduction: Add a band to a raster from an array of doubles.</p> <p>Format:</p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double])</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer)</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer, noDataValue: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>The bandIndex is 1-based and must be between 1 and RS_NumBands(raster) + 1. It throws an exception if the bandIndex is out of range or the raster is null. If not specified, the noDataValue of the band is assumed to be null.</p> <p>When the bandIndex is RS_NumBands(raster) + 1, it appends the band to the end of the raster. Otherwise, it replaces the existing band at the bandIndex.</p> <p>If the bandIndex and noDataValue is not given, a convenience implementation adds a new band with a null noDataValue.</p> <p>Adding a new band with a custom noDataValue requires bandIndex = RS_NumBands(raster) + 1 and non-null noDataValue to be explicitly specified.</p> <p>Modifying or Adding a customNoDataValue is also possible by giving an existing band in RS_AddBandFromArray</p> <p>In order to remove an existing noDataValue from an existing band, pass null as the noDataValue in the RS_AddBandFromArray.</p> <p>Note that: <code>bandIndex == RS_NumBands(raster) + 1</code> is an experimental feature and might lead to the loss of raster metadata and properties such as color models.</p> <p>Note</p> <p>RS_AddBandFromArray typecasts the double band values to the given datatype of the raster. This can lead to overflow values if values beyond the range of the raster's datatype are provided.</p> <p>SQL Example</p> <pre><code>SELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2)) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1, -999) AS raster FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mapalgebra","title":"RS_MapAlgebra","text":"<p>Introduction: Apply a map algebra script on a raster.</p> <p>Format:</p> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String)\n</code></pre> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p><code>RS_MapAlgebra</code> runs a script on a raster. The script is written in a map algebra language called Jiffle. The script takes a raster as input and returns a raster of the same size as output. The script can be used to apply a map algebra expression on a raster. The input raster is named <code>rast</code> in the Jiffle script, and the output raster is named <code>out</code>.</p> <p>SQL Example</p> <p>Calculate the NDVI of a raster with 4 bands (R, G, B, NIR):</p> <pre><code>-- Assume that the input raster has 4 bands: R, G, B, NIR\n-- rast[0] refers to the R band, rast[3] refers to the NIR band.\nSELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') AS ndvi FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p>For more details and examples about <code>RS_MapAlgebra</code>, please refer to the Map Algebra documentation. To learn how to write map algebra script, please refer to Jiffle language summary.</p>"},{"location":"api/sql/Raster-operators/#map-algebra-operators","title":"Map Algebra Operators","text":"<p>Map algebra operators work on a single band of a raster. Each band is represented as an array of doubles. The operators return an array of doubles.</p>"},{"location":"api/sql/Raster-operators/#rs_add","title":"RS_Add","text":"<p>Introduction: Add two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Add (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val sumDF = spark.sql(\"select RS_Add(band1, band2) as sumOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_array","title":"RS_Array","text":"<p>Introduction: Create an array that is filled by the given value</p> <p>Format: <code>RS_Array(length: Integer, value: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Array(height * width, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseand","title":"RS_BitwiseAND","text":"<p>Introduction: Find Bitwise AND between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseAND (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val biwiseandDF = spark.sql(\"select RS_BitwiseAND(band1, band2) as andvalue from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseor","title":"RS_BitwiseOR","text":"<p>Introduction: Find Bitwise OR between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseOR (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val biwiseorDF = spark.sql(\"select RS_BitwiseOR(band1, band2) as or from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_countvalue","title":"RS_CountValue","text":"<p>Introduction: Returns count of a particular value from a spectral band in a raster image</p> <p>Format: <code>RS_CountValue (Band1: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val countDF = spark.sql(\"select RS_CountValue(band1, target) as count from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_divide","title":"RS_Divide","text":"<p>Introduction: Divide band1 with band2 from a geotiff image</p> <p>Format: <code>RS_Divide (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Divide(band1, band2) as divideBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_fetchregion","title":"RS_FetchRegion","text":"<p>Introduction: Fetch a subset of region from given Geotiff image based on minimumX, minimumY, maximumX and maximumY index as well original height and width of image</p> <p>Format:</p> <pre><code>RS_FetchRegion (Band: ARRAY[Double], coordinates: ARRAY[Integer], dimensions: ARRAY[Integer])\n</code></pre> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val region = spark.sql(\"select RS_FetchRegion(Band,Array(0, 0, 1, 2),Array(3, 3)) as Region from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthan","title":"RS_GreaterThan","text":"<p>Introduction: Mask all the values with 1 which are greater than a particular target value</p> <p>Format: <code>RS_GreaterThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val greaterDF = spark.sql(\"select RS_GreaterThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthanequal","title":"RS_GreaterThanEqual","text":"<p>Introduction: Mask all the values with 1 which are greater than equal to a particular target value</p> <p>Format: <code>RS_GreaterThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val greaterEqualDF = spark.sql(\"select RS_GreaterThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthan","title":"RS_LessThan","text":"<p>Introduction: Mask all the values with 1 which are less than a particular target value</p> <p>Format: <code>RS_LessThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val lessDF = spark.sql(\"select RS_LessThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthanequal","title":"RS_LessThanEqual","text":"<p>Introduction: Mask all the values with 1 which are less than equal to a particular target value</p> <p>Format: <code>RS_LessThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val lessEqualDF = spark.sql(\"select RS_LessThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicaldifference","title":"RS_LogicalDifference","text":"<p>Introduction: Return value from band 1 if a value in band1 and band2 are different, else return 0</p> <p>Format: <code>RS_LogicalDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val logicalDifference = spark.sql(\"select RS_LogicalDifference(band1, band2) as logdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicalover","title":"RS_LogicalOver","text":"<p>Introduction: Return value from band1 if it's not equal to 0, else return band2 value</p> <p>Format: <code>RS_LogicalOver (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val logicalOver = spark.sql(\"select RS_LogicalOver(band1, band2) as logover from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mean","title":"RS_Mean","text":"<p>Introduction: Returns Mean value for a spectral band in a Geotiff image</p> <p>Format: <code>RS_Mean (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val meanDF = spark.sql(\"select RS_Mean(band) as mean from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mode","title":"RS_Mode","text":"<p>Introduction: Returns Mode from a spectral band in a Geotiff image in form of an array</p> <p>Format: <code>RS_Mode (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val modeDF = spark.sql(\"select RS_Mode(band) as mode from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_modulo","title":"RS_Modulo","text":"<p>Introduction: Find modulo of pixels with respect to a particular value</p> <p>Format: <code>RS_Modulo (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val moduloDF = spark.sql(\"select RS_Modulo(band, target) as modulo from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiply","title":"RS_Multiply","text":"<p>Introduction: Multiply two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Multiply (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Multiply(band1, band2) as multiplyBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiplyfactor","title":"RS_MultiplyFactor","text":"<p>Introduction: Multiply a factor to a spectral band in a geotiff image</p> <p>Format: <code>RS_MultiplyFactor (Band1: ARRAY[Double], Factor: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyFactorDF = spark.sql(\"select RS_MultiplyFactor(band1, 2) as multiplyfactor from dataframe\")\n</code></pre> <p>This function only accepts integer as factor before <code>v1.5.0</code>.</p>"},{"location":"api/sql/Raster-operators/#rs_normalize","title":"RS_Normalize","text":"<p>Introduction: Normalize the value in the array to [0, 255]. Uniform arrays are set to 0 after normalization.</p> <p>Format: <code>RS_Normalize (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Normalize(band)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_normalizeddifference","title":"RS_NormalizedDifference","text":"<p>Introduction: Returns Normalized Difference between two bands(band2 and band1) in a Geotiff image(example: NDVI, NDBI)</p> <p>Format: <code>RS_NormalizedDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val normalizedDF = spark.sql(\"select RS_NormalizedDifference(band1, band2) as normdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_squareroot","title":"RS_SquareRoot","text":"<p>Introduction: Find Square root of band values in a geotiff image</p> <p>Format: <code>RS_SquareRoot (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val rootDF = spark.sql(\"select RS_SquareRoot(band) as squareroot from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_subtract","title":"RS_Subtract","text":"<p>Introduction: Subtract two spectral bands in a Geotiff image(band2 - band1)</p> <p>Format: <code>RS_Subtract (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val subtractDF = spark.sql(\"select RS_Subtract(band1, band2) as differenceOfOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-visualizer/","title":"Raster visualization","text":"<p>Sedona offers some APIs to aid in easy visualization of a raster object.</p>"},{"location":"api/sql/Raster-visualizer/#image-based-visualization","title":"Image-based visualization","text":"<p>Sedona offers APIs to visualize a raster in an image form. This API only works for rasters with byte data, and bands &lt;= 4 (Grayscale - RGBA). You can check the data type of an existing raster by using RS_BandPixelType or create your own raster by passing 'B' while using RS_MakeEmptyRaster.</p>"},{"location":"api/sql/Raster-visualizer/#rs_asbase64","title":"RS_AsBase64","text":"<p>Introduction: Returns a base64 encoded string of the given raster. If the datatype is integral then this function internally takes the first 4 bands as RGBA, and converts them to the PNG format, finally produces a base64 string. When the datatype is not integral, the function converts the raster to TIFF format, and then generates a base64 string. To visualize other bands, please use it together with <code>RS_Band</code>. You can take the resulting base64 string in an online viewer to check how the image looks like.</p> <p>Warning</p> <p>This is not recommended for large files.</p> <p>Format:</p> <p><code>RS_AsBase64(raster: Raster, maxWidth: Integer)</code></p> <p><code>RS_AsBase64(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsBase64(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>iVBORw0KGgoAAAA...\n</code></pre>"},{"location":"api/sql/Raster-visualizer/#rs_asimage","title":"RS_AsImage","text":"<p>Introduction: Returns a HTML that when rendered using an HTML viewer or via a Jupyter Notebook, displays the raster as a square image of side length <code>imageWidth</code>. Optionally, an imageWidth parameter can be passed to RS_AsImage in order to increase the size of the rendered image (default: 200).</p> <p>Format: <code>RS_AsImage(raster: Raster, imageWidth: Integer = 200)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsImage(raster, 500) from rasters\nSELECT RS_AsImage(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>\"&lt;img src=\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAECAAAAABjWKqcAAAAIElEQVR42mPgPfGfkYUhhfcBNw+DT1KihS6DqLKztjcATWMFp9rkkJgAAAAASUVORK5CYII=\\\" width=\\\"200\\\" /&gt;\";\n</code></pre> <p>Tip</p> <p>RS_AsImage can be paired with SedonaUtils.display_image(df) wrapper inside a Jupyter notebook to directly print the raster as an image in the output, where the 'df' parameter is the dataframe containing the HTML data provided by RS_AsImage</p> <p>Example:</p> <pre><code>from sedona.spark import SedonaUtils\n\n# Or from sedona.spark import *\n\ndf = (\n    sedona.read.format(\"binaryFile\")\n    .load(DATA_DIR + \"raster.tiff\")\n    .selectExpr(\"RS_FromGeoTiff(content) as raster\")\n)\nhtmlDF = df.selectExpr(\"RS_AsImage(raster, 500) as raster_image\")\nSedonaUtils.display_image(htmlDF)\n</code></pre> <p></p>"},{"location":"api/sql/Raster-visualizer/#text-based-visualization","title":"Text-based visualization","text":""},{"location":"api/sql/Raster-visualizer/#rs_asmatrix","title":"RS_AsMatrix","text":"<p>Introduction: Returns a string, that when printed, outputs the raster band as a pretty printed 2D matrix. All the values of the raster are cast to double for the string. RS_AsMatrix allows specifying the number of digits to be considered after the decimal point. RS_AsMatrix expects a raster, and optionally a band (default: 1) and postDecimalPrecision (default: 6). The band parameter is 1-indexed.</p> <p>Note</p> <p>If the provided band is not present in the raster, RS_AsMatrix throws an IllegalArgumentException</p> <p>Note</p> <p>If the provided raster has integral values, postDecimalPrecision (if any) is simply ignored and integers are printed in the resultant string</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format:</p> <pre><code>RS_AsMatrix(raster: Raster, band: Integer = 1, postDecimalPrecision: Integer = 6)\n</code></pre> <p>Since: <code>1.5.0</code></p> <p>SQL Example</p> <pre><code>val inputDf = Seq(Seq(1, 3.333333, 4, 0.0001, 2.2222, 9, 10, 11.11111111, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output:</p> <pre><code>| 1.00000   3.33333   4.00000   0.00010|\n| 2.22220   9.00000  10.00000  11.11111|\n| 3.00000   4.00000   5.00000   6.00000|\n</code></pre> <p>SQL Example</p> <pre><code>val inputDf = Seq(Seq(1, 3, 4, 0, 2, 9, 10, 11, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'i', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output:</p> <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre>"},{"location":"api/sql/Raster-writer/","title":"Raster writer","text":"<p>Note</p> <p>Sedona writers are available in Scala. Java and Python have the same APIs.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-raster-files","title":"Write Raster DataFrame to raster files","text":"<p>To write a Sedona Raster DataFrame to raster files, you need to (1) first convert the Raster DataFrame to a binary DataFrame using <code>RS_AsXXX</code> functions and (2) then write the binary DataFrame to raster files using Sedona's built-in <code>raster</code> data source.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-a-binary-dataframe","title":"Write raster DataFrame to a binary DataFrame","text":"<p>You can use the following RS output functions (<code>RS_AsXXX</code>) to convert a Raster DataFrame to a binary DataFrame. Generally the output format of a raster can be different from the original input format. For example, you can use <code>RS_FromGeoTiff</code> to create rasters and save them using <code>RS_AsArcInfoAsciiGrid</code>.</p>"},{"location":"api/sql/Raster-writer/#rs_asarcgrid","title":"RS_AsArcGrid","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is an ArcGrid image in binary format. ArcGrid only takes 1 source band. If your raster has multiple bands, you need to specify which band you want to use as the source.</p> <p>Possible values for <code>sourceBand</code>: any non-negative value (&gt;=0). If not given, it will use Band 0.</p> <p>Format:</p> <p><code>RS_AsArcGrid(raster: Raster)</code></p> <p><code>RS_AsArcGrid(raster: Raster, sourceBand: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsArcGrid(raster) FROM my_raster_table\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsArcGrid(raster, 1) FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             arcgrid|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n |-- arcgrid: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_asgeotiff","title":"RS_AsGeoTiff","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is a GeoTiff image in binary format.</p> <p>Possible values for <code>compressionType</code>: <code>None</code>, <code>PackBits</code>, <code>Deflate</code>, <code>Huffman</code>, <code>LZW</code> and <code>JPEG</code></p> <p>Possible values for <code>imageQuality</code>: any decimal number between 0 and 1. 0 means the lowest quality and 1 means the highest quality.</p> <p>Format:</p> <p><code>RS_AsGeoTiff(raster: Raster)</code></p> <p><code>RS_AsGeoTiff(raster: Raster, compressionType: String, imageQuality: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsGeoTiff(raster) FROM my_raster_table\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsGeoTiff(raster, 'LZW', '0.75') FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             geotiff|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n |-- geotiff: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_aspng","title":"RS_AsPNG","text":"<p>Introduction: Returns a PNG byte array, that can be written to raster files as PNGs using the sedona function. This function can only accept pixel data type of unsigned integer. PNG can accept 1 or 3 bands of data from the raster, refer to RS_Band for more details.</p> <p>Note</p> <p>Raster having <code>UNSIGNED_8BITS</code> pixel data type will have range of <code>0 - 255</code>, whereas rasters having <code>UNSIGNED_16BITS</code> pixel data type will have range of <code>0 - 65535</code>. If provided pixel value is greater than either <code>255</code> for <code>UNSIGNED_8BITS</code> or <code>65535</code> for <code>UNSIGNED_16BITS</code>, then the extra bit will be truncated.</p> <p>Note</p> <p>Raster that have float or double values will result in an empty byte array. PNG only accepts Integer values, if you want to write your raster to an image file, please refer to RS_AsGeoTiff.</p> <p>Format:</p> <p><code>RS_AsPNG(raster: Raster, maxWidth: Integer)</code></p> <p><code>RS_AsPNG(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsPNG(raster) FROM Rasters\n</code></pre> <p>Output:</p> <pre><code>[-119, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13, 73...]\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsPNG(RS_Band(raster, Array(3, 1, 2)))\n</code></pre> <p>Output:</p> <pre><code>[-103, 78, 94, -26, 61, -16, -91, -103, -65, -116...]\n</code></pre>"},{"location":"api/sql/Raster-writer/#write-a-binary-dataframe-to-raster-files","title":"Write a binary DataFrame to raster files","text":"<p>Introduction: You can write a Sedona binary DataFrame to external storage using Sedona's built-in <code>raster</code> data source. Note that: <code>raster</code> data source does not support reading rasters. Please use Spark built-in <code>binaryFile</code> and Sedona RS constructors together to read rasters.</p> <p>Since: <code>v1.4.1</code></p> <p>Available options:</p> <ul> <li>rasterField:<ul> <li>Default value: the <code>binary</code> type column in the DataFrame. If the input DataFrame has several binary columns, please specify which column you want to use. You can use one of the <code>RS_As*</code> functions mentioned above to convert the raster objects to binary raster file content to write.</li> <li>Allowed values: the name of the to-be-saved binary type column</li> </ul> </li> <li>fileExtension<ul> <li>Default value: <code>.tiff</code></li> <li>Allowed values: any string values such as <code>.png</code>, <code>.jpeg</code>, <code>.asc</code></li> </ul> </li> <li>pathField<ul> <li>No default value. If you use this option, then the column specified in this option must exist in the DataFrame schema. If this option is not used, each produced raster image will have a random UUID file name.</li> <li>Allowed values: any column name that indicates the paths of each raster file</li> </ul> </li> <li>useDirectCommitter (Since: <code>v1.6.1</code>)<ul> <li>Default value: <code>true</code>. If set to <code>true</code>, the output files will be written directly to the target location. If set to <code>false</code>, the output files will be written to a temporary location and finally be committed to their target location. It is usually slower to write large amount of raster files with <code>useDirectCommitter</code> set to <code>false</code>, especially when writing to object stores such as S3.</li> <li>Allowed values: <code>true</code> or <code>false</code></li> </ul> </li> </ul> <p>The schema of the Raster dataframe to be written can be one of the following two schemas:</p> <pre><code>root\n |-- raster_binary: binary (nullable = true)\n</code></pre> <p>or</p> <pre><code>root\n |-- raster_binary: binary (nullable = true)\n |-- path: string (nullable = true)\n</code></pre> <p>Spark SQL example 1:</p> <pre><code>// Assume that df contains a raster column named \"rast\"\ndf.withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(rast)\"))\\\n  .write.format(\"raster\").mode(\"overwrite\").save(\"my_raster_file\")\n</code></pre> <p>Spark SQL example 2:</p> <pre><code>// Assume that df contains a raster column named \"rast\" and a string column named \"path\"\ndf.withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(rast)\"))\\\n  .write.format(\"raster\")\\\n  .option(\"rasterField\", \"raster_binary\")\\\n  .option(\"pathField\", \"path\")\\\n  .option(\"fileExtension\", \".tiff\")\\\n  .mode(\"overwrite\")\\\n  .save(\"my_raster_file\")\n</code></pre> <p>The produced file structure will look like this:</p> <pre><code>my_raster_file\n- part-00000-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test1.tiff\n    - .test1.tiff.crc\n- part-00001-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test2.tiff\n    - .test2.tiff.crc\n- part-00002-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test3.tiff\n    - .test3.tiff.crc\n- _SUCCESS\n</code></pre> <p>To read it back to Sedona Raster DataFrame, you can use the following command (note the <code>*</code> in the path):</p> <pre><code>sparkSession.read.format(\"binaryFile\").load(\"my_raster_file/*\")\n</code></pre> <p>Then you can create Raster type in Sedona like this <code>RS_FromGeoTiff(content)</code> (if the written data was in GeoTiff format).</p> <p>The newly created DataFrame can be written to disk again but must be under a different name such as <code>my_raster_file_modified</code></p>"},{"location":"api/sql/Raster-writer/#write-geometry-to-raster-dataframe","title":"Write Geometry to Raster dataframe","text":""},{"location":"api/sql/Raster-writer/#rs_asraster","title":"RS_AsRaster","text":"<p>Introduction: <code>RS_AsRaster</code> converts a vector geometry into a raster dataset by assigning a specified value to all pixels covered by the geometry. Unlike <code>RS_Clip</code>, which extracts a subset of an existing raster while preserving its original values, <code>RS_AsRaster</code> generates a new raster where the geometry is rasterized onto a raster grid. The function supports all geometry types and takes the following parameters:</p> <ul> <li><code>geom</code>: The geometry to be rasterized.</li> <li><code>raster</code>: The reference raster to be used for overlaying the <code>geom</code> on.</li> <li><code>pixelType</code>: Defines data type of the output raster. This can be one of the following, D (double), F (float), I (integer), S (short), US (unsigned short) or B (byte).</li> <li><code>allTouched</code> (Since: <code>v1.7.1</code>): Decides the pixel selection criteria. If set to <code>true</code>, the function selects all pixels touched by the geometry, else, selects only pixels who's centroids intersect the geometry. Defaults to <code>false</code>.</li> <li><code>Value</code>: The value to be used for assigning pixels covered by the geometry. Defaults to using <code>1.0</code> for cell <code>value</code> if not provided.</li> <li><code>noDataValue</code>: Used for assigning the no data value of the resultant raster. Defaults to <code>null</code> if not provided.</li> <li><code>useGeometryExtent</code>: Defines the extent of the resultant raster. When set to <code>true</code>, it corresponds to the extent of <code>geom</code>, and when set to false, it corresponds to the extent of <code>raster</code>. Default value is <code>true</code> if not set.</li> </ul> <p>Format:</p> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, allTouched: Boolean, value: Double, noDataValue: Double, useGeometryExtent: Boolean)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, allTouched: Boolean, value: Double, noDataValue: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, allTouched: Boolean, value: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, allTouched: Boolean)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Note</p> <p>The function doesn't support rasters that have any one of the following properties: <pre><code>ScaleX &lt; 0\nScaleY &gt; 0\nSkewX != 0\nSkewY != 0\n</code></pre> If a raster is provided with anyone of these properties then IllegalArgumentException is thrown.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>SQL Example</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n        'D', false, 255.0, 0d\n    )\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n        'D'\n    )\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, 215, 2, -2, 0, 0, 0),\n       'D', true, 255, 0d, false\n)\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Reading-legacy-parquet/","title":"Reading Legacy Parquet Files","text":"<p>Due to a breaking change in Apache Sedona 1.4.0 to the SQL type of <code>GeometryUDT</code> (SEDONA-205) as well as the serialization format of geometry values (SEDONA-207), Parquet files containing geometry columns written by Apache Sedona 1.3.1 or earlier cannot be read by Apache Sedona 1.4.0 or later.</p> <p>For parquet files written by <code>\"parquet\"</code> format when using Apache Sedona 1.3.1-incubating or earlier:</p> <pre><code>df.write.format(\"parquet\").save(\"path/to/parquet/files\")\n</code></pre> <p>Reading such files with Apache Sedona 1.4.0 or later using <code>spark.read.format(\"parquet\").load(\"path/to/parquet/files\")</code> will result in an exception:</p> <pre><code>24/01/08 12:52:56 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 11)\norg.apache.spark.sql.AnalysisException: Invalid Spark read type: expected required group geom (LIST) {\n  repeated group list {\n    required int32 element (INTEGER(8,true));\n  }\n} to be list type but found Some(BinaryType)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:745)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertGroupField$3(ParquetSchemaConverter.scala:343)\n    at scala.Option.fold(Option.scala:251)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertGroupField(ParquetSchemaConverter.scala:324)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:188)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3$adapted(ParquetSchemaConverter.scala:117)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n    at scala.collection.immutable.Range.foreach(Range.scala:158)\n    at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n    ...\n</code></pre> <p>Since v1.5.1, GeoParquet supports reading legacy Parquet files. you can use <code>\"geoparquet\"</code> format with the <code>.option(\"legacyMode\", \"true\")</code> option. Here is an example:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre>"},{"location":"api/sql/Spider/","title":"Spider:Spatial Data Generator","text":"<p>Sedona offers a spatial data generator called Spider. It is a data source that generates random spatial data based on the user-specified parameters.</p>"},{"location":"api/sql/Spider/#quick-start","title":"Quick Start","text":"<p>Once you have your <code>SedonaContext</code> object created, you can create a DataFrame with the <code>spider</code> data source.</p> <pre><code>df_random_points = sedona.read.format(\"spider\").load(n=1000, distribution=\"uniform\")\ndf_random_boxes = sedona.read.format(\"spider\").load(\n    n=1000, distribution=\"gaussian\", geometryType=\"box\", maxWidth=0.05, maxHeight=0.05\n)\ndf_random_polygons = sedona.read.format(\"spider\").load(\n    n=1000,\n    distribution=\"bit\",\n    geometryType=\"polygon\",\n    minSegment=3,\n    maxSegment=5,\n    maxSize=0.1,\n)\n</code></pre> <p>Now we have three DataFrames with random spatial data. We can show the first three rows of the <code>df_random_points</code> DataFrame to verify the data is generated correctly.</p> <pre><code>df_random_points.show(3, False)\n</code></pre> <p>Output:</p> <pre><code>+---+---------------------------------------------+\n|id |geometry                                     |\n+---+---------------------------------------------+\n|1  |POINT (0.8781393502074886 0.5925787985028703)|\n|2  |POINT (0.3159498147172185 0.1907316577342276)|\n|3  |POINT (0.2618294441170143 0.3623164670133922)|\n+---+---------------------------------------------+\nonly showing top 3 rows\n</code></pre> <p>The generated DataFrame has two columns: <code>id</code> and <code>geometry</code>. The <code>id</code> column is the unique identifier of each record, and the <code>geometry</code> column is the randomly generated spatial data.</p> <p>We can plot all 3 DataFrames using the following code.</p> <pre><code>import matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Convert DataFrames to GeoDataFrames\ngdf_random_points = gpd.GeoDataFrame(df_random_points.toPandas(), geometry=\"geometry\")\ngdf_random_boxes = gpd.GeoDataFrame(df_random_boxes.toPandas(), geometry=\"geometry\")\ngdf_random_polygons = gpd.GeoDataFrame(\n    df_random_polygons.toPandas(), geometry=\"geometry\"\n)\n\n# Create a figure and a set of subplots\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot each GeoDataFrame on a different subplot\ngdf_random_points.plot(ax=axes[0], color=\"blue\", markersize=5)\naxes[0].set_title(\"Random Points\")\n\ngdf_random_boxes.boundary.plot(ax=axes[1], color=\"red\")\naxes[1].set_title(\"Random Boxes\")\n\ngdf_random_polygons.boundary.plot(ax=axes[2], color=\"green\")\naxes[2].set_title(\"Random Polygons\")\n\n# Adjust the layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>You can browse the SpiderWeb website to play with the parameters and see how they affect the generated data. Once you are satisfied with the parameters, you can use them in your Spider DataFrame creation code. The following sections will explain the parameters in detail.</p>"},{"location":"api/sql/Spider/#common-parameters","title":"Common Parameters","text":"<p>The following parameters are common to all distributions.</p> Parameter Description Default Value n Number of records to generate 100 distribution Distribution type. See Distributions for details. <code>uniform</code> numPartitions Number of partitions to generate The default parallelism of your Spark Context seed Random seed Current timestamp in milliseconds <p>Warning</p> <p>The same <code>seed</code> parameter may produce different results with different Java versions or Sedona versions.</p>"},{"location":"api/sql/Spider/#distributions","title":"Distributions","text":"<p>Spider supports generating random points, boxes and polygons under various distributions. You can explore the capabilities of Spider by visiting the SpiderWeb website. You can specify the distribution type using the <code>distribution</code> parameter. The parameters for each distribution are listed below.</p>"},{"location":"api/sql/Spider/#uniform-distribution","title":"Uniform Distribution","text":"<p>The uniform distribution generates random geometries in the unit square <code>[0, 1] x [0, 1]</code>. This distribution can be selected by setting the <code>distribution</code> parameter to <code>uniform</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=300, distribution=\"uniform\", geometryType=\"box\", maxWidth=0.05, maxHeight=0.05\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").boundary.plot()\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>The Gaussian distribution generates random geometries in a Gaussian distribution with mean <code>[0.5, 0.5]</code> and standard deviation <code>[0.1, 0.1]</code>. This distribution can be selected by setting the <code>distribution</code> parameter to <code>gaussian</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=300, distribution=\"gaussian\", geometryType=\"polygon\", maxSize=0.05\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").boundary.plot()\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#bit-distribution","title":"Bit Distribution","text":"<p>The bit distribution generates random geometries in a bit distribution. This distribution can be selected by setting the <code>distribution</code> parameter to <code>bit</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> probability Probability of setting a bit 0.2 digits Number of digits in the generated data 10 maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=300, distribution=\"bit\", geometryType=\"point\", probability=0.2, digits=10\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#diagonal-distribution","title":"Diagonal Distribution","text":"<p>The diagonal distribution generates random geometries on the diagonal line <code>y = x</code> with some dispersion for geometries that are not exactly on the diagonal. This distribution can be selected by setting the <code>distribution</code> parameter to <code>diagonal</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> percentage The percentage of records that are perfectly on the diagonal 0.5 buffer For points not exactly on the diagonal, the buffer in which they are dispersed 0.5 maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=300, distribution=\"diagonal\", geometryType=\"point\", percentage=0.5, buffer=0.5\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#sierpinski-distribution","title":"Sierpinski Distribution","text":"<p>The Sierpinski distribution generates random geometries distributed on a Sierpinski triangle. This distribution can be selected by setting the <code>distribution</code> parameter to <code>sierpinski</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=2000, distribution=\"sierpinski\", geometryType=\"point\"\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#parcel-distribution","title":"Parcel Distribution","text":"<p>This generator produces boxes that resemble parcel areas. It works by recursively splitting the input domain (unit square) along the longest dimension and then randomly dithering each generated box to add some randomness. This generator can only generate boxes. This distribution can be selected by setting the <code>distribution</code> parameter to <code>parcel</code>.</p> Parameter Description Default Value dither The amount of dithering as a ratio of the side length. Allowed range [0, 1] 0.5 splitRange The allowed range for splitting boxes. Allowed range [0.0, 0.5] 0.0 means all values are allowed. 0.5 means always split in half. 0.5 <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf = sedona.read.format(\"spider\").load(\n    n=300, distribution=\"parcel\", dither=0.5, splitRange=0.5\n)\ngpd.GeoDataFrame(df.toPandas(), geometry=\"geometry\").boundary.plot()\n</code></pre> <p></p> <p>Note</p> <p>The number of partitions generated by the <code>parcel</code> distribution is always power of 4. This is for guaranteeing the quality of the generated data. If the specified <code>numPartitions</code> is not a power of 4, it will be automatically adjusted to the nearest power of 4 smaller or equal to the specified value.</p>"},{"location":"api/sql/Spider/#affine-transformation","title":"Affine Transformation","text":"<p>The random spatial data generated by Spider are mostly in the unit square <code>[0, 1] x [0, 1]</code>. If you need to generate random spatial data in a different region, you can specify affine transformation parameters to scale and translate the data to the target region.</p> <p>The following code demonstrates how to generate random spatial data in a different region using affine transformation.</p> <p>The affine transformation parameters are:</p> Parameter Description Default Value translateX Translate the data horizontally 0 translateY Translate the data vertically 0 scaleX Scale the data horizontally 1 scaleY Scale the data vertically 1 skewX Skew the data horizontally 0 skewY Skew the data vertically 0 <p>The affine transformation is applied to the generated data as follows:</p> <pre><code>x' = translateX + scaleX * x + skewX * y\ny' = translateY + skewY * x + scaleY * y\n</code></pre> <p>Example:</p> <pre><code>import geopandas as gpd\n\ndf_random_points = sedona.read.format(\"spider\").load(\n    n=1000, distribution=\"uniform\", translateX=0.5, translateY=0.5, scaleX=2, scaleY=2\n)\ngpd.GeoDataFrame(df_random_points.toPandas(), geometry=\"geometry\").plot(markersize=1)\n</code></pre> <p>The data is now in the region <code>[0.5, 2.5] x [0.5, 2.5]</code>.</p> <p></p>"},{"location":"api/sql/Spider/#references","title":"References","text":"<ul> <li>Puloma Katiyar, Tin Vu, Sara Migliorini, Alberto Belussi, Ahmed Eldawy. \"SpiderWeb: A Spatial Data Generator on the Web\", ACM SIGSPATIAL 2020, Seattle, WA</li> <li>Beast Spatial Data Generator: https://bitbucket.org/bdlabucr/beast/src/master/doc/spatial-data-generator.md</li> <li>SpiderWeb: A Spatial Data Generator on the Web: https://spider.cs.ucr.edu/</li> <li>SpiderWeb YouTube Video: https://www.youtube.com/watch?v=h0xCG6Swdqw</li> </ul>"},{"location":"api/sql/Visualization-SedonaKepler/","title":"SedonaKepler","text":"<p>SedonaKepler offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaKepler, simply import Sedona using:</p> <pre><code>from sedona.spark import *\n</code></pre> <p>Alternatively it can also be imported using:</p> <pre><code>from sedona.spark import SedonaKepler\n</code></pre> <p>Following are details on all the APIs exposed via SedonaKepler:</p>"},{"location":"api/sql/Visualization-SedonaKepler/#creating-a-map-object-using-sedonakeplercreate_map","title":"Creating a map object using SedonaKepler.create_map","text":"<p>SedonaKepler exposes a create_map API with the following signature:</p> <pre><code>def create_map(\n    df: SedonaDataFrame = None, name: str = \"unnamed\", config: dict = None\n) -&gt; map: ...\n</code></pre> <p>The parameter 'name' is used to associate the passed SedonaDataFrame in the map object and any config applied to the map is linked to this name. It is recommended you pass a unique identifier to the dataframe here.</p> <p>If no SedonaDataFrame object is passed, an empty map (with config applied if passed) is returned. A SedonaDataFrame can be added later using the method <code>add_df</code></p> <p>A map config can be passed optionally to apply pre-apply customizations to the map.</p> <p>Note</p> <p>The map config references every customization with the name assigned to the SedonaDataFrame being displayed, if there is a mismatch in the name, the config will not be applied to the map object.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization-SedonaKepler/#adding-sedonadataframe-to-a-map-object-using-sedonakepleradd_df","title":"Adding SedonaDataFrame to a map object using SedonaKepler.add_df","text":"<p>SedonaKepler exposes an add_df API with the following signature:</p> <pre><code>def add_df(map, df: SedonaDataFrame, name: str = \"unnamed\"): ...\n</code></pre> <p>This API can be used to add a SedonaDataFrame to an already created map object. The map object passed is directly mutated and nothing is returned.</p> <p>The parameters name has the same conditions as 'create_map'</p> <p>Tip</p> <p>This method can be used to add multiple dataframes to a map object to be able to visualize them together.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map()\nSedonaKepler.add_df(map, groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization-SedonaKepler/#setting-a-config-via-the-map","title":"Setting a config via the map","text":"<p>A map rendered by accessing the map object created by SedonaKepler includes a config panel which can be used to customize the map</p>"},{"location":"api/sql/Visualization-SedonaKepler/#saving-and-setting-config","title":"Saving and setting config","text":"<p>A map object's current config can be accessed by accessing its 'config' attribute like <code>map.config</code>. This config can be saved for future use or use across notebooks if the exact same map is to be rendered every time.</p> <p>Note</p> <p>The map config references each applied customization with the name given to the dataframe and hence will work only on maps with the same name of dataframe supplied. For more details refer to keplerGl documentation here</p>"},{"location":"api/sql/Visualization-SedonaPyDeck/","title":"SedonaPyDeck","text":"<p>SedonaPyDeck offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaPyDeck, simply import Sedona using:</p> <pre><code>from sedona.spark import *\n</code></pre> <p>Alternatively it can also be imported using:</p> <pre><code>from sedona.spark import SedonaPyDeck\n</code></pre> <p>Note</p> <p>For more information on the optional parameters please visit PyDeck docs.</p> <p>SedonaPyDeck assumes the map provider to be Mapbox when user selects 'salellite' option for <code>map_style</code>.</p> <p>Following are details on all the APIs exposed via SedonaPyDeck:</p>"},{"location":"api/sql/Visualization-SedonaPyDeck/#geometry-map","title":"Geometry Map","text":"<pre><code>def create_geometry_map(\n    df,\n    fill_color=\"[85, 183, 177, 255]\",\n    line_color=\"[85, 183, 177, 255]\",\n    elevation_col=0,\n    initial_view_state=None,\n    map_style=None,\n    map_provider=None,\n    api_keys=None,\n    stroked=True,\n): ...\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color polygons or point geometries in the map</p> <p>The parameter <code>line_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color the line geometries in the map.</p> <p>The parameter <code>elevation_col</code> can be given a static elevation or elevation based on column values like <code>fill_color</code>, this only works for the polygon geometries in the map.</p> <p>The parameter <code>stroked</code> determines whether to draw an outline around polygons and points, accepts a boolean value. For more information, please refer to this documentation of deck.gl.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization-SedonaPyDeck/#choropleth-map","title":"Choropleth Map","text":"<pre><code>def create_choropleth_map(\n    df,\n    fill_color=None,\n    plot_col=None,\n    initial_view_state=None,\n    map_style=None,\n    map_provider=None,\n    api_keys=None,\n    elevation_col=0,\n    stroked=True,\n): ...\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>The parameter <code>stroked</code> determines whether to draw an outline around polygons and points, accepts a boolean value. For more information please refer to this documentation of deck.gl.</p> <p>For example, all these are valid values of fill_color:</p> <pre><code>fill_color = [255, 12, 250]\nfill_color = [0, 12, 250, 255]\nfill_color = (\n    \"[0, 12, 240, AirportCount * 10]\"  ## AirportCount is a column in the passed df\n)\n</code></pre> <p>Instead of giving a <code>fill_color</code> parameter, a 'plot_col' can be passed which specifies the column to decide the choropleth. SedonaPyDeck then creates a default color scheme based on the values of the column passed.</p> <p>The parameter <code>elevation_col</code> can be given a numeric or a string value (containing the column with/without operations on it) to set a 3D elevation to the plotted polygons if any.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website.</p>"},{"location":"api/sql/Visualization-SedonaPyDeck/#scatterplot","title":"Scatterplot","text":"<pre><code>def create_scatterplot_map(\n    df,\n    fill_color=\"[255, 140, 0]\",\n    radius_col=1,\n    radius_min_pixels=1,\n    radius_max_pixels=10,\n    radius_scale=1,\n    initial_view_state=None,\n    map_style=None,\n    map_provider=None,\n    api_keys=None,\n): ...\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>The parameter <code>radius_col</code> can be given a numeric value or a string value consisting of any operations on the column, in order to specify the radius of the plotted point.</p> <p>The parameter <code>radius_min_pixels</code> can be given a numeric value that would set the minimum radius in pixels. This can be used to prevent the plotted circle from getting too small when zoomed out.</p> <p>The parameter <code>radius_max_pixels</code> can be given a numeric value that would set the maximum radius in pixels. This can be used to prevent the circle from getting too big when zoomed in.</p> <p>The parameter <code>radius_scale</code> can be given a numeric value that sets a global radius multiplier for all points.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization-SedonaPyDeck/#heatmap","title":"Heatmap","text":"<pre><code>def create_heatmap(\n    df,\n    color_range=None,\n    weight=1,\n    aggregation=\"SUM\",\n    initial_view_state=None,\n    map_style=None,\n    map_provider=None,\n    api_keys=None,\n): ...\n</code></pre> <p>The parameter <code>color_range</code> can be optionally given a list of RGB values, SedonaPyDeck by default uses <code>6-class YlOrRd</code> as color_range. More examples can be found on colorbrewer</p> <p>The parameter <code>weight</code> can be given a numeric value or a string with column and operations on it to determine weight of each point while plotting a heatmap. By default, SedonaPyDeck assigns a weight of 1 to each point</p> <p>The parameter <code>aggregation</code> can be used to define aggregation strategy to use when aggregating heatmap to a lower resolution (zooming out). One of \"MEAN\" or \"SUM\" can be provided. By default, SedonaPyDeck uses \"MEAN\" as the aggregation strategy.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/sql/geography/Constructor/","title":"Constructor","text":""},{"location":"api/sql/geography/Constructor/#st_geogfromwkb","title":"ST_GeogFromWKB","text":"<p>Introduction: Construct a Geography from WKB Binary.</p> <p>Format:</p> <p><code>ST_GeogFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeogFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 d6 00 c0 00 00 00 00 80 b5 d6 bf 00 00 00 60 e1 ef f7 bf 00 00 00 80 07 5d e5 bf])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1 -0.4, -1.5 -0.7)\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geogfromewkb","title":"ST_GeogFromEWKB","text":"<p>Introduction: Construct a Geography from EWKB Binary. This function is an alias of ST_GeogFromWKB.</p> <p>Format:</p> <p><code>ST_GeogFromEWKB (EWkb: Binary)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_GeogFromEWKB([01 02 00 00 20 E6 10 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF]))\n</code></pre> <p>Output:</p> <pre><code>SRID: 4326; LINESTRING (-2.1 -0.4, -1.5 -0.7)\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geogfromgeohash","title":"ST_GeogFromGeoHash","text":"<p>Introduction: Create Geography from geohash string and optional precision</p> <p>Format:</p> <p><code>ST_GeogFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeogFromGeoHash('9q9j8ue2v71y5zzy0s4q', 16)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-122.3061 37.554162, -122.3061 37.554162, -122.3061 37.554162, -122.3061 37.554162, -122.3061 37.554162))\"\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geogfromwkt","title":"ST_GeogFromWKT","text":"<p>Introduction: Construct a Geography from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_GeogFromWKT (Wkt: String)</code></p> <p><code>ST_GeogFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeogFromWKT('LINESTRING (1 2, 3 4, 5 6)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4, 5 6)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_GeogFromWKT('LINESTRING (1 2, 3 4, 5 6)', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326; LINESTRING (1 2, 3 4, 5 6)\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geogfromewkt","title":"ST_GeogFromEWKT","text":"<p>Introduction: Construct a Geography from OGC Extended WKT.</p> <p>Format:</p> <p><code>ST_GeogFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsEWKT(ST_GeogFromEWKT('SRID=4326; LINESTRING (0 0, 3 3, 4 4)'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326; LINESTRING (0 0, 3 3, 4 4)\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geogtogeometry","title":"ST_GeogToGeometry","text":"<p>Introduction:</p> <p>This function constructs a planar Geometry object from a Geography. While Sedona makes every effort to preserve the original spatial object, the conversion is not always exact because Geography and Geometry have different underlying models:</p> <ul> <li>Geography represents shapes on the Earth\u2019s surface (spherical).</li> <li>Geometry represents shapes on a flat, Euclidean plane.</li> </ul> <p>This difference can cause certain ambiguities during conversion. For example:</p> <ul> <li>A polygon in Geography always refers to the region on the Earth\u2019s surface that the ring encloses. When converted to Geometry, however, it becomes unclear whether the polygon is intended to represent the \u201cinside\u201d or its complement (the \u201coutside\u201d) on the sphere.</li> <li>Long edges that cross the antimeridian or cover poles may also be represented differently once projected into planar space.</li> </ul> <p>In practice, Sedona preserves the coordinates and ring orientation as closely as possible, but you should be aware that some topological properties (e.g., area, distance) may not match exactly after conversion.</p> <p>Sedona does not validate or enforce the SRID of the input Geography. Whatever SRID is attached to the Geography will be carried over to the resulting Geometry, even if it is not appropriate for planar interpretation. It is the user\u2019s responsibility to ensure that the Geography\u2019s SRID is meaningful in the target Geometry context.</p> <p>Format:</p> <p><code>ST_GeogToGeometry (geog: Geography)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeogToGeometry(ST_GeogFromWKT('MULTILINESTRING ((90 90, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))', 4326))\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((90 90, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))\n</code></pre>"},{"location":"api/sql/geography/Constructor/#st_geomtogeography","title":"ST_GeomToGeography","text":"<p>Introduction:</p> <p>This function constructs a Geography object from a planar Geometry. This function is intended for geometries defined in a Geographic Coordinate Reference System (CRS), most commonly WGS84 (EPSG:4326), where coordinates are expressed in degrees and the longitude/latitude order</p> <p>If the input Geometry is defined in a projected CRS (e.g., Web Mercator EPSG:3857, UTM zones), the conversion may succeed syntactically, but the resulting Geography will not be meaningful. This is because Geography interprets coordinates on the surface of a sphere, not a flat plane.</p> <p>Sedona does not validate or enforce the SRID of the input Geometry. Whatever SRID is attached to the Geometry will simply be carried over to the Geography, even if it is inappropriate for spherical interpretation. It is the user\u2019s responsibility to ensure the input Geometry uses a Geographic CRS.</p> <p>Format:</p> <p><code>ST_GeomToGeography (geom: Geometry)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomToGeography(ST_GeomFromWKT('MULTIPOLYGON (((10 10, 70 10, 70 70, 10 70, 10 10), (20 20, 60 20, 60 60, 20 60, 20 20)), ((30 30, 50 30, 50 50, 30 50, 30 30), (36 36, 44 36, 44 44, 36 44, 36 36)))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((10 10, 70 10, 70 70, 10 70, 10 10), (20 20, 60 20, 60 60, 20 60, 20 20)), ((30 30, 50 30, 50 50, 30 50, 30 30), (36 36, 44 36, 44 44, 36 44, 36 36)))\n</code></pre>"},{"location":"api/sql/geography/Function/","title":"Function","text":""},{"location":"api/sql/geography/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: This function returns the bounding box (envelope) of A. It's important to note that the bounding box is calculated using a cylindrical topology, not a spherical one. If the envelope crosses the antimeridian (the 180\u00b0 longitude line), you can set the split parameter to true. This will return a Geography object containing two separate Polygon objects, split along that line.</p> <p>Format:</p> <p><code>ST_Envelope (A: Geography, splitAtAntiMeridian: Boolean)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Envelope(ST_GeogFromWKT('MULTIPOLYGON (((177.285 -18.28799, 180 -18.28799, 180 -16.02088, 177.285 -16.02088, 177.285 -18.28799)), ((-180 -18.28799, -179.7933 -18.28799, -179.7933 -16.02088, -180 -16.02088, -180 -18.28799)))'), false);\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((177.3 -18.3, -179.8 -18.3, -179.8 -16, 177.3 -16, 177.3 -18.3))\n</code></pre>"},{"location":"api/sql/geography/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geography. EWKT is an extended version of WKT which includes the SRID of the geography. The format originated in PostGIS but is supported by many GIS tools.</p> <p>Format: <code>ST_AsEWKT (A: Geography)</code></p> <p>Since: <code>v1.8.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_GeogFromWKT('LINESTRING (1 2, 3 4, 5 6)', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326; LINESTRING (1 2, 3 4, 5 6)\n</code></pre>"},{"location":"api/stats/sql/","title":"DataFrame","text":""},{"location":"api/stats/sql/#overview","title":"Overview","text":"<p>Sedona's stats module provides Scala and Python functions for conducting geospatial statistical analysis on dataframes with spatial columns. The stats module is built on top of the core module and provides a set of functions that can be used to perform spatial analysis on these dataframes. The stats module is designed to be used with the core module and the viz module to provide a complete set of geospatial analysis tools.</p>"},{"location":"api/stats/sql/#using-dbscan","title":"Using DBSCAN","text":"<p>The DBSCAN function is provided at <code>org.apache.sedona.stats.clustering.DBSCAN.dbscan</code> in scala/java and <code>sedona.stats.clustering.dbscan.dbscan</code> in python.</p> <p>The function annotates a dataframe with a cluster label for each data record using the DBSCAN algorithm. The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and none are named 'geometry', the column name must be provided. The new column will be named 'cluster'.</p>"},{"location":"api/stats/sql/#parameters","title":"Parameters","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - dataframe to cluster. Must contain at least one GeometryType column</li> <li>epsilon - minimum distance parameter of DBSCAN algorithm</li> <li>minPts (min_pts) - minimum number of points parameter of DBSCAN algorithm</li> <li>geometry - name of the geometry column</li> <li>includeOutliers (include_outliers) - whether to include outliers in the output. Default is false</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>The output is the input DataFrame with the cluster label added to each row. Outlier will have a cluster value of -1 if included.</p>"},{"location":"api/stats/sql/#using-local-outlier-factor-lof","title":"Using Local Outlier Factor (LOF)","text":"<p>The LOF function is provided at <code>org.apache.sedona.stats.outlierDetection.LocalOutlierFactor.localOutlierFactor</code> in scala/java and <code>sedona.stats.outlier_detection.local_outlier_factor.local_outlier_factor</code> in python.</p> <p>The function annotates a dataframe with a column containing the local outlier factor for each data record. The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and neither is named 'geometry', the column name must be provided.</p>"},{"location":"api/stats/sql/#parameters_1","title":"Parameters","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - dataframe containing the point geometries</li> <li>k - number of nearest neighbors that will be considered for the LOF calculation</li> <li>geometry - name of the geometry column</li> <li>handleTies (handle_ties) - whether to handle ties in the k-distance calculation. Default is false</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>The output is the input DataFrame with the lof added to each row.</p>"},{"location":"api/stats/sql/#using-getis-ord-gi","title":"Using Getis-Ord Gi(*)","text":"<p>The G Local function is provided at <code>org.apache.sedona.stats.hotspotDetection.GetisOrd.gLocal</code> in scala/java and <code>sedona.stats.hotspot_detection.getis_ord.g_local</code> in python.</p> <p>Performs the Gi or Gi* statistic on the x column of the dataframe.</p> <p>Weights should be the neighbors of this row. The members of the weights should be comprised of structs containing a value column and a neighbor column. The neighbor column should be the contents of the neighbors with the same types as the parent row (minus neighbors). Reference the Using the Distance Weighting Function header for instructions on generating this column. To calculate the Gi* statistic, ensure the focal observation is in the neighbors array (i.e. the row is in the weights column) and <code>star=true</code>. Significance is calculated with a z score.</p>"},{"location":"api/stats/sql/#parameters_2","title":"Parameters","text":"<ul> <li>dataframe - the dataframe to perform the G statistic on</li> <li>x - The column name we want to perform hotspot analysis on</li> <li>weights - The column name containing the neighbors array. The neighbor column should be the contents of the neighbors with the same types as the parent row (minus neighbors). You can use <code>Weighting</code> class functions to achieve this.</li> <li>star - Whether the focal observation is in the neighbors array. If true this calculates Gi*, otherwise Gi</li> </ul> <p>The output is the input DataFrame with the following columns added: G, E[G], V[G], Z, P.</p>"},{"location":"api/stats/sql/#using-the-distance-weighting-function","title":"Using the Distance Weighting Function","text":"<p>The Weighting functions are provided at <code>org.apache.sedona.stats.Weighting</code> in scala/java and <code>sedona.stats.weighting</code> in python.</p> <p>The function generates a column containing an array of structs containing a value column and a neighbor column.</p> <p>The generic <code>addDistanceBandColumn</code> (<code>add_distance_band_column</code> in python) function annotates a dataframe with a weights column containing the other records within the threshold and their weight.</p> <p>The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and neither is named 'geometry', the column name must be provided. The new column will be named 'cluster'.</p>"},{"location":"api/stats/sql/#parameters_3","title":"Parameters","text":""},{"location":"api/stats/sql/#adddistancebandcolumn","title":"addDistanceBandColumn","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - DataFrame with geometry column</li> <li>threshold - Distance threshold for considering neighbors</li> <li>binary - whether to use binary weights or inverse distance weights for neighbors (dist^alpha)</li> <li>alpha - alpha to use for inverse distance weights ignored when binary is true</li> <li>includeZeroDistanceNeighbors (include_zero_distance_neighbors) - whether to include neighbors that are 0 distance. If 0 distance neighbors are included and binary is false, values are infinity as per the floating point spec (divide by 0)</li> <li>includeSelf (include_self) - whether to include self in the list of neighbors</li> <li>selfWeight (self_weight) - the value to use for the self weight</li> <li>geometry - name of the geometry column</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul>"},{"location":"api/stats/sql/#addbinarydistancebandcolumn","title":"addBinaryDistanceBandColumn","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - DataFrame with geometry column</li> <li>threshold - Distance threshold for considering neighbors</li> <li>includeZeroDistanceNeighbors (include_zero_distance_neighbors) - whether to include neighbors that are 0 distance. If 0 distance neighbors are included and binary is false, values are infinity as per the floating point spec (divide by 0)</li> <li>includeSelf (include_self) - whether to include self in the list of neighbors</li> <li>selfWeight (self_weight) - the value to use for the self weight</li> <li>geometry - name of the geometry column</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>In both cases the output is the input DataFrame with the weights column added to each row.</p>"},{"location":"api/stats/sql/#moran-i","title":"Moran I","text":"<p>Moran I is the spatial autocorrelation algorithm, which is using spatial location and non-spatial attribute. When the value is close to the 1 it means that there is spatial correlation, when it is close to 0 then the correlation does not exist and data is randomly distributed. When the MoranI autocorrelation value is close to -1 it means that there is negative correlation. Negative correlation means that close values has dissimilar values.</p> <p>You can see spatial correlation values on the figure below</p> <ul> <li>on the left there is negative correlation (-1)</li> <li>in the middle correlation is positive (1)</li> <li>on the right the correlation is close to zero and data is random.</li> </ul> <p></p> <p>Moran statistics can be used as the Scala/Java and Python functions. As the input function requires weight DataFrame. You can create the weight DataFrame using Apache Sedona weighting functions. You need to keep in mind that your input has to have id column that uniquely identifies the feature and value field. The required minimal schema for the MoranI Apache Sedona function is:</p> <pre><code> |-- id: integer (nullable = true)\n |-- value: double (nullable = true)\n |-- weights: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- neighbor: struct (nullable = false)\n |    |    |    |-- id: integer (nullable = true)\n |    |    |    |-- value: double (nullable = true)\n |    |    |-- value: double (nullable = true)\n</code></pre> <p>You can manipulate the value column name and id using function parameters.</p> <p>To use the Apache Sedona weight functions you need to pass the id column and value column to kept parameters.</p> ScalaPython <pre><code>val weights = Weighting.addDistanceBandColumn(\n      positiveCorrelationFrame,\n      1.0,\n      savedAttributes = Seq(\"id\", \"value\")\n)\n\nval moranResult = Moran.getGlobal(weights, idColumn = \"id\")\n\n// result fields\nmoranResult.getPNorm\nmoranResult.getI\nmoranResult.getZNorm\n</code></pre> <pre><code>from sedona.spark.stats.autocorrelation.moran import Moran\nfrom sedona.spark.stats.weighting import add_binary_distance_band_column\n\nresult = add_binary_distance_band_column(df, 1.0, saved_attributes=[\"id\", \"value\"])\n\nmoran_i_result = Moran.get_global(result)\n\n## result fields\nmoran_i_result.p_norm\nmoran_i_result.i\nmoran_i_result.z_norm\n</code></pre> <p>In the result you get the Z norm, P norm and Moran I value.</p> <p>The full signatures of the functions</p> ScalaPython <pre><code>def getGlobal(\n  dataframe: DataFrame,\n  twoTailed: Boolean = true,\n  idColumn: String = ID_COLUMN,\n  valueColumnName: String = VALUE_COLUMN): MoranResult\n\n// java interface\npublic interface MoranResult {\n    public double getI();\n    public double getPNorm();\n    public double getZNorm();\n}\n</code></pre> <pre><code>def get_global(\n    df: DataFrame,\n    two_tailed: bool = True,\n    id_column: str = \"id\",\n    value_column: str = \"value\",\n) -&gt; MoranResult: ...\n\n\n@dataclass\nclass MoranResult:\n    i: float\n    p_norm: float\n    z_norm: float\n</code></pre>"},{"location":"api/viz/java-api/","title":"RDD","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/viz/sql/","title":"DataFrame/SQL","text":""},{"location":"api/viz/sql/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here: Visualize Spatial DataFrame/RDD.</p> <ol> <li>Add Sedona-core, Sedona-SQL, Sedona-Viz into your project pom.xml or build.sbt</li> <li>Declare your Spark Session</li> </ol> <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\").\nmaster(\"local[*]\").appName(\"mySedonaVizDemo\").getOrCreate()\n</code></pre> <ol> <li>Add the following lines after your SparkSession declaration:</li> </ol> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre>"},{"location":"api/viz/sql/#regular-functions","title":"Regular functions","text":""},{"location":"api/viz/sql/#st_colorize","title":"ST_Colorize","text":"<p>Introduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.</p> <p>Note</p> <p>The color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.</p> <p>Format:</p> <pre><code>ST_Colorize (weight: Double, maxWeight: Double, mandatory color: String (Optional))\n</code></pre> <p>Since: <code>v1.0.0</code></p>"},{"location":"api/viz/sql/#produce-various-colors-heat-map","title":"Produce various colors - heat map","text":"<p>This function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.</p> <p>SQL Example</p> <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999) AS color\nFROM pixels\n</code></pre>"},{"location":"api/viz/sql/#produce-uniform-colors-scatter-plot","title":"Produce uniform colors - scatter plot","text":"<p>If a mandatory color name is put as the third input argument, this function will directly output this color, without considering the weights. In this case, every pixel will possess the same color.</p> <p>SQL Example</p> <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999, 'red') AS color\nFROM pixels\n</code></pre> <p>Here are some example color names can be entered:</p> <pre><code>\"firebrick\"\n\"#aa38e0\"\n\"0x40A8CC\"\n\"rgba(112,36,228,0.9)\"\n</code></pre> <p>Please refer to AWT Colors for a list of pre-defined colors.</p>"},{"location":"api/viz/sql/#st_encodeimage","title":"ST_EncodeImage","text":"<p>Introduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.</p> <p>Format: <code>ST_EncodeImage (A: Image)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_EncodeImage(images.img)\nFROM images\n</code></pre>"},{"location":"api/viz/sql/#st_pixelize","title":"ST_Pixelize","text":"<p>Introduction: Convert a geometry to an array of pixels given a resolution</p> <p>You should use it together with <code>Lateral View</code> and <code>Explode</code></p> <p>Format:</p> <pre><code>ST_Pixelize (A: Geometry, ResolutionX: Integer, ResolutionY: Integer, Boundary: Geometry)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Pixelize(shape, 256, 256, (ST_Envelope_Aggr(shape) FROM pointtable))\nFROM polygondf\n</code></pre>"},{"location":"api/viz/sql/#st_tilename","title":"ST_TileName","text":"<p>Introduction: Return the map tile name for a given zoom level. Please refer to OpenStreetMap ZoomLevel and OpenStreetMap tile name.</p> <p>Note</p> <p>Tile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.</p> <p>Format: <code>ST_TileName (A: Pixel, ZoomLevel: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TileName(pixels.px, 3)\nFROM pixels\n</code></pre>"},{"location":"api/viz/sql/#aggregate-functions","title":"Aggregate functions","text":""},{"location":"api/viz/sql/#st_render","title":"ST_Render","text":"<p>Introduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage. The 3<sup>rd</sup> parameter is optional and it is the zoom level. You should use zoom level when you want to render tiles, instead of a single image.</p> <p>Format: <code>ST_Render (A: Pixel, B: Color, C: Integer - optional zoom level)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT tilename, ST_Render(pixels.px, pixels.color) AS tileimg\nFROM pixels\nGROUP BY tilename\n</code></pre>"},{"location":"asf/asf/","title":"Foundation","text":""},{"location":"asf/asf/#copyright","title":"Copyright","text":"<p>Apache Sedona, Sedona, Apache, the Apache feather logo, and the Apache Sedona project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries. All other marks mentioned may be trademarks or registered trademarks of their respective owners. Please visit Apache Software Foundation for more details.</p>"},{"location":"asf/telemetry/","title":"Telemetry","text":""},{"location":"asf/telemetry/#scarf","title":"Scarf","text":"<p>Apache Sedona uses Scarf to collect anonymous usage data to help us understand how the software is being used and how we can improve it. You can opt out of telemetry collection by setting the environment variable <code>SCARF_NO_ANALYTICS</code> or <code>DO_NOT_TRACK</code> to <code>true</code> on your local machine, or the driver machine of your cluster.</p> <p>Scarf fully supports the GDPR and is allowed by the Apache Software Foundation privacy policy. The privacy policy of Scarf is available at https://about.scarf.sh/privacy-policy.</p>"},{"location":"asf/telemetry/#matomo","title":"Matomo","text":"<p>Apache Sedona uses Matomo to collect anonymous usage data to better understand how users use the system, the website, and the docs and where to focus improvements next.</p> <p>Matomo is allowed by the Apache Software Foundation privacy policy.</p>"},{"location":"blog/","title":"Blog","text":"<p>The official source for Apache Sedona news, technical insights, release updates, and best practices in large-scale spatial data management.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/","title":"Should You Use H3 for Geospatial Analytics? A Deep Dive with Apache Spark and Sedona","text":"<p>TL;DR The H3 spatial index provides a number of spatial functions and a consistent grid system for efficient data aggregation and visualization. H3 is an approximation that makes some computations run faster, but less accurately.  Sedona supports H3 spatial index, but it's often preferable to use precise computations, especially when accuracy is important.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#what-is-h3","title":"What is H3?","text":"<p>The H3 Index is a spatial index that converts real world geometries (points, lines, and polygons) into a hexagonal hierarchical index, also known as a discrete global grid. For example:</p> <pre><code># 40.6892524,-74.044552 - Location of the Statue of Liberty\n\nimport h3\n\nresolution = 8\nh3.latlng_to_cell(40.6892524, -74.044552, resolution)\n\"882a1072b5fffff\"\n</code></pre> <p>Each cell is surrounded by exactly 6 other cells and a group of approximately seven cells at one resolution roughly corresponds to a single larger 'parent' cell in the next coarser resolution. This nesting isn't perfect, as child cells can slightly cross parent boundaries. (see below):</p> <p></p> <p>Image source: Indexing h3geo.org</p> <p>Each cell also has a measurable area-based on its position on the earth ranging from cell size 0 to 15.</p> Resolution Average Cell Area Area in km\u00b2 Area in m\u00b2 0 Largest 4,250,546 km\u00b2 4.25 billion m\u00b2 15 Smallest 0.0000009 km\u00b2 0.9 m\u00b2 <p>Once you have this index you can access a number of functions that allow you to do approximate spatial calculations such as (but not limited to):</p> <ul> <li>Inspection: Find cell resolution, string conversion, etc.</li> <li>Traversal: Distances, path between cells, cell ring distance, etc.</li> <li>Hierarchy: Cell parents and children</li> <li>Regions: Converting polygons or other geometries to H3 cells</li> <li>Relationships: Cell grid relationships</li> <li>Conversion: Cells to centroids, coordinates, or geometries</li> </ul>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#why-was-h3-developed","title":"Why was H3 developed?","text":"<p>It was developed at Uber to provide a common layer for data science functions to analyze ridership data and visualize data in the driver application to show pickup hotspots. This provided an efficient way to analyze and visualize this large scale point data without the need to compute geographic relationships: the data could be sent to the application, in this case the driver mobile app, without latitude and longitude data and only an H3 index, which could then be aggregated and visualized to show hotspots efficiently since the H3 cells were already available as a map layer in the mobile app.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#tessellation-and-tiling","title":"Tessellation and tiling","text":"<p>Tessellation, also known as tiling, is how tiles can be arranged to cover a plane, without any gaps.</p> <p>Covering a two-dimensional plane with tiles is straightforward, think of how a checkerboard can be covered with square tiles.</p> <p>Covering a sphere with tiles is more complex than covering a two-dimensional plane. H3 creates its global grid by projecting hexagons onto the 20 faces of an icosahedron, which is then mapped to the Earth's sphere.  This post does not focus on math, so let's now concentrate on the practical use of H3 for geospatial analyses.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#when-h3-is-used-today","title":"When H3 is used today?","text":"<p>Increasingly H3 has been used to speed up common spatial operations like spatial joins. Since the geometry is converted to either a string or integer, tools that optimize joins around non-spatial data types can perform faster spatial operations. Since cells always have the same location and shape on the globe, it also makes visualization more efficient since no geometry data needs to be passed to the application visualizing the data.</p> <p>There are, however, some significant trade-offs when using the H3 index for spatial data in terms of accuracy, data preservation, data transformation and processing, and more.</p> <ul> <li>Lose the original geometry data: If you drop the original geometry data there is no method to convert back to the original geometry data once converted to an H3 cell</li> <li>Complex, long-running polygon conversions: Converting polygons to H3 cells can be computationally complex and long-running. This is generally required for performing spatial joins between points and polygons.</li> <li>Inaccurate polygon coverage: When filling polygons with cells there isn't a perfect fill level, so some parts of the polygon will be left uncovered by the resulting set of cells, and other areas outside the original polygon will have a cell in them (see below).</li> <li>Overlap effects: H3 cells do not have complete coverage between parents and children resulting in a certain level of inaccuracy within H3 cells (see the image above to view the overlapping areas)</li> </ul> <p></p> <p>Image source: StackOverflow</p> <p>Apache Sedona provides spatial functions for both geometry-based operations without the need to leverage H3 cells and functions to create and read H3 indexes. No matter which option you choose you can leverage the functionality of Apache Sedona to perform spatial queries at scale.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#h3-example-with-spark-and-sedona","title":"H3 example with Spark and Sedona","text":"<p>Let's create a DataFrame with start and end columns using the Empire State Building in New York, the Freedom Tower in New York, and the Wills Tower in Chicago.</p> <pre><code>empire_state_building = Point(-73.985428, 40.748817)\nfreedom_tower = Point(-74.013379, 40.712743)\nwillis_tower = Point(-87.635918, 41.878876)\n\ndf = sedona.createDataFrame(\n    [\n        (empire_state_building, freedom_tower),\n        (empire_state_building, willis_tower),\n    ],\n    [\"start\", \"end\"],\n)\n</code></pre> <p>Let's see how to compute the distance sphere between the start and end points and the H3 cell distance.</p> <pre><code>res = (\n    df.withColumn(\"st_distance_sphere\", ST_DistanceSphere(col(\"start\"), col(\"end\")))\n    .withColumn(\"start_cellid\", ST_H3CellIDs(col(\"start\"), 6, True))\n    .withColumn(\"end_cellid\", ST_H3CellIDs(col(\"end\"), 6, True))\n    .withColumn(\n        \"h3_cell_distance\",\n        ST_H3CellDistance(col(\"start_cellid\")[0], col(\"end_cellid\")[0]),\n    )\n)\n</code></pre> <p>There are a few important points to note in this code snippet:</p> <ul> <li>You must provide a resolution when instantiating the <code>ST_H3CellIDs</code>. For example, <code>ST_H3CellIDs(col(\"start\"), 6, True)</code> uses a resolution of <code>6</code>.</li> <li>The <code>ST_H3CellDistance</code> returns an integer representing the number of steps (hexagons) between the two H3 cells on the H3 grid.</li> </ul> <p>Let's take a look at the resulting DataFrame:</p> <pre><code>res.select(\"st_distance_sphere\", \"h3_cell_distance\").show()\n</code></pre> <pre><code>+------------------+----------------+\n|st_distance_sphere|h3_cell_distance|\n+------------------+----------------+\n|4651.5708314048225|               1|\n|1145748.4514602514|             180|\n+------------------+----------------+\n</code></pre> <p>The <code>ST_DistanceSphere</code> function returns the distance between the two points in meters and the ST_H3CellDistance function returns the number of H3 cells between two H3 cells, for a given resolution.</p> <p>Let's now dive into a more realistic example with H3.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#comparing-h3-and-geometries-in-apache-sedona","title":"Comparing H3 and geometries in Apache Sedona","text":"<p>To understand the nuances of using H3 cells for spatial analytics we can compare the performance of H3 cells with geometries using Apache Sedona. Sedona provides four functions for working with H3 cells:</p> <ul> <li><code>ST_H3CellDistance</code>: Measure the cell distance between two cells (not the real world distance)</li> <li><code>ST_H3CellIDs</code>: Creates an array of H3 cell IDs to cover polygons or linestrings</li> <li><code>ST_H3KRing</code>: Produces the \"filled-in disk\" of cells which are at most grid distance k from the origin cell.</li> <li><code>ST_H3ToGeom</code>: Turns an H3 cell into a hexagonal geometry</li> </ul> <p>To demonstrate a real world use case, we will use the Federal Emergency Management Agency (FEMA) National Flood Hazard Layer (NFHL) data from the United States. This represents flood risk zones as defined by FEMA, specifically in Harris County, Texas where Houston is located.</p> <p>As many of the geometries in this data set are particularly complex and have a high level of accuracy as many different decisions, from building permits to flood insurance, are-based off of this data, it provides a good use case for us to explore. We will limit the test to this specific flood zone polygon:</p> <pre><code># Pull the single area to show on the map\n\narea = sedona.sql(\n    \"\"\"\nselect *\nfrom fema\nwhere FLD_AR_ID = '48201C_9129'\n\"\"\"\n)\n</code></pre> <p></p> <p>As Sedona uses the integer-based cell identifier for H3, we will also create a user defined function to turn these into string-based H3 cells. This is because SedonaKepler accepts the strings in place of geometries, one of the main advantages of using H3.</p> <pre><code># Create a UDF to convert the H3 integer into a string to skip geometry creation for mapping in SedonaKepler\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport h3\n\n\n# Define a Python function that wraps h3.int_to_str\ndef h3_int_to_str(x):\n    return h3.int_to_str(x)\n\n\n# Register the function as a Spark UDF\nh3_int_to_str_udf = udf(h3_int_to_str, StringType())\n\nsedona.udf.register(\"h3_int_to_str\", h3_int_to_str_udf)\n</code></pre>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#coverage-overlaps-with-h3-cells","title":"Coverage overlaps with H3 cells","text":"<p>One of the major issues with H3 cells is that they don't evenly overlap polygons when transforming a polygon to H3 cells. There are two methods for doing this:</p> <ul> <li>Cover: Ensure that every part of the polygon is at least covered by a cell</li> <li>Fill: Will add a cell if the centroid of the cell falls within the polygon shape</li> </ul> <p>Here we can test that out for level 8 and the cover method:</p> <pre><code>h3_cells_8 = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(ST_H3CellIDs(geometry, 8, true)) as hex_id\nfrom fema\nwhere FLD_AR_ID = '48201C_9129')\n\nselect h3_int_to_str(hex_id) as hex_id from a\n\"\"\"\n)\n</code></pre> <p></p> <p>As you can see there is a high amount of extra coverage at this level. Let's go down to level 12 to see how that compares for both the fill and the cover methods.</p> <pre><code># Test the same overlap but with H3 Size 12\n\nh3_cells_12_cover = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(ST_H3CellIDs(geometry, 12, true)) as hex_id\nfrom fema\nwhere FLD_AR_ID = '48201C_9129')\nselect h3_int_to_str(hex_id) as hex_id from a\n\"\"\"\n)\n\nh3_cells_12_fill = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(ST_H3CellIDs(geometry, 12, false)) as hex_id\nfrom fema\nwhere FLD_AR_ID = '48201C_9129')\nselect h3_int_to_str(hex_id) as hex_id from a\n\"\"\"\n)\n</code></pre> <p> You can see that this is a little closer to accurate coverage but there are still overlaps even with the fill method and areas that are uncovered.</p> <p>We can also calculate the coverage overlaps. First for the level 8 layer:</p> <pre><code># Find the excess coverage area for H3 coverage level 8\n\nh3_cells_missing_8 = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(ST_H3ToGeom(ST_H3CellIDs(geometry, 8, true))) as h3_geom\nfrom fema\nwhere FLD_AR_ID = '48201C_9129')\n\nselect\nsum((st_area(st_intersection(fema.geometry, a.h3_geom))) / st_area(fema.geometry)) - 1 percent_missing\nfrom a\njoin fema\non st_intersects(h3_geom, geometry)\nwhere FLD_AR_ID = '48201C_9129'\n\"\"\"\n)\n\nh3_cells_missing_8.show()\n</code></pre> <pre><code>+--------------------+\n|     percent_missing|\n+--------------------+\n|2.220446049250313...|\n+--------------------+\n</code></pre> <p>This means that it has an additional 2.2x coverage than the original geometry</p> <p>We can do the same for the level 12 fill layer which is a bit better:</p> <pre><code># Find the percent of missing area with H3 coverage level 12\n\nh3_cells_missing = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(ST_H3ToGeom(ST_H3CellIDs(geometry, 12, false))) as h3_geom\nfrom fema\nwhere FLD_AR_ID = '48201C_9129')\n\nselect\n1 - sum((st_area(st_intersection(fema.geometry, a.h3_geom))) / st_area(fema.geometry)) percent_missing\nfrom a\njoin fema\non st_intersects(h3_geom, geometry)\nwhere FLD_AR_ID = '48201C_9129'\n\"\"\"\n)\n\nh3_cells_missing.show()\n</code></pre> <pre><code>+--------------------+\n|     percent_missing|\n+--------------------+\n|0.021671094911437705|\n+--------------------+\n</code></pre> <p>This means there is a 2.16% missing coverage, or in other words, the H3 cells fail to cover 2.16% of the original polygon. Not as high but we will see how this is significant when performing a spatial join.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#spatial-join-trade-offs-with-h3","title":"Spatial join trade-offs with H3","text":"<p>Now let's compare how a spatial join compares between the original geometry, H3 level 8, and both methods for H3 level 12. We will count the number of buildings that intersect these layers using the Overture Maps Foundation building footprints dataset.</p> <p>First let's look at a baseline joining the original geometry to the buildings:</p> <pre><code># Compare a spatial join at H3 level 8 with Overture Map Buildings\n\ntrue_spatial_join = sedona.sql(\n    \"\"\"\nselect count(overture.id) as buildings\nfrom overture.buildings_building overture\njoin fema\non st_intersects(fema.geometry, overture.geometry)\nwhere fema.FLD_AR_ID = '48201C_9129'\n\"\"\"\n)\n\ntrue_spatial_join.show()\n</code></pre> <pre><code>+---------+\n|buildings|\n+---------+\n|     1412|\n+---------+\n</code></pre> <p>This means that there are 1,412 buildings that touch at least one point on the flood zone polygons.</p> <p>Let's test that with the level 8 H3 cells. Note that you must first transform the cells to geometries, then aggregate them into a single polygon using <code>ST_Union_Aggr</code> to avoid counting buildings more than one time.</p> <pre><code># Compare a spatial join at H3 level 8 with Overture Map Buildings\n\nh3_cells_join_level_8 = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(\nST_H3ToGeom(\nST_H3CellIDs(geometry, 8, true)\n)\n) as h3_geom\nfrom fema\nwhere FLD_AR_ID = '48201C_9129'),\n\nb as (\nselect ST_Union_Aggr(h3_geom) as h3_geom from a\n)\nselect count(overture.id) as buildings\nfrom overture.buildings_building overture\njoin b\non st_intersects(b.h3_geom, overture.geometry)\n\"\"\"\n)\n\nh3_cells_join_level_8.show()\n</code></pre> <pre><code>+---------+\n|buildings|\n+---------+\n|     5402|\n+---------+\n</code></pre> <p>Far more than we would want in a real world analysis. Now let's try with our level 12 H3 cells using the fill methodology which has the least overlap.</p> <pre><code>h3_cells_join_level_12 = sedona.sql(\n    \"\"\"with a as (\nselect\nexplode(\nST_H3ToGeom(\nST_H3CellIDs(geometry, 12, false)\n)\n) as h3_geom\nfrom fema\nwhere FLD_AR_ID = '48201C_9129'),\nb as (\nselect ST_Union_Aggr(h3_geom) as h3_geom from a\n)\nselect count(overture.id) as buildings\nfrom overture.buildings_building overture\njoin b\non st_intersects(b.h3_geom, overture.geometry)\n\"\"\"\n)\n\nh3_cells_join_level_12.show()\n</code></pre> <pre><code># ... text output ...\n+---------+\n|buildings|\n+---------+\n|     1457|\n+---------+\n</code></pre> <p>Closer but still over-counting by 45 buildings compared to the true polygon. With Apache Sedona you don't need to add any extra steps to process data at scale and you don't need to compromise accuracy.</p> <p>Here is a final map of the three compared layers:</p> <p></p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#data-skew-in-distributed-queries","title":"Data skew in distributed queries","text":"<p>When working with large-scale geospatial data, performance is not just about indexing speed or geometry approximation\u2014it's also about how evenly work is distributed across the cluster.</p> <p>H3 partitions space into a uniform hexagonal grid. While simple, this approach breaks down in real-world datasets, which are almost always skewed: dense urban areas may contain millions of features, while rural regions contain very few. Uniform partitioning causes some partitions to become overloaded while others remain underutilized, leading to load imbalance and straggler tasks that slow down the entire job.</p> <p>It's important to note that H3 was never designed to address this problem. In contrast, Apache Sedona uses adaptive spatial partitioners such as the KDB-tree, which split space-based on data density. This ensures partitions are more balanced, minimizing hotspots and improving query performance at scale.</p> <p></p> <p>For a deeper dive, see Jia Yu's GeoSpark (now Sedona) paper, which illustrates how uniform grids fall short compared to adaptive methods in skewed workloads.</p>"},{"location":"blog/2025/09/05/should-you-use-h3-for-geospatial-analytics-a-deep-dive-with-apache-spark-and-sedona/#conclusion","title":"Conclusion","text":"<p>H3 can be useful for speeding up some spatial operations by approximating computations, but it sacrifices precision and creates additional complexity.</p> <p>Apache Sedona is scalable and can run spatial computations exactly, which is easier.</p> <p>H3 is still a great option for certain types of spatial computations, but it's often easier to simply use the precise computations that are built-in to Apache Sedona.</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/","title":"SedonaDB 0.2.0 Release","text":"<p>The Apache Sedona community is excited to announce the release of SedonaDB version 0.2.0!</p> <p>SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona. This release consists of 136 resolved issues including 40 new functions from 17 contributors.</p> <p>Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#release-highlights","title":"Release Highlights","text":"<ul> <li>Improved spatial function coverage</li> <li>GDAL/OGR spatial file format read support</li> <li>GeoParquet 1.1 write support</li> <li>Python user-defined function support</li> <li>Initial Raster data type implementation</li> <li>Release on <code>crates.io</code></li> <li>Build system improvements</li> </ul> <p>For a complete list of changes since SedonaDB 0.1.0 see the milestone for 0.2.0.</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#improved-spatial-function-coverage","title":"Improved spatial function coverage","text":"<p>Since the 0.1.0 release we have been fortunate to work with contributors add 40 new <code>ST_</code> and <code>RS_</code> functions to our growing catalogue. Users of rs_height, rs_scalex, rs_scaley, rs_skewx, rs_skewy, rs_upperleftx, rs_upperlefty, rs_width, st_azimuth, st_boundary, st_crosses, st_dump, st_endpoint, st_geometryfromtext, st_geometryn, st_isclosed, st_iscollection, st_isring, st_issimple, st_isvalid, st_isvalidreason, st_makevalid, st_minimumclearance, st_minimumclearanceline, st_npoints, st_numgeometries, st_overlaps, st_pointn, st_points, st_polygonize, st_polygonize_agg, st_reverse, st_simplify, st_simplifypreservetopology, st_snap, st_startpoint, st_translate, st_unaryunion, and st_zmflag will be pleased to know that these functions are now available in SedonaDB workflows.</p> <p>Thank you to Abeeujah, ayushjariyal, jesspav, joonaspessi, petern48, and yutannihilation for these contributions! (With a special thanks to petern48 for reviewing nearly all of them!)</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#gdalogr-spatial-file-format-read-support","title":"GDAL/OGR spatial file format read support","text":"<p>Whereas SedonaDB 0.1.0 launched with GeoParquet read support and GeoPandas interoperability, support for file formats like GeoPackage, Shapefile, FlatGeoBuf inherited the limitations of GeoPandas (notably, the materialization of an entire layer in memory as a Pandas dataframe). The package powering GeoPandas read support (pyogrio) also exposes the underlying provider's (GDAL/OGR) native Arrow interface, which is the exact format that SedonaDB uses under the hood! This allowed us to add support for dozens of vector formats at once wired directly in to DataFusion's flexible <code>FileFormat</code> API. Users can now read from spatial file formats just as they can from Parquet:</p> <pre><code># pip install \"apache-sedona[db]\"\nimport sedona.db\n\nsd = sedona.db.connect()\nurl = \"https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb\"\nsd.read_pyogrio(url).to_pandas().plot()\n</code></pre> <p>This works for local files, <code>https://</code> urls (via <code>/vsicurl/</code>), and zipped files (via <code>/vsizip/</code>). Globs (i.e., <code>sd.read_pyogrio(\"path/to/*.gpkg\")</code>) resolving to 1 or more local files are also supported.</p> <p>Like SedonaDB's GeoParquet support, spatial filters like <code>ST_Intersects()</code> are translated into the GDAL/OGR scan where possible to take advantage of embedded spatial indexes in formats like GeoPackage, FlatGeoBuf, and Shapefile. For example, we can query small areas of a huge FlatGeoBuf file hosted elsewhere without scanning the entire file:</p> <pre><code># 12 GB file\nurl = \"https://flatgeobuf.septima.dk/population_areas.fgb\"\nsd.read_pyogrio(url).to_view(\"population_areas\")\n\nwkt = \"POLYGON ((-73.978329 40.767412, -73.950005 40.767412, -73.950005 40.795098, -73.978329 40.795098, -73.978329 40.767412))\"\nsd.sql(\n    f\"\"\"\nSELECT sum(population::INTEGER) FROM population_areas\nWHERE ST_Intersects(wkb_geometry, ST_SetSRID(ST_GeomFromWKT('{wkt}'), 4326))\n\"\"\"\n).show()\n# &gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# &gt; \u2502 sum(population_areas.population) \u2502\n# &gt; \u2502               int64              \u2502\n# &gt; \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# &gt; \u2502                           256251 \u2502\n# &gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/2025/12/01/sedonadb-020-release/#geoparquet-11-write-support","title":"GeoParquet 1.1 Write Support","text":"<p>Whereas the initial version of SedonaDB launched with basic write support for GeoParquet files, the latest version of the specification that enables readers to read small portions of the resulting Parquet file was not supported. With the latest release, <code>DataFrame.to_parquet(\"path/to/parquet\", geoparquet_version=\"1.1\")</code> will add a <code>bbox</code> column enabling functions like <code>sd.read_parquet()</code> with a <code>WHERE ST_Intersects()</code> query to read only a portion of the input file.</p> <pre><code># pip install \"apache-sedona[db]\"\nimport sedona.db\n\nsd = sedona.db.connect()\nurl = \"https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/ns-water_water-point.fgb\"\n\nsd.read_pyogrio(url).to_parquet(\"water_point.parquet\", geoparquet_version=\"1.1\")\n</code></pre> <p>Shrinking the default row group size to ~100,000 and applying a spatial sort may improve read performance by ensuring that row groups contain related features.</p> <pre><code>sd.sql(\"SET datafusion.execution.parquet.max_row_group_size = 100000\")\n\nsd.read_parquet(url).to_view(\"water_point\")\n\nsd.sql(\n    \"\"\"\nSELECT * FROM water_point\nORDER BY sd_order(geometry)\n\"\"\"\n).to_parquet(\"water_point.parquet\", geoparquet_version=\"1.1\")\n</code></pre>"},{"location":"blog/2025/12/01/sedonadb-020-release/#python-user-defined-function-support","title":"Python User-Defined Function Support","text":"<p>User-defined functions (UDFs) are essential components of many workflows in modern DataFrame engines like Spark, DataFusion, and DuckDB to capture user-specific logic that is difficult or impossible to implement by simply composing existing functions. Just as SedonaSpark provides a vectorized UDF framework for geometry types, SedonaDB 0.2.0 exposes a framework that allows user-specific logic (including but not limited to those involving geometry!) to be referenced from SQL workflows. For example, a UDF implementation of <code>ST_Buffer()</code> could be written as:</p> <pre><code>import pyarrow as pa\nimport sedona.db\nfrom sedonadb import udf\nimport shapely\nimport geoarrow.pyarrow as ga\n\nsd = sedona.db.connect()\n\n\n@udf.arrow_udf(ga.wkb(), [udf.GEOMETRY, udf.NUMERIC])\ndef shapely_udf(geom, distance):\n    geom_wkb = pa.array(geom.storage.to_array())\n    distance = pa.array(distance.to_array())\n    geom = shapely.from_wkb(geom_wkb)\n    result_shapely = shapely.buffer(geom, distance)\n    return pa.array(shapely.to_wkb(result_shapely))\n\n\nsd.register_udf(shapely_udf)\nsd.sql(\"SELECT shapely_udf(ST_Point(0, 0), 2.0) as col\").show()\n# &gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# &gt; \u2502                                                col                                               \u2502\n# &gt; \u2502                                             geometry                                             \u2502\n# &gt; \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# &gt; \u2502 POLYGON((2 0,1.9615705608064609 -0.3901806440322565,1.8477590650225735 -0.7653668647301796,1.66\u2026 \u2502\n# &gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See the documentation page for <code>arrow_udf()</code> for more examples and documentation.</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#initial-raster-data-type-implementation","title":"Initial Raster data type implementation","text":"<p>The raster data type support in Sedona Spark is a popular feature and in SedonaDB 0.2.0 we are pleased to offer a raster data type and a few basic functions!</p> <pre><code>import sedona.db\n\nsd = sedona.db.connect()\n\nsd.sql(\"SELECT RS_Width(RS_Example()) as width\").show()\n# &gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# &gt; \u2502  width \u2502\n# &gt; \u2502 uint64 \u2502\n# &gt; \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# &gt; \u2502     64 \u2502\n# &gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>For more information or to get involved see the raster support umbrella issue. Thank you to jesspav for driving this work!</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#release-on-cratesio","title":"Release on <code>crates.io</code>","text":"<p>Because our initial development of SedonaDB was closely tied to improvements we were experimenting with in some of our dependency crates from the GeoRust ecosystem, our first release of SedonaDB contained git dependencies and references to the forks we had used in our experiments. While the first release could be used in Rust projects via a git dependency, this prevented any downstream project from releasing to crates.io themselves and did not make clear that we do expose a public Rust API that can be used alongside any DataFusion-based project! Rust projects can use the components we provide or use the pre-assembled <code>SedonaContext</code>.</p> <p>SedonaDB 0.2.0, in addition to being released to crates.io includes a Rust example to get interested Rust projects started:</p> <pre><code>[package]\n# ...\n\n[dependencies]\ndatafusion = { version = \"50.2.0\"}\nsedona = { version = \"0.2.0\" }\n# ...\n</code></pre> <pre><code>use datafusion::{common::Result, prelude::*};\nuse sedona::context::{SedonaContext, SedonaDataFrame};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let ctx = SedonaContext::new_local_interactive().await?;\n    let url = \"https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/natural-earth/files/natural-earth_cities_geo.parquet\";\n    let df = ctx.read_parquet(url, Default::default()).await?;\n    let output = df\n        .sort_by(vec![col(\"name\")])?\n        .show_sedona(&amp;ctx, Some(5), Default::default())\n        .await?;\n    println!(\"{output}\");\n    Ok(())\n}\n</code></pre> <p>We're still learning about how or if downstream projects are interested in using components from Rust, so feel free to open an issue with ideas or questions.</p> <p>Special thanks to kylebarron for reviewing our PRs upstream into the geo-index and wkb crates!</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#build-system-improvements","title":"Build System Improvements","text":"<p>Build system improvements are some of the least glorious but most important tasks included in the SedonaDB 0.2.0 release; however, we are pleased to be able to release SedonaDB 0.2.0 Python binaries with support for PROJ, GEOS, and S2Geography on MacOS, Windows, and Linux (Python 3.9 to 3.14, including free-threaded variants of Python 3.13 and 3.14). SedonaDB 0.2.0 for R may be installed via R-Universe, which now includes pre-built binaries for MacOS and Windows users. Support for PROJ on Windows and S2Geography on all R platforms is a work in progress and will hopefully be included in the next release.</p> <p>Special thanks to yutannihilation for contributing fixes to the R build system!</p>"},{"location":"blog/2025/12/01/sedonadb-020-release/#contributors","title":"Contributors","text":"<p>This release consists of contributions from 17 contributors. Thank you for your contributions!</p> <pre><code>git shortlog -sn apache-sedona-db-0.2.0.dev..apache-sedona-db-0.2.0\n    25  Dewey Dunnington\n    16  Abeeujah\n    13  Hiroaki Yutani\n    13  Peter Nguyen\n    12  Kristin Cowalcijk\n     8  jp\n     6  Feng Zhang\n     4  Joonas Pessi\n     3  Matthew Powers\n     2  Cancai Cai\n     2  Jia Yu\n     1  L_Sowmya\n     1  Liang Geng\n     1  Peter Von der Porten\n     1  Yongting You\n     1  ayushjariyal\n     1  dentiny\n</code></pre>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/","title":"Introducing SedonaDB: A single-node analytical database engine with geospatial as a first-class citizen","text":"<p>The Apache Sedona community is excited to announce the initial release of SedonaDB! \ud83c\udf89</p> <p>SedonaDB is the first open-source, single-node analytical database engine that treats spatial data as a first-class citizen. It is developed as a subproject of Apache Sedona.</p> <p>Apache Sedona powers large-scale geospatial processing on distributed engines like Spark (SedonaSpark), Flink (SedonaFlink), and Snowflake (SedonaSnow). SedonaDB extends the Sedona ecosystem with a single-node engine optimized for small-to-medium data analytics, delivering the simplicity and speed that distributed systems often cannot.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#what-is-sedonadb","title":"\ud83e\udd14 What is SedonaDB","text":"<p>Written in Rust, SedonaDB is lightweight, blazing fast, and spatial-native. Out of the box, it provides:</p> <ul> <li>\ud83d\uddfa\ufe0f Full support for spatial types, joins, CRS (coordinate reference systems), and functions on top of industry-standard query operations.</li> <li>\u26a1 Query optimizations, indexing, and data pruning features under the hood that make spatial operations just work with high performance.</li> <li>\ud83d\udc0d Pythonic and SQL interfaces familiar to developers, plus APIs for R and Rust.</li> <li>\u2601\ufe0f Flexibility to run in single-machine environments on local files or data lakes.</li> </ul> <p>SedonaDB utilizes Apache Arrow and Apache DataFusion, providing everything you need from a modern, vectorized query engine. What sets it apart is the ability to process spatial workloads natively, without extensions or plugins. Installation is straightforward, and SedonaDB integrates easily into both local development and cloud pipelines, offering a consistent experience across environments.</p> <p>The initial release of SedonaDB provides a comprehensive suite of geometric vector operations and seamlessly integrates with GeoArrow, GeoParquet, and GeoPandas. Future versions will support all popular spatial functions, including functions for raster data.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#sedonadb-quickstart-example","title":"\ud83d\ude80 SedonaDB quickstart example","text":"<p>Start by installing SedonaDB:</p> <pre><code>pip install \"apache-sedona[db]\"\n</code></pre> <p>Now instantiate the connection:</p> <pre><code>import sedona.db\n\nsd = sedona.db.connect()\n</code></pre> <p>Let's perform a spatial join using SedonaDB.</p> <p>Suppose you have a <code>cities</code> table with latitude and longitude points representing the center of each city, and a <code>countries</code> table with a column containing a polygon of the country's geographic boundaries.</p> <p>Here are a few rows from the <code>cities</code> table:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     name     \u2506            geometry           \u2502\n\u2502   utf8view   \u2506      geometry &lt;epsg:4326&gt;     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Vatican City \u2506 POINT(12.4533865 41.9032822)  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 San Marino   \u2506 POINT(12.4417702 43.9360958)  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Vaduz        \u2506 POINT(9.5166695 47.1337238)   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n</code></pre> <p>And here are a few rows from the countries table:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             name            \u2506   continent   \u2506                      geometry                      \u2502\n\u2502           utf8view          \u2506    utf8view   \u2506                geometry &lt;epsg:4326&gt;                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Fiji                        \u2506 Oceania       \u2506 MULTIPOLYGON(((180 -16.067132663642447,180 -16.55\u2026 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 United Republic of Tanzania \u2506 Africa        \u2506 POLYGON((33.90371119710453 -0.9500000000000001,34\u2026 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Western Sahara              \u2506 Africa        \u2506 POLYGON((-8.665589565454809 27.656425889592356,-8\u2026 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n</code></pre> <p>Here\u2019s how to perform a spatial join to compute the country of each city:</p> <pre><code>sd.sql(\n    \"\"\"\nselect\n    cities.name as city_name,\n    countries.name as country_name,\n    continent\nfrom cities\njoin countries\nwhere ST_Intersects(cities.geometry, countries.geometry)\n\"\"\"\n).show(3)\n</code></pre> <p>The code utilizes <code>ST_Intersects</code> to determine if a city is contained within a given country.</p> <p>Here's the result of the query:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   city_name   \u2506         country_name        \u2506 continent \u2502\n\u2502    utf8view   \u2506           utf8view          \u2506  utf8view \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Suva          \u2506 Fiji                        \u2506 Oceania   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Dodoma        \u2506 United Republic of Tanzania \u2506 Africa    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Dar es Salaam \u2506 United Republic of Tanzania \u2506 Africa    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The example above performs a point-in-polygon join, mapping city locations (points) to the countries they fall within (polygons). SedonaDB executes these joins efficiently by leveraging spatial indices where beneficial and dynamically adapting join strategies at runtime using input data samples. While many general-purpose engines struggle with the performance of such operations, SedonaDB is purpose-built for spatial workloads and delivers consistently fast results.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#apache-sedona-spatialbench","title":"\ud83d\udcca Apache Sedona SpatialBench","text":"<p>To test our work on SedonaDB, we also needed to develop a mechanism to evaluate its performance and speed. This led us to develop Apache Sedona SpatialBench, a benchmark for assessing geospatial SQL analytics query performance across database systems.</p> <p>Let's compare the performance of SedonaDB vs. GeoPandas and DuckDB Spatial for some representative spatial queries as defined in SpatialBench.</p> <p>Here are the results from SpatialBench v0.1 for Queries 1\u201312 at scale factor 1 (SF1) and scale factor 10 (SF10).</p> <p> </p> <p>SedonaDB demonstrates balanced performance across all query types and scales effectively to SF 10. DuckDB excels at spatial filters and some geometric operations but faces challenges with complex joins and KNN queries. GeoPandas, while popular in the Python ecosystem, requires manual optimization and parallelization to handle larger datasets effectively. An in-depth performance analysis can be found in the SpatialBench website.</p> <p>Here\u2019s an example of the SpatialBench Query #8 that works for SedonaDB and DuckDB:</p> <pre><code>SELECT b.b_buildingkey, b.b_name, COUNT(*) AS nearby_pickup_count\nFROM trip t JOIN building b ON ST_DWithin(ST_GeomFromWKB(t.t_pickuploc), ST_GeomFromWKB(b.b_boundary), 0.0045) -- ~500m\nGROUP BY b.b_buildingkey, b.b_name\nORDER BY nearby_pickup_count DESC\n</code></pre> <p>This query intentionally performs a distance-based spatial join between points and polygons, followed by an aggregation of the results.</p> <p>Here's what the query returns:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 b_buildingkey \u2506  b_name  \u2506 nearby_pickup_count \u2502\n\u2502     int64     \u2506 utf8view \u2506        int64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502          3779 \u2506 linen    \u2506                  42 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502         19135 \u2506 misty    \u2506                  36 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502          4416 \u2506 sienna   \u2506                  26 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Here\u2019s the equivalent GeoPandas code:</p> <pre><code>trips_df = pd.read_parquet(data_paths[\"trip\"])\ntrips_df[\"pickup_geom\"] = gpd.GeoSeries.from_wkb(\n    trips_df[\"t_pickuploc\"], crs=\"EPSG:4326\"\n)\npickups_gdf = gpd.GeoDataFrame(trips_df, geometry=\"pickup_geom\", crs=\"EPSG:4326\")\n\nbuildings_df = pd.read_parquet(data_paths[\"building\"])\nbuildings_df[\"boundary_geom\"] = gpd.GeoSeries.from_wkb(\n    buildings_df[\"b_boundary\"], crs=\"EPSG:4326\"\n)\nbuildings_gdf = gpd.GeoDataFrame(\n    buildings_df, geometry=\"boundary_geom\", crs=\"EPSG:4326\"\n)\n\nthreshold = 0.0045  # degrees (~500m)\nresult = (\n    buildings_gdf.sjoin(pickups_gdf, predicate=\"dwithin\", distance=threshold)\n    .groupby([\"b_buildingkey\", \"b_name\"], as_index=False)\n    .size()\n    .rename(columns={\"size\": \"nearby_pickup_count\"})\n    .sort_values([\"nearby_pickup_count\", \"b_buildingkey\"], ascending=[False, True])\n    .reset_index(drop=True)\n)\n</code></pre>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#sedonadb-crs-management","title":"\ud83d\uddfa\ufe0f SedonaDB CRS management","text":"<p>SedonaDB manages the CRS when reading/writing files, as well as in DataFrames, making your pipelines safer and saving you from manual work.</p> <p>Let's compute the number of buildings in the state of Vermont to highlight the CRS management features embedded in SedonaDB.</p> <p>Start by reading in a FlatGeobuf file that uses the EPSG 32618 CRS with GeoPandas and then convert it to a SedonaDB DataFrame:</p> <pre><code>import geopandas as gpd\n\npath = \"https://raw.githubusercontent.com/geoarrow/geoarrow-data/v0.2.0/example-crs/files/example-crs_vermont-utm.fgb\"\ngdf = gpd.read_file(path)\nvermont = sd.create_data_frame(gdf)\n</code></pre> <p>Let\u2019s check the schema of the <code>vermont</code> DataFrame:</p> <pre><code>vermont.schema\n\nSedonaSchema with 1 field:\n  geometry: wkb &lt;epsg:32618&gt;\n</code></pre> <p>We can see that the <code>vermont</code> DataFrame maintains the CRS that\u2019s specified in the FlatGeobuf file.  SedonaDB doesn\u2019t have a native FlatGeobuf reader yet, but it\u2019s easy to use the GeoPandas FlatGeobuf reader and then convert it to a SedonaDB DataFrame with a single line of code.</p> <p>Now read a GeoParquet file into a SedonaDB DataFrame.</p> <pre><code>buildings = sd.read_parquet(\n    \"https://github.com/geoarrow/geoarrow-data/releases/download/v0.2.0/microsoft-buildings_point_geo.parquet\"\n)\n</code></pre> <p>Check the schema of the DataFrame:</p> <pre><code>buildings.schema\n\nSedonaSchema with 1 field:\n  geometry: geometry &lt;ogc:crs84&gt;\n</code></pre> <p>Let\u2019s expose these two tables as views and run a spatial join to see how many buildings are in Vermont:</p> <pre><code>buildings.to_view(\"buildings\", overwrite=True)\nvermont.to_view(\"vermont\", overwrite=True)\n\nsd.sql(\n    \"\"\"\nselect count(*) from buildings\njoin vermont\nwhere ST_Intersects(buildings.geometry, vermont.geometry)\n\"\"\"\n).show()\n</code></pre> <p>This command correctly errors out because the tables have different CRSs.  For safety, SedonaDB errors out rather than give you the wrong answer! Here's the error message that's easy to debug:</p> <pre><code>SedonaError: type_coercion\ncaused by\nError during planning: Mismatched CRS arguments: ogc:crs84 vs epsg:32618\nUse ST_Transform() or ST_SetSRID() to ensure arguments are compatible.\n</code></pre> <p>Let\u2019s rewrite the spatial join to convert the <code>vermont</code> CRS to EPSG:4326, so it\u2019s compatible with the <code>buildings</code> CRS.</p> <pre><code>sd.sql(\n    \"\"\"\nselect count(*) from buildings\njoin vermont\nwhere ST_Intersects(buildings.geometry, ST_Transform(vermont.geometry, 'EPSG:4326'))\n\"\"\"\n).show()\n</code></pre> <p>We now get the correct result!</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count(*) \u2502\n\u2502   int64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502   361856 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>SedonaDB tracks the CRS when reading/writing files, converting to/from GeoPandas DataFrames, or when performing DataFrame operations, so your spatial computations run safely and correctly!</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#realistic-example-with-sedonadb","title":"\ud83c\udfaf Realistic example with SedonaDB","text":"<p>Let's now turn our attention to a KNN join, which is a more complex spatial operation.</p> <p>Suppose you're analyzing ride-sharing data and want to identify which buildings are most commonly near pickup points, helping understand the relationship between trip origins and nearby landmarks, businesses, or residential structures that might influence ride demand patterns.</p> <p>This query finds the five closest buildings to each trip pickup location using spatial nearest neighbor analysis. For every trip, it identifies the five buildings that are geographically closest to where the passenger was picked up and calculates the exact distance to each of those buildings.</p> <p>Here\u2019s the query:</p> <pre><code>WITH trip_with_geom AS (\n    SELECT t_tripkey, t_pickuploc, ST_GeomFromWKB(t_pickuploc) as pickup_geom\n    FROM trip\n),\nbuilding_with_geom AS (\n    SELECT b_buildingkey, b_name, b_boundary, ST_GeomFromWKB(b_boundary) as boundary_geom\n    FROM building\n)\nSELECT\n    t.t_tripkey,\n    t.t_pickuploc,\n    b.b_buildingkey,\n    b.b_name AS building_name,\n    ST_Distance(t.pickup_geom, b.boundary_geom) AS distance_to_building\nFROM trip_with_geom t JOIN building_with_geom b\nON ST_KNN(t.pickup_geom, b.boundary_geom, 5, FALSE)\nORDER BY distance_to_building ASC, b.b_buildingkey ASC\n</code></pre> <p>Here are the results of the query:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 t_tripkey \u2506          t_pickuploc          \u2506 b_buildingkey \u2506 building_name \u2506 distance_to_building \u2502\n\u2502   int64   \u2506             binary            \u2506     int64     \u2506      utf8     \u2506        float64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502   5854027 \u2506 01010000001afa27b85825504001\u2026 \u2506            79 \u2506 gainsboro     \u2506                  0.0 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502   3326828 \u2506 01010000001bfcc5b8b7a95d4083\u2026 \u2506           466 \u2506 deep          \u2506                  0.0 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502   1239844 \u2506 0101000000ce471770d6ce2a40f9\u2026 \u2506           618 \u2506 ivory         \u2506                  0.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This is one of the queries from SpatialBench.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#why-sedonadb-was-built-in-rust","title":"\ud83e\udd80 Why SedonaDB was built in Rust","text":"<p>SedonaDB is built in Rust, a high-performance, memory-safe language that offers fine-grained memory management and a mature ecosystem of data libraries. It takes full advantage of this ecosystem by integrating with projects such as Apache DataFusion, GeoArrow, and georust/geo.</p> <p>While Spark provides extension points that let SedonaSpark optimize spatial queries in distributed settings, DataFusion offers stable APIs for pruning, spatial operators, and optimizer rules on a single node. This enabled us to embed deep spatial awareness into the engine while preserving full non-spatial functionality. Thanks to the DataFusion project and community, the experience was both possible and enjoyable.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#why-sedonadb-and-sedonaspark-are-both-needed","title":"\u2696\ufe0f Why SedonaDB and SedonaSpark are Both Needed","text":"<p>SedonaSpark is well-suited for large-scale geospatial workloads or environments where Spark is already part of your production stack. For instance, joining a 100 GB vector dataset with a large raster dataset. For smaller datasets, however, Spark's distributed architecture can introduce unnecessary overhead, making it slower to run locally, harder to install, and more difficult to tune.</p> <p>SedonaDB is better for smaller datasets and when running computations locally. The SedonaDB spatial functions are compatible with the SedonaSpark functions, so SQL chunks that work for one engine will usually work for the other. Over time, we will ensure that both project APIs are fully interoperable. Here's an example of a chunk to analyze the Overture buildings table that works for both engines.</p> <pre><code>nyc_bbox_wkt = (\n    \"POLYGON((-74.2591 40.4774, -74.2591 40.9176, -73.7004 40.9176, -73.7004 40.4774, -74.2591 40.4774))\"\n)\n\nsd.sql(f\"\"\"\nSELECT\n    id,\n    height,\n    num_floors,\n    roof_shape,\n    ST_Centroid(geometry) as centroid\nFROM\n    buildings\nWHERE\n    is_underground = FALSE\n    AND height IS NOT NULL\n    AND height &gt; 20\n    AND ST_Intersects(geometry, ST_SetSRID(ST_GeomFromText('{nyc_bbox_wkt}'), 4326))\nLIMIT 5;\n</code></pre>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#next-steps","title":"\ud83d\ude80 Next steps","text":"<p>While SedonaDB is well-tested and provides a core set of features that can perform numerous spatial analyses, it remains an early-stage project with multiple opportunities for new features.</p> <p>Many more ST functions are required.  Some are relatively straightforward, but others are complex.</p> <p>The community will add built-in support for other spatial file formats, such as GeoPackage and GeoJSON, to SedonaDB. You can read data in these formats into GeoPandas DataFrames and convert them to SedonaDB DataFrames in the meantime.</p> <p>Raster support is also on the roadmap, which is a complex undertaking, so it's an excellent opportunity to contribute if you're interested in solving challenging problems with Rust.</p> <p>Refer to the SedonaDB v0.2 milestone for more details on the specific tasks outlined for the next release. Additionally, feel free to create issues, comment on the Discord, or start GitHub discussions to brainstorm new features.</p>"},{"location":"blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/#join-the-community","title":"\ud83e\udd1d Join the community","text":"<p>The Apache Sedona community has an active Discord community, monthly user meetings, and regular contributor meetings.</p> <p>SedonaDB welcomes contributions from the community. Feel free to request to take ownership of an issue, and we will be happy to assign it to you. You're also welcome to join the contributor meetings, and the other active contributors will be glad to help you get your pull request over the finish line!</p> <p>Info</p> <p>We\u2019re celebrating the launch of SedonaDB &amp; SpatialBench with a special Apache Sedona Community Office Hour!</p> <p>\ud83d\udcc5 October 7, 2025</p> <p>\u23f0 8\u20139 AM Pacific Time</p> <p>\ud83d\udccd Online</p> <p>\ud83d\udd17 Sign up here</p>"},{"location":"blog/2025/07/09/welcome-to-the-apache-sedona-blog/","title":"Welcome to the Apache Sedona Blog!","text":"<p>Welcome to the brand-new blog for Apache Sedona!</p> <p>For several years, Apache Sedona has been the go-to open-source engine for processing massive geospatial datasets, extending Apache Spark to handle complex spatial operations with unparalleled speed and efficiency. Sedona's capabilities also extend beyond Spark, bringing spatial analytics to the Snowflake data warehouse with SedonaSnow and the real-time streaming engine Apache Flink with a Spatial SQL integration.</p> <p>This new blog is our space to share news and best practices for the entire Sedona ecosystem.</p> <p>Whether you're a seasoned data scientist, an engineer building robust data pipelines, or a GIS enthusiast, this is your new home for practical, expert content. We'll dive into advanced tutorials, explore architectural patterns, and discuss the trade-offs of various spatial data strategies. From real-world case studies to our forward-looking vision for the project, we\u2019ll be sharing insights to help you master and innovate with Apache Sedona.</p> <p>More importantly, this is a platform for the entire community\u2014and that includes you. Apache Sedona is driven by its contributors, and this blog is no different. Have you built a compelling use case, mastered a tricky optimization, or have a tutorial you'd like to share? We strongly encourage you to become a guest author. Your insights and real-world stories are invaluable. To share your idea for a post or to submit a draft, please connect with the community on our Apache Sedona Discord.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/","title":"Managing spatial tables in Data Lakehouses with Iceberg","text":"<p>This post explains the benefits of the Lakehouse Architecture for spatial tables and how Lakehouses differ from data warehouses and data lakes.</p> <p>Many of the benefits that Lakehouses (e.g., Iceberg, Delta Lake, Hudi) provide for tabular data also apply to spatial data, for example:</p> <ul> <li>Reliable transactions</li> <li>Versioned data</li> <li>Time travel</li> <li>Schema enforcement</li> <li>Optimizations</li> <li>And many more</li> </ul> <p>The spatial data community can use Lakehouses as of Iceberg v3, which adds geometry/geography types.</p> <p>Spatial data requires different types of metadata and optimizations, but doesn\u2019t always require entirely different file formats.  Iceberg can now store the metadata needed for geometry and geography columns.  It\u2019s excellent that you can now use Iceberg for tabular and spatial data.</p> <p>This post also explains why Lakehouse architecture is often better than warehouses and data lakes.  Let\u2019s start with a detailed description of the Lakehouse architecture.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#data-lakehouse-architecture-overview","title":"Data Lakehouse Architecture Overview","text":"<p>A Lakehouse stores data in a Lakehouse Storage System like Iceberg, Delta Lake, or Hudi.  Iceberg v3 is currently the best option for spatial data because it natively supports geometry and geography columns.</p> <p>Tables in Lakehouses are governed by a catalog like Unity Catalog or Apache Polaris. The catalog allows role-based access control (RBAC) and features like multi-table transactions.</p> <p>You can query tables in the Lakehouse Architecture for BI, reporting, data science, machine learning, and other complex analyses.</p> <p>The following table illustrates the Lakehouse Architecture:</p> <p></p> <p>The Lakehouse Architecture offers several advantages:</p> <ul> <li>Data is stored in open formats so any engine can query it, and there is no vendor lock-in.</li> <li>Lakehouses support all the features familiar to data warehouses, like reliable transactions, DML operations, and role-based access control.</li> <li>Lakehouses are often performant enough for low-latency applications like BI dashboards.</li> <li>Lakehouses are interoperable with proprietary tools like BigQuery, Redshift, or Esri.</li> <li>You can store Lakehouses in cloud-based storage systems without any additional charges.</li> <li>Lakehouses are compatible with any engine. You can use one engine for ingestion, another for ETL, and a third for ML. The architecture encourages using the best engine for the job.</li> </ul> <p>Let\u2019s see how Lakehouses differ from data lakes.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#spatial-tables-in-lakehouses-vs-data-lakes","title":"Spatial tables in Lakehouses vs. Data Lakes","text":"<p>Data Lakes store data in files without a metadata layer, so they don\u2019t guarantee reliable transactions.</p> <p>Here are examples of spatial data lakes:</p> <ul> <li>GeoParquet files stored in AWS S3</li> <li>GeoJSON files stored in Azure Blob Storage</li> <li>CSV files with WKT geometry data stored in GCP</li> </ul> <p>Since data lakes do not support reliable transactions, they cannot support developer-friendly features like delete and merge, require downtime when datasets are mutated, and do not offer advanced performance features.  Data lakes don\u2019t support features like deletion vectors or small file compaction.</p> <p>The Lakehouse metadata layer allows for convenience functions and much better performance.</p> <p>The Lakehouse metadata layer is relatively small, so the storage costs for a Lakehouse and a data lake are about the same. Lakehouses allow for better performance, so compute expenses are generally lower than those of a data lake.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#spatial-tables-in-lakehouses-vs-data-warehouses","title":"Spatial tables in Lakehouses vs. Data Warehouses","text":"<p>A Data Warehouse is an analytics system powered by a proprietary engine and file formats. However, due to market pressures, this definition has shifted, and some Data Warehouses began supporting Lakehouse Storage Systems in addition to proprietary file formats. Many modern customers don\u2019t want vendor lock-in via a proprietary file format.</p> <p>Data Warehouses have the following limitations:</p> <ul> <li>They usually bundle storage with compute in pricing packages, so you must pay for more compute, even if you just want more storage.</li> <li>They store data in proprietary file formats, which aren\u2019t compatible with other engines.</li> <li>Queries can run slower when you store data in open file formats.</li> <li>Sharing compute with other users can cause performance degradation when one user runs a large query.</li> </ul> <p>In modern times, the strict definition of a data warehouse is shifting because engines that used to support only proprietary file formats now support open file formats.  You can now think of a Data Warehouse as a system that contains either a proprietary engine or a proprietary file format.</p> <p>Many modern enterprises prefer the Lakehouse architecture because it\u2019s open, compatible with any engine that builds a connector, vendor-neutral, and low-cost.</p> <p>Let\u2019s now see how to create some Iceberg tables.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#creating-tabular-tables-with-iceberg","title":"Creating tabular tables with Iceberg","text":"<p>Here\u2019s how to create a <code>customers</code> table with <code>id</code> and <code>first_name</code> columns:</p> <pre><code>CREATE TABLE local.db.customers (id string, first_name string)\nUSING iceberg\nTBLPROPERTIES('format-version'='3');\n</code></pre> <p>Let\u2019s append some data to the table:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"a\", \"Bob\"),\n        (\"b\", \"Mary\"),\n        (\"c\", \"Sue\"),\n    ],\n    [\"id\", \"first_name\"],\n)\n\ndf.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.customers\")\n</code></pre> <p>Let\u2019s see how to run a query on the table:</p> <pre><code>sedona.table(\"local.db.customers\").show()\n\n+---+----------+\n| id|first_name|\n+---+----------+\n|  a|       Bob|\n|  b|      Mary|\n|  c|       Sue|\n+---+----------+\n</code></pre> <p>Creating a table with tabular data is straightforward.  Now, let\u2019s see how to make a table with spatial data in Iceberg.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#creating-spatial-tables-with-iceberg-v3","title":"Creating spatial tables with Iceberg v3","text":"<p>Let\u2019s now create a <code>customer_purchases</code> table with a <code>purchase_location</code> column.</p> <p>Here\u2019s how to create this spatial table with Iceberg:</p> <pre><code>CREATE TABLE local.db.customer_purchases (id string, price double, geometry geometry)\nUSING iceberg\nTBLPROPERTIES('format-version'='3');\n</code></pre> <p>Now append some spatial data with longitude/latitude coordinates to the table:</p> <pre><code>coords = [\n    (-88.110352, 24.006326),\n    (-77.080078, 24.006326),\n    (-77.080078, 31.503629),\n    (-88.110352, 31.503629),\n    (-88.110352, 24.006326),\n]\ndf = sedona.createDataFrame(\n    [\n        (\"a\", 10.99, Polygon(coords)),\n        (\"b\", 3.5, Point(1, 2)),\n        (\"c\", 1.95, Point(3, 4)),\n    ],\n    [\"id\", \"price\", \"geometry\"],\n)\n\ndf.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"local.db.customer_purchases\")\n</code></pre> <p>The spatial table contains points and polygons. Some purchases have an exact location, and others occur in a region.</p> <p>Let\u2019s see how to join the tabular table with the spatial table.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#joining-an-iceberg-tabular-table-with-a-spatial-table","title":"Joining an Iceberg tabular table with a spatial table","text":"<p>Here\u2019s how to join the <code>customers</code> and <code>customer_purchases</code> tables.</p> <pre><code>customers = sedona.table(\"local.db.customers\")\npurchases = sedona.table(\"local.db.customer_purchases\")\n\njoined = customers.join(purchases, \"id\")\njoined.show()\n\n+---+----------+-----+--------------------+\n| id|first_name|price|            geometry|\n+---+----------+-----+--------------------+\n|  a|       Bob|10.99|POLYGON ((-88.110...|\n|  b|      Mary|  3.5|         POINT (1 2)|\n|  c|       Sue| 1.95|         POINT (3 4)|\n+---+----------+-----+--------------------+\n</code></pre> <p>Now we can see the customer information and the location of their purchases all in one table.</p> <p>Keep reading for an example with a spatial join, which is a join based on the geometry columns of two tables.</p> <p>It\u2019s easy to join any tables with Sedona, regardless of the underlying file format, because Sedona has so many built-in file readers (e.g., you can easily join one table stored in Shapefiles and another stored in GeoParquet files). But it\u2019s even easier when Iceberg stores the tabular and spatial tables in the same catalog.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#optimizing-spatial-tables-in-lakehouses","title":"Optimizing spatial tables in Lakehouses","text":"<p>This section shows how queries can run faster on Iceberg tables.</p> <p>Let\u2019s query the Overture Maps Foundation buildings dataset stored in GeoParquet files.</p> <pre><code>(\n    sedona.table(\"open_data.overture_2025_03_19_1.buildings_building\")\n    .withColumn(\"geometry\", ST_GeomFromWKB(col(\"geometry\")))\n    .select(\"id\", \"geometry\", \"num_floors\", \"roof_color\")\n    .createOrReplaceTempView(\"my_fun_view\")\n)\n</code></pre> <p>Using this GeoParquet dataset, let\u2019s run a filtering query to get the number of buildings in a small region near Gainesville, Florida.</p> <pre><code>spot = \"POLYGON((-82.258759 29.129371, -82.180481 29.136569, -82.202454 29.173747, -82.258759 29.129371))\"\nsql = f\"\"\"\nselect * from my_fun_view\nwhere ST_Contains(ST_GeomFromWKT('{spot}'), geometry)\n\"\"\"\nsedona.sql(sql).count()\n</code></pre> <p>This query runs in 45 seconds.</p> <p>Let\u2019s convert this dataset to Iceberg:</p> <pre><code>df = sedona.table(\"open_data.overture_2025_03_19_1.buildings_building\")\n\nsql = \"\"\"\nCREATE TABLE local.db.overture_2025_03_19_1_buildings_building (id string, geometry geometry, num_floors integer, roof_color string)\nUSING iceberg\nTBLPROPERTIES('format-version'='3');\n\"\"\"\nsedona.sql(sql)\n\n(\n    df.select(\"id\", \"geometry\", \"num_floors\", \"roof_color\")\n    .write.format(\"iceberg\")\n    .mode(\"overwrite\")\n    .saveAsTable(\"local.db.overture_2025_03_19_1_buildings_building\")\n)\n</code></pre> <p>Now let's rerun the same query on the Iceberg table:</p> <pre><code>spot = \"POLYGON((-82.258759 29.129371, -82.180481 29.136569, -82.202454 29.173747, -82.258759 29.129371))\"\nsql = f\"\"\"\nselect * from local.db.overture_2025_03_19_1_buildings_building\nwhere ST_Contains(ST_GeomFromWKT('{spot}'), geometry)\n\"\"\"\nsedona.sql(sql).count()\n</code></pre> <p>This query executes in 4 seconds.</p> <p>To make this Iceberg query run even faster, we could perform even more optimizations to colocate spatially near data in the same files.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#more-geospatial-examples-in-iceberg","title":"More geospatial examples in Iceberg","text":"<p>This section uses a contrived example to demonstrate some Iceberg features that help you manage your spatial data.  Start by creating two tables.  One for the blue geometric objects and another for the orange polygons in the following graph:</p> <p></p> <p>You can start by creating the Iceberg table:</p> <pre><code>CREATE TABLE some_catalog.matt.icegeometries (id string, geometry geometry)\nUSING iceberg\nTBLPROPERTIES('format-version'='3');\n</code></pre> <p>Append objects <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code>, and <code>e</code> to the table:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"a\", \"LINESTRING(1.0 3.0,3.0 1.0)\"),\n        (\"b\", \"LINESTRING(2.0 5.0,6.0 1.0)\"),\n        (\"c\", \"POLYGON((7.0 4.0,9.0 4.0,9.0 3.0,7.0 3.0,7.0 4.0))\"),\n        (\"d\", \"LINESTRING(2.0 7.0,4.0 9.0,7.0 8.0)\"),\n        (\"e\", \"LINESTRING(10.0 9.0,10.0 6.0)\"),\n    ],\n    [\"id\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\n\ndf.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"some_catalog.matt.icegeometries\")\n</code></pre> <p>Check the content of the table:</p> <pre><code>sedona.sql(\"SELECT * FROM some_catalog.matt.icegeometries;\").show(truncate=False)\n\n+---+-----------------------------------+\n|id |geometry                           |\n+---+-----------------------------------+\n|a  |LINESTRING (1 3, 3 1)              |\n|b  |LINESTRING (2 5, 6 1)              |\n|c  |POLYGON ((7 4, 9 4, 9 3, 7 3, 7 4))|\n|d  |LINESTRING (2 7, 4 9, 7 8)         |\n|e  |LINESTRING (10 9, 10 6)            |\n+---+-----------------------------------+\n</code></pre> <p>Iceberg makes it easy to delete rows of data in a table based on a predicate.</p> <p>Now create a table with the polygons.  Start by creating the Iceberg table:</p> <pre><code>CREATE TABLE some_catalog.matt.icepolygons (id string, geometry geometry)\nUSING iceberg\nTBLPROPERTIES('format-version'='3');\n</code></pre> <p>Append objects <code>polygon_x</code> and <code>polygon_y</code> to the table:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"polygon_x\", \"POLYGON((3.0 5.0,8.0 5.0,8.0 2.0,3.0 2.0,3.0 5.0))\"),\n        (\"polygon_y\", \"POLYGON((5.0 9.0,8.0 9.0,8.0 7.0,5.0 7.0,5.0 9.0))\"),\n    ],\n    [\"id\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\n\ndf.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"some_catalog.matt.icepolygons\")\n</code></pre> <p>Here\u2019s how you can delete all the linestrings that cross any polygon.</p> <pre><code>sql = \"\"\"\nDELETE FROM some_catalog.matt.icegeometries\nWHERE EXISTS (\n    SELECT 1\n    FROM some_catalog.matt.icepolygons\n    WHERE ST_Intersects(icegeometries.geometry, icepolygons.geometry)\n)\n\"\"\"\nsedona.sql(sql)\n</code></pre> <p>Check the table to see that geometries <code>b</code>, <code>c</code>, and <code>d</code> are deleted from the table.</p> <pre><code>sedona.sql(\"SELECT * FROM some_catalog.matt.icegeometries;\").show(truncate=False)\n\n+---+-----------------------+\n|id |geometry               |\n+---+-----------------------+\n|a  |LINESTRING (1 3, 3 1)  |\n|e  |LINESTRING (10 9, 10 6)|\n+---+-----------------------+\n</code></pre> <p>Iceberg also allows for time travel between different versions of a table.  Let\u2019s see all the current versions of the Iceberg table:</p> <pre><code>sql = \"SELECT snapshot_id, committed_at, operation FROM some_catalog.matt.icegeometries.snapshots;\"\nsedona.sql(sql).show(truncate=False)\n\n+-------------------+-----------------------+---------+\n|snapshot_id        |committed_at           |operation|\n+-------------------+-----------------------+---------+\n|1643575804253593143|2025-10-10 19:35:19.539|append   |\n|5206691623836785752|2025-10-10 19:35:41.214|overwrite|\n+-------------------+-----------------------+---------+\n</code></pre> <p>Let\u2019s check the contents of the table before the delete operation is run:</p> <pre><code>sql = \"SELECT * FROM some_catalog.matt.icegeometries FOR SYSTEM_VERSION AS OF 1643575804253593143;\"\nsedona.sql(sql).show(truncate=False)\n\n+---+-----------------------------------+\n|id |geometry                           |\n+---+-----------------------------------+\n|a  |LINESTRING (1 3, 3 1)              |\n|b  |LINESTRING (2 5, 6 1)              |\n|c  |POLYGON ((7 4, 9 4, 9 3, 7 3, 7 4))|\n|d  |LINESTRING (2 7, 4 9, 7 8)         |\n|e  |LINESTRING (10 9, 10 6)            |\n+---+-----------------------------------+\n</code></pre> <p>A new table version is created every time an Iceberg transaction is completed.</p> <p>Geospatial upsert operations with Iceberg</p> <p>Iceberg also supports upsert operations, which allow you to update or insert rows in a table in a single transaction. Upserts are especially useful for incremental updates.</p> <p>Here is the current contents of the Iceberg table:</p> <pre><code>+---+-----------------------+\n|id |geometry               |\n+---+-----------------------+\n|a  |LINESTRING (1 3, 3 1)  |\n|e  |LINESTRING (10 9, 10 6)|\n+---+-----------------------+\n</code></pre> <p>Let\u2019s perform an upsert with the following data:</p> <pre><code>+---+---------------------+\n|id |geometry             |\n+---+---------------------+\n|a  |LINESTRING (1 3, 3 1)| # duplicate\n|e  |LINESTRING (5 9, 10 6)| # updated geometry\n|z  |LINESTRING (6 7, 6 9)| # new data\n+---+---------------------+\n</code></pre> <p>Here\u2019s how the upsert append should run:</p> <ul> <li>New data should be appended</li> <li>Existing data should be updated</li> <li>Duplicate data should be ignored</li> </ul> <p>Here\u2019s the code to execute this operation:</p> <pre><code>merge_sql = \"\"\"\nMERGE INTO some_catalog.matt.icegeometries target\nUSING source\nON target.id = source.id\nWHEN MATCHED THEN\n    UPDATE SET\n        target.geometry = source.geometry\nWHEN NOT MATCHED THEN\n    INSERT (id, geometry) VALUES (source.id, source.geometry)\n\"\"\"\nsedona.sql(merge_sql)\n</code></pre> <p>Here are the contents of the table after running this operation:</p> <pre><code>+---+----------------------+\n|id |geometry              |\n+---+----------------------+\n|a  |LINESTRING (1 3, 3 1) |\n|e  |LINESTRING (5 9, 10 6)|\n|z  |LINESTRING (6 7, 6 9) |\n+---+----------------------+\n</code></pre> <p>The MERGE command has many other practical applications for geospatial tables.</p> <p>Geospatial schema enforcement with Iceberg</p> <p>Iceberg supports schema enforcement, prohibiting appending data with a mismatched schema to the table. If you try to append a DataFrame with a mismatched schema to an Iceberg table, it will error.</p> <p>Let\u2019s create a DataFrame with a different schema than the existing Iceberg table:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"x\", 2, \"LINESTRING(8.0 8.0,3.0 3.0)\"),\n        (\"y\", 3, \"LINESTRING(5.0 5.0,1.0 1.0)\"),\n    ],\n    [\"id\", \"num\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\n</code></pre> <p>Now attempt to append the DataFrame to the table:</p> <pre><code>df.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"some_catalog.matt.icegeometries\")\n</code></pre> <p>Here is the error:</p> <pre><code>AnalysisException: [INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `some_catalog`.`matt`.`icegeometries`, the reason is too many data columns:\nTable columns: `id`, `geometry`.\nData columns: `id`, `num`, `geometry`.\n</code></pre> <p>The append operation is prohibited because the DataFrame schema differs from the Iceberg table schema.</p> <p>Data lakes don\u2019t have built-in schema enforcement, so you can append data with a mismatched schema, which can corrupt a table or require developers to use specific syntax when reading the table.</p> <p>Schema enforcement is a nice feature that protects the integrity of your data tables.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#iceberg-v3-spec-for-geometry-and-geography-columns","title":"Iceberg v3 spec for geometry and geography columns","text":"<p>See this pull request for details on the updated Iceberg spec with geometry and geography columns.</p> <p>The Iceberg v3 spec stores this key information:</p> <ul> <li>The CRS for each geometry/geography column</li> <li>The bounding box (bbox) for each geometry/geography column</li> </ul> <p>From the spec:</p> <p>Geospatial features from OGC \u2013 Simple feature access. Edge-interpolation is always linear/planar. See Appendix G. Parameterized by CRS C. If not specified, C is OGC:CRS84.</p> <p>Here\u2019s Appendix G:</p> <p>The Geometry and Geography class hierarchy and its Well-known text (WKT) and Well-known binary (WKB) serializations (ISO supporting XY, XYZ, XYM, XYZM) are defined by OpenGIS Implementation Specification for Geographic information \u2013 Simple feature access \u2013 Part 1: Common architecture, from OGC (Open Geospatial Consortium). Points are always defined by the coordinates X, Y, Z (optional), and M (optional), in this order. X is the longitude/easting, Y is the latitude/northing, and Z is usually the height, or elevation. M is a fourth optional dimension, for example a linear reference value (e.g., highway milepost value), a timestamp, or some other value as defined by the CRS. The version of the OGC standard first used here is 1.2.1, but future versions may also be used if the WKB representation remains wire-compatible.</p>"},{"location":"blog/2025/10/21/managing-spatial-tables-in-data-lakehouses-with-iceberg/#conclusion","title":"Conclusion","text":"<p>Lakehouses offer amazing features for the data community, and the spatial community can now take advantage of these benefits for vector datasets.</p> <p>You can store geometry and geography data in Iceberg spatial tables to co-locate your tabular and spatial tables in the same catalog.  Organizing your data this way makes your spatial data more findable and improves query performance.</p> <p>The spatial data community still needs to align on the best way to store raster data (e.g., satellite imagery) in Lakehouses.  Stay tuned for more interesting discussions on raster data!</p>"},{"location":"community/contact/","title":"Community","text":""},{"location":"community/contact/#community","title":"Community","text":"<p>Every volunteer project obtains its strength from the people involved in it. We invite you to participate as much or as little as you choose.</p> <p>You can participate in the community as follows:</p> <ul> <li>Use our project and provide a feedback.</li> <li>Provide us with the use-cases.</li> <li>Report bugs and submit patches.</li> <li>Contribute code and documentation.</li> </ul>"},{"location":"community/contact/#community-events","title":"Community events","text":"<p>Everyone is welcome to join our community events. We have a community office hour every 4 weeks. Please register to the event you want to attend: https://bit.ly/3UBmxFY</p>"},{"location":"community/contact/#sedona-on-linkedin","title":"Sedona on LinkedIn","text":"<p>Apache Sedona@LinkedIn</p>"},{"location":"community/contact/#sedona-on-x","title":"Sedona on X","text":"<p>Apache Sedona@X</p>"},{"location":"community/contact/#discord-server","title":"Discord Server","text":""},{"location":"community/contact/#get-help","title":"Get help","text":"<p>You can ask for help or contribute to the project. Before submitting an issue, please:</p> <ul> <li>Verify that the bug does in fact exist.</li> <li>Search the issue tracker to verify there is no existing issue reporting the bug you\u2019ve found.</li> <li>Consider tracking down the bug yourself in the Sedona\u2019s source and submitting a patch along with your bug report. This is a great time saver for the Sedona developers and helps ensure the bug will be fixed quickly.</li> </ul> <p>Enhancement requests for new features are also welcome. The more concrete and rationale the request is, the greater the chance it will be incorporated into future releases.</p> <ul> <li>Sedona JIRA: bug reports and feature requests</li> <li>Sedona GitHub Issues: bug reports and feature requests</li> <li>Sedona GitHub Discussion: project development and general questions</li> <li>Sedona Mailing Lists: dev@sedona.apache.org: project development and general questions.</li> </ul> <p>For the mailing list, Please first subscribe and then post emails. To subscribe, please send an email (leave the subject and content blank) to dev-subscribe@sedona.apache.org</p> <p>Tip</p> <p>All serious conversation such as voting will only take place on the mailing list. This includes voting on each release candidate or major change to the project. Please pay attention to the mailing list!</p>"},{"location":"community/contributor/","title":"Project Management Committee","text":"<p>Sedona has received numerous help from the community. This page lists the committers and project management committee of Apache Sedona. People on this page are ordered by their last name.</p>"},{"location":"community/contributor/#committers","title":"Committers","text":"<p>A contributor who contributes enough code to Sedona will be promoted to a committer. A committer has the write access to Sedona main repository</p>"},{"location":"community/contributor/#project-management-committee-pmc","title":"Project Management Committee (PMC)","text":"<p>A committer will be promoted to a PMC member when the community thinks he/she is able to be in charge at least a major component of this project.</p> <p>Current Sedona PMC members are as follows:</p> Name GitHub ID Apache ID Adam Binford Kimahriman kimahriman@apache.org Kanchan Chowdhury kanchanchy kanchanchy@apache.org Kristin Cowalcijk Kontinuation kontinuation@apache.org Furqaan Khan furqaankhan furqaan@apache.org Pawe\u0142 Koci\u0144ski Imbruced imbruced@apache.org Yitao Li yitao-li yitaoli@apache.org Netanel Malka netanel246 malka@apache.org Mohamed Sarwat Sarwat mosarwat@apache.org Kengo Seki sekikn sekikn@apache.org Sachio Wakai SW186000 swakai@apache.org Jinxuan Wu jinxuan jinxuanw@apache.org Jia Yu jiayuasu jiayu@apache.org Feng Zhang zhangfengcdt fengzhang@apache.org Zongsi Zhang zongsizhang zongsizhang@apache.org Felix Cheung felixcheung@apache.org Von Gosling vongosling@apache.org Sunil Govindan sunilg@apache.org Jean-Baptiste Onofr\u00e9 jbonofre@apache.org George Percivall percivall@apache.org"},{"location":"community/contributor/#become-a-committer","title":"Become a committer","text":"<p>To get started contributing to Sedona, learn how to contribute \u2013 anyone can submit patches, documentation and examples to the project.</p> <p>The PMC regularly adds new committers from the active contributors, based on their contributions to Sedona. The qualifications for new committers include:</p> <ul> <li>Sustained contributions to Sedona: Committers should have a history of major contributions to Sedona.</li> <li>Quality of contributions: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches.</li> <li>Community involvement: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev mailing list &amp; Discord, and help mentor newer contributors and users.</li> </ul> <p>The PMC also adds new PMC members. PMC members are expected to carry out PMC responsibilities as described in Apache Guidance, including helping vote on releases, enforce Apache project trademarks, take responsibility for legal and license issues, and ensure the project follows Apache project mechanics. The PMC periodically adds committers to the PMC who have shown they understand and can help with these activities.</p> <p>Current Sedona Committers are as follows:</p> Name GitHub ID Apache ID John Bampton jbampton johnbam@apache.org Dewey Dunnington paleolimbot paleolimbot@apache.org Nilesh Gajwani iGN5117 nilesh@apache.org Peter Nguyen petern48 petern@apache.org Pranav Toggi prantogg prantogg@apache.org"},{"location":"community/contributor/#nominate-a-committer-or-pmc-member","title":"Nominate a committer or PMC member","text":"<p>Steps are as follows:</p> <ol> <li>Call a vote (templates/committerVote.txt)</li> <li>Close the vote. If the result is positive, invite the new committer.</li> </ol>"},{"location":"community/contributor/#call-for-a-vote","title":"Call for a vote","text":"<p>We do the vote and discussion on the private@sedona.apache.org to enable a frank discussion.</p> <p>Let the Vote thread run for one week. A positive result is achieved by Consensus Approval: at least 3 +1 votes and no vetoes.</p>"},{"location":"community/contributor/#pmc-vote-template","title":"PMC vote template","text":"<p>This is the email to commence a vote for a new PMC candidate. New PMC members need to be voted for by the existing PMC members and subsequently approved by the Board.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE] New PMC candidate: [New PMC NAME]\n\n[ add the reasons behind your nomination here ]\n\nVoting ends one week from today, or until at least 3 +1 votes are cast.\n</code></pre>"},{"location":"community/contributor/#close-a-vote","title":"Close a vote","text":"<p>This email ends the vote and reports the result to the project.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE][RESULT] New PMC candidate: [New PMC NAME]\n\nThe vote has now closed: [paste the vote thread on https://lists.apache.org/list.html?private@sedona.apache.org]. The results are:\n\nBinding Votes:\n\n+1 [TOTAL BINDING +1 VOTES]\n 0 [TOTAL BINDING +0/-0 VOTES]\n-1 [TOTAL BINDING -1 VOTES]\n\nThe vote is ***successful/not successful***\n</code></pre>"},{"location":"community/contributor/#send-a-notice-to-asf-board","title":"Send a notice to ASF Board","text":"<p>The nominating PMC member should send a message to the ASF Board (board@apache.org) with a reference to the vote result in the following form:</p> <pre><code>To: board at apache.org\nCC: private at sedona.apache.org\nSubject: [NOTICE] New PMC NAME for Apache Sedona PMC\nBody:\n\nNew PMC NAME has been voted as a new member of the Apache Sedona PMC. The vote thread is at: *link to the vote result thread*\n</code></pre>"},{"location":"community/contributor/#send-the-invitation","title":"Send the invitation","text":"<pre><code>To: New PMC Email address\nCC: private@sedona.apache.org\n\nHello [New PMC NAME],\n\nThe Sedona Project Management Committee (PMC)\nhereby offers you committer privileges to the project\n[as well as membership in the PMC]. These privileges are\noffered on the understanding that you'll use them\nreasonably and with common sense. We like to work on trust\nrather than unnecessary constraints.\n\nBeing a committer enables you to more easily make\nchanges without needing to go through the patch\nsubmission process. Being a PMC member enables you\nto guide the direction of the project.\n\nBeing a committer does not require you to\nparticipate any more than you already do. It does\ntend to make one even more committed. You will\nprobably find that you spend more time here.\n\nOf course, you can decline and instead remain as a\ncontributor, participating as you do now.\n\nA. This personal invitation is a chance for you to\naccept or decline in private. Either way, please\nlet us know in reply to the private@sedona.apache.org\naddress only.\n\nB. If you accept, the next step is to register an iCLA:\n    1. Details of the iCLA and the forms are found\n    through this link: https://www.apache.org/licenses/#clas\n\n    2. Instructions for its completion and return to\n    the Secretary of the ASF are found at\n    https://www.apache.org/licenses/#submitting\n\n    3. When you transmit the completed iCLA, request\n    to notify the Apache Sedona project and choose a\n    unique Apache ID. Look to see if your preferred\n    ID is already taken at\n    https://people.apache.org/committer-index.html\n    This will allow the Secretary to notify the PMC\n    when your iCLA has been recorded.\n\nWhen recording of your iCLA is noted, you will\nreceive a follow-up message with the next steps for\nestablishing you as a committer.\n</code></pre>"},{"location":"community/contributor/#pmc-accept-and-icla-instructions","title":"PMC Accept and ICLA instructions","text":"<pre><code>To: New PMC Email address\nCc: private@sedona.apache.org\nSubject: Re: invitation to become Apache Sedona PMC\n\nWelcome. Here are the next steps in becoming a project committer. After that we will make an announcement to the dev@sedona.apache.org\n\n1. You need to send a Contributor License Agreement to the ASF.\nNormally you would send an Individual CLA. If you also make\ncontributions done in work time or using work resources,\nsee the Corporate CLA. Ask us if you have any issues.\nhttps://www.apache.org/licenses/#clas.\n\nYou need to choose a preferred ASF user name and alternatives.\nIn order to ensure it is available you can view a list of taken IDs at\nhttps://people.apache.org/committer-index.html\n\nPlease notify us when you have submitted the CLA and by what means\nyou did so. This will enable us to monitor its progress.\n\nWe will arrange for your Apache user account when the CLA has\nbeen recorded.\n\n2. After that is done, please use your ASF email to subscribe to the dev@sedona.apache.org\nand private@sedona.apache.org by sending an email to dev-subscribe@sedona.apache.org and\nprivate-subscribe@sedona.apache.org. We generally discuss everything on the dev list and\nkeep the private@sedona.apache.org list for occasional matters which must be private.\n\nThe developer section of the website describes roles within the ASF and provides other\nresources:\n  https://www.apache.org/foundation/how-it-works.html\n  https://www.apache.org/dev/\n\nJust as before you became a committer, participation in any ASF community\nrequires adherence to the ASF Code of Conduct:\n  https://www.apache.org/foundation/policies/conduct.html\n\nYours,\nThe Apache Sedona PMC\n</code></pre>"},{"location":"community/contributor/#create-asf-account","title":"Create ASF account","text":"<p>Once the ICLA has been filed, use the ASF New Account Request form to generate the request. Sedona mentors will request the account.</p> <p>Once Sedona graduates, the PMC chair will make the request.</p>"},{"location":"community/contributor/#add-to-the-system","title":"Add to the system","text":"<p>Once the new PMC subscribes to the Sedona mailing lists using his/her ASF account, one of the PMC needs to add the new PMC to the Whimsy system (https://whimsy.apache.org/roster/pmc/sedona).</p>"},{"location":"community/contributor/#pmc-announcement","title":"PMC announcement","text":"<p>This is the email to announce the new committer to sedona-dev once the account has been created.</p> <pre><code>To: dev@sedona.apache.org\nSubject: new committer: ###New PMC NAME\n\nThe Project Management Committee (PMC) for Apache Sedona\nhas invited New PMC NAME to become a committer and we are pleased\nto announce that they have accepted.\n\n### add specific details here ###\n\nBeing a committer enables easier contribution to the\nproject since there is no need to go via the patch\nsubmission process. This should enable better productivity.\nA PMC member helps manage and guide the direction of the project.\n</code></pre>"},{"location":"community/contributor/#committer-done-template","title":"Committer Done Template","text":"<p>After the committer account is established.</p> <pre><code>To: New Committer Email\nCC: private@sedona.apache.org\nSubject: account request: New Committer NAME\n\nNew Committer NAME, as you know, the ASF Infrastructure has set up your\ncommitter account with the username '####'.\n\nYou have commit access to specific sections of the\nASF repository, as follows:\nhttps://github.com/apache/sedona\n\nYou need to link your ASF Account with your GitHub account.\n\nHere are the steps\n\n1. Verify you have a GitHub ID enabled with 2FA\n    * https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/\n2. Enter your GitHub ID into your Apache ID profile https://id.apache.org/\n3. Merge your Apache and GitHub accounts using\n    * GitBox (Apache Account Linking utility) https://gitbox.apache.org/setup/\n    * You should see 3 green checks in GitBox.\n    * Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation\n4. After accepting the GitHub Invitation verify that you are a\nmember of the team https://github.com/orgs/apache/teams/sedona-committers\n\nOptionally, if you want, please follow the instructions to set up your GitHub, SSH, svn password, svn configuration, email forwarding, etc.\nhttps://www.apache.org/dev/#committers\n\nAdditionally, if you have been elected to the Sedona\n Project Mgmt. Committee (PMC): Verify you are part of the LDAP sedona\n  pmc https://whimsy.apache.org/roster/pmc/sedona\n</code></pre>"},{"location":"community/develop/","title":"Develop","text":""},{"location":"community/develop/#develop-sedona","title":"Develop Sedona","text":""},{"location":"community/develop/#scalajava-developers","title":"Scala/Java developers","text":""},{"location":"community/develop/#ide","title":"IDE","text":"<p>We recommend Intellij IDEA with Scala plugin installed. Please make sure that the Project has the SDK set to a JDK 1.8.</p>"},{"location":"community/develop/#import-the-project","title":"Import the project","text":""},{"location":"community/develop/#choose-open","title":"Choose <code>Open</code>","text":""},{"location":"community/develop/#go-to-the-sedona-root-folder-not-a-submodule-folder-and-choose-open","title":"Go to the Sedona root folder (not a submodule folder) and choose <code>open</code>","text":""},{"location":"community/develop/#the-ide-might-show-errors","title":"The IDE might show errors","text":"<p>The IDE usually has trouble understanding the complex project structure in Sedona.</p> <p></p>"},{"location":"community/develop/#fix-errors-by-changing-pomxml","title":"Fix errors by changing <code>pom.xml</code>","text":"<p>You need to comment out the following lines in <code>pom.xml</code> at the root folder, as follows. Remember that you should NOT submit this change to Sedona.</p> <pre><code>&lt;!--    &lt;parent&gt;--&gt;\n&lt;!--        &lt;groupId&gt;org.apache&lt;/groupId&gt;--&gt;\n&lt;!--        &lt;artifactId&gt;apache&lt;/artifactId&gt;--&gt;\n&lt;!--        &lt;version&gt;23&lt;/version&gt;--&gt;\n&lt;!--        &lt;relativePath /&gt;--&gt;\n&lt;!--    &lt;/parent&gt;--&gt;\n</code></pre>"},{"location":"community/develop/#reload-pomxml","title":"Reload <code>pom.xml</code>","text":"<p>Make sure you reload the <code>pom.xml</code> or reload the maven project. The IDE will ask you to remove some modules. Please select <code>yes</code>.</p> <p></p>"},{"location":"community/develop/#the-final-project-structure-should-be-like-this","title":"The final project structure should be like this:","text":""},{"location":"community/develop/#run-unit-tests","title":"Run unit tests","text":""},{"location":"community/develop/#run-all-unit-tests","title":"Run all unit tests","text":"<p>In a terminal, go to the Sedona root folder. Run <code>mvn clean install</code>. All tests will take more than 15 minutes. To only build the project jars, run <code>mvn clean install -DskipTests</code>.</p> <p>Note</p> <p><code>mvn clean install</code> will compile Sedona with Spark 3.3 and Scala 2.12. If you have a different version of Spark in $SPARK_HOME, make sure to specify that using -Dspark command line arg. For example, to compile sedona with Spark 3.4 and Scala 2.12, use: <code>mvn clean install -Dspark=3.4 -Dscala=2.12</code></p> <p>More details can be found on Compile Sedona</p>"},{"location":"community/develop/#run-a-single-unit-test","title":"Run a single unit test","text":"<p>In the IDE, right-click a test case and run this test case.</p> <p></p> <p>When you run a test case written in Scala, the IDE might tell you that the \"Path does not exist\" as follows:</p> <p></p> <p>Go to <code>Edit Configuration</code></p> <p></p> <p>Change the value of <code>Working Directory</code> to <code>$MODULE_WORKING_DIR$</code>.</p> <p></p> <p>Re-run the test case. Do NOT right-click the test case to re-run. Instead, click the button as shown in the figure below.</p> <p></p> <p>If you don't want to change the <code>Working Directory</code> configuration every time, you can change the default value of <code>Working Directory</code> in the <code>Run/Debug Configurations</code> window. Click <code>Edit configuration templates...</code> and change the value of <code>Working Directory</code> of ScalaTest to <code>$MODULE_WORKING_DIR$</code>.</p> <p> </p> <p>Now newly created run configurations for ScalaTest will have the correct value set for <code>Working Directory</code>.</p>"},{"location":"community/develop/#ide-configurations-when-using-java-11","title":"IDE Configurations When Using Java 11","text":"<p>If you are using Java 11, you may encounter the following error when running tests:</p> <pre><code>/path/to/sedona/common/src/main/java/org/apache/sedona/common/geometrySerde/UnsafeGeometryBuffer.java\npackage sun.misc does not exist\nsun.misc.Unsafe\n</code></pre> <p>You can fix this issue by disabling <code>Use '--release' option for cross-compilation</code> in the IDE settings.</p> <p></p>"},{"location":"community/develop/#run-tests-with-different-sparkscala-versions","title":"Run Tests with Different Spark/Scala Versions","text":"<p>If you want to test changes with different Spark/Scala versions, you can select the Spark and Scala profile in the Maven panel. Once you have selected the desired versions, reload the sedona-parent project. See picture below</p> <p>Note</p> <p>The profile change won't update the module names in the IDE. Don't be misled if a module still has a <code>-3.3-2.12</code> suffix in the name.</p> <p>Note</p> <p>Not all combinations of spark and scala versions are supported and so they will fail to compile.</p> <p></p>"},{"location":"community/develop/#python-developers","title":"Python developers","text":""},{"location":"community/develop/#ide_1","title":"IDE","text":"<p>We recommend PyCharm.</p>"},{"location":"community/develop/#run-tests","title":"Run tests","text":""},{"location":"community/develop/#run-all-python-tests","title":"Run all Python tests","text":"<p>To run all Python test cases, follow steps mentioned here.</p> <p>Once the environment is set up, you can run all tests using the following command in python directory:</p> <pre><code>cd python\nuv run pytest -v tests\n</code></pre>"},{"location":"community/develop/#run-a-single-test","title":"Run a single test","text":"<p>To run a particular Python test file, specify the path of the <code>.py</code>.</p> <p>For example, to run all tests in <code>test_function.py</code> located in <code>python/tests/sql/</code>, use:</p> <pre><code>cd python\nuv run pytest -v tests/sql/test_function.py\n</code></pre> <p>To run a particular test in a particular <code>.py</code> test file, specify <code>file_name::class_name::test_name</code> to the <code>pytest</code> command.</p> <p>For example, to run the test on <code>ST_Contains</code> function located in <code>sql/test_predicate.py</code>, use:</p> <pre><code>cd python\nuv run pytest -v tests/sql/test_predicate.py::TestPredicate::test_st_contains\n</code></pre>"},{"location":"community/develop/#build-packages","title":"Build packages","text":"<p>The following command will build the sdist and whl packages in the <code>dist</code> folder.</p> <pre><code>cd python\nuv build\n</code></pre>"},{"location":"community/develop/#r-developers","title":"R developers","text":"<p>More details to come.</p>"},{"location":"community/develop/#ide_2","title":"IDE","text":"<p>We recommend RStudio</p>"},{"location":"community/develop/#import-the-project_1","title":"Import the project","text":""},{"location":"community/geopandas/","title":"Contribute to Geopandas API on Sedona","text":""},{"location":"community/geopandas/#geopandas-on-sedona","title":"Geopandas on Sedona","text":"<p>This guide outlines a few important considerations when contributing changes to the GeoPandas component of Apache Sedona as a developer. Again, this guide is targeted towards contributors; the official documentation is more tailored towards users.</p> <p>General Approach: This component is built on top of the PySpark Pandas API. The <code>GeoDataFrame</code> and <code>GeoSeries</code> classes both inherit from pyspark pandas' <code>ps.DataFrame</code> and <code>ps.Series</code> classes, respectively. When possible, it is generally preferred to leverage pyspark pandas' implementation of a function and extending it from there (i.e. find a way to leverage <code>super()</code> rather than copying over parts of logic). The code structure resembles the structure of the Geopandas repo.</p> <p>Lazy Evaluation: Spark uses lazy evaluation. Spark's distributed and lazy nature occasionally comes in the way of implementing functionality in the same way the original GeoPandas library does so. For example, GeoPandas has many checks for invalid crs in many places (e.g <code>GeoSeries.__init__()</code>, <code>set_crs()</code>). Sedona's implementation for getting the <code>crs</code> currently is expensive compared to GeoPandas because it requires us to run an eager <code>ST_SRID()</code> query. If we eagerly query for the crs in every initialization of <code>GeoSeries</code>, all of our function calls (e.g <code>.area()</code>, etc) would also become eager and would incur a noticeable slowdown, resulting in a poor user experience.</p> <p>Maintaining Order: Because Spark uses distributed data, maintaining the order of data across operations takes extra time and effort. Maintaining order for some operations is not very meaningful. In those cases, it's reasonable to skip the post-sorting to avoid an unnecessary performance hit. Documentation should document this behavior. For example, <code>sjoin</code> currently does not maintain the traditional pandas dataframe order after performing the join. This follows the same convention as traditional PySpark Pandas. The user can always post-sort using a separate function such as <code>sort_index()</code>, but we should avoid sorting unnecessarily by default.</p> <p>Conventions: The conventional shorthand for the Sedona Geopandas package is <code>sgpd</code>. Notice it's the same as the geopandas shorthand (<code>gpd</code>), except prefixed with an 's'. The conventional short hands for adjacent packages are shown below also.</p> <pre><code>import pandas as pd\nimport geopandas as gpd\nimport pyspark.pandas as ps\nimport sedona.spark.geopandas as sgpd\n</code></pre> <p>Conversion Methods: Sedona's Implementation of Geopandas provides useful methods to convert to and from other dataframes using the following methods. These apply to both <code>GeoDataFrame</code> and <code>GeoSeries</code>:</p> <ul> <li><code>to_geopandas()</code>: Sedona Geo(DataFrame/Series) to Geopandas</li> <li><code>to_geoframe()</code>: Sedona GeoSeries to Sedona GeoDataFrame</li> <li><code>to_spark_pandas()</code>: Sedona Geo(DataFrame/Series) to Pandas on PySpark</li> <li><code>to_spark()</code> (inherited): Sedona GeoDataFrame to Spark DataFrame</li> <li><code>to_frame()</code> (inherited): Sedona GeoSeries to PySpark Pandas DataFrame</li> </ul> <p>GeoSeries functions: Most geometry manipulation operations in Geopandas are considered GeoSeries functions. However, we can call them from a <code>GeoDataFrame</code> object as well to execute on its active geometry column. We implement the functions in the <code>GeoSeries</code> class. However in <code>base.py</code>, we add a <code>_delegate_to_geometry_column()</code> call to allow the <code>GeoDataFrame</code> to also execute the function on its active geometry column. We also specify the docstring for the function here instead of <code>GeoSeries</code>, so that both <code>GeoDataFrame</code> and <code>GeoSeries</code> will inherit the shared docstring (avoiding duplicated docstrings).</p> <p>Explain Query Plans: Because these dataframe abstractions are built on Spark, we can retrieve the query plan for an operation for a Dataframe by using the <code>.spark.explain()</code> method.</p> <p>Example:</p> <pre><code>geoseries = GeoSeries([Polygon([(0, 0), (1, 0), (1, 1), (0, 0)])])\n# Currently PySpark pandas Series does not have the spark.explain() method, so a workaround is to convert it to a dataframe first\nprint(geoseries.area.to_frame().spark.explain(extended=True))\n</code></pre> <pre><code>== Parsed Logical Plan ==\nProject [__index_level_0__#19L, 0#27 AS None#31]\n+- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Area**   AS 0#27, __index_level_0__#19L, __natural_order__#23L]\n   +- Project [__index_level_0__#19L, 0#20, monotonically_increasing_id() AS __natural_order__#23L]\n      +- LogicalRDD [__index_level_0__#19L, 0#20], false\n\n== Analyzed Logical Plan ==\n...\n\n== Optimized Logical Plan ==\n...\n\n== Physical Plan ==\nProject [__index_level_0__#19L,  **org.apache.spark.sql.sedona_sql.expressions.ST_Area**   AS None#31]\n+- *(1) Scan ExistingRDD[__index_level_0__#19L,0#20]\n</code></pre>"},{"location":"community/geopandas/#suggested-short-readings","title":"Suggested Short Readings:","text":"<ul> <li>PySpark Pandas Best Practices - This mentions other useful notes to consider such as why <code>__iter__()</code> is not supported</li> <li>Geopandas User Guide - Specifically it's useful to understand the GeoDataFrame's \"active geometry column\"</li> </ul>"},{"location":"community/geopandas/#other-references","title":"Other References","text":"<ul> <li>Public API Pages for Pandas API on Spark</li> <li>Geopandas API Page</li> </ul>"},{"location":"community/publication/","title":"Publications","text":""},{"location":"community/publication/#publication","title":"Publication","text":"<p>Apache Sedona was formerly called GeoSpark, initiated by Arizona State University Data Systems Lab.</p>"},{"location":"community/publication/#key-publications","title":"Key publications","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.</p>"},{"location":"community/publication/#third-party-evaluation","title":"Third-party evaluation","text":"<p>GeoSpark were evaluated by papers published on database top venues. It is worth noting that we do not have any collaboration with the authors.</p> <ul> <li>SIGMOD 2020 paper \"Architecting a Query Compiler for Spatial Workloads\" Ruby Y. Tahboub, Tiark  Rompf (Purdue University).</li> </ul> <p>In Figure 16a, GeoSpark distance join query runs around 7x - 9x faster than Simba, a spatial extension on Spark, on 1 - 24 core machines.</p> <ul> <li>PVLDB 2018 paper \"How Good Are Modern Spatial Analytics Systems?\" Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows:</li> </ul> <p>GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.</p>"},{"location":"community/publication/#full-publications","title":"Full publications","text":""},{"location":"community/publication/#geospark-ecosystem","title":"GeoSpark Ecosystem","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2019.</p> <p>\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\" (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016</p> <p>\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\" (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015</p>"},{"location":"community/publication/#geosparkviz-visualization-system","title":"GeoSparkViz Visualization System","text":"<p>\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\" (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018</p>"},{"location":"community/publication/#geosparksim-traffic-simulator","title":"GeoSparkSim Traffic Simulator","text":"<p>\"Dissecting GeoSparkSim: a scalable microscopic road network traffic simulator in Apache Spark\" (journal paper) Jia Yu, Zishan Fu, Mohamed Sarwat. Distributed Parallel Databases 38(4): 963-994 (2020)</p> <p>\"Demonstrating GeoSparkSim: A Scalable Microscopic Road Network Traffic Simulator Based on Apache Spark\". Zishan Fu, Jia Yu, Mohamed Sarwat. International Symposium on Spatial and Temporal Databases, SSTD, 2019</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019</p>"},{"location":"community/publication/#a-tutorial-about-geospatial-data-management-in-spark","title":"A Tutorial about Geospatial Data Management in Spark","text":"<p>\"Geospatial Data Management in Apache Spark: A Tutorial\" (Tutorial) Jia Yu and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p>"},{"location":"community/publish/","title":"Make a release","text":""},{"location":"community/publish/#make-a-sedona-release","title":"Make a Sedona release","text":"<p>This page is for Sedona PMC to publish Sedona releases.</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/publish/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run</li> </ol> <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre> <ol> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this script file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/publish/#1-check-asf-copyright-in-all-file-headers","title":"1. Check ASF copyright in all file headers","text":"<ol> <li>Run the following script:</li> </ol> <pre><code>#!/bin/bash\nwget -q https://archive.apache.org/dist/creadur/apache-rat-0.15/apache-rat-0.15-bin.tar.gz\ntar -xvf  apache-rat-0.15-bin.tar.gz\ngit clone --shared --branch master https://github.com/apache/sedona.git sedona-src\njava -jar apache-rat-0.15/apache-rat-0.15.jar -d sedona-src &gt; report.txt\n</code></pre> <ol> <li>Read the generated report.txt file and make sure all source code files have ASF header.</li> <li>Delete the generated report and cloned files</li> </ol> <pre><code>#!/bin/bash\nrm -rf apache-rat-0.15\nrm -rf sedona-src\nrm report.txt\n</code></pre>"},{"location":"community/publish/#2-update-sedona-python-r-and-zeppelin-versions","title":"2. Update Sedona Python, R and Zeppelin versions","text":"<p>Make sure the Sedona version in the following files are 1.8.0.</p> <ol> <li>https://github.com/apache/sedona/blob/master/python/sedona/version.py</li> <li>https://github.com/apache/sedona/blob/master/R/DESCRIPTION</li> <li>https://github.com/apache/sedona/blob/99239524f17389fc4ae9548ea88756f8ea538bb9/R/R/dependencies.R#L42</li> <li>https://github.com/apache/sedona/blob/master/zeppelin/package.json</li> </ol>"},{"location":"community/publish/#3-update-mkdocsyml","title":"3. Update mkdocs.yml","text":"<ul> <li>Please change the following variables in <code>mkdocs.yml</code> to the version you want to publish.<ul> <li><code>sedona_create_release.current_version</code></li> <li><code>sedona_create_release.current_rc</code></li> <li><code>sedona_create_release.current_git_tag</code></li> <li><code>sedona_create_release.current_snapshot</code></li> </ul> </li> <li>Then compile the website by <code>mkdocs serve</code>. This will generate the scripts listed on this page in your local browser.</li> <li>You can also publish this website if needed. See the instruction at bottom.</li> </ul>"},{"location":"community/publish/#4-stage-and-upload-release-candidates","title":"4. Stage and upload release candidates","text":"<pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\necho \"*****Step 1. Stage the Release Candidate to GitHub.\"\n\nmvn -q -B clean release:prepare -Dtag=sedona-1.8.0-rc1 -DreleaseVersion=1.8.0 -DdevelopmentVersion=1.8.1-SNAPSHOT -Dresume=false -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\necho \"*****Step 2: Upload the Release Candidate to https://repository.apache.org.\"\n\n## Note that we use maven-release-plugin 2.3.2 instead of more recent version (e.g., 3.0.1) to get rid of a bug of maven-release-plugin,\n## which prevent us from cloning git repo with user specified -Dtag=&lt;tag&gt;.\n## Please refer to https://issues.apache.org/jira/browse/MRELEASE-933 and https://issues.apache.org/jira/browse/SCM-729 for details.\n##\n## Please also note that system properties `-Dspark` and `-Dscala` has to be specified both for release:perform and the actual build parameters\n## in `-Darguments`, because the build profiles activated for release:perform task will also affect the actual build task. It is safer to specify\n## these system properties for both tasks.\n\n# Define repository details\nREPO_URL=\"https://github.com/apache/sedona.git\"\nRC_VERSION=\"1.8.0-rc1\"\nSEDONA_VERSION=\"1.8.0\"\nTAG=\"sedona-${RC_VERSION}\"\nLOCAL_DIR=\"sedona-release\"\n\n# Remove existing directory if it exists and clone the repository\nrm -rf $LOCAL_DIR &amp;&amp; git clone --depth 1 --branch $TAG $REPO_URL $LOCAL_DIR &amp;&amp; cd $LOCAL_DIR\n\n# Define the Maven release plugin version\nMAVEN_PLUGIN_VERSION=\"2.3.2\"\n\n# Define Spark and Scala versions\ndeclare -a SPARK_VERSIONS=(\"3.4\" \"3.5\" \"4.0\")\ndeclare -a SCALA_VERSIONS=(\"2.12\" \"2.13\")\n\n# Function to get Java version for Spark version\nget_java_version() {\n  local spark_version=$1\n  if [[ \"$spark_version\" == \"4.0\" ]]; then\n    echo \"17\"\n  else\n    echo \"11\"\n  fi\n}\n\n# Function to find Maven installation path\nfind_maven_path() {\n  # Try different methods to find Maven\n  local mvn_path=\"\"\n\n  # Method 1: Check if mvn is in PATH\n  if command -v mvn &gt;/dev/null 2&gt;&amp;1; then\n    mvn_path=$(command -v mvn)\n  fi\n\n  # Method 2: Check common Homebrew locations\n  if [[ -z \"$mvn_path\" ]]; then\n    for version_dir in /opt/homebrew/Cellar/maven/*/libexec/bin/mvn; do\n      if [[ -x \"$version_dir\" ]]; then\n        mvn_path=\"$version_dir\"\n        break\n      fi\n    done\n  fi\n\n  # Method 3: Check /usr/local (older Homebrew installations)\n  if [[ -z \"$mvn_path\" ]]; then\n    for version_dir in /usr/local/Cellar/maven/*/libexec/bin/mvn; do\n      if [[ -x \"$version_dir\" ]]; then\n        mvn_path=\"$version_dir\"\n        break\n      fi\n    done\n  fi\n\n  # Method 4: Check system locations\n  if [[ -z \"$mvn_path\" ]]; then\n    for path in /usr/bin/mvn /usr/local/bin/mvn; do\n      if [[ -x \"$path\" ]]; then\n        mvn_path=\"$path\"\n        break\n      fi\n    done\n  fi\n\n  if [[ -z \"$mvn_path\" ]]; then\n    echo \"ERROR: Could not find Maven installation\" &gt;&amp;2\n    echo \"Please ensure Maven is installed and available in PATH or in standard locations\" &gt;&amp;2\n    exit 1\n  fi\n\n  echo \"$mvn_path\"\n}\n\n# Function to create Maven wrapper with specific Java version\ncreate_mvn_wrapper() {\n  local java_version=$1\n  local mvn_wrapper=\"/tmp/mvn-java${java_version}\"\n  local mvn_path=$(find_maven_path)\n\n  echo \"Using Maven at: $mvn_path\" &gt;&amp;2\n\n  # Create a wrapper script that sets JAVA_HOME and executes Maven\n  cat &gt; \"$mvn_wrapper\" &lt;&lt; EOF\n#!/bin/bash\nJAVA_HOME=\"\\${JAVA_HOME:-\\$(/usr/libexec/java_home -v ${java_version})}\" exec \"${mvn_path}\" \"\\$@\"\nEOF\n\n  chmod +x \"$mvn_wrapper\"\n  echo \"$mvn_wrapper\"\n}\n\n# Function to verify Java version using Maven wrapper\nverify_java_version() {\n  local mvn_wrapper=$1\n  local expected_java_version=$2\n\n  echo \"Verifying Java version with Maven wrapper...\"\n  local mvn_java_version=$($mvn_wrapper --version | grep \"Java version\" | sed 's/.*Java version: \\([0-9]*\\).*/\\1/')\n  if [[ \"$mvn_java_version\" != \"$expected_java_version\" ]]; then\n    echo \"ERROR: Maven wrapper is using Java $mvn_java_version, but expected Java $expected_java_version\"\n    echo \"Please ensure the correct Java version is installed\"\n    exit 1\n  fi\n  echo \"\u2713 Verified: Maven wrapper is using Java $mvn_java_version\"\n}\n\n# Iterate through Spark and Scala versions\nfor SPARK in \"${SPARK_VERSIONS[@]}\"; do\n  for SCALA in \"${SCALA_VERSIONS[@]}\"; do\n    # Skip Spark 4.0 + Scala 2.12 combination as it's not supported\n    if [[ \"$SPARK\" == \"4.0\" &amp;&amp; \"$SCALA\" == \"2.12\" ]]; then\n      echo \"Skipping Spark $SPARK with Scala $SCALA (not supported)\"\n      continue\n    fi\n\n    JAVA_VERSION=$(get_java_version $SPARK)\n    echo \"Running release:perform for Spark $SPARK and Scala $SCALA with Java $JAVA_VERSION...\"\n\n    # Create Maven wrapper with appropriate Java version\n    MVN_WRAPPER=$(create_mvn_wrapper $JAVA_VERSION)\n    echo \"Created Maven wrapper: $MVN_WRAPPER\"\n\n    # Verify Java version\n    verify_java_version $MVN_WRAPPER $JAVA_VERSION\n\n    # Execute Maven with the wrapper\n    $MVN_WRAPPER org.apache.maven.plugins:maven-release-plugin:$MAVEN_PLUGIN_VERSION:perform \\\n      -DconnectionUrl=scm:git:file://$(pwd) \\\n      -Dtag=$TAG \\\n      -Dresume=false \\\n      -Darguments=\"-DskipTests -Dspark=$SPARK -Dscala=$SCALA\" \\\n      -Dspark=$SPARK \\\n      -Dscala=$SCALA\n\n    # Clean up the wrapper\n    rm -f $MVN_WRAPPER\n  done\ndone\n\necho \"*****Step 3: Upload Release Candidate on ASF SVN: https://dist.apache.org/repos/dist/dev/sedona\"\n\necho \"Creating ${RC_VERSION} folder on SVN...\"\n\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}\n\necho \"Creating release files locally...\"\n\n# Go back to parent directory for file operations\ncd ../..\n\necho \"Downloading source code...\"\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-${RC_VERSION}.tar.gz\ntar -xvf sedona-${RC_VERSION}.tar.gz\nmkdir apache-sedona-${SEDONA_VERSION}-src\ncp -r sedona-sedona-${RC_VERSION}/* apache-sedona-${SEDONA_VERSION}-src/\ntar czf apache-sedona-${SEDONA_VERSION}-src.tar.gz apache-sedona-${SEDONA_VERSION}-src\nrm sedona-${RC_VERSION}.tar.gz\nrm -rf sedona-sedona-${RC_VERSION}\n\n# Create checksums and signatures for source files\nshasum -a 512 apache-sedona-${SEDONA_VERSION}-src.tar.gz &gt; apache-sedona-${SEDONA_VERSION}-src.tar.gz.sha512\ngpg -ab apache-sedona-${SEDONA_VERSION}-src.tar.gz\n\necho \"Uploading source files...\"\n\n# Upload source files first\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-src.tar.gz https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-src.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-src.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-src.tar.gz.sha512\n\necho \"Compiling the source code...\"\n\nmkdir apache-sedona-${SEDONA_VERSION}-bin\n\n# Function to get Java version for Spark version\nget_java_version() {\n  local spark_version=$1\n  if [[ \"$spark_version\" == \"4.0\" ]]; then\n    echo \"17\"\n  else\n    echo \"11\"\n  fi\n}\n\n# Function to find Maven installation path\nfind_maven_path() {\n  # Try different methods to find Maven\n  local mvn_path=\"\"\n\n  # Method 1: Check if mvn is in PATH\n  if command -v mvn &gt;/dev/null 2&gt;&amp;1; then\n    mvn_path=$(command -v mvn)\n  fi\n\n  # Method 2: Check common Homebrew locations\n  if [[ -z \"$mvn_path\" ]]; then\n    for version_dir in /opt/homebrew/Cellar/maven/*/libexec/bin/mvn; do\n      if [[ -x \"$version_dir\" ]]; then\n        mvn_path=\"$version_dir\"\n        break\n      fi\n    done\n  fi\n\n  # Method 3: Check /usr/local (older Homebrew installations)\n  if [[ -z \"$mvn_path\" ]]; then\n    for version_dir in /usr/local/Cellar/maven/*/libexec/bin/mvn; do\n      if [[ -x \"$version_dir\" ]]; then\n        mvn_path=\"$version_dir\"\n        break\n      fi\n    done\n  fi\n\n  # Method 4: Check system locations\n  if [[ -z \"$mvn_path\" ]]; then\n    for path in /usr/bin/mvn /usr/local/bin/mvn; do\n      if [[ -x \"$path\" ]]; then\n        mvn_path=\"$path\"\n        break\n      fi\n    done\n  fi\n\n  if [[ -z \"$mvn_path\" ]]; then\n    echo \"ERROR: Could not find Maven installation\" &gt;&amp;2\n    echo \"Please ensure Maven is installed and available in PATH or in standard locations\" &gt;&amp;2\n    exit 1\n  fi\n\n  echo \"$mvn_path\"\n}\n\n# Function to create Maven wrapper with specific Java version\ncreate_mvn_wrapper() {\n  local java_version=$1\n  local mvn_wrapper=\"/tmp/mvn-java${java_version}\"\n  local mvn_path=$(find_maven_path)\n\n  echo \"Using Maven at: $mvn_path\" &gt;&amp;2\n\n  # Create a wrapper script that sets JAVA_HOME and executes Maven\n  cat &gt; \"$mvn_wrapper\" &lt;&lt; EOF\n#!/bin/bash\nJAVA_HOME=\"\\${JAVA_HOME:-\\$(/usr/libexec/java_home -v ${java_version})}\" exec \"${mvn_path}\" \"\\$@\"\nEOF\n\n  chmod +x \"$mvn_wrapper\"\n  echo \"$mvn_wrapper\"\n}\n\n# Function to verify Java version using Maven wrapper\nverify_java_version() {\n  local mvn_wrapper=$1\n  local expected_java_version=$2\n\n  echo \"Verifying Java version with Maven wrapper...\"\n  local mvn_java_version=$($mvn_wrapper --version | grep \"Java version\" | sed 's/.*Java version: \\([0-9]*\\).*/\\1/')\n  if [[ \"$mvn_java_version\" != \"$expected_java_version\" ]]; then\n    echo \"ERROR: Maven wrapper is using Java $mvn_java_version, but expected Java $expected_java_version\"\n    echo \"Please ensure the correct Java version is installed\"\n    exit 1\n  fi\n  echo \"\u2713 Verified: Maven wrapper is using Java $mvn_java_version\"\n}\n\n# Compile for Spark 3.4 with Java 11\nJAVA_VERSION=$(get_java_version \"3.4\")\nMVN_WRAPPER=$(create_mvn_wrapper $JAVA_VERSION)\nverify_java_version $MVN_WRAPPER $JAVA_VERSION\n\necho \"Compiling for Spark 3.4 with Scala 2.12 using Java $JAVA_VERSION...\"\ncd apache-sedona-${SEDONA_VERSION}-src &amp;&amp; $MVN_WRAPPER -q clean install -DskipTests -Dspark=3.4 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-${SEDONA_VERSION}-src/spark-shaded/target/sedona-*${SEDONA_VERSION}.jar apache-sedona-${SEDONA_VERSION}-bin/\n\necho \"Compiling for Spark 3.4 with Scala 2.13 using Java $JAVA_VERSION...\"\ncd apache-sedona-${SEDONA_VERSION}-src &amp;&amp; $MVN_WRAPPER -q clean install -DskipTests -Dspark=3.4 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-${SEDONA_VERSION}-src/spark-shaded/target/sedona-*${SEDONA_VERSION}.jar apache-sedona-${SEDONA_VERSION}-bin/\n\n# Compile for Spark 3.5 with Java 11\nJAVA_VERSION=$(get_java_version \"3.5\")\nMVN_WRAPPER=$(create_mvn_wrapper $JAVA_VERSION)\nverify_java_version $MVN_WRAPPER $JAVA_VERSION\n\necho \"Compiling for Spark 3.5 with Scala 2.12 using Java $JAVA_VERSION...\"\ncd apache-sedona-${SEDONA_VERSION}-src &amp;&amp; $MVN_WRAPPER -q clean install -DskipTests -Dspark=3.5 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-${SEDONA_VERSION}-src/spark-shaded/target/sedona-*${SEDONA_VERSION}.jar apache-sedona-${SEDONA_VERSION}-bin/\n\necho \"Compiling for Spark 3.5 with Scala 2.13 using Java $JAVA_VERSION...\"\ncd apache-sedona-${SEDONA_VERSION}-src &amp;&amp; $MVN_WRAPPER -q clean install -DskipTests -Dspark=3.5 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-${SEDONA_VERSION}-src/spark-shaded/target/sedona-*${SEDONA_VERSION}.jar apache-sedona-${SEDONA_VERSION}-bin/\n\n# Compile for Spark 4.0 with Java 17\nJAVA_VERSION=$(get_java_version \"4.0\")\nMVN_WRAPPER=$(create_mvn_wrapper $JAVA_VERSION)\nverify_java_version $MVN_WRAPPER $JAVA_VERSION\n\necho \"Compiling for Spark 4.0 with Scala 2.13 using Java $JAVA_VERSION...\"\ncd apache-sedona-${SEDONA_VERSION}-src &amp;&amp; $MVN_WRAPPER -q clean install -DskipTests -Dspark=4.0 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-${SEDONA_VERSION}-src/spark-shaded/target/sedona-*${SEDONA_VERSION}.jar apache-sedona-${SEDONA_VERSION}-bin/\n\n# Clean up Maven wrappers\nrm -f /tmp/mvn-java11 /tmp/mvn-java17\n\ntar czf apache-sedona-${SEDONA_VERSION}-bin.tar.gz apache-sedona-${SEDONA_VERSION}-bin\n\n# Create checksums and signatures for binary files\nshasum -a 512 apache-sedona-${SEDONA_VERSION}-bin.tar.gz &gt; apache-sedona-${SEDONA_VERSION}-bin.tar.gz.sha512\ngpg -ab apache-sedona-${SEDONA_VERSION}-bin.tar.gz\n\necho \"Uploading binary files...\"\n\n# Upload binary files\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-bin.tar.gz https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-bin.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-${SEDONA_VERSION}-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/${RC_VERSION}/apache-sedona-${SEDONA_VERSION}-bin.tar.gz.sha512\n\necho \"Removing local release files...\"\n\nrm apache-sedona-${SEDONA_VERSION}-src.tar.gz\nrm apache-sedona-${SEDONA_VERSION}-src.tar.gz.asc\nrm apache-sedona-${SEDONA_VERSION}-src.tar.gz.sha512\nrm apache-sedona-${SEDONA_VERSION}-bin.tar.gz\nrm apache-sedona-${SEDONA_VERSION}-bin.tar.gz.asc\nrm apache-sedona-${SEDONA_VERSION}-bin.tar.gz.sha512\nrm -rf apache-sedona-${SEDONA_VERSION}-src\nrm -rf apache-sedona-${SEDONA_VERSION}-bin\n</code></pre>"},{"location":"community/publish/#5-vote-in-dev-sedonaapacheorg","title":"5. Vote in dev sedona.apache.org","text":""},{"location":"community/publish/#vote-email","title":"Vote email","text":"<p>Please add changes at the end if needed:</p> <pre><code>Subject: [VOTE] Release Apache Sedona 1.8.0-rc1\n\nHi all,\n\nThis is a call for vote on Apache Sedona 1.8.0-rc1. Please refer to the changes listed at the bottom of this email.\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.8.0-rc1/docs/setup/release-notes.md\n\nBuild instructions:\nhttps://github.com/apache/sedona/blob/sedona-1.8.0-rc1/docs/setup/compile.md\n\nGitHub tag:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.8.0-rc1\n\nGPG public key to verify the Release:\nhttps://downloads.apache.org/sedona/KEYS\n\nSource code and binaries:\nhttps://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/\n\nThe vote will be open for at least 72 hours or until at least 3 \"+1\" PMC votes are cast\n\nInstruction for checking items on the checklist: https://sedona.apache.org/latest/community/vote/\n\nWe recommend you use this Jupyter notebook on MyBinder to perform this task: https://mybinder.org/v2/gh/jiayuasu/sedona-tools/HEAD?labpath=binder%2Fverify-release.ipynb\n\n**Please vote accordingly and you must provide your checklist for your vote**.\n\n\n[ ] +1 approve\n\n[ ] +0 no opinion\n\n[ ] -1 disapprove with the reason\n\nChecklist:\n\n[ ] Download links are valid.\n\n[ ] Checksums and PGP signatures are valid.\n\n[ ] Source code artifacts have correct names matching the current release.\n\nFor a detailed checklist  please refer to:\nhttps://cwiki.apache.org/confluence/display/INCUBATOR/Incubator+Release+Checklist\n\n------------\n\nChanges according to the comments on the previous release\nOriginal comment (Permalink from https://lists.apache.org/list.html):\n</code></pre>"},{"location":"community/publish/#pass-email","title":"Pass email","text":"<p>Please count the votes and add the Permalink of the vote thread at the end.</p> <pre><code>Subject: [RESULT][VOTE] Release Apache Sedona 1.8.0-rc1\n\nDear all,\n\nThe vote closes now as 72hr have passed. The vote PASSES with\n\n+? (binding): NAME1, NAME2, NAME3\n+? (non-binding): NAME4\nNo -1 votes\n\nThe vote thread (Permalink from https://lists.apache.org/list.html):\n\nI will make an announcement soon.\n</code></pre>"},{"location":"community/publish/#announce-email","title":"Announce email","text":"<ol> <li>This email should be sent to dev@sedona.apache.org</li> <li>Please add the permalink of the vote thread</li> <li>Please add the permalink of the vote result thread</li> </ol> <pre><code>Subject: [ANNOUNCE] Apache Sedona 1.8.0 released\n\nDear all,\n\nWe are happy to report that we have released Apache Sedona 1.8.0. Thank you again for your help.\n\nApache Sedona is a cluster computing system for processing large-scale spatial data.\n\n\nVote thread (Permalink from https://lists.apache.org/list.html):\n\n\nVote result thread (Permalink from https://lists.apache.org/list.html):\n\n\nWebsite:\nhttp://sedona.apache.org/\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.8.0/docs/setup/release-notes.md\n\nDownload links:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.8.0\n\nAdditional resources:\nMailing list: dev@sedona.apache.org\nTwitter: https://twitter.com/ApacheSedona\nGitter: https://gitter.im/apache/sedona\n\nRegards,\nApache Sedona Team\n</code></pre>"},{"location":"community/publish/#7-failed-vote","title":"7. Failed vote","text":"<p>If a vote failed, do the following:</p> <ol> <li>In the vote email, say that we will create another release candidate.</li> <li>Restart from Step 3 <code>Update mkdocs.yml</code>. Please increment the release candidate ID (e.g., <code>1.8.0-rc2</code>) and update <code>sedona_create_release.current_rc</code> and <code>sedona_create_release.current_git_tag</code> in <code>mkdocs.yml</code> to generate the script listed on this webpage.</li> </ol>"},{"location":"community/publish/#8-release-source-code-and-maven-package","title":"8. Release source code and Maven package","text":""},{"location":"community/publish/#upload-releases","title":"Upload releases","text":"<pre><code>#!/bin/bash\n\necho \"Move all files in https://dist.apache.org/repos/dist/dev/sedona to https://dist.apache.org/repos/dist/release/sedona, using svn\"\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/release/sedona/1.8.0\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-src.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-src.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-src.tar.gz.sha512\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-bin.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-bin.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.8.0-rc1/apache-sedona-1.8.0-bin.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.8.0-src.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.8.0-src.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.8.0-src.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.8.0-bin.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.8.0-bin.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.8.0-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.8.0/apache-sedona-1.8.0-bin.tar.gz.sha512\nrm apache-sedona-1.8.0-src.tar.gz\nrm apache-sedona-1.8.0-src.tar.gz.asc\nrm apache-sedona-1.8.0-src.tar.gz.sha512\nrm apache-sedona-1.8.0-bin.tar.gz\nrm apache-sedona-1.8.0-bin.tar.gz.asc\nrm apache-sedona-1.8.0-bin.tar.gz.sha512\n</code></pre>"},{"location":"community/publish/#manually-close-and-release-the-package","title":"Manually close and release the package","text":"<ol> <li>Click <code>Close</code> on the Sedona staging repo on https://repository.apache.org under <code>staging repository</code></li> <li>Once the staging repo is closed, click <code>Release</code> on this repo.</li> </ol> <p>NOTICE: The staging repo will be automatically dropped after 3 days without closing. If you find the staging repo being dropped, you can re-stage the release using the following script.</p> <pre><code>#!/bin/bash\n\necho \"Re-staging releases to https://repository.apache.org\"\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# Function to get Java version for Spark version\nget_java_version() {\n  local spark_version=$1\n  if [[ \"$spark_version\" == \"4.0\" ]]; then\n    echo \"17\"\n  else\n    echo \"11\"\n  fi\n}\n\n# Function to set JAVA_HOME based on Java version\nset_java_home() {\n  local java_version=$1\n  if [[ \"$java_version\" == \"17\" ]]; then\n    # Try to find Java 17 installation\n    if command -v /usr/libexec/java_home &gt;/dev/null 2&gt;&amp;1; then\n      export JAVA_HOME=$(/usr/libexec/java_home -v 17 2&gt;/dev/null || /usr/libexec/java_home -v 1.17 2&gt;/dev/null || echo \"\")\n    fi\n    if [[ -z \"$JAVA_HOME\" ]]; then\n      echo \"Warning: Java 17 not found, using system default\"\n    else\n      echo \"Using Java 17: $JAVA_HOME\"\n    fi\n  else\n    # Try to find Java 11 installation\n    if command -v /usr/libexec/java_home &gt;/dev/null 2&gt;&amp;1; then\n      export JAVA_HOME=$(/usr/libexec/java_home -v 11 2&gt;/dev/null || /usr/libexec/java_home -v 1.11 2&gt;/dev/null || echo \"\")\n    fi\n    if [[ -z \"$JAVA_HOME\" ]]; then\n      echo \"Warning: Java 11 not found, using system default\"\n    else\n      echo \"Using Java 11: $JAVA_HOME\"\n    fi\n  fi\n\n  # Verify Java version using Maven\n  echo \"Verifying Java version with Maven...\"\n  local mvn_java_version=$(mvn --version | grep \"Java version\" | sed 's/.*Java version: \\([0-9]*\\).*/\\1/')\n  if [[ \"$mvn_java_version\" != \"$java_version\" ]]; then\n    echo \"ERROR: Maven is using Java $mvn_java_version, but expected Java $java_version\"\n    echo \"Please ensure the correct Java version is installed and JAVA_HOME is set properly\"\n    exit 1\n  fi\n  echo \"\u2713 Verified: Maven is using Java $mvn_java_version\"\n}\n\n# For Spark 3.4 and Scala 2.12 (Java 11)\nJAVA_VERSION=$(get_java_version \"3.4\")\nset_java_home $JAVA_VERSION\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.8.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13 (Java 11)\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.8.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\n# For Spark 3.5 and Scala 2.12 (Java 11)\nJAVA_VERSION=$(get_java_version \"3.5\")\nset_java_home $JAVA_VERSION\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.8.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.12\" -Dspark=3.5 -Dscala=2.12\n\n# For Spark 3.5 and Scala 2.13 (Java 11)\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.8.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.13\" -Dspark=3.5 -Dscala=2.13\n\n# For Spark 4.0 and Scala 2.13 (Java 17)\n# Note: Spark 4.0 + Scala 2.12 is not supported, so we skip it\nJAVA_VERSION=$(get_java_version \"4.0\")\nset_java_home $JAVA_VERSION\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.8.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=4.0 -Dscala=2.13\" -Dspark=4.0 -Dscala=2.13\n</code></pre>"},{"location":"community/publish/#9-release-sedona-python-and-zeppelin","title":"9. Release Sedona Python and Zeppelin","text":"<p>You must have the maintainer privilege of <code>https://pypi.org/project/apache-sedona/</code> and <code>https://www.npmjs.com/package/apache-sedona</code></p> <p>To publish Sedona pythons, you have to use GitHub actions since we release wheels for different platforms. Please use this repo: https://github.com/jiayuasu/sedona-publish-python</p> <pre><code>#!/bin/bash\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.8.0-rc1.tar.gz\ntar -xvf sedona-1.8.0-rc1.tar.gz\nmkdir apache-sedona-1.8.0-src\ncp -r sedona-sedona-1.8.0-rc1/* apache-sedona-1.8.0-src/\n\nrm -rf apache-sedona-1.8.0-rc1\n\ncd apache-sedona-1.8.0-src/zeppelin &amp;&amp; npm publish &amp;&amp; cd ..\nrm -rf apache-sedona-1.8.0-src\n</code></pre>"},{"location":"community/publish/#10-release-sedona-r-to-cran","title":"10. Release Sedona R to CRAN.","text":"<pre><code>#!/bin/bash\nR CMD build .\nR CMD check --as-cran apache.sedona_*.tar.gz\n</code></pre> <p>Then submit to CRAN using this web form.</p>"},{"location":"community/publish/#11-publish-the-doc-website","title":"11. Publish the doc website","text":"<ol> <li>Check out the 1.8.0 Git tag on your local repo to a branch namely <code>branch-1.8.0</code></li> <li>Add the download link to Download page.</li> <li>Add the news to <code>docs/index.md</code>.</li> <li>Push the changes to this branch on GitHub.</li> <li>GitHub CI will pick up the changes and deploy to <code>website</code> branch.</li> <li>Normally this repo will automatically publish the website on a daily basis.</li> </ol>"},{"location":"community/release-manager/","title":"Become a release manager","text":""},{"location":"community/release-manager/#become-a-release-manager","title":"Become a release manager","text":"<p>You only need to perform these steps if this is your first time being a release manager.</p>"},{"location":"community/release-manager/#0-software-requirement","title":"0. Software requirement","text":"<ul> <li>JDK 11/17: <code>brew install openjdk@11</code> or <code>brew install openjdk@17</code></li> <li>Maven 3.X. Your Maven must point to JDK 11 or 17. Check it by <code>mvn --version</code></li> <li>Git and SVN</li> </ul> <p>If your Maven (<code>mvn --version</code>) points to other JDK versions, you must change it to JDK 11 or 17. Steps are as follows:</p> <ol> <li>Find all Java installed on your machine: <code>/usr/libexec/java_home -V</code>. You should see multiple JDK versions including JDK 11 and 17.</li> <li>Run <code>whereis mvn</code> to get the installation location of your Maven. The result is a symlink to the actual location.</li> <li>Open it in the terminal (with <code>sudo</code> if needed). It will be like this</li> </ol> <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre> <ol> <li>Change <code>JAVA_HOME:-$(/usr/libexec/java_home)}</code> to <code>JAVA_HOME:-$(/usr/libexec/java_home -v 11)}</code> or <code>JAVA_HOME:-$(/usr/libexec/java_home -v 17)}</code>. The resulting content will be like this:</li> </ol> <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home -v 11)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre> <ol> <li>Run <code>mvn --version</code> again. It should now point to JDK 11 or 17.</li> </ol>"},{"location":"community/release-manager/#1-obtain-write-access-to-sedona-github-repo","title":"1. Obtain Write Access to Sedona GitHub repo","text":"<ol> <li>Verify you have a GitHub ID enabled with 2FA https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/</li> <li>Enter your GitHub ID into your Apache ID profile https://id.apache.org/</li> <li>Merge your Apache and GitHub accounts using GitBox (Apache Account Linking utility): https://gitbox.apache.org/setup/<ul> <li>You should see 5 green checks in GitBox</li> <li>Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation</li> </ul> </li> <li>After accepting the GitHub Invitation, verify that you are a member of the team https://github.com/orgs/apache/teams/sedona-committers</li> <li>Additionally, if you have been elected to the Sedona PMC, verify you are part of the LDAP Sedona PMC https://whimsy.apache.org/roster/pmc/sedona</li> </ol>"},{"location":"community/release-manager/#2-prepare-secret-gpg-key","title":"2. Prepare Secret GPG key","text":"<ol> <li>Install GNUPG if it was not installed before. On Mac: <code>brew install gnupg gnupg2</code></li> <li>Generate a secret key. It must be RSA4096 (4096 bits long).</li> <li>Run <code>gpg --full-generate-key</code>. If not work, run <code>gpg --default-new-key-algo rsa4096 --gen-key</code></li> <li>At the prompt, specify the kind of key you want: Select <code>RSA</code>, then press <code>enter</code></li> <li>At the prompt, specify the key size you want: Enter <code>4096</code></li> <li>At the prompt, enter the length of time the key should be valid: Press <code>enter</code> to make the key never expire.</li> <li>Verify that your selections are correct.</li> <li>Enter your user ID information: use your real name and Apache email address.</li> <li>Type a secure passphrase. Make sure you remember this because we will use it later.</li> <li>Use the <code>gpg --list-secret-keys --keyid-format=long</code> command to list the long form of the GPG keys.</li> <li>From the list of GPG keys, copy the long form of the GPG key ID you'd like to use (e.g., <code>3AA5C34371567BD2</code>)</li> <li>Run <code>gpg --export --armor 3AA5C34371567BD2</code>, substituting in the GPG key ID you'd like to use.</li> <li>Copy your GPG key, beginning with <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending with <code>-----END PGP PUBLIC KEY BLOCK-----</code>.</li> <li>There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Publish your armored key in major key servers: https://keyserver.pgp.com/</li> </ol>"},{"location":"community/release-manager/#3-use-svn-to-update-keys","title":"3. Use SVN to update KEYS","text":"<p>Use SVN to append your armored PGP public key to the <code>KEYS</code> files</p> <ul> <li>https://dist.apache.org/repos/dist/dev/sedona/KEYS</li> <li> <p>https://dist.apache.org/repos/dist/release/sedona/KEYS</p> </li> <li> <p>Check out both KEYS files</p> </li> </ul> <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/sedona/ sedona-dev --depth files\nsvn checkout https://dist.apache.org/repos/dist/release/sedona/ sedona-release --depth files\n</code></pre> <ol> <li>Use your favorite text editor to open <code>sedona-dev/KEYS</code> and <code>sedona-release/KEYS</code>.</li> <li>Paste your armored key to the end of both files. Note: There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Commit both KEYS. SVN might ask you to enter your ASF ID and password. Make sure you do it so SVN can always store your ID and password locally.</li> </ol> <pre><code>svn commit -m \"Update KEYS\" sedona-dev/KEYS\nsvn commit -m \"Update KEYS\" sedona-release/KEYS\n</code></pre> <ol> <li>Then remove both svn folders</li> </ol> <pre><code>rm -rf sedona-dev\nrm -rf sedona-release\n</code></pre>"},{"location":"community/release-manager/#4-add-gpg_tty-environment-variable","title":"4. Add GPG_TTY environment variable","text":"<p>In your <code>~/.bashrc</code> file, add the following content. Then restart your terminal.</p> <pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre>"},{"location":"community/release-manager/#5-get-github-personal-access-token-classic","title":"5. Get GitHub personal access token (classic)","text":"<p>You need to create a GitHub personal access token (classic). You can follow the instruction on GitHub.</p> <p>In short:</p> <ol> <li>On your GitHub interface -&gt; Settings</li> <li>In the left sidebar, click Developer settings.</li> <li>In the left sidebar, under  Personal access tokens, click Tokens (classic).</li> <li>Select Generate new token, then click Generate new token (classic).</li> <li>Give your token a descriptive name.</li> <li>To give your token an expiration, select the Expiration drop-down menu. Make sure you set the <code>Expiration</code> to <code>No expiration</code>.</li> <li>Select the scopes you'd like to grant this token. To use your token to access repositories from the command line, select <code>repo</code> and <code>admin:org</code>.</li> <li>Click <code>Generate token</code>.</li> <li>Please save your token somewhere because we will use it in the next step.</li> </ol>"},{"location":"community/release-manager/#6-set-up-credentials-for-maven","title":"6. Set up credentials for Maven","text":"<p>In your <code>~/.m2/settings.xml</code> file, add the following content. Please create this file or <code>.m2</code> folder if it does not exist.</p> <p>Please replace all capitalized text with your own ID and password.</p> <pre><code>&lt;settings&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;github&lt;/id&gt;\n      &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n      &lt;password&gt;YOUR_GITHUB_TOKEN&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.snapshots.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.releases.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n  &lt;profiles&gt;\n    &lt;profile&gt;\n      &lt;id&gt;gpg&lt;/id&gt;\n      &lt;properties&gt;\n        &lt;gpg.passphrase&gt;YOUR_GPG_PASSPHRASE&lt;/gpg.passphrase&gt;\n      &lt;/properties&gt;\n    &lt;/profile&gt;\n  &lt;/profiles&gt;\n  &lt;activeProfiles&gt;\n    &lt;activeProfile&gt;gpg&lt;/activeProfile&gt;\n  &lt;/activeProfiles&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"community/rule/","title":"Rules","text":""},{"location":"community/rule/#contributing-to-apache-sedona","title":"Contributing to Apache Sedona","text":"<p>The project welcomes contributions. You can contribute to Sedona code or documentation by making Pull Requests on Sedona GitHub Repo.</p> <p>The following sections brief the workflow of how to complete a contribution.</p>"},{"location":"community/rule/#pick-announce-a-task","title":"Pick / Announce a task","text":"<p>It is important to confirm that your contribution is acceptable. Before starting a contribution, you should announce your intention to the community via tickets. Sedona allows tickets from both GitHub issues and JIRA. A new JIRA ticket will be automatically sent to <code>dev@sedona.apache.org</code></p>"},{"location":"community/rule/#develop-a-code-contribution","title":"Develop a code contribution","text":"<p>Code contributions should include the following:</p> <ul> <li>Detailed documentations on classes and methods.</li> <li>Unit Tests to demonstrate code correctness and allow this to be maintained going forward. In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any). Unit Tests can be JUnit test or Scala test. Some Sedona functions need to be tested in both Scala and Java.</li> <li>Updates on corresponding Sedona documentation if necessary.</li> </ul> <p>Code contributions must include an Apache 2.0 license header at the top of each file.</p> <p>Please run <code>mvn spotless:apply</code> to format the code before making a pull request. If you've modified code for a specific spark version (for example, source files in spark/spark-3.5/), please add additional Maven CLI arguments to format that code: <code>mvn spotless:apply -Dscala=2.12 -Dspark=3.5</code>.</p>"},{"location":"community/rule/#develop-a-document-contribution","title":"Develop a document contribution","text":"<p>Documentation contributions should satisfy the following requirements:</p> <ul> <li>Detailed explanation with examples.</li> <li>Place a newly added document in a proper folder</li> <li>Change the mkdocs.yml if necessary</li> </ul> <p>Note</p> <p>Please read Compile the source code to learn how to compile Sedona website.</p>"},{"location":"community/rule/#make-a-pull-request","title":"Make a Pull Request","text":"<p>After developing a contribution, the easiest and most visible way to submit a Pull Request (PR) to the GitHub repo.</p> <p>Please use the JIRA ticket ID or GitHub Issue ID in the PR name, such as \"[SEDONA-1] my subject\" or \"\"[GH-1] my subject\".</p> <p>When creating a PR, please answer the questions in the PR template.</p> <p>When a PR is submitted, GitHub Action will check the build correctness. Please check the PR status, and fix any reported problems.</p>"},{"location":"community/rule/#review-a-pull-request","title":"Review a Pull Request","text":"<ul> <li>Every PR requires (1) at least 1 approval from a committer and (2) no disapproval from a committer. Everyone is welcome to review a PR but only the committer can make the final decision.</li> <li>Other reviewers, including community members and committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.</li> <li>Lively, polite, rapid technical debate is encouraged from everyone in the community even if the outcome may be a rejection of the entire change.</li> <li>Keep in mind that changes to more critical parts of Sedona, like Sedona core and spatial join algorithms, will be subjected to more review, and may require more testing and proof of its correctness than other changes.</li> <li>Sometimes, other changes will be merged which conflict with your pull request\u2019s changes. The PR can\u2019t be merged until the conflict is resolved. This can be resolved by resolving the conflicts by hand, then pushing the result to your branch.</li> </ul>"},{"location":"community/rule/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read Apache Software Foundation Code of Conduct.</p> <p>We expect everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF in any role to honor this code of conduct.</p>"},{"location":"community/snapshot/","title":"Publish a snapshot version","text":""},{"location":"community/snapshot/#publish-a-snapshot-version","title":"Publish a SNAPSHOT version","text":"<p>This step is to publish Maven SNAPSHOTs to https://repository.apache.org</p> <p>This is a good practice for a release manager to try out his/her credential setup.</p> <p>The detailed requirement is on ASF Infra website</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/snapshot/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run</li> </ol> <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre> <ol> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this text file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/snapshot/#1-upload-snapshot-versions","title":"1. Upload snapshot versions","text":"<p>In your Sedona GitHub repo, run this script:</p> <pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# Validate the POMs and your credential setup\nmvn -q -B clean release:prepare -Dtag=sedona-1.8.0-rc1 -DreleaseVersion=1.8.0 -DdevelopmentVersion=1.8.1-SNAPSHOT -Dresume=false -DdryRun=true -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\n# Spark 3.3 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.3 -Dscala=2.12\n\n# Spark 3.3 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.3 -Dscala=2.13\n\n# Spark 3.4 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.12\n\n# Spark 3.4 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/vote/","title":"Vote a release","text":""},{"location":"community/vote/#vote-a-sedona-release","title":"Vote a Sedona release","text":"<p>This page is for Sedona community to vote a Sedona release. The script below is tested on macOS.</p> <p>In order to vote a Sedona release, you must provide your checklist including the following minimum requirement:</p> <ul> <li>Download links are valid</li> <li>Checksums and PGP signatures are valid</li> <li>DISCLAIMER and NOTICE are included</li> <li>Source code artifacts have correct names matching the current release</li> <li>The project can compile from the source code</li> </ul> <p>To make your life easier, we have provided an online Jupyter notebook using MyBinder. Please click this button to open the notebook and verify the release: . Then you can vote <code>+1</code> in the vote email.</p> <p>If you prefer to run the steps on your local machine, please read the steps below. If you can successfully finish the steps, you will pass the items mentioned above. Then you can vote <code>+1</code> in the vote email and provide your checklist.</p>"},{"location":"community/vote/#install-necessary-software","title":"Install necessary software","text":"<ol> <li>GPG: On Mac <code>brew install gnupg gnupg2</code>. You can check in a terminal <code>gpg --version</code>.</li> <li>JDK 1.8 or 1.11. Your Mac might have many different Java versions installed. You can try to use it but not sure if it can pass. You can check in a terminal <code>java --version</code>.</li> <li>Apache Maven 3.3.1+. On Mac <code>brew install maven</code>. You can check it in a terminal <code>mvn -version</code>.</li> <li>Python3 installed on your machine. MacOS comes with Python3 by default. You can check in a terminal <code>python3 --version</code>.</li> </ol> <p>You can skip this step if you installed these software before.</p>"},{"location":"community/vote/#run-the-verify-script","title":"Run the verify script","text":"<p>Please replace SEDONA_CURRENT_RC and SEDONA_CURRENT_VERSION with the correct versions. Then paste the content in a script called <code>verify.sh</code> and re-direct the output to a file. To run a script, do the following:</p> <pre><code>#!/bin/bash\n\n## Change the permission of the script to executable\nchmod 777 verify.sh\n\n## Run and redirect the output to a file\n./verify.sh &amp;&gt; verify.out\n</code></pre> <p>The content of the <code>verify.sh</code> script is as follows. If you copy the following content, a line break is automatically added to a long line of code. Please remove it in your local script.</p> <pre><code>#!/bin/bash\n\nSEDONA_CURRENT_RC=1.8.0-rc1\nSEDONA_CURRENT_VERSION=1.8.0\n\n## Download a Sedona release\nwget -q https://downloads.apache.org/sedona/KEYS\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Verify the signature and checksum\ngpg --import KEYS\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Uncompress the source code folder\ntar -xvf apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\n\n## Compile the project from source\n(cd apache-sedona-$SEDONA_CURRENT_VERSION-src;mvn clean install -DskipTests)\n</code></pre> <ul> <li>If successful, in the output file, you should be able to see something similar to the following text. It should include <code>Good signature from</code> and the final 4 lines should be two pairs of checksum matching each other.</li> </ul> <pre><code>gpg: key 3A79A47AC26FF4CD: \"Jia Yu &lt;jiayu@apache.org&gt;\" not changed\ngpg: key 6C883CA80E7FD299: \"PawelKocinski &lt;imbruced@apache.org&gt;\" not changed\ngpg: Total number processed: 2\ngpg:              unchanged: 2\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-src.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:31 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-bin.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:42 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n</code></pre> <ul> <li>At the end of the output, you should also see the <code>BUILD SUCCESS</code> if you can compile the source code. If this step fails, you can contact Sedona PMC and see if this is just because of your environment.</li> </ul>"},{"location":"community/vote/#check-files-manually","title":"Check files manually","text":"<ol> <li> <p>Check if the downloaded files have the correct version.</p> </li> <li> <p>In the unzipped source code folder, and check if DISCLAIMER and NOTICE files and included and up to date.</p> </li> </ol>"},{"location":"setup/azure-synapse-analytics/","title":"Install on Azure Synapse Analytics","text":"<p>This tutorial will guide you through the process of installing Sedona on Azure Synapse Analytics when Data Exfiltration Protection (DEP) is enabled or when you have no internet connection from the Spark pools due to other networking constraints.</p>"},{"location":"setup/azure-synapse-analytics/#before-you-begin","title":"Before you begin","text":"<p>This tutorial focuses on getting you up and running with Sedona 1.6.1 on Spark 3.4 Python 3.10</p> <p>If you want to run newer version, you will need to dive into the detailed build and diagnose process detailed in the lower part of this document.</p>"},{"location":"setup/azure-synapse-analytics/#strong-recommendations","title":"Strong recommendations","text":"<ol> <li>Start with a clean Spark pool with no other packages installed to avoid package conflicts.</li> <li>Apache Spark pool -&gt; Apache Spark configuration: Use default configuration</li> </ol>"},{"location":"setup/azure-synapse-analytics/#sedona-161-on-spark-34-python-310","title":"Sedona 1.6.1 on Spark 3.4 Python 3.10","text":""},{"location":"setup/azure-synapse-analytics/#step1-download-packages-9","title":"Step1: Download packages (9)","text":"<p>Caution: Precise versions are critical, latest is not always greatest here.</p> <p>From Maven</p> <ul> <li> <p>sedona-spark-shaded-3.4_2.12-1.6.1.jar</p> </li> <li> <p>geotools-wrapper-1.6.1-28.2.jar</p> </li> </ul> <p>From PyPI</p> <ul> <li> <p>rasterio-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>apache_sedona-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>click_plugins-1.1.1-py2.py3-none-any.whl</p> </li> <li> <p>cligj-0.7.2-py3-none-any.whl</p> </li> <li> <p>affine-2.4.0-py3-none-any.whl</p> </li> <li> <p>numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> </ul>"},{"location":"setup/azure-synapse-analytics/#step-2-upload-packages-to-synapse-workspace","title":"Step 2: Upload packages to Synapse Workspace","text":"<p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-workspace-packages</p>"},{"location":"setup/azure-synapse-analytics/#step-3-add-packages-to-spark-pool","title":"Step 3: Add packages to Spark Pool","text":"<p>This tutorial used the second method on this page: If you are updating from the Synapse Studio</p> <p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-pool-packages#manage-packages-from-synapse-studio-or-azure-portal</p>"},{"location":"setup/azure-synapse-analytics/#step-4-notebook","title":"Step 4: Notebook","text":"<p>Start your notebook with:</p> <pre><code>from sedona.spark import SedonaContext\n\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-shaded-3.4_2.12-1.6.1,\"\n        \"org.datasyslab:geotools-wrapper-1.6.1-28.2\",\n    )\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n    .config(\n        \"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\"\n    )\n    .config(\n        \"spark.sql.extensions\",\n        \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Run a test</p> <pre><code>sedona.sql(\"SELECT ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)')\").show()\n</code></pre> <p>If you see the output of the point, then the installation is successful. Are you are all done with the setup.</p>"},{"location":"setup/azure-synapse-analytics/#packages-for-sedona-160-on-spark-34-python-10","title":"Packages for Sedona 1.6.0 on Spark 3.4 Python 10","text":"<pre><code>spark-xml_2.12-0.17.0.jar\nsedona-spark-shaded-3.4_2.12-1.6.0.jar\n\nclick_plugins-1.1.1-py2.py3-none-any.whl\naffine-2.4.0-py3-none-any.whl\napache_sedona-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ncligj-0.7.2-py3-none-any.whl\nrasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl\nshapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nsnuggs-1.4.7-py3-none-any.whl\ngeotools-wrapper-1.6.0-28.2.jar\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#background-how-to-identify-packages-for-otherfuture-versions-of-spark-andor-sedona","title":"Background: How to identify packages for other/future versions of Spark and/or Sedona","text":"<p>Warning: this process is going to require some tenacious technical skills and troubleshooting.</p> <p>Broad steps: build a linux VM from the same image as the deployed Spark Pool, configure for Synapse, install Sedona packages, identify required packages over and above baseline Synapse config.</p> <p>This is the process for Sedona 1.6.1 on Spark 3.4 Python 3.10. (The same process was used for Sedona 1.6.0)</p>"},{"location":"setup/azure-synapse-analytics/#step-1-identify-the-linux-image-of-the-spark-pool-by-version","title":"Step 1: Identify the Linux image of the Spark Pool by version","text":"<p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-34-runtime</p>"},{"location":"setup/azure-synapse-analytics/#step-2-download-the-iso","title":"Step 2 : Download the ISO","text":"<p>https://github.com/microsoft/azurelinux/tree/2.0</p>"},{"location":"setup/azure-synapse-analytics/#step-3-build-the-vm","title":"Step 3: build the VM","text":"<p>https://github.com/microsoft/azurelinux/blob/2.0/toolkit/docs/quick_start/quickstart.md#iso-image</p> <p>Important settings if using Hyper-V</p> <ul> <li>Enable Secure Boot: Microsoft UEFI Certificate authority</li> <li>Cores 2</li> <li>Disable Dynamic Memory (fix at 8Gb), forgetting this setting causes havoc.</li> </ul>"},{"location":"setup/azure-synapse-analytics/#step-4-patch-the-vm","title":"Step 4: patch the VM","text":"<p>Connect the VM. Note: it will take longer to first boot than you'd expect</p> <pre><code>sudo dnf upgrade\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-5-optional-but-strongly-recommended-install-ssh-server-for-best-copy-and-paste-experience","title":"Step 5: optional but strongly recommended - install ssh-server (for best copy and paste experience)","text":"<pre><code>sudo tdnf install -y openssh-server\n</code></pre> <p>Enable root and password auth</p> <pre><code>sudo vi /etc/ssh/sshd_config\n-   PasswordAuthentication yes\n-   PermitRootLogin yes\n</code></pre> <p>Start ssh-server</p> <pre><code>sudo systemctl enable --now sshd.service\n</code></pre> <p>Identify the ip of the VM (I'm using Hyper-V on windows 10 desktop)</p> <pre><code>Get-VMNetworkAdapter -VMName \"Synapse Spark 3.4 Python 3.10 Sedona 1.6.1\" | Select-Object -ExpandProperty IPAddresses\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-6-install-miniconda","title":"Step 6: install Miniconda","text":"<pre><code>cd /tmp\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-7-install-compilers","title":"Step 7: install compilers","text":"<pre><code>sudo tdnf -y install gcc g++\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-8-create-baseline-synapse-virtual-env","title":"Step 8: create baseline synapse virtual env","text":"<p>Download the virtual env spec</p> <pre><code>wget -O Synapse-Python310-CPU.yml https://raw.githubusercontent.com/microsoft/synapse-spark-runtime/refs/heads/main/Synapse/spark3.4/Synapse-Python310-CPU.yml source\n</code></pre> <pre><code>conda env create -f Synapse-Python310-CPU.yml -n synapse\n</code></pre> <p>if you get errors due to <code>fsspec_wrapper</code> then remove <code>fsspec_wrapper==0.1.13=py_3</code> from the yml and run again</p> <p>if you get further but different errors from <code>pip</code> after making the above change, ignore them you can still proceed</p>"},{"location":"setup/azure-synapse-analytics/#step-9-install-sedona-python-packages","title":"Step 9: install sedona python packages","text":"<pre><code>conda activate synapse\necho \"apache-sedona==1.6.1\" &gt; requirements.txt\npip install -r requirements.txt &gt; pip-output.txt\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-10-identify-python-packages-to-download","title":"Step 10: identify Python packages to download","text":"<pre><code>grep Downloading pip-output.txt\n</code></pre> <p>This will be the list of packages you need to locate and download from PyPI</p> <p>Example output</p> <pre><code>Downloading apache_sedona-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\nDownloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\nDownloading rasterio-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\nDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-11-identify-package-conflicts-in-your-deployed-azure-synapse-spark-pool-the-real-one-not-the-vm","title":"Step 11: identify package conflicts in your deployed Azure Synapse Spark Pool (the real one, not the VM)","text":"<ul> <li>upload packages to workspace</li> <li>add packages to your (clean!) Spark pool</li> </ul> <p>Pay careful attention to errors reported back from Synapse and troubleshoot to resolve conflicts.</p> <p>Note: We didn't have issues with Sedona 1.6.0 on Spark 3.4, but Sedona 1.6.1 and supporting packages had a conflict around <code>numpy</code> which requires us to download a specific version and add it to the packages list. <code>numpy</code> was not listed in the output of the grep.</p>"},{"location":"setup/cluster/","title":"Set up Spark cluster manually","text":""},{"location":"setup/cluster/#set-up-your-apache-spark-cluster","title":"Set up your Apache Spark cluster","text":"<p>Download a Spark distribution from Spark download page.</p>"},{"location":"setup/cluster/#preliminary","title":"Preliminary","text":"<ol> <li>Set up a password-less SSH on your cluster. Each master-worker pair should have bidirectional password-less SSH.</li> <li>Make sure you have installed JRE 1.8 or later.</li> <li>Add the list of your workers' IP address in ./conf/slaves</li> <li>Besides the necessary Spark settings, you may need to add the following lines in the Spark configuration files to avoid Sedona memory errors:</li> </ol> <p>In <code>./conf/spark-defaults.conf</code></p> <pre><code>spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n</code></pre> <ul> <li><code>spark.driver.memory</code> tells Spark to allocate enough memory for the driver program because Sedona needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.</li> <li><code>spark.network.timeout</code> is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.</li> <li><code>spark.driver.maxResultSize</code> is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.</li> </ul> <p>For more details of Spark parameters, please visit Spark Website.</p>"},{"location":"setup/cluster/#start-your-cluster","title":"Start your cluster","text":"<p>Go the root folder of the uncompressed Apache Spark folder. Start your Spark cluster via a terminal</p> <pre><code>./sbin/start-all.sh\n</code></pre>"},{"location":"setup/compile/","title":"Compile the code","text":""},{"location":"setup/compile/#compile-sedona-source-code","title":"Compile Sedona source code","text":""},{"location":"setup/compile/#compile-scala-java-source-code","title":"Compile Scala / Java source code","text":"<p>Sedona Scala/Java code is a project with multiple modules. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.</p> <ul> <li>Make sure your Linux/Mac machine has Java 11/17, Apache Maven 3.3.1+, and Python3.8+. The compilation of Sedona is not tested on Windows machines.</li> </ul> <p>To compile all modules, please make sure you are in the root folder of all modules. Then enter the following command in the terminal:</p> Without unit testsWith unit testsWith Geotools jars packaged <p><pre><code>mvn clean install -DskipTests\n</code></pre> This command will first delete the old binary files and compile all modules. This compilation will skip the unit tests. To compile a single module, please make sure you are in the folder of that module. Then enter the same command.</p> <p><pre><code>mvn clean install\n</code></pre> The maven unit tests of all modules may take up to 30 minutes.</p> <p><pre><code>mvn clean install -DskipTests -Dgeotools\n</code></pre> Geotools jars will be packaged into the produced fat jars.</p> <p>Note</p> <p>By default, this command will compile Sedona with Spark 3.4 and Scala 2.12</p>"},{"location":"setup/compile/#compile-with-different-targets","title":"Compile with different targets","text":"<p>User can specify <code>-Dspark</code> and <code>-Dscala</code> command line options to compile with different targets. Available targets are:</p> <ul> <li><code>-Dspark</code>: <code>{major}.{minor}</code>: For example, specify <code>-Dspark=3.4</code> to build for Spark 3.4.</li> <li><code>-Dscala</code>: <code>2.12</code> or <code>2.13</code></li> </ul> Spark 3.4+ Scala 2.12Spark 3.4+ Scala 2.13 <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.12\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre> Please replace <code>3.4</code> with Spark major.minor version when building for higher Spark versions.</p> <p>Tip</p> <p>To get the Sedona Spark Shaded jar with all GeoTools jars included, simply append <code>-Dgeotools</code> option. The command is like this:<code>mvn clean install -DskipTests -Dscala=2.12 -Dspark=3.4 -Dgeotools</code></p>"},{"location":"setup/compile/#download-staged-jars","title":"Download staged jars","text":"<p>Sedona uses GitHub Actions to automatically generate jars per commit. You can go here and download the jars by clicking the commits Artifacts tag.</p>"},{"location":"setup/compile/#run-python-test","title":"Run Python test","text":"<p>1) Set up Spark (download if needed) and environment variables</p> <pre><code>export SPARK_VERSION=3.4.0   # or another supported version\nwget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz\ntar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz\nrm spark-${SPARK_VERSION}-bin-hadoop3.tgz\nexport SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-hadoop3\nexport PYTHONPATH=$SPARK_HOME/python\n</code></pre> <p>2) Add required JAI jars into $SPARK_HOME/jars</p> <pre><code>export JAI_CORE_VERSION=\"1.1.3\"\nexport JAI_CODEC_VERSION=\"1.1.3\"\nexport JAI_IMAGEIO_VERSION=\"1.1\"\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_core/${JAI_CORE_VERSION}/jai_core-${JAI_CORE_VERSION}.jar\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_codec/${JAI_CODEC_VERSION}/jai_codec-${JAI_CODEC_VERSION}.jar\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_imageio/${JAI_IMAGEIO_VERSION}/jai_imageio-${JAI_IMAGEIO_VERSION}.jar\n</code></pre> <p>3) Build Sedona Scala/Java jars with GeoTools shaded (from repo root)</p> <pre><code>mvn clean install -DskipTests -Dgeotools\ncp spark-shaded/target/sedona-spark-shaded-*.jar $SPARK_HOME/jars/\n</code></pre> <p>4) Setup Python development environment</p> <p>The Python package uses <code>pyproject.toml</code> (PEP 517/518) with setuptools as the build backend. We recommend using uv to manage virtual environments and dependencies.</p> <pre><code>cd python\npython -m pip install --upgrade uv\nuv venv --python 3.10   # or any supported version (&gt;=3.8)\n</code></pre> <p>5) Install the PySpark version and the other dependency</p> <pre><code>cd python\n# Use the correct PySpark version, otherwise latest version will be installed\nuv add pyspark==${SPARK_VERSION} --optional spark\nuv sync\n</code></pre> <p>6) Install Sedona (editable) and run the Python tests</p> <pre><code>cd python\nuv pip install -e .\nuv run pytest -v tests\n</code></pre>"},{"location":"setup/compile/#compile-the-documentation","title":"Compile the documentation","text":"<p>The website is automatically built after each commit. The built website can be downloaded here:</p>"},{"location":"setup/compile/#mkdocs-website","title":"MkDocs website","text":"<p>The source code of the documentation website is written in Markdown and then compiled by MkDocs. The website is built upon the Material for MkDocs template.</p> <p>In the Sedona repository, the MkDocs configuration file mkdocs.yml is in the root folder and all documentation source code is in docs folder.</p> <p>To compile the source code and test the website on your local machine, please read the MkDocs Tutorial and Materials for MkDocs Tutorial.</p> <p>In short, you need to run:</p> <pre><code>python3 -m pip install uv\nuv sync --group docs\n</code></pre> <p>After installing MkDocs and MkDocs-Material, run these commands in the Sedona root folder:</p> <pre><code>uv run mkdocs build\nuv run mike deploy --update-aliases latest-snapshot -b website -p\nuv run mike serve\n</code></pre>"},{"location":"setup/compile/#pre-commit","title":"pre-commit","text":"<p>We run pre-commit with GitHub Actions so installation on your local machine is currently optional.</p> <p>The pre-commit configuration file is in the repository root. Before you can run the hooks, you need to have pre-commit installed.</p> <p>The hooks run when running <code>git commit</code> and also from the command line with <code>pre-commit</code>. Some of the hooks will auto fix the code after the hooks fail whilst most will print error messages from the linters. If a hook fails the overall commit will fail, and you will need to fix the issues or problems and <code>git add</code> and git commit again. On git commit the hooks will run mostly only against modified files so if you want to test all hooks against all files and when you are adding a new hook you should always run:</p> <p><code>pre-commit run --all-files</code></p> <p>Sometimes you might need to skip a hook to commit because the hook is stopping you from committing or your computer might not have all the installation requirements for all the hooks. The <code>SKIP</code> variable is comma separated for two or more hooks:</p> <p><code>SKIP=codespell git commit -m \"foo\"</code></p> <p>The same applies when running pre-commit:</p> <p><code>SKIP=codespell pre-commit run --all-files</code></p> <p>Occasionally you can have more serious problems when using <code>pre-commit</code> with <code>git commit</code>. You can use <code>--no-verify</code> to commit and stop <code>pre-commit</code> from checking the hooks. For example:</p> <p><code>git commit --no-verify -m \"foo\"</code></p> <p>If you just want to run one hook for example just run the <code>markdownlint</code> hook:</p> <p><code>pre-commit run markdownlint --all-files</code></p> <p>We have a Makefile in the repository root which has three pre-commit convenience commands.</p> <p>For example, you can run the following to setup pre-commit to run before each commit</p> <pre><code>make checkinstall\n</code></pre>"},{"location":"setup/databricks/","title":"Install on Databricks","text":"<p>You can run Sedona in Databricks to leverage the functionality that Sedona provides.  Here\u2019s an example of a Databricks notebook that\u2019s running Sedona code:</p> <p></p> <p>Sedona isn\u2019t available in all Databricks environments because of the platform's limitations. This post explains how and where you can run Sedona in Databricks.</p>"},{"location":"setup/databricks/#databricks-and-sedona-version-requirements","title":"Databricks and Sedona version requirements","text":"<p>Databricks and Sedona depend on Spark, Scala, and other libraries.</p> <p>For example, one Databricks Runtime 16.4 depends on Scala 2.12 and Spark 3.5.  Here are the version requirements for a few Databricks runtimes.</p> <p></p> <p>If you use a Databricks Runtime compiled with Spark 3.5 and Scala 2.12, then you should use a Sedona version compiled with Spark 3.5 and Scala 2.12.  You need to make sure the Scala versions are aligned, even if you\u2019re using the Python or SQL APIs.</p>"},{"location":"setup/databricks/#install-the-sedona-library-in-databricks","title":"Install the Sedona library in Databricks","text":"<p>Download the required Sedona packages by executing the following commands:</p> <pre><code>%sh\n# Create JAR directory for Sedona\nmkdir -p /Workspace/Shared/sedona/1.8.0\n\n# Download the dependencies from Maven into DBFS\ncurl -o /Workspace/Shared/sedona/1.8.0/geotools-wrapper-1.8.0-33.1.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.8.0-33.1/geotools-wrapper-1.8.0-33.1.jar\"\n\ncurl -o /Workspace/Shared/sedona/1.8.0/sedona-spark-shaded-3.5_2.12-1.8.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.5_2.12/1.8.0/sedona-spark-shaded-3.5_2.12-1.8.0.jar\"\n</code></pre> <p>Here are the software versions used to compile <code>sedona-spark-shaded-3.5_2.12-1.8.0.jar</code>:</p> <ul> <li>Spark 3.5</li> <li>Scala 2.12</li> <li>Sedona 1.8.0</li> </ul> <p>Ensure that you use a Databricks Runtime with versions compatible with this jar.</p> <p>You will be able to see these in your Databricks environment after downloading them:</p> <p></p> <p>Create an init script as follows:</p> <pre><code>%sh\n\n# Create init script directory for Sedona\nmkdir -p /Workspace/Shared/sedona/\n\n# Create init script\ncat &gt; /Workspace/Shared/sedona/sedona-init.sh &lt;&lt;'EOF'\n#!/bin/bash\n#\n# File: sedona-init.sh\n#\n# On cluster startup, this script will copy the Sedona jars to the cluster's default jar directory.\n\ncp /Workspace/Shared/sedona/1.8.0/*.jar /databricks/jars\n\nEOF\n</code></pre>"},{"location":"setup/databricks/#create-a-databricks-cluster","title":"Create a Databricks cluster","text":"<p>You need to create a Databricks cluster compatible with the Sedona JAR files.  If you use Sedona JAR files compiled with Scala 2.12, you must use a Databricks cluster that runs Scala 2.12.</p> <p>Go to the compute tab and configure the cluster:</p> <p></p> <p>Set the proper cluster configurations:</p> <p></p> <p>Here\u2019s a list of the cluster configurations that\u2019s easy to copy and paste:</p> <pre><code>spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.sedona.core.serde.SedonaKryoRegistrator\nspark.sedona.enableParserExtension false\n</code></pre> <p>Specify the path to the init script:</p> <p></p> <p>If you are creating a Shared cluster, you won't be able to use init scripts and jars stored under Workspace. Please store them in volumes instead. The overall process should be the same.</p> <p>Add the required dependencies in the Library tab:</p> <p></p> <p>Here\u2019s the full list of libraries:</p> <pre><code>apache-sedona==1.8.0\ngeopandas==1.0.1\nkeplergl==0.3.7\npydeck==0.9.1\n</code></pre> <p>Then click \u201cCreate compute\u201d to start the cluster.</p>"},{"location":"setup/databricks/#create-a-databricks-notebook","title":"Create a Databricks notebook","text":"<p>Create a Databricks notebook and connect it to the cluster.  Verify that you can run a Python computation with a Sedona function:</p> <p></p> <p>If you want to use Sedona Python functions such as DataFrame APIs or StructuredAdapter, you need to initialize Sedona as follows:</p> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(spark)\n</code></pre> <p>You can also use the SQL API as follows:</p> <p></p>"},{"location":"setup/databricks/#saving-geometry-in-databricks-delta-lake-tables","title":"Saving geometry in Databricks Delta Lake tables","text":"<p>Here\u2019s how to create a Sedona DataFrame with a geometry column</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"a\", \"POLYGON((1.0 1.0,1.0 3.0,2.0 3.0,2.0 1.0,1.0 1.0))\"),\n        (\"b\", \"LINESTRING(4.0 1.0,4.0 2.0,6.0 4.0)\"),\n        (\"c\", \"POINT(9.0 2.0)\"),\n    ],\n    [\"id\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(geometry)\"))\n</code></pre> <p>Write the Sedona DataFrame to a Delta Lake table:</p> <pre><code>df.write.saveAsTable(\"your_org.default.geotable\")\n</code></pre> <p>Here\u2019s how to read the table: <code>sedona.table(\"your_org.default.geotable\").display()</code></p> <p>This is what the results look like in Databricks:</p> <p></p>"},{"location":"setup/databricks/#known-bugs","title":"Known bugs","text":"<p>To ensure stability, we recommend using a currently supported Long-Term Support (LTS) version, such as Databricks Runtime 16.4 LTS or 15.4 LTS.  Some Databricks Runtimes, such as 16.2 (non-LTS), are not compatible with Apache Sedona, as this particular runtime introduced a change in the json4s dependency.</p>"},{"location":"setup/docker/","title":"Play Sedona in Docker","text":""},{"location":"setup/docker/#sedona-jupyterlab-docker-image","title":"Sedona JupyterLab Docker Image","text":"<p>Sedona Docker images are available on Sedona official DockerHub repo.</p> <p>We provide a Docker image for Apache Sedona with Python JupyterLab, Apache Zeppelin and 1 master node and 1 worker node.</p>"},{"location":"setup/docker/#how-to-use","title":"How to use","text":""},{"location":"setup/docker/#pull-the-image-from-dockerhub","title":"Pull the image from DockerHub","text":"<p>Format:</p> <pre><code>docker pull apache/sedona:&lt;sedona_version&gt;\n</code></pre> <p>Example 1: Pull the latest image of Sedona master branch</p> <pre><code>docker pull apache/sedona:latest\n</code></pre> <p>Example 2: Pull the image of a specific Sedona release</p> <pre><code>docker pull apache/sedona:1.8.0\n</code></pre>"},{"location":"setup/docker/#start-the-container","title":"Start the container","text":"<p>Format:</p> <pre><code>docker run -d -e DRIVER_MEM=&lt;driver_mem&gt; -e EXECUTOR_MEM=&lt;executor_mem&gt; -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 -p 8085:8085 apache/sedona:&lt;sedona_version&gt;\n</code></pre> <p>Driver memory and executor memory are optional. If their values are not given, the container will take 4GB RAM for the driver and 4GB RAM for the executor. The -d (or --detach) flag ensures the container runs in detached mode, allowing it to run in the background.</p> <p>Example 1:</p> <pre><code>docker run -d -e DRIVER_MEM=6g -e EXECUTOR_MEM=8g -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 -p 8085:8085 apache/sedona:latest\n</code></pre> <p>This command will start a container with 6GB RAM for the driver and 8GB RAM for the executor and use the latest Sedona image. The container will run in detached mode.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040, 8085 to the host's ports 8888, 8080, 8081, 4040, 8085 respectively.</p> <p>Example 2:</p> <pre><code>docker run -d -e -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 -p 8085:8085 apache/sedona:1.8.0\n</code></pre> <p>This command will start a container with 4GB RAM for the driver and 4GB RAM for the executor and use Sedona 1.8.0 image.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040, 8085 to the host's ports 8888, 8080, 8081, 4040, 8085 respectively.</p> <p>Example 3: Persisting <code>/opt</code> (Jupyter &amp; Zeppelin Data) with Docker Volume</p> <p>To ensure that Jupyter workspace, Zeppelin notebooks, and configurations persist, mount <code>/opt</code> as a Docker volume:</p> <pre><code>docker run -d -e DRIVER_MEM=6g -e EXECUTOR_MEM=8g \\\n    -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 -p 8085:8085 \\\n    -v sedona_opt:/opt \\\n    apache/sedona:latest\n</code></pre> <ul> <li>The <code>-v sedona_opt:/opt</code> flag creates (if not existing) and mounts a Docker volume named <code>sedona_opt</code> to the <code>/opt</code> directory inside the container.</li> <li>This ensures that Jupyter and Zeppelin notebooks, configurations, and workspaces persist even if the container is stopped or removed.</li> </ul>"},{"location":"setup/docker/#start-coding","title":"Start coding","text":"<p>Open your browser and go to http://localhost:8888/ to start coding with Sedona in Jupyter Notebook. You can also access Apache Zeppelin at http://localhost:8085/classic/ using your browser.</p>"},{"location":"setup/docker/#notes","title":"Notes","text":"<ul> <li>This container assumes you have at least 8GB RAM and takes all your CPU cores and 8GM RAM. The 1 worker will take 4GB and the Jupyter program will take the remaining 4GB.</li> <li>Sedona in this container runs in the cluster mode. Only 1 notebook can be run at a time. If you want to run another notebook, please shut down the kernel of the current notebook first (How?).</li> </ul>"},{"location":"setup/docker/#how-to-build","title":"How to build","text":"<p>Clone the Sedona GitHub repository</p>"},{"location":"setup/docker/#build-the-image-against-a-sedona-release","title":"Build the image against a Sedona release","text":"<p>Requirements: docker (How?)</p> <p>Format:</p> <pre><code>./docker/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/build.sh 3.4.1 1.8.0\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#build-the-image-against-the-latest-sedona-master","title":"Build the image against the latest Sedona master","text":"<p>Requirements: docker (How?), JDK &lt;= 19, maven3</p> <p>Format:</p> <pre><code>./docker/build.sh &lt;spark_version&gt; latest &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/build.sh 3.4.1 latest\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#notes_1","title":"Notes","text":"<p>This docker image can only be built against Sedona 1.7.0+ and Spark 3.3+</p>"},{"location":"setup/docker/#cluster-configuration","title":"Cluster Configuration","text":""},{"location":"setup/docker/#software","title":"Software","text":"<ul> <li>OS: Ubuntu 22.02</li> <li>JDK: openjdk-19</li> <li>Python: 3.10</li> <li>Spark 3.5.5</li> </ul>"},{"location":"setup/docker/#web-ui","title":"Web UI","text":"<ul> <li>JupyterLab: http://localhost:8888/</li> <li>Spark master URL: spark://localhost:7077</li> <li>Spark job UI: http://localhost:4040</li> <li>Spark master web UI: http://localhost:8080/</li> <li>Spark work web UI: http://localhost:8081/</li> <li>Apache Zeppelin: http://localhost:8085/</li> </ul> <p>A Zeppelin tutorial notebook is bundled with Sedona tutorials. See Sedona-Zeppelin tutorial for details.</p>"},{"location":"setup/docker/#how-to-push-to-dockerhub","title":"How to push to DockerHub","text":"<p>Format:</p> <pre><code>docker login\n./docker/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; release\n</code></pre> <p>Example:</p> <pre><code>docker login\n./docker/build.sh 3.4.1 1.8.0 release\n</code></pre>"},{"location":"setup/emr/","title":"Install on AWS EMR","text":"<p>We recommend Sedona-1.3.1-incubating and above for EMR. In the tutorial, we use AWS Elastic MapReduce (EMR) 6.9.0. It has the following applications installed: Hadoop 3.3.3, JupyterEnterpriseGateway 2.6.0, Livy 0.7.1, Spark 3.3.0.</p> <p>This tutorial is tested on EMR on EC2 with EMR Studio (notebooks). EMR on EC2 uses YARN to manage resources.</p> <p>Note</p> <p>If you are using Spark 3.4+ and Scala 2.12, please use <code>sedona-spark-shaded-3.4_2.12</code>. Please pay attention to the Spark version postfix and Scala version postfix.</p>"},{"location":"setup/emr/#prepare-initialization-script","title":"Prepare initialization script","text":"<p>In your S3 bucket, add a script that has the following content:</p> <pre><code>#!/bin/bash\n\n# EMR clusters only have ephemeral local storage. It does not really matter where we store the jars.\nsudo mkdir /jars\n\n# Download Sedona jar\nsudo curl -o /jars/sedona-spark-shaded-3.3_2.12-1.8.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.8.0/sedona-spark-shaded-3.3_2.12-1.8.0.jar\"\n\n# Download GeoTools jar\nsudo curl -o /jars/geotools-wrapper-1.8.0-33.1.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.8.0-33.1/geotools-wrapper-1.8.0-33.1.jar\"\n\n# Install necessary python libraries\nsudo python3 -m pip install pandas\nsudo python3 -m pip install shapely\nsudo python3 -m pip install geopandas\nsudo python3 -m pip install keplergl==0.3.2\nsudo python3 -m pip install pydeck==0.8.0\nsudo python3 -m pip install attrs matplotlib descartes apache-sedona==1.8.0\n</code></pre> <p>When you create an EMR cluster, in the <code>bootstrap action</code>, specify the location of this script.</p>"},{"location":"setup/emr/#add-software-configuration","title":"Add software configuration","text":"<p>When you create an EMR cluster, in the software configuration, add the following content:</p> <pre><code>[\n  {\n    \"Classification\":\"spark-defaults\",\n    \"Properties\":{\n      \"spark.yarn.dist.jars\": \"/jars/sedona-spark-shaded-3.3_2.12-1.8.0.jar,/jars/geotools-wrapper-1.8.0-33.1.jar\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n      \"spark.sql.extensions\": \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n      }\n  }\n]\n</code></pre>"},{"location":"setup/emr/#verify-installation","title":"Verify installation","text":"<p>After the cluster is created, you can verify the installation by running the following code in a Jupyter notebook:</p> <pre><code>spark.sql(\"SELECT ST_Point(0, 0)\").show()\n</code></pre> <p>Note that: you don't need to run the <code>SedonaRegistrator.registerAll(spark)</code> or <code>SedonaContext.create(spark)</code> because <code>org.apache.sedona.sql.SedonaSqlExtensions</code> in the config will take care of that.</p>"},{"location":"setup/fabric/","title":"Install on Microsoft Fabric","text":"<p>This tutorial will guide you through the process of installing Sedona on Microsoft Fabric Synapse Data Engineering's Spark environment.</p>"},{"location":"setup/fabric/#step-1-open-microsoft-fabric-synapse-data-engineering","title":"Step 1: Open Microsoft Fabric Synapse Data Engineering","text":"<p>Go to the Microsoft Fabric portal and choose the <code>Data Engineering</code> option.</p> <p></p>"},{"location":"setup/fabric/#step-2-create-a-microsoft-fabric-data-engineering-environment","title":"Step 2: Create a Microsoft Fabric Data Engineering environment","text":"<p>On the left side, click <code>My Workspace</code> and then click <code>+ New</code> to create a new <code>Environment</code>. Let's name it <code>ApacheSedona</code>.</p> <p></p>"},{"location":"setup/fabric/#step-3-select-the-apache-spark-version","title":"Step 3: Select the Apache Spark version","text":"<p>In the <code>Environment</code> page, click the <code>Home</code> tab and select the appropriate version of Apache Spark. You will need this version to install the correct version of Apache Sedona.</p> <p></p>"},{"location":"setup/fabric/#step-4-install-the-sedona-python-package","title":"Step 4: Install the Sedona Python package","text":"<p>In the <code>Environment</code> page, click the <code>Public libraries</code> tab and then type in <code>apache-sedona</code>. Please select the appropriate version of Apache Sedona. The source is <code>PyPI</code>.</p> <p></p>"},{"location":"setup/fabric/#step-5-set-spark-properties","title":"Step 5: Set Spark properties","text":"<p>In the <code>Environment</code> page, click the <code>Spark properties</code> tab, then create the following 3 properties:</p> <ul> <li><code>spark.sql.extensions</code>: <code>org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code></li> <li><code>spark.serializer</code>: <code>org.apache.spark.serializer.KryoSerializer</code></li> <li><code>spark.kryo.registrator</code>: <code>org.apache.sedona.core.serde.SedonaKryoRegistrator</code></li> </ul> <p></p>"},{"location":"setup/fabric/#step-6-save-and-publish-the-environment","title":"Step 6: Save and publish the environment","text":"<p>Click the <code>Save</code> button and then click the <code>Publish</code> button to save and publish the environment. This will create the environment with the Apache Sedona Python package installed. The publishing process will take about 10 minutes.</p> <p></p>"},{"location":"setup/fabric/#step-7-find-the-download-links-of-sedona-jars","title":"Step 7: Find the download links of Sedona jars","text":"<ol> <li>Learn the Sedona jars you need from our Sedona maven coordinate</li> <li>Find the <code>sedona-spark-shaded</code> jar from Maven Central. Please pay attention to the Spark version and Scala version of the jars. If you select Spark 3.4 in the Fabric environment, you should download the Sedona jars with Spark 3.4 and Scala 2.12 and the jar name should be like <code>sedona-spark-shaded-3.4_2.12-1.5.1.jar</code>.</li> <li>Find the <code>geotools-wrapper</code> jar from Maven Central. Please pay attention to the Sedona versions of the jar. If you select Sedona 1.5.1, you should download the <code>geotools-wrapper</code> jar with version 1.5.1 and the jar name should be like <code>geotools-wrapper-1.5.1-28.2.jar</code>.</li> </ol> <p>The download links are like:</p> <pre><code>https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar\nhttps://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\n</code></pre>"},{"location":"setup/fabric/#step-8-start-the-notebook-with-the-sedona-environment-and-install-the-jars","title":"Step 8: Start the notebook with the Sedona environment and install the jars","text":"<p>In the notebook page, select the <code>ApacheSedona</code> environment you created before.</p> <p></p> <p>In the notebook, you can install the jars by running the following code. Please replace the <code>jars</code> with the download links of the 2 jars from the previous step.</p> <pre><code>%%configure -f\n{\n    \"jars\": [\"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\", \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar\"]\n}\n</code></pre>"},{"location":"setup/fabric/#step-9-verify-the-installation","title":"Step 9: Verify the installation","text":"<p>You can verify the installation by running the following code in the notebook.</p> <pre><code>from sedona.spark import *\n\n\nsedona = SedonaContext.create(spark)\n\n\nsedona.sql(\"SELECT ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)')\").show()\n</code></pre> <p>If you see the output of the point, then the installation is successful.</p> <p></p>"},{"location":"setup/fabric/#optional-manually-upload-sedona-jars-to-the-fabric-environment-lakehouse-storage","title":"Optional: manually upload Sedona jars to the Fabric environment LakeHouse storage","text":"<p>If your cluster has no internet access or you want to skip the slow on-the-fly download, you can manually upload the Sedona jars to the Fabric environment LakeHouse storage.</p> <p>In the notebook page, choose the <code>Explorer</code> and click the <code>LakeHouses</code> option. If you don't have a LakeHouse, you can create one. Then choose <code>Files</code> and upload the 2 jars you downloaded in the previous step.</p> <p>After the upload, you should be able to see the 2 jars in the LakeHouse storage. Then please copy the <code>ABFS</code> paths of the 2 jars. In this example, the paths are</p> <pre><code>abfss://9e9d4196-870a-4901-8fa5-e24841492ab8@onelake.dfs.fabric.microsoft.com/e15f3695-af7e-47de-979e-473c3caa9f5b/Files/sedona-spark-shaded-3.4_2.12-1.5.1.jar\n\nabfss://9e9d4196-870a-4901-8fa5-e24841492ab8@onelake.dfs.fabric.microsoft.com/e15f3695-af7e-47de-979e-473c3caa9f5b/Files/geotools-wrapper-1.5.1-28.2.jar\n</code></pre> <p></p> <p></p> <p>If you use this option, the config files in your notebook should be</p> <pre><code>%%configure -f\n{\n    \"conf\": {\n        \"spark.jars\": \"abfss://XXX/Files/sedona-spark-shaded-3.4_2.12-1.5.1.jar,abfss://XXX/Files/geotools-wrapper-1.5.1-28.2.jar\",\n    }\n}\n</code></pre>"},{"location":"setup/glue/","title":"Install on AWS Glue","text":"<p>This tutorial will cover how to configure both a glue notebook and a glue ETL job. The tutorial is written assuming you have a working knowledge of AWS Glue jobs.</p> <p>In the tutorial, we use Sedona 1.8.0 and Glue 4.0 which runs on Spark 3.3.0, Java 8, Scala 2.12, and Python 3.10. We recommend Sedona-1.3.1-incubating and above for Glue.</p> <p>Warning</p> <p>Important: Since Sedona 1.8.0, Java 8 support is dropped and Spark 3.3 support is dropped. For Sedona 1.8.0+, you need to use Glue 5.0+ which supports Java 11 and Spark 3.4+.</p>"},{"location":"setup/glue/#gather-maven-links","title":"Gather Maven Links","text":"<p>You will need to point your glue job to the Sedona and Geotools jars. We recommend using the jars available from maven. The links below are those intended for Glue 4.0</p> <p>Sedona Jar: Maven Central</p> <p>Geotools Jar: Maven Central</p> <p>Note</p> <p>Ensure you pick a version for Scala 2.12 and Spark 3.3. The Spark 3.4 and Scala 2.13 jars are not compatible with Glue 4.0.</p>"},{"location":"setup/glue/#configure-glue-job","title":"Configure Glue Job","text":"<p>Once you have your jar links, you can configure your Glue job to use them, as well as the apache-sedona Python package. How you do this varies slightly between the notebook and the script job types.</p> <p>Note</p> <p>Always ensure that the Sedona version of the jars and the Python package match.</p>"},{"location":"setup/glue/#notebook-job","title":"Notebook Job","text":"<p>Add the following cell magics before starting your sparkContext or glueContext. The first points to the jars, and the second installs the Sedona Python package directly from pip.</p> <pre><code># Sedona Config\n%extra_jars https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.8.0/sedona-spark-shaded-3.3_2.12-1.8.0.jar, https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.8.0-33.1/geotools-wrapper-1.8.0-33.1.jar\n%additional_python_modules apache-sedona==1.8.0\n</code></pre> <p>If you are using the example notebook from glue, the first cell should now look like this:</p> <pre><code>%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n# Sedona Config\n%extra_jars https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.8.0/sedona-spark-shaded-3.3_2.12-1.8.0.jar, https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.8.0-33.1/geotools-wrapper-1.8.0-33.1.jar\n%additional_python_modules apache-sedona==1.8.0\n\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n</code></pre> <p>You can confirm your installation by running the following cell:</p> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(spark)\nsedona.sql(\"SELECT ST_POINT(1., 2.) as geom\").show()\n</code></pre>"},{"location":"setup/glue/#etl-job","title":"ETL Job","text":"<p>Glue also calls these Scripts. From your job's page, navigate to the \"Job details\" tab. At the bottom of the page expand the \"Advanced properties\" section. In the \"Dependent JARs path\" field, add the paths to the jars, separated by a comma.</p> <p>To add the Sedona Python package, navigate to the \"Job Parameters\" section and add a new parameter with the key <code>--additional-python-modules</code> and the value <code>apache-sedona==1.8.0</code>.</p> <p>To confirm the installation add the follow code to the script:</p> <pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder().getOrCreate()\nsedona = SedonaContext.create(config)\n\nsedona.sql(\"SELECT ST_POINT(1., 2.) as geom\").show()\n</code></pre> <p>Once added to the script, save and run the job. If the job runs successfully, the installation was successful.</p>"},{"location":"setup/install-python/","title":"Install Sedona Python","text":"<p>Apache Sedona extends pyspark functions which depends on libraries:</p> <ul> <li>pyspark</li> <li>shapely</li> <li>attrs</li> </ul> <p>You need to install necessary packages if your system does not have them installed. Sedona now uses uv for Python dependency management. See the dependency definitions in our pyproject.toml.</p>"},{"location":"setup/install-python/#install-sedona","title":"Install sedona","text":"<ul> <li>Installing from PyPI repositories. You can find the latest Sedona Python on PyPI. There is a known issue in Sedona v1.0.1 and earlier versions.</li> </ul> <pre><code>pip install apache-sedona\n</code></pre> <ul> <li>Since Sedona v1.1.0, pyspark is an optional dependency of Sedona Python because spark comes pre-installed on many spark platforms. To install pyspark along with Sedona Python in one go, use the <code>spark</code> extra:</li> </ul> <pre><code>pip install apache-sedona[spark]\n</code></pre> <ul> <li>Installing from Sedona Python source</li> </ul> <p>Clone Sedona GitHub source code and run the following command</p> <pre><code>cd python\npython3 -m pip install .\n</code></pre>"},{"location":"setup/install-python/#prepare-sedona-spark-jar","title":"Prepare sedona-spark jar","text":"<p>Sedona Python needs one additional jar file called <code>sedona-spark-shaded</code> or <code>sedona-spark</code> to work properly. Please make sure you use the correct version for Spark and Scala.</p> <p>Please use Spark major.minor version number in artifact names.</p> <p>You can get it using one of the following methods:</p> <ol> <li>If you run Sedona in Databricks, AWS EMR, or other cloud platform's notebook, use the <code>shaded jar</code>: Download sedona-spark-shaded jar and geotools-wrapper jar from Maven Central, and put them in SPARK_HOME/jars/ folder.</li> <li>If you run Sedona in an IDE or a local Jupyter notebook, use the <code>unshaded jar</code>. Call the Maven Central coordinate in your python program. For example, Sedona &gt;= 1.4.1</li> </ol> <pre><code>from sedona.spark import *\n\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-3.3_2.12:1.8.0,\"\n        \"org.datasyslab:geotools-wrapper:1.8.0-33.1\",\n    )\n    .config(\n        \"spark.jars.repositories\",\n        \"https://artifacts.unidata.ucar.edu/repository/unidata-all\",\n    )\n    .getOrCreate()\n)\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>SedonaRegistrator is deprecated in Sedona 1.4.1 and later versions. Please use the above method instead.</p> <pre><code>from pyspark.sql import SparkSession\nfrom sedona.spark import SedonaRegistrator\nfrom sedona.spark import SedonaKryoRegistrator, KryoSerializer\n\nspark = (\n    SparkSession.builder.appName(\"appName\")\n    .config(\"spark.serializer\", KryoSerializer.getName)\n    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.8.0,\"\n        \"org.datasyslab:geotools-wrapper:1.8.0-33.1\",\n    )\n    .getOrCreate()\n)\nSedonaRegistrator.registerAll(spark)\n</code></pre>"},{"location":"setup/install-python/#setup-environment-variables","title":"Setup environment variables","text":"<p>If you manually copy the sedona-spark-shaded jar to <code>SPARK_HOME/jars/</code> folder, you need to setup two environment variables</p> <ul> <li>SPARK_HOME. For example, run the command in your terminal</li> </ul> <pre><code>export SPARK_HOME=~/Downloads/spark-3.0.1-bin-hadoop2.7\n</code></pre> <ul> <li>PYTHONPATH. For example, run the command in your terminal</li> </ul> <pre><code>export PYTHONPATH=$SPARK_HOME/python\n</code></pre> <p>You can then play with Sedona Python Jupyter notebook.</p>"},{"location":"setup/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Spark cluster is ready.</p> <p>There are two ways to use a Scala or Java library with Apache Spark. You can use either one to run Sedona.</p> <ul> <li>Spark interactive Scala or SQL shell: easy to start, good for new learners to try simple functions</li> <li>Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects</li> </ul>"},{"location":"setup/install-scala/#spark-scala-shell","title":"Spark Scala shell","text":""},{"location":"setup/install-scala/#download-sedona-jar-automatically","title":"Download Sedona jar automatically","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Run Spark shell with <code>--packages</code> option. This command will automatically download Sedona jars from Maven Central.</p> </li> </ol> <pre><code>./bin/spark-shell --packages MavenCoordinates\n</code></pre> <p>Please refer to Sedona Maven Central coordinates to select the corresponding Sedona packages for your Spark version.</p> <pre><code>* Local mode: test Sedona without setting up a cluster\n```\n./bin/spark-shell --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.8.0,org.datasyslab:geotools-wrapper:1.8.0-33.1\n```\n\n* Cluster mode: you need to specify Spark Master IP\n```\n./bin/spark-shell --master spark://localhost:7077 --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.8.0,org.datasyslab:geotools-wrapper:1.8.0-33.1\n```\n</code></pre>"},{"location":"setup/install-scala/#download-sedona-jar-manually","title":"Download Sedona jar manually","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Download Sedona jars:</p> <ul> <li>Download the pre-compiled jars from Sedona Releases</li> <li>Download / Git clone Sedona source code and compile the code by yourself (see Compile Sedona)</li> </ul> </li> <li>Run Spark shell with <code>--jars</code> option.</li> </ol> <pre><code>./bin/spark-shell --jars /Path/To/SedonaJars.jar\n</code></pre> <p>Please use jars with Spark major.minor versions in the filename, such as <code>sedona-spark-shaded-3.3_2.12-1.8.0</code>.</p> <pre><code>* Local mode: test Sedona without setting up a cluster\n```\n./bin/spark-shell --jars /path/to/sedona-spark-shaded-3.3_2.12-1.8.0.jar,/path/to/geotools-wrapper-1.8.0-33.1.jar\n```\n\n* Cluster mode: you need to specify Spark Master IP\n```\n./bin/spark-shell --master spark://localhost:7077 --jars /path/to/sedona-spark-shaded-3.3_2.12-1.8.0.jar,/path/to/geotools-wrapper-1.8.0-33.1.jar\n```\n</code></pre>"},{"location":"setup/install-scala/#spark-sql-shell","title":"Spark SQL shell","text":"<p>Please see Use Sedona in a pure SQL environment</p>"},{"location":"setup/install-scala/#self-contained-spark-projects","title":"Self-contained Spark projects","text":"<p>A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use Sedona in your self-contained Spark project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using SBT. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:</li> </ol> <pre><code>./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n</code></pre> <p>Note</p> <p>The detailed explanation of spark-submit is available on Spark website.</p>"},{"location":"setup/maven-coordinates/","title":"Maven Central coordinate","text":""},{"location":"setup/maven-coordinates/#maven-coordinates","title":"Maven Coordinates","text":""},{"location":"setup/maven-coordinates/#use-sedona-shaded-fat-jars","title":"Use Sedona shaded (fat) jars","text":"<p>Warning</p> <p>For Scala/Java/Python users, this is the most common way to use Sedona in your environment. Do not use separate Sedona jars unless you are sure that you do not need shaded jars.</p> <p>Warning</p> <p>For R users, this is the only way to use Sedona in your environment.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <p>Please use the artifact with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-shaded-3.4_2.12</code>.</p> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.4 and Scala 2.12Spark 3.5 and Scala 2.12Spark 4.0 and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.4_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.5_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-4.0_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.4 and Scala 2.13Spark 3.5 and Scala 2.13Spark 4.0 and Scala 2.13 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.4_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.5_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-4.0_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-flink-shaded_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Snowflake</p> Snowflake 7.0+ (Year 2023 and later) <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-snowflake&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#use-sedona-unshaded-jars","title":"Use Sedona unshaded jars","text":"<p>Warning</p> <p>For Scala, Java, Python users, please use the following jars only if you satisfy these conditions: (1) you know how to exclude transient dependencies in a complex application. (2) your environment has internet access (3) you are using some sort of Maven package resolver, or pom.xml, or build.sbt. It usually directly takes an input like this <code>GroupID:ArtifactID:Version</code>. If you don't understand what we are talking about, the following jars are not for you.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <p>Please use the artifacts with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-3.4_2.12</code>.</p> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license, so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.4 and Scala 2.12Spark 3.5 and Scala 2.12Spark 4.0 and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.4_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.5_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-4.0_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.4 and Scala 2.13Spark 3.5 and Scala 2.13Spark 4.0 and Scala 2.13 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.4_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.5_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-4.0_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-flink_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.8.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.8.0-33.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona Snowflake does not have an unshaded version.</p>"},{"location":"setup/maven-coordinates/#netcdf-java-542","title":"netCDF-Java 5.4.2","text":"<p>This is required only if you want to read HDF/NetCDF files using <code>RS_FromNetCDF</code>. Note that this JAR is not in Maven Central so you will need to add this repository to your pom.xml or build.sbt, or specify the URL in Spark Config <code>spark.jars.repositories</code> or spark-submit <code>--repositories</code> option.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your pom.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n    &lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n    &lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n    &lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#snapshot-versions","title":"SNAPSHOT versions","text":"<p>Sometimes Sedona has a SNAPSHOT version for the upcoming release. It follows the same naming conversion but has \"SNAPSHOT\" as suffix in the version. For example, <code>1.8.1-SNAPSHOT</code></p> <p>In order to download SNAPSHOTs, you need to add the following repositories in your pom.xml or build.sbt</p>"},{"location":"setup/maven-coordinates/#buildsbt","title":"build.sbt","text":"<p>resolvers +=   \"Apache Software Foundation Snapshots\" at \"https://repository.apache.org/content/groups/snapshots\"</p>"},{"location":"setup/maven-coordinates/#pomxml","title":"pom.xml","text":"<pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;snapshots-repo&lt;/id&gt;\n        &lt;url&gt;https://repository.apache.org/content/groups/snapshots&lt;/url&gt;\n        &lt;releases&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/releases&gt;\n        &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre>"},{"location":"setup/modules/","title":"Modules","text":""},{"location":"setup/modules/#sedona-modules-for-apache-spark","title":"Sedona modules for Apache Spark","text":"Name API Introduction spark RDD / SQL / DataFrame SpatialRDD and Spatial DataFrame spark-shaded Shaded version python Python interface for SpatialRDD and Spatial DataFrame Zeppelin Apache Zeppelin Plugin for Apache Zeppelin 0.8.1+"},{"location":"setup/modules/#api-availability","title":"API availability","text":"Core/RDD DataFrame/SQL Viz RDD/SQL Scala/Java \u2705 \u2705 \u2705 Python \u2705 \u2705 SQL only R \u2705 \u2705 \u2705"},{"location":"setup/overview/","title":"Overview","text":""},{"location":"setup/overview/#download-statistics","title":"Download statistics","text":"Download statistics Maven PyPI Conda-forge CRAN DockerHub Apache Sedona 330k/month Archived GeoSpark releases 10k/month"},{"location":"setup/overview/#what-can-sedona-do","title":"What can Sedona do?","text":""},{"location":"setup/overview/#distributed-spatial-datasets","title":"Distributed spatial datasets","text":"<ul> <li> Spatial RDD on Spark</li> <li> Spatial DataFrame/SQL on Spark</li> <li> Spatial DataStream on Flink</li> <li> Spatial Table/SQL on Flink</li> <li> Spatial SQL on Snowflake</li> </ul>"},{"location":"setup/overview/#complex-spatial-objects","title":"Complex spatial objects","text":"<ul> <li> Vector geometries / trajectories</li> <li> Raster images with Map Algebra</li> <li> Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, ArcGrid, NetCDF/HDF</li> </ul>"},{"location":"setup/overview/#distributed-spatial-queries","title":"Distributed spatial queries","text":"<ul> <li> Spatial query: range query, range join query, distance join query, K Nearest Neighbor query</li> <li> Spatial index: R-Tree, Quad-Tree</li> </ul>"},{"location":"setup/overview/#rich-spatial-analytics-tools","title":"Rich spatial analytics tools","text":"<ul> <li> Coordinate Reference System / Spatial Reference System Transformation</li> <li> Apache Zeppelin dashboard integration</li> <li> Integrate with a variety of Python tools including Jupyter notebook, GeoPandas, Shapely</li> <li> Integrate with a variety of visualization tools including KeplerGL, DeckGL</li> <li> High resolution and scalable map generation: Visualize Spatial DataFrame/RDD</li> <li> Support Scala, Java, Python, R</li> </ul>"},{"location":"setup/platform/","title":"Language wrappers","text":"<p>Sedona binary releases are compiled by Java 11/17 and Scala 2.12/2.13 and tested in the following environments:</p> <p>Java Requirements:</p> <ul> <li>Spark 3.4 &amp; 3.5: Java 11</li> <li>Spark 4.0: Java 17</li> </ul> <p>Note: Java 8 support is dropped since Sedona 1.8.0. Spark 3.3 support is dropped since Sedona 1.8.0.</p> Sedona Scala/JavaSedona PythonSedona R Spark 3.4 Spark 3.5 Spark 4.0 Scala 2.12 \u2705 \u2705 \u2705 Scala 2.13 \u2705 \u2705 \u2705 Spark 3.4 (Scala 2.12) Spark 3.5 (Scala 2.12) Spark 4.0 (Scala 2.12) Python 3.7 \u2705 \u2705 \u2705 Python 3.8 \u2705 \u2705 \u2705 Python 3.9 \u2705 \u2705 \u2705 Python 3.10 \u2705 \u2705 \u2705 Spark 3.4 Spark 3.5 Spark 4.0 Scala 2.12 \u2705 \u2705 \u2705"},{"location":"setup/release-notes/","title":"Release notes","text":""},{"location":"setup/release-notes/#sedona-180","title":"Sedona 1.8.0","text":"<p>Sedona 1.8.0 is compiled against:</p> <ul> <li>Spark: 3.4, 3.5, 4.0</li> <li>Flink: 1.19</li> <li>Snowflake: 7+</li> </ul> <p>Java Requirements:</p> <ul> <li>Spark 3.4 &amp; 3.5: Java 11</li> <li>Spark 4.0: Java 17</li> </ul> <p>Spark 3.3 and Java 8 support are dropped since the 1.8.0 release.</p> <p>This is a major release that introduces significant new features including GeoPandas Compatible API, S2Geography support, and major platform upgrades.</p>"},{"location":"setup/release-notes/#new-contributors","title":"New Contributors","text":"<ul> <li>@jesspav made their first contribution in https://github.com/apache/sedona/pull/1942</li> <li>@raveendra11 made their first contribution in https://github.com/apache/sedona/pull/1981</li> <li>@petern48 made their first contribution in https://github.com/apache/sedona/pull/1990</li> <li>@Aashish-Jha-11 made their first contribution in https://github.com/apache/sedona/pull/2000</li> <li>@SumitGupta016 made their first contribution in https://github.com/apache/sedona/pull/1999</li> <li>@giswqs made their first contribution in https://github.com/apache/sedona/pull/2003</li> <li>@ZhuochengShang made their first contribution in https://github.com/apache/sedona/pull/1992</li> <li>@Subham-KRLX made their first contribution in https://github.com/apache/sedona/pull/2114</li> <li>@Gautam3994 made their first contribution in https://github.com/apache/sedona/pull/2190</li> <li>@jjestrada2 made their first contribution in https://github.com/apache/sedona/pull/2239</li> </ul>"},{"location":"setup/release-notes/#highlights","title":"Highlights","text":"<ul> <li> [SEDONA-720] GeoPandas Compatible API - Complete implementation of GeoPandas-like interface for Sedona</li> <li> [SEDONA-721] Vectorized UDF for Python - High-performance vectorized user-defined functions</li> <li> [SEDONA-738] Moran I Autocorrelation - Spatial autocorrelation analysis</li> <li> [SEDONA-743] MySQL Geometry Support - Direct loading of MySQL geometry objects</li> <li> [SEDONA-725] PyFlink Support - Python API for Apache Flink</li> <li> S2Geography Support - Complete S2-based geography data type with comprehensive functions</li> <li> [SEDONA-741] Enhanced Spatial Filter Pushdown - Improved query performance with ST_DWithin and ST_DistanceSpheroid</li> <li> [SEDONA-742] ST_DistanceSphere Filter Pushdown - Additional spatial filter optimizations</li> <li> Java 11 Upgrade - Modern Java runtime support</li> <li> GeoTools 33 Upgrade - Latest geospatial library support</li> <li> Spark 4.0 Support - Full compatibility with Apache Spark 4.0</li> <li> Libpostal Integration - Address parsing and geocoding capabilities</li> </ul>"},{"location":"setup/release-notes/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>[GH-2354] - SedonaRegistrator Import Breaking Change: The <code>SedonaRegistrator</code> import path has been changed in Sedona 1.8.0. The module has been moved from <code>sedona.register.geo_registrator</code> to <code>sedona.spark.register.geo_registrator</code>. <code>SedonaRegistrator</code> has been soft deprecated since version 1.4.1 but due to a bug its path has now changed. The solution is to either use <code>SedonaContext</code> or use the try/except pattern:</li> </ul> <pre><code>try:\n    # Sedona 1.7\n    from sedona.register.geo_registrator import PackageImporter\nexcept ImportError:\n    # Sedona 1.8\n    from sedona.spark.register.geo_registrator import PackageImporter\n</code></pre>"},{"location":"setup/release-notes/#new-features","title":"New Features","text":""},{"location":"setup/release-notes/#geopandas-compatible-api","title":"GeoPandas Compatible API","text":"<ul> <li>[SEDONA-720] - Complete GeoPandas-compatible API implementation</li> <li>[GH-1990] - Geopandas.Series repr() and to_geopandas()</li> <li>[GH-2005] - Geopandas.GeoSeries: Implement Test Framework</li> <li>[GH-2008] - Geopandas.Dataframe: Fix constructor for pandas-on-pyspark and Sedona Geopandas input types</li> <li>[GH-2014] - Geopandas.Series: Implement crs property and set_crs</li> <li>[GH-2017] - Geopandas.GeoSeries: Implement x(), y(), z(), has_z</li> <li>[GH-2019] - Geopandas.GeoSeries: Setup skeleton of functions and tests</li> <li>[GH-2021] - Geopandas.GeoSeries: Implement .length and refactor process_geometry_column()</li> <li>[GH-2023] - Geopandas.GeoSeries: Implement geom_type</li> <li>[GH-2026] - Geopandas.GeoSeries: Implement is_valid, is_empty, is_simple</li> <li>[GH-2032] - Geopandas.GeoSeries: Implement isna, notna and aliases (isnull, notnull)</li> <li>[GH-2038] - Implement row_wise_operation + intersection, intersect</li> <li>[GH-2040] - Geopandas.GeoSeries: Implement from_wkb, from_wkt, from_xy + temp init fix</li> <li>[GH-2042] - Implement boundary, centroid, envelope</li> <li>[GH-2044] - Geopandas.GeoSeries: Implement is_valid_reason, make_valid</li> <li>[GH-2046] - Geopandas.GeoSeries: Implement fillna</li> <li>[GH-2048] - Geopandas.GeoSeries: Implement to_crs</li> <li>[GH-2056] - Geopandas.GeoSeries: Implement estimate_utm_crs, bounds, total_bounds, getitem</li> <li>[GH-2057] - Geopandas.GeoSeries: Implement align=False for row_wise_operation + support indexes</li> <li>[GH-2058] - Geopandas: set_geometry, rename_geometry, active_geometry_name + refactor tests</li> <li>[GH-2061] - Geopandas.GeoSeries: Implement crosses, overlaps, touches, within, covers, covered_by, contains, distance</li> <li>[GH-2071] - Geopandas.GeoSeries: Implement <code>get_geometry</code></li> <li>[GH-2080] - Store geometries in EWKB format in spark df and enforce it</li> <li>[GH-2086] - Retain index information in query results</li> <li>[GH-2105] - Geopandas: Implement <code>to_json</code>, <code>to_arrow</code>, <code>from_arrow</code></li> <li>[GH-2109] - Geopandas.GeoSeries: Implement <code>union_all</code> aggregation</li> <li>[GH-2111] - Geopandas.GeoSeries: Implement simplify and snap</li> <li>[GH-2113] - Geopandas.GeoSeries: Implement <code>to_wkt</code> and <code>to_wkb</code></li> <li>[GH-2117] - Geopandas.GeoSeries: Implement <code>difference</code> and <code>dwithin</code></li> <li>[GH-2121] - Geopandas: Implement Scalable workaround and store as geometries instead of EWKB</li> <li>[GH-2124] - Support serializing LinearRing in C bindings for Python interface</li> <li>[GH-2131] - Geopandas: Refactor to use Spark Dataframe API instead of Spark SQL</li> <li>[GH-2145] - Geopandas: Clean up tests, reduce test time, add LinearRing tests</li> <li>[GH-2150] - Geopandas: Implement <code>to_file</code>, <code>from_file</code>, <code>read_file</code></li> <li>[GH-2162] - Add constructor tests and re-enable more linear ring tests</li> <li>[GH-2163] - Geopandas: Share geoseries functions in base class to implement them for geodataframe</li> <li>[GH-2167] - More pathing fixes for Python package refactor</li> <li>[GH-2170] - Clean up Geopandas Documentation after the refactor</li> <li>[GH-2179] - Implement ST_Segmentize function and geopandas.segmentize()</li> <li>[GH-2184] - Fix ST_Buffer logic cap_style when specifying 'side' parameter + Implement rest of geoseries.buffer()</li> <li>[GH-2193] - Geopandas: Implement <code>plot()</code> for GeoDataFrame and GeoSeries + <code>read_parquet</code></li> <li>[GH-2203] - Geopandas: Optimize crs operations, getitem, and set_geometry + impl scalable getitem</li> <li>[GH-2209] - Geopandas: Fix sjoin implementation + proper naming and index behavior</li> <li>[GH-2216] - Retain sindex, make sindex.query return geometries, implement has_sindex</li> <li>[GH-2219] - Fix index retaining behavior for row_wise_operations on dataframes</li> <li>[GH-2225] - Move sedona.geopandas to sedona.spark.geopandas</li> <li>[GH-2229] - Add Geopandas Developer Guide README.md</li> <li>[GH-2232] - Implement is_ring</li> <li>[GH-2243] - Geopandas: Remove orderBy in query_geometry_column</li> <li>[GH-2265] - Geopandas: Support other of type GeoDataframe in row_wise_operations</li> <li>[GH-2267] - Geopandas: Remove unnecessary old_crs check and in <code>to_crs</code></li> <li>[GH-2270] - Geopandas: Fix geodataframe.copy() to properly create a copy</li> <li>[GH-2274] - Fix GeoPandas errors and issues in the examples</li> <li>[GH-2280] - Geopandas: Support sjoin with <code>covers</code> and <code>covered_by</code> predicates</li> <li>[GH-2282] - Geopandas: set ps option 'compute.ops_on_diff_frames' to True by default</li> <li>[GH-2289] - Make it clear sindex queries doesn't support GeoSeries as input</li> <li>[GH-2319] - Reapply Geopandas doc improvements and add io section to docs</li> <li>[GH-2332] - Geopandas: Document differences of sindex compared to gpd + sindex fixes</li> <li>[GH-2067] - Implement spatial index sindex query() and unit tests</li> <li>[GH-2084] - Implement nearest Method for GeoPandas Spatial Index</li> <li>[GH-2088] - Implement intersection Method for GeoPandas.SpatialIndex</li> <li>[GH-2099] - Fix the Geopandas sindex query method predicate logic</li> <li>[GH-2101] - Enhance geopandas sjoin implementation</li> </ul>"},{"location":"setup/release-notes/#s2geography-support","title":"S2Geography Support","text":"<ul> <li>[GH-1992] - Create object of S2Geography, and implement Point/Polyline/PolygonGeography with its encoder/decoder</li> <li>[GH-2054] - Shade S2 and Guava into runtime JARs</li> <li>[GH-2027] - Implementations of GeographyCollection &amp; ShapeIndexGeography on S2Geography</li> <li>[GH-2221] - Add Singlepoint, Singlepolyline for correctly round trip WKT write</li> <li>[GH-2223] - Register S2Geography UDT as Geography UDT into Apache Sedona</li> <li>[GH-2051] - Implement geospatial functions of Distance, Predicates, Projection</li> <li>[GH-2090] - WKB, WKT reader &amp; writer</li> <li>[GH-2262] - Add new constructor ST_GeogFromEWKT</li> <li>[GH-2260] - Add new constructor ST_GeogFromEKWB</li> <li>[GH-2271] - Add new constructor ST_GeogFromGeoHash</li> <li>[GH-2275] - New constructor of convert Geography to Geometry</li> <li>[GH-2298] - Add constructor of Geometry to Geography</li> <li>[GH-2285] - ST_Envelope on Geography</li> <li>[GH-2309] - Add function of ST_AsEWKT</li> <li>[GH-2303] - Add Multipolygon encode / decode</li> <li>[GH-2305] - Update more robust instream count for Geography decode</li> </ul>"},{"location":"setup/release-notes/#other-new-features","title":"Other New Features","text":"<ul> <li>[SEDONA-721] - Add Sedona vectorized udf for Python</li> <li>[SEDONA-738] - Add moran i autocorrelation</li> <li>[SEDONA-743] - Add geom from mysql function, to support loading MySQL geometry object directly to Sedona</li> <li>[SEDONA-741] - Make ST_DWithin and ST_DistanceSpheroid filters to push down</li> <li>[SEDONA-742] - Push down ST_DistanceSphere filter</li> <li>[GH-2077] - Add libpostal integration</li> <li>[GH-1755] - Implement ST_Segmentize function and geopandas.segmentize()</li> <li>[GH-2003] - Add params to KeplerGl constructor</li> <li>[GH-1885] - Create EnvelopeAccumulator class</li> <li>[GH-2295] - Introduce XYZM support in GeoJSON reader and writer</li> </ul>"},{"location":"setup/release-notes/#platform-infrastructure","title":"Platform &amp; Infrastructure","text":""},{"location":"setup/release-notes/#major-upgrades","title":"Major Upgrades","text":"<ul> <li>[GH-1942] - Upgrade to Java 11</li> <li>[GH-1967] - Upgrade to GeoTools 33</li> <li>[GH-1919] - Spark 4 support</li> <li>[GH-1958] - Drop support for Spark 3.3 and Python 3.7</li> </ul>"},{"location":"setup/release-notes/#pyflink-support","title":"PyFlink Support","text":"<ul> <li>[GH-1875] - Add pyflink to Sedona</li> <li>[GH-1930] - Restructure spark python package</li> </ul>"},{"location":"setup/release-notes/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>[GH-1964] - Shade Jiffle and its dependencies</li> <li>[GH-2098] - Use graphframes 0.9.0</li> <li>[GH-2317] - Fix graphframes dependency across Spark major versions</li> <li>[GH-2320] - Remove unexpected mandatory Python dependencies</li> <li>[GH-2322] - Remove unnecessary commons-lang dependency</li> <li>[GH-2328] - Add SedonaDB extra</li> </ul>"},{"location":"setup/release-notes/#bug","title":"Bug","text":""},{"location":"setup/release-notes/#rasterization-rs-functions","title":"Rasterization &amp; RS Functions","text":"<ul> <li>[SEDONA-740] - Fix rasterization rounding issue</li> </ul>"},{"location":"setup/release-notes/#stac-reader","title":"STAC Reader","text":"<ul> <li>[GH-2125] - Fix STAC reader to accept tuples for datetime parameter</li> <li>[GH-2126] - Add Shapely and WKT geometry support to STAC reader</li> <li>[GH-2128] - Fix STAC python unit tests for remote endpoint</li> <li>[GH-2153] - Fix: prevent information disclosure through exception messages in STAC client</li> <li>[GH-2168] - Security issues in the STAC client api</li> </ul>"},{"location":"setup/release-notes/#osm-reader","title":"OSM Reader","text":"<ul> <li>[GH-2308] - Fix the precision lost issue in OSM PBF reader</li> </ul>"},{"location":"setup/release-notes/#geopackage-reader","title":"GeoPackage Reader","text":"<ul> <li>[GH-2339] - Fix the DateTimeParseException in GeoPackage reader</li> </ul>"},{"location":"setup/release-notes/#python-core","title":"Python &amp; Core","text":"<ul> <li>[GH-2073] - Fix Structured Adapter Issue with Spatial Queries</li> <li>[GH-2123] - Fix deserialization of geometry SRID in python</li> <li>[GH-2135] - Fix bug where get_crs logic won't work if first element is null</li> <li>[GH-2253] - Deprecation warnings pop up in the wrong place</li> <li>[GH-2297] - Making GeoParquetFileFormat self-contained to avoid problems on Databricks</li> <li>[GH-2301] - Usage of transformUpWithPruning prevents sedona from working on Databricks</li> <li>[GH-2165] - Fix RasterType legacy import path</li> <li>[GH-2177] - Always use native byte order when writing WKBs</li> </ul>"},{"location":"setup/release-notes/#other-bug-fixes","title":"Other Bug Fixes","text":"<ul> <li>[GH-1981] - Hardcoded file paths to local user files</li> <li>[GH-2011] - Fix issues in the Compile Sedona source code page</li> <li>[GH-2015] - Add 'Closes #' to PR template <li>[GH-2029] - Rename <code>lint</code> workflow to <code>pre-commit</code></li> <li>[GH-2033] - Remove tabs and indent code</li> <li>[GH-2076] - Remove duplicate versions matrix entry in python.yml</li> <li>[GH-2089] - Remove ShapefileRDD Test</li> <li>[GH-2132] - Make <code>ST_</code> and <code>RS_</code> function expression classes package private</li> <li>[GH-2141] - Make DataFrameShims usable outside of sedona</li> <li>[GH-2146] - Add redirection and warnings for old Python public functions</li> <li>[GH-2172] - Add hook <code>check-zip-file-is-not-committed</code></li> <li>[GH-2188] - Add <code>mvn spotless:apply</code> as a pre-commit hook</li> <li>[GH-2196] - Fix the python api build failed to import module errors</li> <li>[GH-2205] - Fix snowflake tests for ST_Segmentize</li> <li>[GH-2220] - Add three hooks that target <code>RST</code> files</li> <li>[GH-2239] - Update mkdocs.yml and improve documentation structure</li> <li>[GH-2248] - Fix RST formatting: Use double backticks for inline code</li> <li>[GH-2249] - Fix documentation structure after moving sedona.geopandas</li> <li>[GH-2256] - Add test for ST_GeogFromWKB(byte array)</li> <li>[GH-2257] - Add MULTIPOLYGON Type in Geography</li> <li>[GH-2259] - Replace old pypistats badges with Pepy tech badges</li> <li>[GH-2268] - Add the Python documentation links to the main navigation</li> <li>[GH-2278] - Fix to_file inferring parquet format instead of geoparquet</li> <li>[GH-2293] - Fix the missing import paths for some stats functions</li> <li>[GH-2315] - Update the Databricks instruction for the Python API</li>"},{"location":"setup/release-notes/#sedona-172","title":"Sedona 1.7.2","text":"<p>Sedona 1.7.2 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a minor release that only includes bug fixes. No API breaking changes and behavior changes are expected.</p>"},{"location":"setup/release-notes/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@MadhuriRathod30 made their first contribution in https://github.com/apache/sedona/pull/1906</li> <li>@jgoday made their first contribution in https://github.com/apache/sedona/pull/1923</li> <li>@oliverbeagley made their first contribution in https://github.com/apache/sedona/pull/1929</li> <li>@cgauvi made their first contribution in https://github.com/apache/sedona/pull/1962</li> </ul>"},{"location":"setup/release-notes/#bug_1","title":"Bug","text":"<ul> <li>SEDONA-722: Fix precision loss problems caused by casting world coordinates from double to float</li> <li>SEDONA-704: Optimize STAC reader and fix few issues</li> <li>SEDONA-715: Unify Zeppelin and Jupyter setting in Docker</li> <li>GH-1868: Fix spark sql extension load failure when parser failed to load</li> <li>SEDONA-726: Fix ST_Force_2D and add ST_Force2D</li> <li>SEDONA-704: Add the grid extension to the stac reader</li> <li>SEDONA-724: Fix RS_ZonalStats and RS_ZonalStatsAll edge case bug</li> <li>SEDONA-728: Fix Rasterization clamping bug</li> <li>SEDONA-690: Set default metric to use Haversine for KNN join and code refactoring</li> <li>SEDONA-731: Add osm nodes parser</li> <li>GH-1922: ST_X/Y/Z ON null geometries</li> <li>GH-1761: Error when invalid ST_Subdivide maxVertices argument</li> <li>GH-1910: Import geopandas only when type checking</li> <li>GH-1931: Move packaging module import to geopandas try-except block</li> <li>SEDONA-734 Fix relation parsing in OSM reader</li> <li>SEDONA-735 Fix RS_Clip bug caused by AOI geometries smaller than pixel size</li> <li>GH-1945 Shade Jiffle and its dependencies</li> </ul>"},{"location":"setup/release-notes/#sedona-171","title":"Sedona 1.7.1","text":"<p>Sedona 1.7.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a minor release that includes new features, improvements, bug fixes. No API breaking changes and behavior changes are expected.</p>"},{"location":"setup/release-notes/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@MrPowers made their first contribution in https://github.com/apache/sedona/pull/1735</li> <li>@paleolimbot made their first contribution in https://github.com/apache/sedona/pull/1748</li> <li>@kadolor made their first contribution in https://github.com/apache/sedona/pull/1762</li> <li>@BaseMax made their first contribution in https://github.com/apache/sedona/pull/1808</li> <li>@ruanqizhen made their first contribution in https://github.com/apache/sedona/pull/1822</li> <li>@sshiv012 made their first contribution in https://github.com/apache/sedona/pull/1826</li> </ul>"},{"location":"setup/release-notes/#highlights_1","title":"Highlights","text":"<ul> <li> [SEDONA-689] SQL interface for GeoStats including ST_DBSCAN, ST_GLocal, and ST_LocalOutlierFactor</li> <li> [SEDONA-690] Broadcast join support for distributed KNN Join</li> <li> GeoArrow-enhanced GeoPandas input and output of Sedona DataFrame</li> <li> [SEDONA-695] Expose spatial partitioning structure from SpatialRDD to enable better partitioning for GeoParquet</li> <li> [SEDONA-704] STAC catalog reader</li> <li> [SEDONA-713] OpenStreetMap (OSM) PBF reader</li> <li> [SEDONA-707] Significant performance improvement on geometry rasterization such as RS_AsRaster</li> <li> Several cool ST functions such as ST_RemoveRepeatedPoints</li> </ul>"},{"location":"setup/release-notes/#bug_2","title":"Bug","text":"<ul> <li>[SEDONA-688] -         running ST_KNN() error : java.lang.ArithmeticException: / by zero </li> <li>[SEDONA-690] -         Optimize query side broadcast KNN join </li> <li>[SEDONA-694] -         Error message for optional includes refers to old pypi package name </li> <li>[SEDONA-696] -         Fix issue with geopackage datasource on databricks. </li> <li>[SEDONA-698] -         Fix ST_RemoveRepeatedPoints </li> <li>[SEDONA-699] -         When loading metadata for relatively huge geoparquet files Sedona application stops responding. </li> <li>[SEDONA-700] -         ST_KNN fails on null and empty geometries </li> <li>[SEDONA-706] -         Python DataFrame API have problem working in multi-threaded environment </li> <li>[SEDONA-716] -         MERGE INTO TABLE Does't Work with Sedona 1.7.0 </li> <li>[SEDONA-718] -         Auto Detect geometry column in GeoJSON writer </li> </ul>"},{"location":"setup/release-notes/#new-feature","title":"New Feature","text":"<ul> <li>[SEDONA-693] -         Add ST_Perimeter2D </li> <li>[SEDONA-704] -         Add the STAC datasource reader </li> <li>[SEDONA-707] -         Add allTouched parameter for RS_functions that perform rasterization (Geometry to Raster) </li> </ul>"},{"location":"setup/release-notes/#improvement","title":"Improvement","text":"<ul> <li>[SEDONA-685] -         R \u2013 Switch shapefile and geojson readers to DataFrame API sources </li> <li>[SEDONA-689] -         Geostats in SQL </li> <li>[SEDONA-695] -         Expose spatial partitioning structure from SpatialRDD </li> <li>[SEDONA-705] -         Add spatial partitioners that don't introduce duplicates </li> <li>[SEDONA-708] -         SedonaPython should use PyArrow to get GeoPandas DataFrame </li> <li>[SEDONA-711] -         Add Geography user-defined type </li> <li>[SEDONA-715] -         Add Zeppelin Notebook support along with visualization plugin for the docker image </li> <li>[SEDONA-717] -         Fix dataframe_to_arrow() for zero-row case </li> <li>[SEDONA-719] -         Support reading Shapefile with Z/M ordinates </li> </ul>"},{"location":"setup/release-notes/#task","title":"Task","text":"<ul> <li>[SEDONA-25] -         Change Scala Seq type in Adapter </li> <li>[SEDONA-46] -         Add Postgis equivalent ST_ClusterDBSCAN to Apache Sedona. </li> <li>[SEDONA-713] -         Create OSM reader to Apache Sedona </li> <li>[SEDONA-714] -         Add conversion from geopandas to Sedona using arrow. </li> </ul>"},{"location":"setup/release-notes/#sedona-170","title":"Sedona 1.7.0","text":"<p>Sedona 1.7.0 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a major release that includes new features, improvements, bug fixes, API breaking changes, and behavior changes.</p>"},{"location":"setup/release-notes/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@mvaaltola made their first contribution in https://github.com/apache/sedona/pull/1574</li> <li>@emmanuel-ferdman made their first contribution in https://github.com/apache/sedona/pull/1658</li> <li>@MohammadLotfiA made their first contribution in https://github.com/apache/sedona/pull/1659</li> <li>@golfalot made their first contribution in https://github.com/apache/sedona/pull/1673</li> <li>@AmirTallap made their first contribution in https://github.com/apache/sedona/pull/1675</li> <li>@freamdx made their first contribution in https://github.com/apache/sedona/pull/1704</li> </ul>"},{"location":"setup/release-notes/#highlights_2","title":"Highlights","text":"<ul> <li> Add a new join algorithm for distributed K Nearest Neighbor Join and a corresponding ST_KNN function</li> <li> Add new spatial statistics algorithms DBSCAN, Local Outlier Factor, and Getis Ord Hot Spot Analysis</li> <li> Add new DataFrame based readers for Shapefile, and GeoPackage</li> <li> Add 10 new ST functions</li> </ul>"},{"location":"setup/release-notes/#api-breaking-changes","title":"API breaking changes","text":"<ul> <li> The support of Spark 3.0, 3.1, 3.2 is dropped. Sedona is now only compatible with Spark 3.3, 3.4, and 3.5.</li> <li> Rasterio is no longer a mandatory dependency. You can still use Sedona Raster without rasterio. If you need to write rasterio UDF in Sedona, you can install it separately.</li> </ul>"},{"location":"setup/release-notes/#behavior-changes","title":"Behavior changes","text":"<ul> <li> JTS version is upgraded to 1.20.0. This may cause some behavior changes in ST functions that rely on JTS.</li> <li> ST_Length, ST_Length2D and ST_LengthSpheroid now only return the length for line objects. It now returns 0 for polygon objects.</li> <li> ST_Perimeter now only returns the perimeter for polygon objects. It now returns 0 for line objects.</li> </ul>"},{"location":"setup/release-notes/#bug_3","title":"Bug","text":"<ul> <li>[SEDONA-650] -         Fiona-Geopandas Compatibility Issue in Python 3.8 </li> <li>[SEDONA-665] -         Docker build failed at ubuntu 22 with rasterio 1.4.0+ </li> <li>[SEDONA-669] -         GeoParquet format should handle timestamp_ntz columns properly </li> <li>[SEDONA-670] -         GeoJSON reader does not work properly on DBR </li> <li>[SEDONA-672] -         Bug fix for ST_LengthSpheroid </li> <li>[SEDONA-673] -         Cannot load GeoParquet without bbox metadata when spatial filter is applied </li> <li>[SEDONA-677] -         Kryo deserialization for null envelopes results in unit envelopes </li> <li>[SEDONA-682] -         Sedona Spark 3.3 does not compile on Scala 2.13 </li> </ul>"},{"location":"setup/release-notes/#new-feature_1","title":"New Feature","text":"<ul> <li>[SEDONA-646] -         Shapefile data source for DataFrame API </li> <li>[SEDONA-647] -         Add ST_RemoveRepeatedPoints </li> <li>[SEDONA-648] -         Implement Distributed K Nearest Neighbor Join </li> <li>[SEDONA-652] -         Add ST_MakeEnvelope </li> <li>[SEDONA-654] -         Add ST_RotateY </li> <li>[SEDONA-655] -         DBSCAN </li> <li>[SEDONA-656] -         Add ST_Project </li> <li>[SEDONA-658] -         Add ST_Simplify </li> <li>[SEDONA-659] -         Upgrade jts version to 1.20.0 </li> <li>[SEDONA-661] -         Local Outlier Factor </li> <li>[SEDONA-664] -         Add native GeoPackage reader </li> <li>[SEDONA-666] -         Add ST_Scale and ST_ScaleGeom </li> <li>[SEDONA-667] -         Getis Ord G Local </li> <li>[SEDONA-671] -         Spider random spatial data generator </li> <li>[SEDONA-675] -         Add ST_InterpolatePoint </li> <li>[SEDONA-676] -         Add ST_Perimeter </li> </ul>"},{"location":"setup/release-notes/#improvement_1","title":"Improvement","text":"<ul> <li>[SEDONA-636] -         datatype geometry is not supported when 'create table xxx (geom geometry) </li> <li>[SEDONA-640] -         Refactor support for multiple spark versions in the build </li> <li>[SEDONA-642] -         R \u2013 Adapt R package for split version of jars </li> <li>[SEDONA-644] -         R \u2013 Update for SedonaContext </li> <li>[SEDONA-649] -         Fix spelling in Java files </li> <li>[SEDONA-653] -         Add lenient mode for RS_Clip </li> <li>[SEDONA-663] -         Support spark connect in dataframe api </li> <li>[SEDONA-678] -         Fix ST_Length and ST_Length2D behavior </li> <li>[SEDONA-679] -         Fix ST_LengthSpheroid behavior </li> </ul>"},{"location":"setup/release-notes/#task_1","title":"Task","text":"<ul> <li>[SEDONA-651] -         Add spark prefix to all sedona spark config </li> <li>[SEDONA-662] -         Clean Up Dead Code from DBSCAN </li> <li>[SEDONA-668] -         Drop the support of Spark 3.0, 3.1, 3.2 </li> <li>[SEDONA-674] -         Make the rasterio binding for sedona-python work with GDAL 3.10 </li> <li>[SEDONA-680] -         Remove rasterio from mandatory dependency </li> <li>[SEDONA-681] -         Bump GeoTools version from 28.2 to 28.5 </li> <li>[SEDONA-683] -         Exclude some repetitive dependencies </li> </ul>"},{"location":"setup/release-notes/#sedona-161","title":"Sedona 1.6.1","text":"<p>Sedona 1.6.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes bug fixes and minor improvements.</p>"},{"location":"setup/release-notes/#new-contributors_4","title":"New Contributors","text":"<ul> <li>@zhangfengcdt made their first contribution in https://github.com/apache/sedona/pull/1431</li> <li>@james-willis made their first contribution in https://github.com/apache/sedona/pull/1453</li> </ul>"},{"location":"setup/release-notes/#highlights_3","title":"Highlights","text":"<ul> <li> Add native DataFrame based GeoJSON reader and writer</li> <li> 48 new ST functions added</li> <li> GeoParquet reader and writer supports GeoParquet 1.1.0 covering column</li> <li> Improve the error handling of ST functions so that the error message includes the geometry that caused the error</li> </ul>"},{"location":"setup/release-notes/#api-breaking-changes_1","title":"API breaking changes","text":"<ul> <li> The following raster functions now return struct type outputs instead of array types.</li> <li>RS_Metadata</li> <li>RS_SummaryStatsAll</li> <li>RS_ZonalStatsAll</li> <li>RS_GeoTransform</li> </ul>"},{"location":"setup/release-notes/#bug_4","title":"Bug","text":"<ul> <li>[SEDONA-560] -         Spatial join involving dataframe containing 0 partition throws exception </li> <li>[SEDONA-561] -         Failed to run examples in the core.showcase package </li> <li>[SEDONA-580] -         New instances of RasterUDT object is not equal to the RasterUDT case object </li> <li>[SEDONA-581] -         SedonaKepler fails to reload if a raster column exists </li> <li>[SEDONA-605] -         RS_AsRaster(useGeometryExtent=false) does not work with reference rasters with scaleX/Y &lt; 1 </li> <li>[SEDONA-608] -         Fix ST_IsPolygonCW, ST_IsPolygonCCW, ST_ForcePolygonCW and ST_ForcePolygonCCW </li> <li>[SEDONA-609] -         Fix python 3.12 build issue caused by binary compatibility issues with numpy 2.0.0 </li> <li>[SEDONA-611] -         Cannot write rasters to S3 on EMR </li> <li>[SEDONA-618] -         Maven build failed with javadoc classes and package list files missing </li> <li>[SEDONA-624] -         Distance join throws java.lang.reflect.InvocationTargetException when working with aggregation functions </li> <li>[SEDONA-626] -         SRID of geometries returned by many ST functions are incorrect </li> <li>[SEDONA-628] -         Python DataFrame Functions Cannot Be Imported As Documented </li> <li>[SEDONA-639] -         ST_Split may produce inaccurate results when splitting linestrings </li> </ul>"},{"location":"setup/release-notes/#new-feature_2","title":"New Feature","text":"<ul> <li>[SEDONA-462] -         ST_IsValidDetail </li> <li>[SEDONA-486] -         Implement ST_MMin </li> <li>[SEDONA-487] -         Implement ST_MMax </li> <li>[SEDONA-562] -         Add native DataFrame based GeoJSON reader and writer </li> <li>[SEDONA-563] -         Add ST_GeomFromEWKB </li> <li>[SEDONA-564] -         Add ST_NumInteriorRing </li> <li>[SEDONA-565] -         Add ST_ForceRHR </li> <li>[SEDONA-566] -         Add ST_TriangulatePolygon </li> <li>[SEDONA-567] -         Add ST_M </li> <li>[SEDONA-569] -         Add ST_PointZM </li> <li>[SEDONA-570] -         Add ST_PointM </li> <li>[SEDONA-571] -         Add ST_MMin </li> <li>[SEDONA-572] -         Add ST_PointFromWKB </li> <li>[SEDONA-573] -         Add ST_HasM </li> <li>[SEDONA-574] -         Add ST_MMax </li> <li>[SEDONA-575] -         Add ST_LineFromWKB </li> <li>[SEDONA-576] -         Add ST_HasZ </li> <li>[SEDONA-577] -         Add ST_GeometryFromText </li> <li>[SEDONA-578] -         Add ST_Points </li> <li>[SEDONA-579] -         Add ST_AsHEXEWKB </li> <li>[SEDONA-582] -         Add ST_PointFromGeoHash </li> <li>[SEDONA-583] -         Add ST_Length2D </li> <li>[SEDONA-584] -         Add ST_Zmflag </li> <li>[SEDONA-585] -         Add ST_ForceCollection </li> <li>[SEDONA-586] -         Add ST_Force3DZ </li> <li>[SEDONA-587] -         Add ST_Force3DM </li> <li>[SEDONA-588] -         Add ST_Force4D </li> <li>[SEDONA-589] -         Add ST_LongestLine </li> <li>[SEDONA-590] -         Add ST_GeomColFromText </li> <li>[SEDONA-591] -         Add ST_MaxDistance </li> <li>[SEDONA-592] -         Add ST_MPointFromText </li> <li>[SEDONA-593] -         Add ST_Relate </li> <li>[SEDONA-594] -         Add ST_RelatedMatch </li> <li>[SEDONA-595] -         Add ST_LineStringFromWKB </li> <li>[SEDONA-596] -         Add ST_SimplifyVW </li> <li>[SEDONA-597] -         Add ST_SimplifyPolygonHull </li> <li>[SEDONA-598] -         Add ST_UnaryUnion </li> <li>[SEDONA-599] -         Add ST_MinimumClearance </li> <li>[SEDONA-600] -         Add ST_MinimumClearanceLine </li> <li>[SEDONA-601] -         Add ST_DelaunyTriangles </li> <li>[SEDONA-602] -         Add ST_LocateAlong </li> <li>[SEDONA-603] -         Add ST_MakePointM </li> <li>[SEDONA-604] -         Add ST_AddMeasure </li> <li>[SEDONA-606] -         Add ST_IsValidDetail </li> <li>[SEDONA-607] -         Include Geometry in ST Function Exceptions </li> <li>[SEDONA-610] -         Add ST_IsValidTrajectory </li> <li>[SEDONA-615] -         Add ST_MaximumInscribedCircle </li> <li>[SEDONA-617] -         Add ST_Rotate </li> <li>[SEDONA-625] -         Add ST_GeneratePoints </li> <li>[SEDONA-627] -         Writing covering column metadata to GeoParquet files </li> <li>[SEDONA-631] -         Add ST_Expand </li> <li>[SEDONA-643] -         Fix Flink constructor functions signatures </li> <li>[SEDONA-645] -         Add ST_RotateX </li> </ul>"},{"location":"setup/release-notes/#improvement_2","title":"Improvement","text":"<ul> <li>[SEDONA-558] -         Fix and improve SedonaPyDeck behavior </li> <li>[SEDONA-559] -         Make the flink example work </li> <li>[SEDONA-568] -         Refactor TestBaseScala to use method instead of a class-level variable for sparkSession </li> <li>[SEDONA-616] -         Apply spotless to snowflake module </li> <li>[SEDONA-620] -         Simplify Java if statements </li> <li>[SEDONA-621] -         Remove redundant call to `toString()` </li> <li>[SEDONA-622] -         Improve SedonaPyDeck behavior </li> <li>[SEDONA-623] -         Simplify Java `if` statements </li> <li>[SEDONA-629] -         Return Structs for RS_ Functions </li> <li>[SEDONA-632] -         Don't use a conventional output committer when writing raster files using df.write.format(\"raster\") </li> <li>[SEDONA-633] -         Add tileWidth and tileHeight fields to the result of RS_Metadata </li> <li>[SEDONA-634] -         Support omitting tileWidth and tileHeight parameters when calling RS_Tile or RS_TileExplode on rasters with decent tiling scheme </li> <li>[SEDONA-635] -         Allow feature and feature collection format in ST_AsGeoJSON </li> <li>[SEDONA-637] -         Show spatial filters pushed to GeoParquet scans in the query plan </li> <li>[SEDONA-638] -         Send telemetry data asynchronously to avoid blocking the initialization of SedonaContext </li> </ul>"},{"location":"setup/release-notes/#task_2","title":"Task","text":"<ul> <li>[SEDONA-101] -         Add Scala Formatter to MVN </li> <li>[SEDONA-102] -         Java Code Formatting using formatter plugin </li> <li>[SEDONA-553] -         Update Sedona docker to use newer GeoPandas </li> </ul>"},{"location":"setup/release-notes/#sedona-160","title":"Sedona 1.6.0","text":"<p>Sedona 1.6.0 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p>"},{"location":"setup/release-notes/#new-contributors_5","title":"New Contributors","text":"<ul> <li>@mpetazzoni made their first contribution in https://github.com/apache/sedona/pull/1216</li> <li>@sebdiem made their first contribution in https://github.com/apache/sedona/pull/1217</li> <li>@guilhem-dvr made their first contribution in https://github.com/apache/sedona/pull/1229</li> <li>@niklas-petersen made their first contribution in https://github.com/apache/sedona/pull/1252</li> <li>@mebrein made their first contribution in https://github.com/apache/sedona/pull/1334</li> <li>@docete made their first contribution in https://github.com/apache/sedona/pull/1409</li> </ul>"},{"location":"setup/release-notes/#highlights_4","title":"Highlights","text":"<ul> <li> Sedona is now compatible with Shapely 2.0 and GeoPandas 0.11.1+.</li> <li> Sedona added enhanced support for geography data. This includes<ul> <li>ST_Buffer with spheroid distance</li> <li>ST_BestSRID to find the best SRID for a geometry</li> <li>ST_ShiftLongitude to shift the longitude of a geometry to mitigate the issue of crossing the date line</li> <li>ST_CrossesDateLine to check if a geometry crosses the date line</li> <li>ST_DWithin now supports spheroid distance</li> </ul> </li> <li> Sedona Spark Sedona Raster allows RS_ReropjectMatch to wrap the extent of one raster to another raster, similar to RasterArray.reproject_match function in rioxarray</li> <li> Sedona Spark Sedona Raster now supports Rasterio and NumPy UDF by <code>raster.as_numpy</code>, <code>raster.as_numpy_masked</code>, <code>raster.as_rasterio</code>. You can perform any native function from rasterio and numpy and run them in parallel. See the example below.</li> </ul> <pre><code>from pyspark.sql.types import DoubleType\n\n\ndef mean_udf(raster):\n    return float(raster.as_numpy().mean())\n\n\nsedona.udf.register(\"mean_udf\", mean_udf, DoubleType())\ndf_raster.withColumn(\"mean\", expr(\"mean_udf(rast)\")).show()\n</code></pre>"},{"location":"setup/release-notes/#bug_5","title":"Bug","text":"<ul> <li>[SEDONA-532] - Sedona Spark SQL optimizer cannot optimize joins with complex conditions </li> <li>[SEDONA-543] - RS_Union_aggr gives referenceRaster is null error when run on cluster </li> </ul>"},{"location":"setup/release-notes/#new-feature_3","title":"New Feature","text":"<ul> <li>[SEDONA-467] - Add optimized join support for ST_DWithin </li> <li>[SEDONA-468] - Add provision to use spheroid distance in ST_DWithin </li> <li>[SEDONA-475] - Add RS_NormalizeAll to normalize all bands of a raster </li> <li>[SEDONA-480] - Implement ST_S2ToGeom </li> <li>[SEDONA-481] - Implements ST_Snap </li> <li>[SEDONA-484] - Implement ST_IsPolygonCW </li> <li>[SEDONA-488] - ST_Buffer with spheroid distance </li> <li>[SEDONA-498] - Add ST_BestSRID </li> <li>[SEDONA-499] - Add Spheroidal ST_Buffer </li> <li>[SEDONA-504] - Add ST_ShiftLongitude </li> <li>[SEDONA-508] - Add ST_CrossesDateLine </li> <li>[SEDONA-509] - Add Single Statistic RS_SummaryStats </li> <li>[SEDONA-514] - Add RS_SetPixelType </li> <li>[SEDONA-516] - Add RS_Interpolate </li> <li>[SEDONA-517] - Add RS_MakeRaster for constructing a new raster using given array data as band data </li> <li>[SEDONA-518] - Add RS_ReprojectMatch for wrapping the extent of one raster to another raster </li> <li>[SEDONA-522] - Add ST_Union with array of Geometry as input </li> <li>[SEDONA-533] - Implement ST_Polygonize </li> <li>[SEDONA-539] - Support Snowflake geography type </li> </ul>"},{"location":"setup/release-notes/#improvement_3","title":"Improvement","text":"<ul> <li>[SEDONA-483] - Implements ST_IsPolygonCCW </li> <li>[SEDONA-493] - Update default behavior of RS_NormalizeAll </li> <li>[SEDONA-503] - Support Shapely 2.0 in PySpark binding </li> <li>[SEDONA-521] - Change ST_H3ToGeom Behavior </li> <li>[SEDONA-549] - RS_Union_aggr should support combining all bands in multi-band rasters </li> </ul>"},{"location":"setup/release-notes/#task_3","title":"Task","text":"<ul> <li>[SEDONA-540] - Fix failed ST_Buffer and ST_Snap Snowflake tests </li> <li>[SEDONA-550] - Remove the version upper bound of Pandas, GeoPandas </li> <li>[SEDONA-557] - Bump Flink from 1.14.x to 1.19.0"},{"location":"setup/release-notes/#sedona-153","title":"Sedona 1.5.3","text":"<p>Sedona 1.5.3 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes one bug fix on top of Sedona 1.5.2. No new features or major changes are added in this release.</p>"},{"location":"setup/release-notes/#bug_6","title":"Bug","text":"<ul> <li>[SEDONA-556] - Hidden requirement for geopandas in apache-sedona 1.5.2 </li> <li>[SEDONA-555] - Snowflake Native App should not always create a new role </li> </ul>"},{"location":"setup/release-notes/#sedona-152","title":"Sedona 1.5.2","text":"<p>Sedona 1.5.2 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes bug fixes and minor improvements. No new features or major changes are added in this release.</p>"},{"location":"setup/release-notes/#new-contributors_6","title":"New Contributors","text":"<ul> <li>@mpetazzoni made their first contribution in https://github.com/apache/sedona/pull/1216</li> <li>@sebdiem made their first contribution in https://github.com/apache/sedona/pull/1217</li> <li>@guilhem-dvr made their first contribution in https://github.com/apache/sedona/pull/1229</li> <li>@niklas-petersen made their first contribution in https://github.com/apache/sedona/pull/1252</li> <li>@mebrein made their first contribution in https://github.com/apache/sedona/pull/1334</li> </ul>"},{"location":"setup/release-notes/#bug_7","title":"Bug","text":"<ul> <li>[SEDONA-470] - Cannot distinguish between missing or null crs from the result of geoparquet.metadata </li> <li>[SEDONA-471] - SedonaKepler cannot work with Uber H3 hex since 1.5.1 </li> <li>[SEDONA-472] - Adapter API no longer works with unshaded jar </li> <li>[SEDONA-473] - cdm-core mistakenly becomes a compile dependency for sedona-spark-shaded </li> <li>[SEDONA-477] - Avoid producing rasters with images having non-zero origins </li> <li>[SEDONA-478] - Sedona 1.5.1 context initialization fails without GeoTools coverage </li> <li>[SEDONA-479] - Fix RS_Normalize: Incorrect behavior for double arrays </li> <li>[SEDONA-494] - Raster data source cannot write to HDFS </li> <li>[SEDONA-495] - Raster data source uses shared FileSystem connections which lead to race condition </li> <li>[SEDONA-497] - SpatialRDD read from multiple Shapefiles has incorrect fieldName property </li> <li>[SEDONA-500] - Cannot correctly read data from directories containing multiple shapefiles </li> <li>[SEDONA-501] - ST_Split maps to wrong Java-call </li> <li>[SEDONA-505] - Treat geometry with SRID=0 as if it was in EPSG:4326 in various raster functions </li> <li>[SEDONA-507] - RS_AsImage cannot visualize rasters with non-integral band data </li> <li>[SEDONA-510] - geometry columns with snake_case names in GeoParquet files cannot be recognized as geometry column </li> <li>[SEDONA-511] - geometry columns with snake_case names in GeoParquet files cannot be recognized as geometry column </li> <li>[SEDONA-519] - ST_SubDivide (Snowflake) fails even on documentation example </li> <li>[SEDONA-520] - Missing dependencies in Snowflake JAR </li> <li>[SEDONA-531] - RDD spatial join in Python throws Not available error </li> <li>[SEDONA-534] - Disable Python warning message of finding jars </li> <li>[SEDONA-545] - Sedona Python DataFrame API fail due to missing commas </li> <li>[SEDONA-548] - Fix Python Dataframe API Constructor registrations </li> </ul>"},{"location":"setup/release-notes/#improvement_4","title":"Improvement","text":"<ul> <li>[SEDONA-474] - Remove manipulation of warnings config </li> <li>[SEDONA-506] - Add lenient mode for RS_ZonalStats and RS_ZonalStatsAll </li> <li>[SEDONA-512] - Python serializer should report the object type in the error message </li> <li>[SEDONA-515] - Add handling for noDataValues in RS_Resample </li> <li>[SEDONA-529] - Add basic `EditorConfig` file </li> <li>[SEDONA-535] - Add the pull request labeler </li> <li>[SEDONA-536] - Add CODEOWNERS file </li> <li>[SEDONA-541] - Allow concurrent snowflake testers </li> </ul>"},{"location":"setup/release-notes/#test","title":"Test","text":"<ul> <li>[SEDONA-513] - Add pre-commit hook `mixed-line-ending` </li> <li>[SEDONA-523] - Add pre-commit hook `fix-byte-order-marker` </li> <li>[SEDONA-524] - Clean up the `pre-commit` config </li> <li>[SEDONA-525] - Add two more pre-commit hooks </li> <li>[SEDONA-528] - Add `pre-commit` hook `check-yaml` </li> <li>[SEDONA-530] - Add `pre-commit` hook `debug-statements` </li> <li>[SEDONA-537] - Add pre-commit hook `requirements-txt-fixer` </li> <li>[SEDONA-538] - Add four more pre-commit hooks </li> <li>[SEDONA-542] - Add `pre-commit` hook `check-executables-have-shebangs` </li> <li>[SEDONA-544] - Add `ruff-pre-commit` for `Python` linting </li> <li>[SEDONA-546] - Python linting enable rule `E712` </li> </ul>"},{"location":"setup/release-notes/#task_4","title":"Task","text":"<ul> <li>[SEDONA-469] - Update Sedona docker and binder to use 1.5.1 </li> <li>[SEDONA-496] - Dependabot: reduce the open pull requests limit to 2 </li> <li>[SEDONA-526] - Upgrade `actions/setup-java` to `v4` </li> </ul>"},{"location":"setup/release-notes/#sedona-151","title":"Sedona 1.5.1","text":"<p>Sedona 1.5.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p>"},{"location":"setup/release-notes/#highlights_5","title":"Highlights","text":"<ul> <li> Sedona Snowflake Add support for Snowflake</li> <li> Sedona Spark Support Spark 3.5</li> <li> Sedona Spark Support Snowflake 7+</li> <li> Sedona Spark Added 20+ raster functions (or variants)</li> <li> Sedona Spark/Flink/Snowflake Added 7 vector functions (or variants)</li> <li> Sedona Spark GeoParquet reader and writer supports projjson in metadata</li> <li> Sedona Spark GeoParquet reader and writer conform to GeoParquet spec 1.0.0 instead of 1.0.0-beta1</li> <li> Sedona Spark Added a legacyMode in GeoParquet reader for 1.5.1+ users to read Parquet files written by Sedona 1.3.1 and earlier</li> <li> Sedona Spark Fixed a bug in GeoParquet writer so 1.3.1 and earlier users can read Parquet files written by 1.5.1+</li> </ul>"},{"location":"setup/release-notes/#behavior-change","title":"Behavior change","text":"<ul> <li>All raster functions that take a geometry will implicitly transform the CRS of the geometry if needed.</li> <li>The default CRS for these functions is 4326 for raster and geometry involved in raster functions, if not specified.</li> <li>KeplerGL and DeckGL become optional dependencies for Sedona Spark Python.</li> </ul>"},{"location":"setup/release-notes/#new-contributors_7","title":"New Contributors","text":"<ul> <li>@hongbo-miao made their first contribution in https://github.com/apache/sedona/pull/1063</li> <li>@prantogg made their first contribution in https://github.com/apache/sedona/pull/1122</li> <li>@MyEnthusiastic made their first contribution in https://github.com/apache/sedona/pull/1130</li> <li>@duhaode520 made their first contribution in https://github.com/apache/sedona/pull/1193</li> </ul>"},{"location":"setup/release-notes/#bug_8","title":"Bug","text":"<ul> <li>[SEDONA-414] - ST_MakeLine in sedona-spark does not work with array inputs </li> <li>[SEDONA-417] - Fix SedonaUtils.display_image </li> <li>[SEDONA-419] - SedonaKepler and SedonaPyDeck should not be in `sedona.spark` </li> <li>[SEDONA-420] - Make SedonaKepler and SedonaPydeck optional dependencies </li> <li>[SEDONA-424] - Specify jt-jiffle as a provided dependency </li> <li>[SEDONA-426] - Change cloning of rasters to be able to include metadata. </li> <li>[SEDONA-440] - GeoParquet reader should support filter pushdown on nested fields </li> <li>[SEDONA-443] - Upload-artifact leads to 503 error </li> <li>[SEDONA-453] - Performance degrade when indexing points using Quadtree </li> <li>[SEDONA-456] - SedonaKepler cannot work with geopandas &gt;= 0.13.0 correctly </li> </ul>"},{"location":"setup/release-notes/#new-feature_4","title":"New Feature","text":"<ul> <li>[SEDONA-369] - Add ST_DWITHIN </li> <li>[SEDONA-411] - Add RS_Rotation </li> <li>[SEDONA-413] - Add buffer parameters to ST_Buffer </li> <li>[SEDONA-415] - Add optional parameter to ST_Transform </li> <li>[SEDONA-421] - Add RS_Clip </li> <li>[SEDONA-422] - Add a feature in RS_SetBandNoDataValue and fix NoDataValue in RS_Clip </li> <li>[SEDONA-427] - Add RS_RasterToWorldCoord </li> <li>[SEDONA-428] - Add RS_ZonalStats &amp; RS_ZonalStatsAll </li> <li>[SEDONA-430] - geoparquet writer should have an option called `writeToCrs` </li> <li>[SEDONA-431] - Add RS_PixelAsPoints </li> <li>[SEDONA-432] - Add RS_PixelAsCentroids </li> <li>[SEDONA-433] - Improve RS_SummaryStats performance </li> <li>[SEDONA-435] - Add RS_PixelAsPolygons </li> <li>[SEDONA-438] - Add NetCDF reader to Sedona </li> <li>[SEDONA-439] - Add RS_Union_Aggr </li> <li>[SEDONA-441] - Implement ST_LineLocatePoint </li> <li>[SEDONA-449] - Add two raster column support to RS_MapAlgebra </li> <li>[SEDONA-455] - Add a new data source namely geoparquet.metadata </li> <li>[SEDONA-459] - Add Snowflake support </li> <li>[SEDONA-460] - RS_Tile and RS_TileExplode </li> <li>[SEDONA-461] - ST_IsValidReason </li> <li>[SEDONA-465] - Support reading legacy parquet files written by Apache Sedona &lt;= 1.3.1-incubating </li> </ul>"},{"location":"setup/release-notes/#improvement_5","title":"Improvement","text":"<ul> <li>[SEDONA-339] - Skip irrelevant GitHub actions </li> <li>[SEDONA-416] - importing SedonaContext, kepler.gl is not found. </li> <li>[SEDONA-429] - geoparquet reader/writer should print \"1.0.0\" in its version </li> <li>[SEDONA-434] - Improve reliability by resolve the nondeterministic of the order of the Map </li> <li>[SEDONA-436] - Fix RS_SetValues bug </li> <li>[SEDONA-437] - Add implicit CRS transformation </li> <li>[SEDONA-446] - Add floating point datatype support in RS_AsBase64 </li> <li>[SEDONA-448] - RS_SetBandNoDataValue should have `replace` option </li> <li>[SEDONA-454] - Change the default value of sedona.global.indextype from quadtree to rtree </li> <li>[SEDONA-457] - Don't write GeometryUDT into org.apache.spark.sql.parquet.row.metadata when writing GeoParquet files </li> <li>[SEDONA-464] - ST_Valid should have integer flags </li> <li>[SEDONA-466] - RS_AsRaster does not use the weight and height of the raster in its parameters. </li> </ul>"},{"location":"setup/release-notes/#test_1","title":"Test","text":"<ul> <li>[SEDONA-410] - pre-commit: check that scripts with shebangs are executable </li> <li>[SEDONA-412] - pre-commit: add hook `end-of-file-fixer` </li> <li>[SEDONA-423] - pre-commit: apply hook `end-of-file-fixer` to more files </li> <li>[SEDONA-442] - pre-commit: add hook markdown-lint </li> <li>[SEDONA-444] - pre-commit: add hook to trim trailing whitespace </li> <li>[SEDONA-445] - pre-commit: apply hook end-of-file-fixer to more files </li> <li>[SEDONA-447] - pre-commit: apply end-of-file-fixer to more files </li> <li>[SEDONA-463] - Add a Makefile for convenience </li> </ul>"},{"location":"setup/release-notes/#task_5","title":"Task","text":"<ul> <li>[SEDONA-450] - Support Spark 3.5 </li> <li>[SEDONA-458] - The docs should have examples for UDF </li> </ul>"},{"location":"setup/release-notes/#sedona-150","title":"Sedona 1.5.0","text":"<p>Sedona 1.5.0 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_6","title":"Highlights","text":"<p>API breaking changes:</p> <ul> <li>The following functions in Sedona requires the input data must be in longitude/latitude order otherwise they might throw errors. You can use <code>FlipCoordinates</code> to swap X and Y.<ul> <li>ST_Transform</li> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_GeoHash</li> <li>All ST_H3 functions</li> <li>All ST_S2 functions</li> <li>All RS constructors</li> <li>All RS predicates</li> <li>Spark RDD: CRStransform</li> </ul> </li> <li>Rename <code>RS_Count</code> to <code>RS_CountValue</code></li> <li>Drop <code>RS_HTML</code></li> <li>Unshaded Sedona Spark code are all merged to a single jar <code>sedona-spark</code></li> </ul> <p>New features</p> <ul> <li>Add 18 more ST functions for vector data processing in Sedona Spark and Sedona Flink</li> <li>Add 36 more RS functions in Sedona Spark to support comprehensive raster data ETL and analytics<ul> <li>You can now directly join vector and raster datasets together</li> <li>Flexible map algebra equations: <code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table</code></li> </ul> </li> <li>Add native support of Uber H3 functions in Sedona Spark and Sedona Flink.</li> <li>Add SedonaKepler and SedonaPyDeck for interactive map visualization on Sedona Spark.</li> </ul>"},{"location":"setup/release-notes/#bug_9","title":"Bug","text":"<ul> <li>[SEDONA-318] - SerDe for RasterUDT performs poorly </li> <li>[SEDONA-319] - RS_AddBandFromArray does not always produce serializable rasters </li> <li>[SEDONA-322] - The \"Scala and Java build\" CI job occasionally fail </li> <li>[SEDONA-325] - RS_FromGeoTiff is leaking file descriptors </li> <li>[SEDONA-329] - Remove geometry_col parameter from SedonaKepler APIs </li> <li>[SEDONA-330] - Fix bugs in SedonaPyDeck </li> <li>[SEDONA-332] - RS_Value and RS_Values don't need to fetch all the pixel data </li> <li>[SEDONA-337] - Failure falling back to pure python implementation when geomserde_speedup is unavailable </li> <li>[SEDONA-338] - Refactor Raster construction in sedona to use AffineTransform instead of envelope </li> <li>[SEDONA-358] - Refactor Functions to remove geotools dependency for most vector functions </li> <li>[SEDONA-362] - RS_BandAsArray truncates the decimal part of float/double pixel values. </li> <li>[SEDONA-373] - Move RasterPredicates to correct raster package to prevent redundant imports </li> <li>[SEDONA-394] - fix RS_Band data type bug </li> <li>[SEDONA-401] - Handle null values in RS_AsMatrix </li> <li>[SEDONA-402] - Floor grid coordinates received from geotools </li> <li>[SEDONA-403] - Add Null tolerance to RS_AddBandFromArray </li> <li>[SEDONA-405] - Sedona driver Out of Memory on 1.4.1 </li> </ul>"},{"location":"setup/release-notes/#new-feature_5","title":"New Feature","text":"<ul> <li>[SEDONA-200] - Add ST_CoordDim to Sedona </li> <li>[SEDONA-213] - Add ST_BoundingDiagonal to Sedona </li> <li>[SEDONA-237] - Implement ST_Dimension </li> <li>[SEDONA-238] - Implement OGC GeometryType </li> <li>[SEDONA-293] - Implement ST_IsCollection </li> <li>[SEDONA-294] - Implement ST_Angle </li> <li>[SEDONA-295] - Implement ST_LineInterpolatePoint in Flink </li> <li>[SEDONA-296] - Implement ST_Multi in Sedona Flink </li> <li>[SEDONA-298] - Implement ST_ClosestPoint </li> <li>[SEDONA-299] - Implement ST_FrechetDistance </li> <li>[SEDONA-300] - Implement ST_HausdorffDistance </li> <li>[SEDONA-301] - Implement ST_Affine </li> <li>[SEDONA-303] - Port all Sedona Spark functions to Sedona Flink </li> <li>[SEDONA-310] - Add ST_Degrees to sedona </li> <li>[SEDONA-314] - Support Optimized join on ST_HausdorffDistance </li> <li>[SEDONA-315] - Support Optimized join on ST_FrechetDistance </li> <li>[SEDONA-321] - Implement RS_Intersects(raster, geom) </li> <li>[SEDONA-323] - Add wrapper for KeplerGl visualization in sedona </li> <li>[SEDONA-328] - Add wrapper for pydeck visualizations in sedona </li> <li>[SEDONA-331] - Add RS_Height and RS_Width </li> <li>[SEDONA-334] - Add ScaleX and ScaleY </li> <li>[SEDONA-335] - Add RS_PixelAsPoint </li> <li>[SEDONA-336] - Add RS_UpperLeftX and RS_UpperLeftY </li> <li>[SEDONA-340] - Add RS_ConvexHull </li> <li>[SEDONA-343] - Add raster predicates: Contains and Within </li> <li>[SEDONA-344] - Add RS_RasterToWorldCoordX, RS_RasterToWorldCoordY </li> <li>[SEDONA-346] - Add RS_WorldToRaster APIs </li> <li>[SEDONA-353] - Add RS_BandNoDataValue </li> <li>[SEDONA-354] - Add RS_SkewX and RS_SkewY </li> <li>[SEDONA-355] - Add RS_BandPixelType </li> <li>[SEDONA-357] - Implement ST_VoronoiPolygons </li> <li>[SEDONA-359] - Add RS_GeoReference </li> <li>[SEDONA-361] - Add RS_MapAlgebra for performing map algebra operations using simple expressions </li> <li>[SEDONA-363] - Add RS_PixelAsPolygon </li> <li>[SEDONA-364] - Add RS_MinConvexHull </li> <li>[SEDONA-366] - Add RS_Count </li> <li>[SEDONA-367] - Add RS_PixelAsCentroid </li> <li>[SEDONA-368] - Add RS_SummaryStats </li> <li>[SEDONA-371] - Add optimized join support for raster-vector and raster-raster(if any) joins </li> <li>[SEDONA-372] - Add RS_SetGeoReference </li> <li>[SEDONA-375] - Add RS_SetBandNoDataValue </li> <li>[SEDONA-376] - Add RS_SetValues </li> <li>[SEDONA-378] - Add RS_SetValue </li> <li>[SEDONA-379] - Add RS_AsBase64 </li> <li>[SEDONA-383] - Add RS_Band </li> <li>[SEDONA-387] - Add RS_BandIsNoData </li> <li>[SEDONA-388] - Add RS_AsRaster </li> <li>[SEDONA-391] - Add RS_AsMatrix </li> <li>[SEDONA-393] - Add RS_AsPNG </li> <li>[SEDONA-395] - Add RS_AsImage </li> <li>[SEDONA-396] - Add RS_SetValues Geometry variant </li> <li>[SEDONA-398] - Add RS_AddBand </li> <li>[SEDONA-404] - Add RS_Resample </li> </ul>"},{"location":"setup/release-notes/#improvement_6","title":"Improvement","text":"<ul> <li>[SEDONA-39] - Fix the Lon/lat order issue in Sedona </li> <li>[SEDONA-114] - Add ST_MakeLine to Apache Sedona </li> <li>[SEDONA-142] - Add ST_Collect to Flink Catalog </li> <li>[SEDONA-311] - Refactor InferredExpression to handle functions with arbitrary arity </li> <li>[SEDONA-313] - Refactor ST_Affine to support signature like PostGIS </li> <li>[SEDONA-324] - R \u2013 Fix failing tests </li> <li>[SEDONA-326] - Improve raster band algebra functions for easier preprocessing of raster data </li> <li>[SEDONA-327] - Refactor InferredExpression to handle GridCoverage2D </li> <li>[SEDONA-333] - Support EWKT parser in ST_GeomFromWKT </li> <li>[SEDONA-347] - Centralize usages of transform() </li> <li>[SEDONA-350] - Refactor RS_AddBandFromArray to allow adding a custom noDataValue </li> <li>[SEDONA-352] - Refactor MakeEmptyRaster to allow setting custom datatype for the raster </li> <li>[SEDONA-360] - Handle nodata values of raster bands in a more concise way </li> <li>[SEDONA-365] - Refactor RS_Count to RS_CountValue </li> <li>[SEDONA-374] - RS predicates should support (geom, rast) and (rast, rast) as arguments, and use the convex hull of rasters for spatial relationship testing </li> <li>[SEDONA-385] - Set the Maven Central to be the first repository to check </li> <li>[SEDONA-386] - Speed up GridCoverage2D serialization </li> <li>[SEDONA-392] - Add five more pre-commit hooks </li> <li>[SEDONA-399] - Support Uber H3 cells </li> <li>[SEDONA-400] - pre-commit add hook to ensure that links to vcs websites are permalinks </li> <li>[SEDONA-408] - Set a reasonable default size for RasterUDT </li> </ul>"},{"location":"setup/release-notes/#task_6","title":"Task","text":"<ul> <li>[SEDONA-316] - Refactor Sedona Jupyter notebook examples with unified SedonaContext entrypoint </li> <li>[SEDONA-317] - Change map visualization in Jupyter notebooks with KeplerGL </li> <li>[SEDONA-341] - Move RS_Envelope to GeometryFunctions </li> <li>[SEDONA-356] - Change CRS transformation from lat/lon to lon/lat order </li> <li>[SEDONA-370] - Completely drop the old GeoTiff reader and writer </li> <li>[SEDONA-377] - Change sphere/spheroid functions to work with coordinates in lon/lat order </li> <li>[SEDONA-380] - Merge all Sedona Spark module to a single module </li> <li>[SEDONA-381] - Merge python-adapter to sql module </li> <li>[SEDONA-382] - Merge SQL and Core module to a single Spark module </li> <li>[SEDONA-384] - Merge viz module to the spark module </li> <li>[SEDONA-397] - Move Map Algebra functions </li> </ul>"},{"location":"setup/release-notes/#sedona-141","title":"Sedona 1.4.1","text":"<p>Sedona 1.4.1 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_7","title":"Highlights","text":"<ul> <li> Sedona Spark More raster functions and bridge RasterUDT and Map Algebra operators. See Raster based operators and Raster to Map Algebra operators.</li> <li> Sedona Spark &amp; Flink Added geodesic / geography functions:<ul> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_AreaSpheroid</li> <li>ST_LengthSpheroid</li> </ul> </li> <li> Sedona Spark &amp; Flink Introduced <code>SedonaContext</code> to unify Sedona entry points.</li> <li> Sedona Spark Support Spark 3.4.</li> <li> Sedona Spark Added a number of new ST functions.</li> <li> Zeppelin Zeppelin helium plugin supports plotting geometries like linestring, polygon.</li> </ul>"},{"location":"setup/release-notes/#api-change","title":"API change","text":"<ul> <li> <p>Sedona Spark &amp; Flink Introduced a new entry point called SedonaContext to unify all Sedona entry points in different compute engines and deprecate old Sedona register entry points. Users no longer have to register Sedona kryo serializer and import many tedious Python classes.</p> <ul> <li> <p>Sedona Spark:</p> <ul> <li>Scala:</li> </ul> <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(SedonaContext.builder().master(\"local[*]\").getOrCreate())\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre> <ul> <li>Python:</li> </ul> <pre><code>from sedona.spark import *\n\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.4.1,\"\n        \"org.datasyslab:geotools-wrapper:1.4.0-28.2\",\n    )\n    .getOrCreate()\n)\nsedona = SedonaContext.create(config)\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre> </li> <li> <p>Sedona Flink:</p> </li> </ul> <pre><code>import org.apache.sedona.flink.SedonaContext;\nStreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\nsedona.sqlQuery(\"SELECT ST_GeomFromWKT(XXX) FROM\");\n</code></pre> </li> </ul>"},{"location":"setup/release-notes/#bug_10","title":"Bug","text":"<ul> <li>[SEDONA-266] - RS_Values throws UnsupportedOperationException for shuffled point arrays </li> <li>[SEDONA-267] - Cannot pip install apache-sedona 1.4.0 from source distribution </li> <li>[SEDONA-273] - Set a upper bound for Shapely, Pandas and GeoPandas </li> <li>[SEDONA-277] - Sedona spark artifacts for scala 2.13 do not have proper POMs </li> <li>[SEDONA-283] - Artifacts were deployed twice when running mvn clean deploy </li> <li>[SEDONA-284] - Property values in dependency deduced POMs for shaded modules were not substituted </li> </ul>"},{"location":"setup/release-notes/#new-feature_6","title":"New Feature","text":"<ul> <li>[SEDONA-196] - Add ST_Force3D to Sedona </li> <li>[SEDONA-239] - Implement ST_NumPoints </li> <li>[SEDONA-264] - zeppelin helium plugin supports plotting geometry like linestring, polygon </li> <li>[SEDONA-280] - Add ST_GeometricMedian </li> <li>[SEDONA-281] - Support geodesic / geography functions </li> <li>[SEDONA-286] - Support optimized distance join on ST_DistanceSpheroid and ST_DistanceSphere </li> <li>[SEDONA-287] - Use SedonaContext to unify Sedona entry points </li> <li>[SEDONA-292] - Bridge Sedona Raster and Map Algebra operators </li> <li>[SEDONA-297] - Implement ST_NRings </li> <li>[SEDONA-302] - Implement ST_Translate </li> </ul>"},{"location":"setup/release-notes/#improvement_7","title":"Improvement","text":"<ul> <li>[SEDONA-167] - Add __pycache__ to Python .gitignore </li> <li>[SEDONA-265] - Migrate all ST functions to Sedona Inferred Expressions </li> <li>[SEDONA-269] - Add data source for writing binary files </li> <li>[SEDONA-270] - Remove redundant serialization for rasters </li> <li>[SEDONA-271] - Add raster function RS_SRID </li> <li>[SEDONA-274] - Move all ST function logics to Sedona common </li> <li>[SEDONA-275] - Add raster function RS_SetSRID </li> <li>[SEDONA-276] - Add support for Spark 3.4 </li> <li>[SEDONA-279] - Sedona-Flink should not depend on Sedona-Spark modules </li> <li>[SEDONA-282] - R \u2013 Add raster write function </li> <li>[SEDONA-290] - RDD Spatial Joins should follow the iterator model </li> </ul>"},{"location":"setup/release-notes/#sedona-140","title":"Sedona 1.4.0","text":"<p>Sedona 1.4.0 is compiled against, Spark 3.3 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_8","title":"Highlights","text":"<ul> <li> Sedona Spark &amp; Flink Serialize and deserialize geometries 3 - 7X faster</li> <li> Sedona Spark &amp; Flink Google S2 based spatial join for fast approximate point-in-polygon join. See Join query in Spark and Join query in Flink</li> <li> Sedona Spark Pushdown spatial predicate on GeoParquet to reduce memory consumption by 10X: see explanation</li> <li> Sedona Spark Automatically use broadcast index spatial join for small datasets</li> <li> Sedona Spark New RasterUDT added to Sedona GeoTiff reader.</li> <li> Sedona Spark A number of bug fixes and improvement to the Sedona R module.</li> </ul>"},{"location":"setup/release-notes/#api-change_1","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Packaging strategy changed. See Maven Coordinate. Please change your Sedona dependencies if needed. We recommend <code>sedona-spark-shaded-3.0_2.12-1.4.0</code> and <code>sedona-flink-shaded_2.12-1.4.0</code></li> <li>Sedona Spark &amp; Flink GeoTools-wrapper version upgraded. Please use <code>geotools-wrapper-1.4.0-28.2</code>.</li> </ul>"},{"location":"setup/release-notes/#behavior-change_1","title":"Behavior change","text":"<ul> <li>Sedona Flink Sedona Flink no longer outputs any LinearRing type geometry. All LinearRing are changed to LineString.</li> <li>Sedona Spark Join optimization strategy changed. Sedona no longer optimizes spatial join when use a spatial predicate together with an equijoin predicate. By default, it prefers equijoin whenever possible. SedonaConf adds a config option called <code>sedona.join.optimizationmode</code>, it can be configured as one of the following values:<ul> <li><code>all</code>: optimize all joins having spatial predicate in join conditions. This was the behavior of Apache Sedona prior to 1.4.0.</li> <li><code>none</code>: disable spatial join optimization.</li> <li><code>nonequi</code>: only enable spatial join optimization on non-equi joins. This is the default mode.</li> </ul> </li> </ul> <p>When <code>sedona.join.optimizationmode</code> is configured as <code>nonequi</code>, it won't optimize join queries such as <code>SELECT * FROM A, B WHERE A.x = B.x AND ST_Contains(A.geom, B.geom)</code>, since it is an equi-join with equi-condition <code>A.x = B.x</code>. Sedona will optimize for <code>SELECT * FROM A, B WHERE ST_Contains(A.geom, B.geom)</code></p>"},{"location":"setup/release-notes/#bug_11","title":"Bug","text":"<ul> <li>[SEDONA-218] - Flaky test caused by improper handling of null struct values in Adapter.toDf </li> <li>[SEDONA-221] - Outer join throws NPE for null geometries </li> <li>[SEDONA-222] - GeoParquet reader does not work in non-local mode </li> <li>[SEDONA-224] - java.lang.NoSuchMethodError when loading GeoParquet files using Spark 3.0.x ~ 3.2.x </li> <li>[SEDONA-225] - Cannot count dataframes loaded from GeoParquet files </li> <li>[SEDONA-227] - Python SerDe Performance Degradation </li> <li>[SEDONA-230] - rdd.saveAsGeoJSON should generate feature properties with field names </li> <li>[SEDONA-233] - Incorrect results for several joins in a single stage </li> <li>[SEDONA-236] - Flakey python tests in tests.serialization.test_[de]serializers </li> <li>[SEDONA-242] - Update jars dependencies in Sedona R to Sedona 1.4.0 version </li> <li>[SEDONA-250] - R Deprecate use of Spark 2.4 </li> <li>[SEDONA-252] - Fix disabled RS_Base64 test </li> <li>[SEDONA-255] - R \u2013 Translation issue for ST_Point and ST_PolygonFromEnvelope </li> <li>[SEDONA-258] - Cannot directly assign raw spatial RDD to CircleRDD using Python binding </li> <li>[SEDONA-259] - Adapter.toSpatialRdd in Python binding does not have valid implementation for specifying custom field names for user data </li> <li>[SEDONA-261] - Cannot run distance join using broadcast index join when the distance expression references to attributes from the right-side relation </li> </ul>"},{"location":"setup/release-notes/#new-feature_7","title":"New Feature","text":"<ul> <li>[SEDONA-156] - predicate pushdown support for GeoParquet </li> <li>[SEDONA-215] - Add ST_ConcaveHull </li> <li>[SEDONA-216] - Upgrade jts version to 1.19.0 </li> <li>[SEDONA-235] - Create ST_S2CellIds in Sedona </li> <li>[SEDONA-246] - R GeoTiff read/write </li> <li>[SEDONA-254] - R \u2013 Add raster type </li> <li>[SEDONA-262] - Don't optimize equi-join by default, add an option to configure when to optimize spatial joins </li> </ul>"},{"location":"setup/release-notes/#improvement_8","title":"Improvement","text":"<ul> <li>[SEDONA-205] - Use BinaryType in GeometryUDT in Sedona Spark </li> <li>[SEDONA-207] - Faster serialization/deserialization of geometry objects </li> <li>[SEDONA-212] - Move shading to separate maven modules </li> <li>[SEDONA-217] - Automatically broadcast small datasets </li> <li>[SEDONA-220] - Upgrade Ubuntu build image from 18.04 to 20.04 </li> <li>[SEDONA-226] - Support reading and writing GeoParquet file metadata </li> <li>[SEDONA-228] - Standardize logging dependencies </li> <li>[SEDONA-231] - Redundant Serde Removal </li> <li>[SEDONA-234] - ST_Point inconsistencies </li> <li>[SEDONA-243] - Improve Sedona R file readers: GeoParquet and Shapefile </li> <li>[SEDONA-244] - Align R read/write functions with the Sparklyr framework </li> <li>[SEDONA-249] - Add jvm flags for running tests on Java 17 </li> <li>[SEDONA-251] - Add raster type to Sedona </li> <li>[SEDONA-253] - Upgrade geotools to version 28.2 </li> <li>[SEDONA-260] - More intuitive configuration of partition and index-build side of spatial joins in Sedona SQL </li> </ul>"},{"location":"setup/release-notes/#sedona-131","title":"Sedona 1.3.1","text":"<p>This version is a minor release on Sedona 1.3.0 line. It fixes a few critical bugs in 1.3.0. We suggest all 1.3.0 users to migrate to this version.</p>"},{"location":"setup/release-notes/#bug-fixes","title":"Bug fixes","text":"<ul> <li>SEDONA-204 - Init value in X/Y/Z max should be -Double.MAX</li> <li>SEDONA-206 - Performance regression of ST_Transform in 1.3.0-incubating</li> <li>SEDONA-210 - 1.3.0-incubating doesn't work with Scala 2.12 sbt projects</li> <li>SEDONA-211 - Enforce release managers to use JDK 8</li> <li>SEDONA-201 - Implement ST_MLineFromText and ST_MPolyFromText methods</li> </ul>"},{"location":"setup/release-notes/#new-feature_8","title":"New Feature","text":"<ul> <li>SEDONA-196 - Add ST_Force3D to Sedona</li> <li>SEDONA-197 - Add ST_ZMin, ST_ZMax to Sedona</li> <li>SEDONA-199 - Add ST_NDims to Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_9","title":"Improvement","text":"<ul> <li>SEDONA-194 - Merge org.datasyslab.sernetcdf into Sedona</li> <li>SEDONA-208 - Use Spark RuntimeConfig in SedonaConf</li> </ul> <p>Note</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ releases binary for both Scala 2.12 and 2.13.</p>"},{"location":"setup/release-notes/#sedona-130","title":"Sedona 1.3.0","text":"<p>This version is a major release on Sedona 1.3.0 line and consists of 50 PRs. It includes many new functions, optimization and bug fixes.</p>"},{"location":"setup/release-notes/#highlights_9","title":"Highlights","text":"<ul> <li> Sedona on Spark in this release is compiled against Spark 3.3.</li> <li> Sedona on Flink in this release is compiled against Flink 1.14.</li> <li> Scala 2.11 support is removed.</li> <li> Spark 2.X support is removed.</li> <li> Python 3.10 support is added.</li> <li> Aggregators in Flink are added</li> <li> Correctness fixes for corner cases in range join and distance join.</li> <li> Native GeoParquet read and write (../../tutorial/sql/#load-geoparquet).<ul> <li><code>df = spark.read.format(\"geoparquet\").option(\"fieldGeometry\", \"myGeometryColumn\").load(\"PATH/TO/MYFILE.parquet\")</code></li> <li><code>df.write.format(\"geoparquet\").save(\"PATH/TO/MYFILE.parquet\")</code></li> </ul> </li> <li> DataFrame style API (../../tutorial/sql.md/#dataframe-style-api)<ul> <li><code>df.select(ST_Point(min_value, max_value).as(\"point\"))</code></li> </ul> </li> <li> Allow WKT format CRS in ST_Transform<ul> <li><code>ST_Transform(geom, \"srcWktString\", \"tgtWktString\")</code></li> </ul> </li> </ul> <pre><code>GEOGCS[\"WGS 84\",\n  DATUM[\"WGS_1984\",\n  SPHEROID[\"WGS 84\",6378137,298.257223563,\n  AUTHORITY[\"EPSG\",\"7030\"]],\n  AUTHORITY[\"EPSG\",\"6326\"]],\n  PRIMEM[\"Greenwich\",0,\n  AUTHORITY[\"EPSG\",\"8901\"]],\n  UNIT[\"degree\",0.0174532925199433,\n  AUTHORITY[\"EPSG\",\"9122\"]],\n  AUTHORITY[\"EPSG\",\"4326\"]]\n</code></pre>"},{"location":"setup/release-notes/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>SEDONA-119 - ST_Touches join query returns true for polygons whose interiors intersect</li> <li>SEDONA-136 - Enable testAsEWKT for Flink</li> <li>SEDONA-137 - Fix ST_Buffer for Flink to work</li> <li>SEDONA-138 - Fix ST_GeoHash for Flink to work</li> <li>SEDONA-153 - Python Serialization Fails with Nulls</li> <li>SEDONA-158 - Fix wrong description about ST_GeometryN in the API docs</li> <li>SEDONA-169 - Fix ST_RemovePoint in accordance with the API document</li> <li>SEDONA-178 - Correctness issue in distance join queries</li> <li>SEDONA-182 - ST_AsText should not return SRID</li> <li>SEDONA-186 - collecting result rows of a spatial join query with SELECT * fails with serde error</li> <li>SEDONA-188 - Python warns about missing <code>jars</code> even when some are found</li> <li>SEDONA-193 - ST_AsBinary produces EWKB by mistake</li> </ul>"},{"location":"setup/release-notes/#new-features_1","title":"New Features","text":"<ul> <li>SEDONA-94 - GeoParquet  Support For Sedona</li> <li>SEDONA-125 - Allows customized CRS in ST_Transform</li> <li>SEDONA-166 - Provide Type-safe DataFrame Style API</li> <li>SEDONA-168 - Add ST_Normalize to Apache Sedona</li> <li>SEDONA-171 - Add ST_SetPoint to Apache Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_10","title":"Improvement","text":"<ul> <li>SEDONA-121 - Add equivalent constructors left over from Spark to Flink</li> <li>SEDONA-132 - Create common module for SQL functions</li> <li>SEDONA-133 - Allow user-defined schemas in Adapter.toDf()</li> <li>SEDONA-139 - Fix wrong argument order in Flink unit tests</li> <li>SEDONA-140 - Update Sedona Dependencies in R Package</li> <li>SEDONA-143 - Add missing unit tests for the Flink predicates</li> <li>SEDONA-144 - Add ST_AsGeoJSON to the Flink API</li> <li>SEDONA-145 - Fix ST_AsEWKT to reserve the Z coordinate</li> <li>SEDONA-146 - Add missing output functions to the Flink API</li> <li>SEDONA-147 - Add SRID functions to the Flink API</li> <li>SEDONA-148 - Add boolean functions to the Flink API</li> <li>SEDONA-149 - Add Python 3.10 support</li> <li>SEDONA-151 - Add ST aggregators to Sedona Flink</li> <li>SEDONA-152 - Add reader/writer functions for GML and KML</li> <li>SEDONA-154 - Add measurement functions to the Flink API</li> <li>SEDONA-157 - Add coordinate accessors to the Flink API</li> <li>SEDONA-159 - Add Nth accessor functions to the Flink API</li> <li>SEDONA-160 - Fix geoparquetIOTests.scala to cleanup after test</li> <li>SEDONA-161 - Add ST_Boundary to the Flink API</li> <li>SEDONA-162 - Add ST_Envelope to the Flink API</li> <li>SEDONA-163 - Better handle of unsupported types in shapefile reader</li> <li>SEDONA-164 - Add geometry count functions to the Flink API</li> <li>SEDONA-165 - Upgrade Apache Rat to 0.14</li> <li>SEDONA-170 - Add ST_AddPoint and ST_RemovePoint to the Flink API</li> <li>SEDONA-172 - Add ST_LineFromMultiPoint to Apache Sedona</li> <li>SEDONA-176 - Make ST_Contains conform with OGC standard, and add ST_Covers and ST_CoveredBy functions.</li> <li>SEDONA-177 - Support spatial predicates other than INTERSECTS and COVERS/COVERED_BY in RangeQuery.SpatialRangeQuery and JoinQuery.SpatialJoinQuery</li> <li>SEDONA-181 - Build fails with java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$</li> <li>SEDONA-189 - Prepare geometries in broadcast join</li> <li>SEDONA-192 - Null handling in predicates</li> <li>SEDONA-195 - Add wkt validation and an optional srid to ST_GeomFromWKT/ST_GeomFromText</li> </ul>"},{"location":"setup/release-notes/#task_7","title":"Task","text":"<ul> <li>SEDONA-150 - Drop Spark 2.4 and Scala 2.11 support</li> </ul>"},{"location":"setup/release-notes/#sedona-121","title":"Sedona 1.2.1","text":"<p>This version is a maintenance release on Sedona 1.2.0 line. It includes bug fixes.</p> <p>Sedona on Spark is now compiled against Spark 3.3, instead of Spark 3.2.</p>"},{"location":"setup/release-notes/#sql-for-spark","title":"SQL (for Spark)","text":"<p>Bug fixes:</p> <ul> <li>SEDONA-104: Bug in reading band values of GeoTiff images</li> <li>SEDONA-118: Fix the wrong result in ST_Within</li> <li>SEDONA-123: Fix the check for invalid lat/lon in ST_GeoHash</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-96: Refactor ST_MakeValid to use GeometryFixer</li> <li>SEDONA-108: Write support for GeoTiff images</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> <li>SEDONA-127: Add null safety to ST_GeomFromWKT/WKB/Text</li> <li>SEDONA-129: Support Spark 3.3</li> <li>SEDONA-135: Consolidate and upgrade hadoop dependency</li> </ul> <p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-106: Add ST_LineFromText</li> <li>SEDONA-117: Add RS_AppendNormalizedDifference</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_YMin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-124: Add ST_CollectionExtract</li> <li>SEDONA-109: Add ST_OrderingEquals</li> </ul>"},{"location":"setup/release-notes/#flink","title":"Flink","text":"<p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_YMin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-110: Add ST_GeomFromGeoHash</li> <li>SEDONA-121: More ST constructors to Flink</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> </ul>"},{"location":"setup/release-notes/#sedona-120","title":"Sedona 1.2.0","text":"<p>This version is a major release on Sedona 1.2.0 line. It includes bug fixes and new features: Sedona with Apache Flink.</p>"},{"location":"setup/release-notes/#rdd","title":"RDD","text":"<p>Bug fix:</p> <ul> <li>SEDONA-18: Fix an error reading Shapefile</li> <li>SEDONA-73: Exclude scala-library from scala-collection-compat</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-77: Refactor Format readers and spatial partitioning functions to be standalone libraries. So they can be used by Flink and others.</li> </ul>"},{"location":"setup/release-notes/#sql","title":"SQL","text":"<p>New features:</p> <ul> <li>SEDONA-4: Handle nulls in SQL functions</li> <li>SEDONA-65: Create ST_Difference function</li> <li>SEDONA-68 Add St_Collect function.</li> <li>SEDONA-82: Create ST_SymDifference function</li> <li>SEDONA-75: Add support for \"3D\" geometries: Preserve Z coordinates on geometries when serializing, ST_AsText, ST_Z, ST_3DDistance</li> <li>SEDONA-86: Support empty geometries in ST_AsBinary and ST_AsEWKB</li> <li>SEDONA-90: Add ST_Union</li> <li>SEDONA-100: Add st_multi function</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-89: GeometryUDT equals should test equivalence of the other object</li> </ul>"},{"location":"setup/release-notes/#flink_1","title":"Flink","text":"<p>Major update:</p> <ul> <li>SEDONA-80: Geospatial stream processing support in Flink Table API</li> <li>SEDONA-85: ST_Geohash function in Flink</li> <li>SEDONA-87: Support Flink Table and DataStream conversion</li> <li>SEDONA-93: Add ST_GeomFromGeoJSON</li> </ul>"},{"location":"setup/release-notes/#sedona-111","title":"Sedona 1.1.1","text":"<p>This version is a maintenance release on Sedona 1.1.X line. It includes bug fixes and a few new functions.</p>"},{"location":"setup/release-notes/#global","title":"Global","text":"<p>New feature:</p> <ul> <li>SEDONA-73: Scala source code supports Scala 2.13</li> </ul>"},{"location":"setup/release-notes/#sql_1","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-67: Support Spark 3.2</li> </ul> <p>New features:</p> <ul> <li>SEDONA-43: Add ST_GeoHash and ST_GeomFromGeoHash</li> <li>SEDONA-45: Add ST_MakePolygon</li> <li>SEDONA-71: Add ST_AsBinary, ST_AsEWKB, ST_SRID, ST_SetSRID</li> </ul>"},{"location":"setup/release-notes/#sedona-110","title":"Sedona 1.1.0","text":"<p>This version is a major release on Sedona 1.1.0 line. It includes bug fixes and new features: R language API, Raster data and Map algebra support</p>"},{"location":"setup/release-notes/#global_1","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-30: Use Geotools-wrapper 1.1.0-24.1 to include geotools GeoTiff libraries.</li> </ul> <p>Improvement on join queries in core and SQL:</p> <ul> <li>SEDONA-63: Skip empty partitions in NestedLoopJudgement</li> <li>SEDONA-64: Broadcast dedupParams to improve performance</li> </ul> <p>Behavior change:</p> <ul> <li>SEDONA-62: Ignore HDF test in order to avoid NASA copyright issue</li> </ul>"},{"location":"setup/release-notes/#core","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-41: Fix rangeFilter bug when the leftCoveredByRight para is false</li> <li>SEDONA-53: Fix SpatialKnnQuery NullPointerException</li> </ul>"},{"location":"setup/release-notes/#sql_2","title":"SQL","text":"<p>Major update:</p> <ul> <li>SEDONA-30: Add GeoTiff raster I/O and Map Algebra function</li> </ul> <p>New function:</p> <ul> <li>SEDONA-27: Add ST_Subdivide and ST_SubdivideExplode functions</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-56: Fix broadcast join with Adapter Query Engine enabled</li> <li>SEDONA-22, SEDONA-60: Fix join queries in SparkSQL when one side has no rows or only one row</li> </ul>"},{"location":"setup/release-notes/#viz","title":"Viz","text":"<p>N/A</p>"},{"location":"setup/release-notes/#python","title":"Python","text":"<p>Improvement:</p> <ul> <li>SEDONA-59: Make pyspark dependency of Sedona Python optional</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-50: Remove problematic logging conf that leads to errors on Databricks</li> <li>Fix the issue: Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake.</li> </ul>"},{"location":"setup/release-notes/#r","title":"R","text":"<p>Major update:</p> <ul> <li>SEDONA-31: Add R interface for Sedona</li> </ul>"},{"location":"setup/release-notes/#sedona-101","title":"Sedona 1.0.1","text":"<p>This version is a maintenance release on Sedona 1.0.0 line. It includes bug fixes, some new features, one API change</p>"},{"location":"setup/release-notes/#known-issue","title":"Known issue","text":"<p>In Sedona v1.0.1 and earlier versions, the Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake. When you install Sedona Python (apache-sedona v1.0.1) from PyPI, pip might uninstall PySpark 3.1.1 and install PySpark 3.0.2 on your machine.</p> <p>Three ways to fix this:</p> <ol> <li> <p>After install apache-sedona v1.0.1, uninstall PySpark 3.0.2 and reinstall PySpark 3.1.1</p> </li> <li> <p>Ask pip not to install Sedona dependencies: <code>pip install --no-deps apache-sedona</code></p> </li> <li> <p>Install Sedona from the latest setup.py (on GitHub) manually.</p> </li> </ol>"},{"location":"setup/release-notes/#global_2","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-16: Use a GeoTools Maven Central wrapper to fix failed Jupyter notebook examples</li> <li>SEDONA-29: upgrade to Spark 3.1.1</li> <li>SEDONA-33: jts2geojson version from 0.14.3 to 0.16.1</li> </ul>"},{"location":"setup/release-notes/#core_1","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-35: Address user-data mutability issue with Adapter.toDF()</li> </ul>"},{"location":"setup/release-notes/#sql_3","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-14: Saving dataframe to CSV or Parquet fails due to unknown type</li> <li>SEDONA-15: Add ST_MinimumBoundingRadius and ST_MinimumBoundingCircle functions</li> <li>SEDONA-19: Global indexing does not work with SQL joins</li> <li>SEDONA-20: Case object GeometryUDT and GeometryUDT instance not equal in Spark 3.0.2</li> </ul> <p>New function:</p> <ul> <li>SEDONA-21: allows Sedona to be used in pure SQL environment</li> <li>SEDONA-24: Add ST_LineSubString and ST_LineInterpolatePoint</li> <li>SEDONA-26: Add broadcast join support</li> </ul>"},{"location":"setup/release-notes/#viz_1","title":"Viz","text":"<p>Improvement:</p> <ul> <li>SEDONA-32: Speed up ST_Render</li> </ul> <p>API change:</p> <ul> <li>SEDONA-29: Upgrade to Spark 3.1.1 and fix ST_Pixelize</li> </ul>"},{"location":"setup/release-notes/#python_1","title":"Python","text":"<p>Bug fix:</p> <ul> <li>SEDONA-19: Global indexing does not work with SQL joins</li> </ul>"},{"location":"setup/release-notes/#sedona-100","title":"Sedona 1.0.0","text":"<p>This version is the first Sedona release since it joins the Apache Incubator. It includes new functions, bug fixes, and API changes.</p>"},{"location":"setup/release-notes/#global_3","title":"Global","text":"<p>Key dependency upgrade:</p> <ul> <li>SEDONA-1: upgrade to JTS 1.18</li> <li>upgrade to GeoTools 24.0</li> <li>upgrade to jts2geojson 0.14.3</li> </ul> <p>Key dependency packaging strategy change:</p> <ul> <li>JTS, GeoTools, jts2geojson are no longer packaged in Sedona jars. End users need to add them manually. See here.</li> </ul> <p>Key compilation target change:</p> <ul> <li>SEDONA-3: Paths and class names have been changed to Apache Sedona</li> <li>SEDONA-7: build the source code for Spark 2.4, 3.0, Scala 2.11, 2.12, Python 3.7, 3.8, 3.9. See here.</li> </ul>"},{"location":"setup/release-notes/#sedona-core","title":"Sedona-core","text":"<p>Bug fix:</p> <ul> <li>PR 443: read multiple Shape Files by multiPartitions</li> <li>PR 451 (API change): modify CRSTransform to ignore datum shift</li> </ul> <p>New function:</p> <ul> <li>SEDONA-8: spatialRDD.flipCoordinates()</li> </ul> <p>API / behavior change:</p> <ul> <li>PR 488: JoinQuery.SpatialJoinQuery/DistanceJoinQuery now returns <code>&lt;Geometry, List&gt;</code> instead of <code>&lt;Geometry, HashSet&gt;</code> because we can no longer use HashSet in Sedona for duplicates removal. All original duplicates in both input RDDs will be preserved in the output.</li> </ul>"},{"location":"setup/release-notes/#sedona-sql","title":"Sedona-sql","text":"<p>Bug fix:</p> <ul> <li>SEDONA-8 (API change): ST_Transform slow due to lock contention.</li> <li>PR 427: ST_Point and ST_PolygonFromEnvelope now allows Double type</li> </ul> <p>New function:</p> <ul> <li>PR 499: ST_Azimuth, ST_X, ST_Y, ST_StartPoint, ST_Boundary, ST_EndPoint, ST_ExteriorRing, ST_GeometryN, ST_InteriorRingN, ST_Dump, ST_DumpPoints, ST_IsClosed, ST_NumInteriorRings, ST_AddPoint, ST_RemovePoint, ST_IsRing</li> <li>PR 459: ST_LineMerge</li> <li>PR 460: ST_NumGeometries</li> <li>PR 469: ST_AsGeoJSON</li> <li>SEDONA-8: ST_FlipCoordinates</li> </ul> <p>Behavior change:</p> <ul> <li>PR 480: Aggregate Functions rewrite for new Aggregator API. The functions can be used as typed functions in code and enable compilation-time type check.</li> </ul> <p>API change:</p> <ul> <li>SEDONA-11: Adapter.toDf() will directly generate a geometry type column. ST_GeomFromWKT is no longer needed.</li> </ul>"},{"location":"setup/release-notes/#sedona-viz","title":"Sedona-viz","text":"<p>API change: Drop the function which can generate SVG vector images because the required library has an incompatible license and the SVG image is not good at plotting big data</p>"},{"location":"setup/release-notes/#sedona-python","title":"Sedona Python","text":"<p>API/Behavior change:</p> <ul> <li>Python-to-Sedona adapter is moved to a separate module. To use Sedona Python, see here</li> </ul> <p>New function:</p> <ul> <li>PR 448: Add support for partition number in spatialPartitioning function <code>spatial_rdd.spatialPartitioning(grid_type, NUM_PARTITION)</code></li> </ul>"},{"location":"setup/wherobots/","title":"Install on Wherobots","text":""},{"location":"setup/wherobots/#wherobotsdb","title":"WherobotsDB","text":"<p>Wherobots Cloud offers fully-managed and fully provisioned cloud services for WherobotsDB, a comprehensive spatial analytics database system. You can play with it using in a cloud-hosted Jupyter notebook with Python, Java or Scala kernels; no installation is needed.</p> <p>WherobotsDB is 100% compatible with Apache Sedona in terms of public APIs but provides more functionality and better performance.</p> <p>It is easy to migrate your existing Sedona workflow to Wherobots Cloud. Please sign up here to create your account.</p>"},{"location":"setup/zeppelin/","title":"Install Sedona-Zeppelin","text":""},{"location":"setup/zeppelin/#install-sedona-zeppelin","title":"Install Sedona-Zeppelin","text":"<p>Warning</p> <p>Known issue: due to an issue in Leaflet JS, Sedona can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fledged visualization, please use SedonaViz to plot scatter plots and heat maps on Zeppelin map.</p>"},{"location":"setup/zeppelin/#compatibility","title":"Compatibility","text":"<p>Apache Spark 2.3+</p> <p>Apache Zeppelin 0.8.1+</p> <p>Sedona 1.0.0+: Sedona-core, Sedona-SQL, Sedona-Viz</p>"},{"location":"setup/zeppelin/#installation","title":"Installation","text":"<p>Note</p> <p>You only need to do Step 1 and 2 only if you cannot see Apache-sedona or GeoSpark Zeppelin in Zeppelin Helium package list.</p>"},{"location":"setup/zeppelin/#create-helium-folder-optional","title":"Create Helium folder (optional)","text":"<p>Create a folder called <code>helium</code> in Zeppelin root folder.</p>"},{"location":"setup/zeppelin/#add-sedona-zeppelin-description-optional","title":"Add Sedona-Zeppelin description (optional)","text":"<p>Create a file called <code>sedona-zeppelin.json</code> in this folder and put the following content in this file. You need to change the artifact path!</p> <pre><code>{\n  \"type\": \"VISUALIZATION\",\n  \"name\": \"sedona-zeppelin\",\n  \"description\": \"Zeppelin visualization support for Sedona\",\n  \"artifact\": \"/Absolute/Path/sedona/zeppelin\",\n  \"license\": \"BSD-2-Clause\",\n  \"icon\": \"&lt;i class='fa fa-globe'&gt;&lt;/i&gt;\"\n}\n</code></pre>"},{"location":"setup/zeppelin/#enable-sedona-zeppelin","title":"Enable Sedona-Zeppelin","text":"<p>Restart Zeppelin then open Zeppelin Helium interface and enable Sedona-Zeppelin.</p> <p></p>"},{"location":"setup/zeppelin/#add-sedona-dependencies-in-zeppelin-spark-interpreter","title":"Add Sedona dependencies in Zeppelin Spark Interpreter","text":""},{"location":"setup/zeppelin/#visualize-sedonasql-results","title":"Visualize SedonaSQL results","text":""},{"location":"setup/zeppelin/#display-sedonaviz-results","title":"Display SedonaViz results","text":"<p>Now, you are good to go! Please read Sedona-Zeppelin tutorial for a hands-on tutorial.</p>"},{"location":"setup/flink/install-python/","title":"Install Sedona Python","text":"<p>To install Apache Sedona Python, you need to install the following steps:</p> <p>Install the required Python packages.</p> <pre><code>pip install apache-sedona[flink] shapely attr\n</code></pre> <p>Download the required JAR files from Maven Central:</p> <ul> <li>sedona-flink-shaded_2.12:jar:1.8.0</li> <li>geotools-wrapper-1.8.0-33.1.jar</li> </ul> <p>Follow the official Flink documentation to install the JAR files in your Flink cluster or PyFlink application. https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/dependency_management/</p>"},{"location":"setup/flink/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Flink cluster is ready.</p> <p>Then you can create a self-contained Scala / Java project. A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place.</p> <p>To use Sedona in your self-contained Flink project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Read Sedona Flink guide and use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using Maven. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Flink cluster. Make sure you are in the root folder of Flink distribution. Then run the following command:</li> </ol> <pre><code>./bin/flink run /Path/To/YourJar.jar\n</code></pre>"},{"location":"setup/flink/modules/","title":"Modules","text":""},{"location":"setup/flink/modules/#sedona-modules-for-apache-flink","title":"Sedona modules for Apache Flink","text":"Name Introduction flink Spatial Table and DataStream implementation flink-shaded shaded version"},{"location":"setup/flink/modules/#api-availability","title":"API availability","text":"DataStream Table Scala/Java \u2705 \u2705 Python no no R no no"},{"location":"setup/flink/platform/","title":"Language wrappers","text":"<p>Sedona Flink binary releases are compiled by Java 11/17 and Scala 2.12, and tested with Flink 1.12 - 1.19.</p> <p>Java Requirements:</p> <ul> <li>Flink 1.12 - 1.18: Java 11</li> <li>Flink 1.19+: Java 17</li> </ul> <p>Note: Java 8 support is dropped since Sedona 1.8.0.</p>"},{"location":"setup/snowflake/install/","title":"Install Sedona SQL","text":"<p>Note</p> <p>This tutorial is for you to manually install Sedona on Snowflake. If you want to use Sedona on Snowflake without manually installing it, you can use the free SedonaSnow native app shipped by Wherobots.</p>"},{"location":"setup/snowflake/install/#prerequisites","title":"Prerequisites","text":"<p>To install Sedona on Snowflake, you need to prepare a Snowflake account and a Snowflake user that can access at least one <code>DATABASE</code> and run at least one <code>WAREHOUSE</code>. Then you can follow the steps below to install Sedona on Snowflake.</p> <p>You can refer to Snowflake Documentation to how to create a DATABASE.</p> <p>In this tutorial, we will use a database created by the following SQL statement. But you can use any database you want.</p> <pre><code>CREATE DATABASE SEDONA_TEST;\n</code></pre>"},{"location":"setup/snowflake/install/#step-1-create-a-stage-in-the-database","title":"Step 1: Create a stage in the database","text":"<p>A stage is a Snowflake object that maps to a location in a cloud storage provider, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage. You can use a stage to load data into a table, or unload data from a table.</p> <p>In this case, we will create a stage named <code>ApacheSedona</code> in the <code>public</code> schema of the database created in the previous step. The stage will be used to load Sedona's JAR files into the database. We will choose a <code>Snowflake managed</code> stage.</p> <p></p> <p>After creating the stage, you should be able to see the stage in the database.</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a stage.</p>"},{"location":"setup/snowflake/install/#step-2-upload-sedonas-jar-files-to-the-stage","title":"Step 2: Upload Sedona's JAR files to the stage","text":"<p>You will need to download the following 2 JAR files:</p> <ul> <li>sedona-snowflake-1.8.0.jar: Sedona's Maven Central repository</li> <li>geotools-wrapper-1.8.0-33.1.jar: GeoTools-wrapper's Maven Central repository</li> </ul> <p>Then you can upload the 2 JAR files to the stage created in the previous step.</p> <p>After uploading the 2 JAR files, you should be able to see the 2 JAR files in the stage.</p> <p></p> <p>You can refer to Snowflake Documentation to how to upload files to a stage.</p>"},{"location":"setup/snowflake/install/#step-3-create-a-schema-in-the-database","title":"Step 3: Create a schema in the database","text":"<p>A schema is a Snowflake object that maps to a database. You can use a schema to organize your tables into groups based on business functions or other categories.</p> <p>In this case, we will create a schema named <code>SEDONA</code> in the database created in the previous step. The schema will be used to create Sedona's functions.</p> <p></p> <p>You can find your schema in the database as follows:</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a schema.</p>"},{"location":"setup/snowflake/install/#step-4-get-the-sql-script-for-creating-sedonas-functions","title":"Step 4: Get the SQL script for creating Sedona's functions","text":"<p>You will need to get this SQL script by running the following command:</p> <pre><code>java -jar sedona-snowflake-1.8.0.jar --geotools-version 1.8.0-33.1 &gt; sedona-snowflake.sql\n</code></pre> <p>sedona-snowflake-1.8.0.jar is the JAR file downloaded in Step 2.</p>"},{"location":"setup/snowflake/install/#step-5-run-the-sql-script-to-create-sedonas-functions","title":"Step 5: Run the SQL script to create Sedona's functions","text":"<p>We will create a worksheet in the database created in the previous step, and run the SQL script to create Sedona's functions.</p> <p>In this case, we will choose the option <code>Create Worksheet from SQL File</code>.</p> <p></p> <p>In the worksheet, choose <code>SEDONA_TEST</code> as the database, and <code>PUBLIC</code> as the schema. The SQL script should be in the worksheet. Then right-click the worksheet and choose <code>Run All</code>. Snowflake will take 3 minutes to create Sedona's functions.</p> <p></p>"},{"location":"setup/snowflake/install/#step-6-verify-the-installation","title":"Step 6: Verify the installation","text":"<p>Open a new worksheet, choose <code>SEDONA_TEST</code> as the database, and any schema as the schema. Then run the following SQL statement:</p> <pre><code>SELECT SEDONA.ST_AsEWKT(SEDONA.ST_SETSRID(SEDONA.ST_POINT(1, 2), 4326));\n</code></pre> <p>You should be able to see the following result:</p> <pre><code>SRID=4326;POINT (1 2)\n</code></pre> <p>The worksheet should look like this:</p> <p></p>"},{"location":"setup/snowflake/modules/","title":"Modules","text":""},{"location":"setup/snowflake/modules/#sedona-modules-for-snowflake","title":"Sedona modules for Snowflake","text":"Name Introduction snowflake Spatial SQL functions for Snowflake snowflake-tester Automated tester of Sedona Snowflake functions"},{"location":"setup/snowflake/modules/#api-availability","title":"API availability","text":"Table SnowSQL \u2705 Snowpark \u2705"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/","title":"Tune RDD application","text":""},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#advanced-tutorial-tune-your-sedona-rdd-application","title":"Advanced tutorial: Tune your Sedona RDD application","text":"<p>Before getting into this advanced tutorial, please make sure that you have tried several Sedona functions on your local machine.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#pick-a-proper-sedona-version","title":"Pick a proper Sedona version","text":"<p>The versions of Sedona have three levels: X.X.X (i.e., 0.8.1)</p> <p>The first level means that this version contains big structure redesign which may bring big changes in APIs and performance.</p> <p>The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old Sedona user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read Sedona version release notes and make sure you are ready to accept the API changes.</p> <p>The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all Sedona users that stay at the same level move to the latest version in this level.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#choose-a-proper-spatial-rdd-constructor","title":"Choose a proper Spatial RDD constructor","text":"<p>Sedona provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.</p> <ol> <li>Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:</li> </ol> <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel)\n</code></pre> <ol> <li>Initialize a SpatialRDD from an existing RDD. A typical example is as follows:</li> </ol> <pre><code>public PointRDD(JavaRDD&lt;Point&gt; rawSpatialRDD, StorageLevel newLevel)\n</code></pre> <p>You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why Sedona does this is that Sedona wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.</p> <p>However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:</p> <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, Envelope datasetBoundary, Integer approximateTotalCount) {\n</code></pre> <p>Manually providing the dataset boundary and approximate total count helps Sedona avoiding several slow \"Action\"s during initialization.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#cache-the-spatial-rdd-that-is-repeatedly-used","title":"Cache the Spatial RDD that is repeatedly used","text":"<p>Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:</p> <ol> <li>rawSpatialRDD: The RDD generated by SpatialRDD constructors.</li> <li>spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.</li> <li>indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.</li> <li>indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.</li> </ol> <p>These four RDDs don't co-exist so you don't need to worry about the memory issue. These four RDDs are invoked in different queries:</p> <ol> <li>Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.</li> <li>Spatial Range Query / KNN Query, use index: indexedRawRDD is used.</li> <li>Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.</li> <li>Spatial Join Query / Distance Join Query, use index: indexed RDD is used.</li> </ol> <p>Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:</p> <ol> <li>In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.</li> <li>In Spark RDD sharing applications such as Livy and Spark Job Server, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.</li> </ol>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#be-aware-of-spatial-rdd-partitions","title":"Be aware of Spatial RDD partitions","text":"<p>Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.</p> <p>After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.</p>"},{"location":"tutorial/benchmark/","title":"Benchmark","text":""},{"location":"tutorial/benchmark/#benchmark","title":"Benchmark","text":"<p>We welcome people to use Sedona for benchmark purpose. To achieve the best performance or enjoy all features of Sedona,</p> <ul> <li>Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.</li> <li>Please open Sedona kryo serializer to reduce the memory footprint.</li> </ul>"},{"location":"tutorial/demo/","title":"Scala/Java","text":""},{"location":"tutorial/demo/#scala-and-java-examples","title":"Scala and Java Examples","text":"<p>Scala and Java Examples contains template projects for Sedona Spark (RDD, SQL and Viz) and Sedona Flink. The template projects have been configured properly.</p> <p>Note that, although the template projects are written in Scala, the same APIs can be  used in Java as well.</p>"},{"location":"tutorial/demo/#folder-structure","title":"Folder structure","text":"<p>The folder structure of this repository is as follows.</p> <ul> <li>spark-sql: a Scala template shows how to use Sedona RDD, DataFrame and SQL API</li> <li>flink-sql: a Java template show how to use Sedona SQL via Flink Table APIs</li> </ul>"},{"location":"tutorial/demo/#compile-and-package","title":"Compile and package","text":""},{"location":"tutorial/demo/#prerequisites","title":"Prerequisites","text":"<p>Please make sure you have the following software installed on your local machine:</p> <ul> <li>For Scala: Scala 2.12</li> <li>For Java: JDK 1.8, Apache Maven 3</li> </ul>"},{"location":"tutorial/demo/#compile","title":"Compile","text":"<p>Run a terminal command <code>mvn clean package</code> within the folder of each template</p>"},{"location":"tutorial/demo/#submit-your-fat-jar-to-spark","title":"Submit your fat jar to Spark","text":"<p>After running the command mentioned above, you are able to see a fat jar in <code>./target</code> folder. Please take it and use <code>./bin/spark-submit</code> to submit this jar.</p> <p>To run the jar in this way, you need to:</p> <ul> <li> <p>Either change Spark Master Address in template projects or simply delete it. Currently, they are hard coded to <code>local[*]</code> which means run locally with all cores.</p> </li> <li> <p>Change the dependency packaging scope of Apache Spark from \"compile\" to \"provided\". This is a common packaging strategy in Maven and SBT which means do not package Spark into your fat jar. Otherwise, this may lead to a huge jar and version conflicts!</p> </li> <li> <p>Make sure the dependency versions in build.sbt are consistent with your Spark version.</p> </li> </ul>"},{"location":"tutorial/demo/#run-template-projects-locally","title":"Run template projects locally","text":"<p>We highly suggest you use IDEs to run template projects on your local machine. For Scala, we recommend IntelliJ IDEA with Scala plug-in. For Java, we recommend IntelliJ IDEA and Eclipse. With the help of IDEs, you don't have to prepare anything (even don't need to download and set up Spark!). As long as you have Scala and Java, everything works properly!</p>"},{"location":"tutorial/demo/#scala","title":"Scala","text":"<p>Import the Scala template project as SBT project. Then run the Main file in this project.</p>"},{"location":"tutorial/geopandas-api/","title":"GeoPandas API on Sedona","text":""},{"location":"tutorial/geopandas-api/#geopandas-api-for-apache-sedona","title":"GeoPandas API for Apache Sedona","text":"<p>The GeoPandas API for Apache Sedona provides a familiar GeoPandas interface that scales your geospatial analysis beyond single-node limitations. This API combines the intuitive GeoPandas DataFrame syntax with the distributed processing power of Apache Sedona on Apache Spark, enabling you to work with planetary-scale datasets using the same code patterns you already know.</p>"},{"location":"tutorial/geopandas-api/#overview","title":"Overview","text":""},{"location":"tutorial/geopandas-api/#what-is-the-geopandas-api-for-apache-sedona","title":"What is the GeoPandas API for Apache Sedona?","text":"<p>The GeoPandas API for Apache Sedona is a compatibility layer that allows you to use GeoPandas-style operations on distributed geospatial data. Instead of being limited to single-node processing, your GeoPandas code can leverage the full power of Apache Spark clusters for large-scale geospatial analysis.</p>"},{"location":"tutorial/geopandas-api/#key-benefits","title":"Key Benefits","text":"<ul> <li>Familiar API: Use the same GeoPandas syntax and methods you're already familiar with</li> <li>Distributed Processing: Scale beyond single-node limitations to handle large datasets</li> <li>Lazy Evaluation: Benefit from Apache Sedona's query optimization and lazy execution</li> <li>Performance: Leverage distributed computing for complex geospatial operations</li> <li>Seamless Migration: Minimal code changes required to migrate existing GeoPandas workflows</li> </ul>"},{"location":"tutorial/geopandas-api/#setup","title":"Setup","text":"<p>The GeoPandas API for Apache Sedona automatically handles SparkSession management through PySpark's pandas-on-Spark integration. You have two options for setup:</p>"},{"location":"tutorial/geopandas-api/#option-1-automatic-sparksession-recommended","title":"Option 1: Automatic SparkSession (Recommended)","text":"<p>The GeoPandas API automatically uses the default SparkSession from PySpark:</p> <pre><code>from sedona.spark.geopandas import GeoDataFrame, read_parquet\n\n# No explicit SparkSession setup needed - uses default session\n# The API automatically handles Sedona context initialization\n</code></pre>"},{"location":"tutorial/geopandas-api/#option-2-manual-sparksession-setup","title":"Option 2: Manual SparkSession Setup","text":"<p>If you need to configure a custom SparkSession or are working in an environment where you need explicit control:</p> <pre><code>from sedona.spark.geopandas import GeoDataFrame, read_parquet\nfrom sedona.spark import SedonaContext\n\n# Create and configure SparkSession\nconfig = SedonaContext.builder().getOrCreate()\nsedona = SedonaContext.create(config)\n\n# The GeoPandas API will use this configured session\n</code></pre>"},{"location":"tutorial/geopandas-api/#option-3-using-existing-sparksession","title":"Option 3: Using Existing SparkSession","text":"<p>If you already have a SparkSession (e.g., in Databricks, EMR, or other managed environments):</p> <pre><code>from sedona.spark.geopandas import GeoDataFrame, read_parquet\nfrom sedona.spark import SedonaContext\n\n# Use existing SparkSession (e.g., 'spark' in Databricks)\nsedona = SedonaContext.create(spark)  # 'spark' is the existing session\n</code></pre>"},{"location":"tutorial/geopandas-api/#how-sparksession-management-works","title":"How SparkSession Management Works","text":"<p>The GeoPandas API leverages PySpark's pandas-on-Spark functionality, which automatically manages the SparkSession lifecycle:</p> <ol> <li> <p>Default Session: When you import <code>sedona.spark.geopandas</code>, it automatically uses PySpark's default session via <code>pyspark.pandas.utils.default_session()</code></p> </li> <li> <p>Automatic Sedona Registration: The API automatically registers Sedona's spatial functions and optimizations with the SparkSession when needed</p> </li> <li> <p>Transparent Integration: All GeoPandas operations are translated to Spark SQL operations under the hood, using the configured SparkSession</p> </li> <li> <p>No Manual Context Management: Unlike traditional Sedona usage, you don't need to explicitly call <code>SedonaContext.create()</code> unless you need custom configuration</p> </li> </ol> <p>This design makes the API more user-friendly by hiding the complexity of SparkSession management while still providing the full power of distributed processing.</p>"},{"location":"tutorial/geopandas-api/#s3-configuration","title":"S3 Configuration","text":"<p>When working with S3 data, the GeoPandas API uses Spark's built-in S3 support rather than external libraries like s3fs. Configure anonymous access to public S3 buckets using Spark configuration:</p> <pre><code>from sedona.spark import SedonaContext\n\n# For anonymous access to public S3 buckets\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.hadoop.fs.s3a.bucket.bucket-name.aws.credentials.provider\",\n        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>For authenticated S3 access, use appropriate AWS credential providers:</p> <pre><code># For IAM roles (recommended for EC2/EMR)\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n        \"com.amazonaws.auth.InstanceProfileCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\n# For access keys (not recommended for production)\nconfig = (\n    SedonaContext.builder()\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"your-access-key\")\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"your-secret-key\")\n    .getOrCreate()\n)\n</code></pre>"},{"location":"tutorial/geopandas-api/#basic-usage","title":"Basic Usage","text":""},{"location":"tutorial/geopandas-api/#importing-the-api","title":"Importing the API","text":"<p>Instead of importing GeoPandas directly, import from the Sedona GeoPandas module:</p> <pre><code># Traditional GeoPandas import\n# import geopandas as gpd\n\n# Sedona GeoPandas API import\nimport sedona.spark.geopandas as gpd\n\n# or\nfrom sedona.spark.geopandas import GeoDataFrame, read_parquet\n</code></pre>"},{"location":"tutorial/geopandas-api/#reading-data","title":"Reading Data","text":"<p>The API supports reading from various geospatial formats, including Parquet files from cloud storage. For S3 access with anonymous credentials, configure Spark to use anonymous AWS credentials:</p> <pre><code>from sedona.spark import SedonaContext\n\n# Configure Spark for anonymous S3 access\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\n        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n\n# Load GeoParquet file directly from S3\ns3_path = \"s3://wherobots-examples/data/onboarding_1/nyc_buildings.parquet\"\nnyc_buildings = gpd.read_parquet(s3_path)\n\n# Display basic information\nprint(f\"Dataset shape: {nyc_buildings.shape}\")\nprint(f\"Columns: {nyc_buildings.columns.tolist()}\")\nnyc_buildings.head()\n</code></pre>"},{"location":"tutorial/geopandas-api/#spatial-filtering","title":"Spatial Filtering","text":"<p>Use spatial indexing and filtering methods. Note that <code>cx</code> spatial indexing is not yet implemented in the current version:</p> <pre><code>from shapely.geometry import box\n\n# Define bounding box for Central Park\ncentral_park_bbox = box(\n    -73.973,\n    40.764,  # bottom-left (longitude, latitude)\n    -73.951,\n    40.789,  # top-right (longitude, latitude)\n)\n\n# Filter buildings within the bounding box using spatial index\n# Note: This requires collecting data to driver for spatial filtering\n# For large datasets, consider using spatial joins instead\nbuildings_sample = nyc_buildings.sample(1000)  # Sample for demonstration\ncentral_park_buildings = buildings_sample[\n    buildings_sample.geometry.intersects(central_park_bbox)\n]\n\n# Display results\nprint(\n    central_park_buildings[[\"BUILD_ID\", \"PROP_ADDR\", \"height_val\", \"geometry\"]].head()\n)\n</code></pre> <p>Alternative approach for large datasets using spatial joins:</p> <pre><code># Create a GeoDataFrame with the bounding box\nbbox_gdf = gpd.GeoDataFrame({\"id\": [1]}, geometry=[central_park_bbox], crs=\"EPSG:4326\")\n\n# Use spatial join to filter buildings within the bounding box\ncentral_park_buildings = nyc_buildings.sjoin(bbox_gdf, predicate=\"intersects\")\n</code></pre>"},{"location":"tutorial/geopandas-api/#advanced-operations","title":"Advanced Operations","text":""},{"location":"tutorial/geopandas-api/#spatial-joins","title":"Spatial Joins","text":"<p>Perform spatial joins using the same syntax as GeoPandas:</p> <pre><code># Load two datasets\nleft_df = gpd.read_parquet(\"s3://bucket/left_data.parquet\")\nright_df = gpd.read_parquet(\"s3://bucket/right_data.parquet\")\n\n# Spatial join with distance predicate\nresult = left_df.sjoin(right_df, predicate=\"dwithin\", distance=50)\n\n# Other spatial predicates\nintersects_result = left_df.sjoin(right_df, predicate=\"intersects\")\ncontains_result = left_df.sjoin(right_df, predicate=\"contains\")\n</code></pre>"},{"location":"tutorial/geopandas-api/#coordinate-reference-system-operations","title":"Coordinate Reference System Operations","text":"<p>Transform geometries between different coordinate reference systems:</p> <pre><code># Set initial CRS\nbuildings = gpd.read_parquet(\"buildings.parquet\")\nbuildings = buildings.set_crs(\"EPSG:4326\")\n\n# Transform to projected CRS for area calculations\nbuildings_projected = buildings.to_crs(\"EPSG:3857\")\n\n# Calculate areas\nbuildings_projected[\"area\"] = buildings_projected.geometry.area\n</code></pre>"},{"location":"tutorial/geopandas-api/#geometric-operations","title":"Geometric Operations","text":"<p>Apply geometric transformations and analysis:</p> <pre><code># Buffer operations\nbuffered = buildings.geometry.buffer(100)  # 100 meter buffer\n\n# Geometric properties\nbuildings[\"is_valid\"] = buildings.geometry.is_valid\nbuildings[\"is_simple\"] = buildings.geometry.is_simple\nbuildings[\"bounds\"] = buildings.geometry.bounds\n\n# Distance calculations\nfrom shapely.geometry import Point\n\nreference_point = Point(-73.9857, 40.7484)  # Times Square\nbuildings[\"distance_to_times_square\"] = buildings.geometry.distance(reference_point)\n\n# Area and length calculations (requires projected CRS)\nbuildings_projected = buildings.to_crs(\"EPSG:3857\")  # Web Mercator\nbuildings_projected[\"area\"] = buildings_projected.geometry.area\nbuildings_projected[\"perimeter\"] = buildings_projected.geometry.length\n</code></pre>"},{"location":"tutorial/geopandas-api/#performance-considerations","title":"Performance Considerations","text":""},{"location":"tutorial/geopandas-api/#use-traditional-geopandas-when","title":"Use Traditional GeoPandas when:","text":"<ul> <li>Working with small datasets (&lt; 1GB)</li> <li>Simple operations on local data</li> <li>Complete functional coverage is required</li> <li>Single-node processing is sufficient</li> </ul>"},{"location":"tutorial/geopandas-api/#use-geopandas-api-for-apache-sedona-when","title":"Use GeoPandas API for Apache Sedona when:","text":"<ul> <li>Working with large datasets (&gt; 1GB)</li> <li>Complex geospatial analyses</li> <li>Distributed processing is needed</li> <li>Data is stored in cloud storage (S3, HDFS, etc.)</li> </ul>"},{"location":"tutorial/geopandas-api/#supported-operations","title":"Supported Operations","text":"<p>The GeoPandas API for Apache Sedona has implemented 39 GeoSeries functions and 10 GeoDataFrame functions, covering the most commonly used GeoPandas operations:</p>"},{"location":"tutorial/geopandas-api/#data-io","title":"Data I/O","text":"<ul> <li><code>read_parquet()</code> - Read GeoParquet files</li> <li><code>read_file()</code> - Read various geospatial formats</li> <li><code>to_parquet()</code> - Write to Parquet format</li> </ul>"},{"location":"tutorial/geopandas-api/#spatial-operations","title":"Spatial Operations","text":"<ul> <li><code>sjoin()</code> - Spatial joins with various predicates</li> <li><code>buffer()</code> - Geometric buffering</li> <li><code>distance()</code> - Distance calculations</li> <li><code>intersects()</code>, <code>contains()</code>, <code>within()</code> - Spatial predicates</li> <li><code>sindex</code> - Spatial indexing (limited functionality)</li> </ul>"},{"location":"tutorial/geopandas-api/#crs-operations","title":"CRS Operations","text":"<ul> <li><code>set_crs()</code> - Set coordinate reference system</li> <li><code>to_crs()</code> - Transform between CRS</li> <li><code>crs</code> - Access CRS information</li> </ul>"},{"location":"tutorial/geopandas-api/#geometric-properties","title":"Geometric Properties","text":"<ul> <li><code>area</code>, <code>length</code>, <code>bounds</code> - Geometric measurements</li> <li><code>is_valid</code>, <code>is_simple</code>, <code>is_empty</code> - Geometric validation</li> <li><code>centroid</code>, <code>envelope</code>, <code>boundary</code> - Geometric properties</li> <li><code>x</code>, <code>y</code>, <code>z</code>, <code>has_z</code> - Coordinate access</li> <li><code>total_bounds</code>, <code>estimate_utm_crs</code> - Bounds and CRS utilities</li> </ul>"},{"location":"tutorial/geopandas-api/#spatial-operations_1","title":"Spatial Operations","text":"<ul> <li><code>buffer()</code> - Geometric buffering</li> <li><code>distance()</code> - Distance calculations</li> <li><code>intersects()</code>, <code>contains()</code>, <code>within()</code> - Spatial predicates</li> <li><code>intersection()</code> - Geometric intersection</li> <li><code>make_valid()</code> - Geometry validation and repair</li> <li><code>sindex</code> - Spatial indexing (limited functionality)</li> </ul>"},{"location":"tutorial/geopandas-api/#data-conversion","title":"Data Conversion","text":"<ul> <li><code>to_geopandas()</code> - Convert to traditional GeoPandas</li> <li><code>to_wkb()</code>, <code>to_wkt()</code> - Convert to WKB/WKT formats</li> <li><code>from_xy()</code> - Create geometries from coordinates</li> <li><code>geom_type</code> - Get geometry types</li> </ul>"},{"location":"tutorial/geopandas-api/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import sedona.spark.geopandas as gpd\nfrom sedona.spark import SedonaContext\n\n# Configure Spark for anonymous S3 access\nconfig = (\n    SedonaContext.builder()\n    .config(\n        \"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\n        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n\n# Load data\nDATA_DIR = \"s3://wherobots-examples/data/geopandas_blog/\"\noverture_size = \"1M\"\npostal_codes_path = DATA_DIR + \"postal-code/\"\noverture_path = DATA_DIR + overture_size + \"/\" + \"overture-buildings/\"\n\npostal_codes = gpd.read_parquet(postal_codes_path)\nbuildings = gpd.read_parquet(overture_path)\n\n# Spatial analysis\nbuildings = buildings.set_crs(\"EPSG:4326\")\nbuildings_projected = buildings.to_crs(\"EPSG:3857\")\n\n# Calculate areas and filter\nbuildings_projected[\"area\"] = buildings_projected.geometry.area\nlarge_buildings = buildings_projected[buildings_projected[\"area\"] &gt; 1000]\n\nresult = large_buildings.sjoin(postal_codes, predicate=\"intersects\")\n\n# Aggregate by postal code\nsummary = (\n    result.groupby(\"postal_code\")\n    .agg({\"area\": \"sum\", \"BUILD_ID\": \"count\"})\n    .rename(columns={\"BUILD_ID\": \"building_count\"})\n)\n\nprint(summary.head())\n</code></pre>"},{"location":"tutorial/geopandas-api/#resources-and-contributing","title":"Resources and Contributing","text":"<p>For detailed and up-to-date API documentation, including complete method signatures, parameters, and examples, see:</p> <p>\ud83d\udcda GeoPandas API Documentation</p> <p>The GeoPandas API for Apache Sedona is an open-source project. Contributions are welcome through the GitHub issue tracker for reporting bugs, requesting features, or contributing code. For more information on contributing, see the Contributor Guide.</p>"},{"location":"tutorial/geopandas-shapely/","title":"Work with GeoPandas and Shapely","text":""},{"location":"tutorial/geopandas-shapely/#work-with-geopandas-and-shapely","title":"Work with GeoPandas and Shapely","text":"<p>Note</p> <p>Sedona before 1.6.0 only works with Shapely 1.x. If you want to work with Shapely 2.x, please use Sedona no earlier than 1.6.0.</p> <p>If you use Sedona &lt; 1.6.0, please use GeoPandas &lt;= <code>0.11.1</code> since GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.5</code>.</p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-geopandas","title":"Interoperate with GeoPandas","text":"<p>Sedona Python has implemented serializers and deserializers which allows to convert Sedona Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.</p>"},{"location":"tutorial/geopandas-shapely/#from-geopandas-to-sedona-dataframe","title":"From GeoPandas to Sedona DataFrame","text":"<p>Loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ngdf = gpd.read_file(\"gis_osm_pois_free_1.shp\")\n\nsedona.createDataFrame(gdf).show()\n</code></pre> <p>This query will show the following outputs:</p> <pre><code>+---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+\n</code></pre> <p>To leverage Arrow optimization and speed up the conversion, you can use the <code>create_spatial_dataframe</code> that takes a SparkSession and GeoDataFrame as parameters and returns a Sedona DataFrame.</p> <pre><code>def create_spatial_dataframe(\n    spark: SparkSession, gdf: gpd.GeoDataFrame\n) -&gt; DataFrame: ...\n</code></pre> <ul> <li>spark: SparkSession</li> <li>gdf: gpd.GeoDataFrame</li> <li>return: DataFrame</li> </ul> <p>Example:</p> <pre><code>from sedona.spark.geoarrow import create_spatial_dataframe\n\ncreate_spatial_dataframe(spark, gdf)\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#from-sedona-dataframe-to-geopandas","title":"From Sedona DataFrame to GeoPandas","text":"<p>Reading data with Spark and converting to GeoPandas</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ncounties = (\n    sedona.read.option(\"delimiter\", \"|\").option(\"header\", \"true\").csv(\"counties.csv\")\n)\n\ncounties.createOrReplaceTempView(\"county\")\n\ncounties_geom = sedona.sql(\"SELECT *, st_geomFromWKT(geom) as geometry from county\")\n\ndf = counties_geom.toPandas()\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\n\ngdf.plot(\n    figsize=(10, 8),\n    column=\"value\",\n    legend=True,\n    cmap=\"YlOrBr\",\n    scheme=\"quantiles\",\n    edgecolor=\"lightgray\",\n)\n</code></pre> <p> </p> <p></p> <p> </p> <p>You may also wish to try converting to GeoPandas via GeoArrow, which can be significantly faster for large results (requires geopandas &gt;= 1.0).</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import dataframe_to_arrow\n\nconfig = SedonaContext.builder().getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ntest_wkt = [\"POINT (0 1)\", \"LINESTRING (0 1, 2 3)\"]\ndf = sedona.createDataFrame(zip(test_wkt), [\"wkt\"]).selectExpr(\n    \"ST_GeomFromText(wkt) as geom\"\n)\n\ngpd.GeoDataFrame.from_arrow(dataframe_to_arrow(df))\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-shapely-objects","title":"Interoperate with shapely objects","text":""},{"location":"tutorial/geopandas-shapely/#supported-shapely-objects","title":"Supported Shapely objects","text":"shapely object Available Point MultiPoint LineString MultiLinestring Polygon MultiPolygon GeometryCollection <p>To create Spark DataFrame based on mentioned Geometry types, please use  GeometryType  from   sedona.sql.types  module. Converting works for list or tuple with shapely objects.</p> <p>Schema for target table with integer id and geometry type can be defined as follows:</p> <pre><code>from pyspark.sql.types import IntegerType, StructField, StructType\n\nfrom sedona.spark import *\n\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), False),\n        StructField(\"geom\", GeometryType(), False),\n    ]\n)\n</code></pre> <p>Also, Spark DataFrame with geometry type can be converted to list of shapely objects with  collect  method.</p>"},{"location":"tutorial/geopandas-shapely/#point-example","title":"Point example","text":"<pre><code>from shapely.geometry import Point\n\ndata = [[1, Point(21.0, 52.0)], [1, Point(23.0, 42.0)], [1, Point(26.0, 32.0)]]\n\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show()\n</code></pre> <pre><code>+---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+\n</code></pre> <pre><code>gdf.printSchema()\n</code></pre> <pre><code>root\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipoint-example","title":"MultiPoint example","text":"<pre><code>from shapely.geometry import MultiPoint\n\ndata = [[1, MultiPoint([[19.511463, 51.765158], [19.446408, 51.779752]])]]\n\ngdf = sedona.createDataFrame(data, schema).show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#linestring-example","title":"LineString example","text":"<pre><code>from shapely.geometry import LineString\n\nline = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [[1, LineString(line)]]\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multilinestring-example","title":"MultiLineString example","text":"<pre><code>from shapely.geometry import MultiLineString\n\nline1 = [(10, 10), (20, 20), (10, 40)]\nline2 = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [[1, MultiLineString([line1, line2])]]\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#polygon-example","title":"Polygon example","text":"<pre><code>from shapely.geometry import Polygon\n\npolygon = Polygon(\n    [\n        [19.51121, 51.76426],\n        [19.51056, 51.76583],\n        [19.51216, 51.76599],\n        [19.51280, 51.76448],\n        [19.51121, 51.76426],\n    ]\n)\n\ndata = [[1, polygon]]\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipolygon-example","title":"MultiPolygon example","text":"<pre><code>from shapely.geometry import MultiPolygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\n\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\npolygons = [Polygon(exterior_p1, [interior_p1]), Polygon(exterior_p2)]\n\ndata = [[1, MultiPolygon(polygons)]]\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#geometrycollection-example","title":"GeometryCollection example","text":"<pre><code>from shapely.geometry import GeometryCollection, Point, LineString, Polygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\ngeoms = [\n    Polygon(exterior_p1, [interior_p1]),\n    Polygon(exterior_p2),\n    Point(1, 1),\n    LineString([(0, 0), (1, 1), (2, 2)]),\n]\n\ndata = [[1, GeometryCollection(geoms)]]\n\ngdf = sedona.createDataFrame(data, schema)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                                                                                     |\n+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1  |GEOMETRYCOLLECTION (POLYGON ((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1 1.5, 1.5 1.5, 1.5 1, 1 1)), POLYGON ((0 0, 1 0, 1 1, 0 1, 0 0)), POINT (1 1), LINESTRING (0 0, 1 1, 2 2))|\n+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/jupyter-notebook/","title":"Python","text":""},{"location":"tutorial/jupyter-notebook/#python-jupyter-notebook-examples","title":"Python Jupyter Notebook Examples","text":"<p>Sedona Python provides a number of Jupyter Notebook examples.</p> <p>Please use the following steps to run Jupyter notebook with Pipenv on your machine</p> <ol> <li>Clone Sedona GitHub repo or download the source code</li> <li>Install Sedona Python from PyPI or GitHub source: Read Install Sedona Python to learn.</li> <li>Prepare spark-shaded jar: Read Install Sedona Python to learn.</li> <li>Setup pipenv python version. Please use your desired Python version.</li> </ol> <pre><code>cd docs/usecases\npipenv --python 3.8\n</code></pre> <ol> <li>Install dependencies</li> </ol> <pre><code>cd docs/usecases\npipenv install\n</code></pre> <ol> <li>Install jupyter notebook kernel for pipenv</li> </ol> <pre><code>pipenv install ipykernel\npipenv shell\n</code></pre> <ol> <li>In the pipenv shell, do</li> </ol> <pre><code>python -m ipykernel install --user --name=apache-sedona\n</code></pre> <ol> <li>Setup environment variables <code>SPARK_HOME</code> and <code>PYTHONPATH</code> if you didn't do it before. Read Install Sedona Python to learn.</li> <li>Launch jupyter notebook: <code>jupyter notebook</code></li> <li>Select Sedona notebook. In your notebook, Kernel -&gt; Change Kernel. Your kernel should now be an option.</li> </ol>"},{"location":"tutorial/raster/","title":"Raster DataFrame / SQL app","text":"<p>Note</p> <p>Sedona uses 1-based indexing for all raster functions except map algebra function, which uses 0-based indexing.</p> <p>Note</p> <p>Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>Starting from <code>v1.1.0</code>, Sedona SQL supports raster data sources and raster operators in DataFrame and SQL. Raster support is available in all Sedona language bindings including Scala, Java, Python, and R.</p> <p>This page outlines the steps to manage raster data using SedonaSQL.</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example raster data in Sedona GitHub repo.</p>"},{"location":"tutorial/raster/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example.</li> </ol>"},{"location":"tutorial/raster/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and use <code>spark</code> directly.</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.8.0,'\n           'org.datasyslab:geotools-wrapper:1.8.0-33.1'). \\\n    getOrCreate()\n</code></pre> Please replace the <code>3.3</code> in the package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.8.0</code>.</p>"},{"location":"tutorial/raster/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating the Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/raster/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a single raster data file called rasterData.tiff, at Path.</p> <p>Use the following code to load the data and create a raw Dataframe.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n</code></pre> <p>For multiple raster data files use the following code to load the data from path and create raw DataFrame.</p> <p>Note</p> <p>The above code works too for loading multiple raster data files. If the raster files are in separate directories and the option also makes sure that only <code>.tif</code> or <code>.tiff</code> files are being loaded.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder);\nrawDf.createOrReplaceTempView(\"rawdf\");\nrawDf.show();\n</code></pre> <pre><code>rawDf = (\n    sedona.read.format(\"binaryFile\")\n    .option(\"recursiveFileLookup\", \"true\")\n    .option(\"pathGlobFilter\", \"*.tif*\")\n    .load(path_to_raster_data_folder)\n)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|209199|[4D 4D 00 2A 00 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|  6619|[49 49 2A 00 08 0...|\n</code></pre> <p>The content column in the raster table is still in the raw form, binary form.</p>"},{"location":"tutorial/raster/#create-a-raster-type-column","title":"Create a Raster type column","text":"<p>All raster operations in SedonaSQL require Raster type objects. Therefore, this should be the next step after loading the data.</p>"},{"location":"tutorial/raster/#from-geotiff","title":"From Geotiff","text":"<pre><code>SELECT RS_FromGeoTiff(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>rasterDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- rast: raster (nullable = true)\n |-- modificationTime: timestamp (nullable = true)\n |-- length: long (nullable = true)\n |-- path: string (nullable = true)\n</code></pre>"},{"location":"tutorial/raster/#from-arc-grid","title":"From Arc Grid","text":"<p>The raster data is loaded the same way as <code>tiff</code> file, but the raster data is stored with the extension <code>.asc</code>, ASCII format. The following code creates a Raster type objects from binary data:</p> <pre><code>SELECT RS_FromArcInfoAsciiGrid(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre>"},{"location":"tutorial/raster/#rasters-metadata","title":"Raster's metadata","text":"<p>Sedona has a function to get the metadata for the raster, and also a function to get the world file of the raster.</p>"},{"location":"tutorial/raster/#metadata","title":"Metadata","text":"<p>This function will return an array of metadata, it will have all the necessary information about the raster, Please refer to RS_MetaData.</p> <pre><code>SELECT RS_MetaData(rast) FROM rasterDf\n</code></pre> <p>Output for the following function will be:</p> <pre><code>[-1.3095817809482181E7, 4021262.7487925636, 512.0, 517.0, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857.0, 1.0]\n</code></pre> <p>The first two elements of the array represent the real-world geographic coordinates (like longitude/latitude) of the raster image's top left pixel, while the next two elements represent the pixel dimensions of the raster.</p>"},{"location":"tutorial/raster/#world-file","title":"World File","text":"<p>There are two kinds of georeferences, GDAL and ESRI seen in world files. For more information please refer to RS_GeoReference.</p> <pre><code>SELECT RS_GeoReference(rast, \"ESRI\") FROM rasterDf\n</code></pre> <p>The Output will be as follows:</p> <pre><code>72.328613\n0.000000\n0.000000\n-72.328613\n-13095781.645176\n4021226.584486\n</code></pre> <p>World files are used to georeference and geolocate images by establishing an image-to-world coordinate transformation that assigns real-world geographic coordinates to the pixels of the image.</p>"},{"location":"tutorial/raster/#raster-manipulation","title":"Raster Manipulation","text":"<p>Since <code>v1.5.0</code> there have been many additions to manipulate raster data, we will show you a few example queries.</p> <p>Note</p> <p>Read SedonaSQL Raster operators to learn how you can use Sedona for raster manipulation.</p>"},{"location":"tutorial/raster/#coordinate-translation","title":"Coordinate translation","text":"<p>Sedona allows you to translate coordinates as per your needs. It can translate pixel locations to world coordinates and vice versa.</p>"},{"location":"tutorial/raster/#pixelaspoint","title":"PixelAsPoint","text":"<p>Use RS_PixelAsPoint to translate pixel coordinates to world location.</p> <pre><code>SELECT RS_PixelAsPoint(rast, 450, 400) FROM rasterDf\n</code></pre> <p>Output:</p> <pre><code>POINT (-13063342 3992403.75)\n</code></pre>"},{"location":"tutorial/raster/#world-to-raster-coordinate","title":"World to Raster Coordinate","text":"<p>Use RS_WorldToRasterCoord to translate world location to pixel coordinates. To just get X coordinate use RS_WorldToRasterCoordX and for just Y coordinate use RS_WorldToRasterCoordY.</p> <pre><code>SELECT RS_WorldToRasterCoord(rast, -1.3063342E7, 3992403.75)\n</code></pre> <p>Output:</p> <pre><code>POINT (450 400)\n</code></pre>"},{"location":"tutorial/raster/#pixel-manipulation","title":"Pixel Manipulation","text":"<p>Use RS_Values to fetch values for a specified array of Point Geometries. The coordinates in the point geometry are indicative of real-world location.</p> <pre><code>SELECT RS_Values(rast, Array(ST_Point(-13063342, 3992403.75), ST_Point(-13074192, 3996020)))\n</code></pre> <p>Output:</p> <pre><code>[132.0, 148.0]\n</code></pre> <p>To change values over a grid or area defined by geometry, we will use RS_SetValues.</p> <pre><code>SELECT RS_SetValues(\n        rast, 1, 250, 260, 3, 3,\n        Array(10, 12, 17, 26, 28, 37, 43, 64, 66)\n    )\n</code></pre> <p>Follow the links to get more information on how to use the functions appropriately.</p>"},{"location":"tutorial/raster/#band-manipulation","title":"Band Manipulation","text":"<p>Sedona provides APIs to select specific bands from a raster image and create a new raster. For example, to select 2 bands from a raster, you can use the RS_Band API to retrieve the desired multi-band raster.</p> <p>Let's use a multi-band raster for this example. The process of loading and converting it to raster type is the same.</p> <pre><code>SELECT RS_Band(colorRaster, Array(1, 2))\n</code></pre> <p>Let's say you have many single-banded rasters and want to add a band to the raster to perform map algebra operations. You can do so using RS_AddBand Sedona function.</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 1, 2)\n</code></pre> <p>This will result in <code>raster1</code> having <code>raster2</code>'s specified band.</p>"},{"location":"tutorial/raster/#resample-raster-data","title":"Resample raster data","text":"<p>Sedona allows you to resample raster data using different interpolation methods like the nearest neighbor, bilinear, and bicubic to change the cell size or align raster grids, using RS_Resample.</p> <pre><code>SELECT RS_Resample(rast, 50, -50, -13063342, 3992403.75, true, \"bicubic\")\n</code></pre> <p>For more information please follow the link.</p>"},{"location":"tutorial/raster/#execute-map-algebra-operations","title":"Execute map algebra operations","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations.</p> <p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements from a space platform and assess whether the target being observed contains live green vegetation or not.</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <pre><code>SELECT RS_MapAlgebra(raster, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>For more information please refer to Map Algebra API.</p>"},{"location":"tutorial/raster/#interoperability-between-raster-and-vector-data","title":"Interoperability between raster and vector data","text":""},{"location":"tutorial/raster/#geometry-as-raster","title":"Geometry As Raster","text":"<p>Sedona allows you to rasterize a geometry by using RS_AsRaster.</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((150 150, 220 260, 190 300, 300 220, 150 150))'),\n        RS_MakeEmptyRaster(1, 'b', 4, 6, 1, -1, 1),\n        'b', 230\n    )\n</code></pre> <p>The image created is as below for the vector:</p> <p></p> <p>Note</p> <p>The vector coordinates are buffed up to showcase the output, the real use case, may or may not match the example.</p>"},{"location":"tutorial/raster/#spatial-range-query","title":"Spatial range query","text":"<p>Sedona provides raster predicates to do a range query using a geometry window, for example, let's use RS_Intersects.</p> <pre><code>SELECT rast FROM rasterDf WHERE RS_Intersect(rast, ST_GeomFromWKT('POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))'))\n</code></pre>"},{"location":"tutorial/raster/#spatial-join-query","title":"Spatial join query","text":"<p>Sedona's raster predicates also can do a spatial join using the raster column and geometry column, using the same function as above.</p> <pre><code>SELECT r.rast, g.geom FROM rasterDf r, geomDf g WHERE RS_Interest(r.rast, g.geom)\n</code></pre> <p>Note</p> <p>These range and join queries will filter rasters using the provided geometric boundary and the spatial boundary of the raster.</p> <p>Sedona offers more raster predicates to do spatial range queries and spatial join queries. Please refer to raster predicates docs.</p>"},{"location":"tutorial/raster/#visualize-raster-images","title":"Visualize raster images","text":"<p>Sedona provides APIs to visualize raster data in an image form.</p>"},{"location":"tutorial/raster/#base64-string","title":"Base64 String","text":"<p>The RS_AsBase64 encodes the raster data as a Base64 string and can be visualized using online decoder.</p> <pre><code>SELECT RS_AsBase64(rast) FROM rasterDf\n</code></pre>"},{"location":"tutorial/raster/#html-image","title":"HTML Image","text":"<p>The RS_AsImage returns an HTML image tag, that can be visualized using an HTML viewer or in Jupyter Notebook. For more information please click on the link.</p> <pre><code>SELECT RS_AsImage(rast, 500) FROM rasterDf\n</code></pre> <p>The output looks like this:</p> <p></p>"},{"location":"tutorial/raster/#2-d-matrix","title":"2-D Matrix","text":"<p>Sedona offers an API to visualize raster data that is not sufficient for the other APIs mentioned above.</p> <pre><code>SELECT RS_AsMatrix(rast) FROM rasterDf\n</code></pre> <p>Output will be as follows:</p> <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre> <p>Please refer to Raster visualizer docs to learn how to make the most of the visualizing APIs.</p>"},{"location":"tutorial/raster/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>Sedona has APIs that can save an entire raster column to files in a specified location. Before saving, the raster type column needs to be converted to a binary format. Sedona provides several functions to convert a raster column into a binary column suitable for file storage. Once in binary format, the raster data can then be written to files on disk using the Sedona file storage APIs.</p> <pre><code>rasterDf.write.format(\"raster\").option(\"rasterField\", \"raster\").option(\"fileExtension\", \".tiff\").mode(SaveMode.Overwrite).save(dirPath)\n</code></pre> <p>Sedona has a few writer functions that create the binary DataFrame necessary for saving the raster images.</p>"},{"location":"tutorial/raster/#as-arc-grid","title":"As Arc Grid","text":"<p>Use RS_AsArcGrid to get the binary Dataframe of the raster in Arc Grid format.</p> <pre><code>SELECT RS_AsArcGrid(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-geotiff","title":"As GeoTiff","text":"<p>Use RS_AsGeoTiff to get the binary Dataframe of the raster in GeoTiff format.</p> <pre><code>SELECT RS_AsGeoTiff(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-png","title":"As PNG","text":"<p>Use RS_AsPNG to get the binary Dataframe of the raster in PNG format.</p> <pre><code>SELECT RS_AsPNG(raster)\n</code></pre> <p>Please refer to Raster writer docs for more details.</p>"},{"location":"tutorial/raster/#collecting-raster-dataframes-and-working-with-them-locally-in-python","title":"Collecting raster Dataframes and working with them locally in Python","text":"<p>Sedona allows collecting Dataframes with raster columns and working with them locally in Python since <code>v1.6.0</code>. The raster objects are represented as <code>SedonaRaster</code> objects in Python, which can be used to perform raster operations.</p> <pre><code>df_raster = (\n    sedona.read.format(\"binaryFile\")\n    .load(\"/path/to/raster.tif\")\n    .selectExpr(\"RS_FromGeoTiff(content) as rast\")\n)\nrows = df_raster.collect()\nraster = rows[0].rast\nraster  # &lt;sedona.raster.sedona_raster.InDbSedonaRaster at 0x1618fb1f0&gt;\n</code></pre> <p>You can retrieve the metadata of the raster by accessing the properties of the <code>SedonaRaster</code> object.</p> <pre><code>raster.width  # width of the raster\nraster.height  # height of the raster\nraster.affine_trans  # affine transformation matrix\nraster.crs_wkt  # coordinate reference system as WKT\n</code></pre> <p>You can get a numpy array containing the band data of the raster using the <code>as_numpy</code> or <code>as_numpy_masked</code> method. The band data is organized in CHW order.</p> <pre><code>raster.as_numpy()  # numpy array of the raster\nraster.as_numpy_masked()  # numpy array with nodata values masked as nan\n</code></pre> <p>If you want to work with the raster data using <code>rasterio</code>, you can retrieve a <code>rasterio.DatasetReader</code> object using the <code>as_rasterio</code> method.</p> <p>Note</p> <p>You need to have the <code>rasterio</code> package installed (version &gt;= 1.2.10) to use this method. You can install it using <code>pip install rasterio</code>.</p> <pre><code>ds = raster.as_rasterio()  # rasterio.DatasetReader object\n# Work with the raster using rasterio\nband1 = ds.read(1)  # read the first band\n</code></pre>"},{"location":"tutorial/raster/#writing-python-udf-to-work-with-raster-data","title":"Writing Python UDF to work with raster data","text":"<p>You can write Python UDFs to work with raster data in Python. The UDFs can take <code>SedonaRaster</code> objects as input and return any Spark data type as output. This is an example of a Python UDF that calculates the mean of the raster data.</p> <pre><code>from pyspark.sql.types import DoubleType\n\n\ndef mean_udf(raster):\n    return float(raster.as_numpy().mean())\n\n\nsedona.udf.register(\"mean_udf\", mean_udf, DoubleType())\ndf_raster.withColumn(\"mean\", expr(\"mean_udf(rast)\")).show()\n</code></pre> <pre><code>+--------------------+------------------+\n|                rast|              mean|\n+--------------------+------------------+\n|GridCoverage2D[\"g...|1542.8092886117788|\n+--------------------+------------------+\n</code></pre> <p>It is much trickier to write an UDF that returns a raster object, since Sedona does not support serializing Python raster objects yet. However, you can write a UDF that returns the band data as an array and then construct the raster object using <code>RS_MakeRaster</code>. This is an example of a Python UDF that creates a mask raster based on the first band of the input raster.</p> <pre><code>from pyspark.sql.types import ArrayType, DoubleType\nimport numpy as np\n\n\ndef mask_udf(raster):\n    band1 = raster.as_numpy()[0, :, :]\n    mask = (band1 &lt; 1400).astype(np.float64)\n    return mask.flatten().tolist()\n\n\nsedona.udf.register(\"mask_udf\", band_udf, ArrayType(DoubleType()))\ndf_raster.withColumn(\"mask\", expr(\"mask_udf(rast)\")).withColumn(\n    \"mask_rast\", expr(\"RS_MakeRaster(rast, 'I', mask)\")\n).show()\n</code></pre> <pre><code>+--------------------+--------------------+--------------------+\n|                rast|                mask|           mask_rast|\n+--------------------+--------------------+--------------------+\n|GridCoverage2D[\"g...|[0.0, 0.0, 0.0, 0...|GridCoverage2D[\"g...|\n+--------------------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/raster/#performance-optimization","title":"Performance optimization","text":"<p>When working with large raster datasets, refer to the documentation on storing raster geometries in Parquet format for recommendations to optimize performance.</p>"},{"location":"tutorial/rdd/","title":"Spatial RDD app","text":"<p>The page outlines the steps to create Spatial RDDs and run spatial queries using Sedona-core.</p>"},{"location":"tutorial/rdd/#set-up-dependencies","title":"Set up dependencies","text":"<p>Please refer to Set up dependencies to set up dependencies.</p>"},{"location":"tutorial/rdd/#create-sedona-config","title":"Create Sedona config","text":"<p>Please refer to Create Sedona config to create a Sedona config.</p>"},{"location":"tutorial/rdd/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Please refer to Initiate SedonaContext to initiate a SedonaContext.</p>"},{"location":"tutorial/rdd/#create-a-spatialrdd-from-sedonasql-dataframe","title":"Create a SpatialRDD from SedonaSQL DataFrame","text":"<p>Please refer to Create a Geometry type column to create a Geometry type column. Then you can create a SpatialRDD from the DataFrame.</p> ScalaJavaPython <pre><code>var spatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>SpatialRDD spatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>from sedona.spark import StructuredAdapter\n\nspatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <p>\"usacounty\" is the name of the geometry column. It is an optional parameter. If you don't provide it, the first geometry column will be used.</p>"},{"location":"tutorial/rdd/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in Sedona is same as the unit of all geometries in an SpatialRDD.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use spatialRDD.flipCoordinates to swap X and Y.</p> <p>To convert Coordinate Reference System of an SpatialRDD, use the following code:</p> ScalaJavaPython <pre><code>val sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nval targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>String sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nString targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\ntargetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, False)\n</code></pre> <p><code>false</code> in CRSTransform(sourceCrsCode, targetCrsCode, false) means that it will not tolerate Datum shift. If you want it to be lenient, use <code>true</code> instead.</p> <p>Warning</p> <p>CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instance, use something like this:</p> ScalaJavaPython <pre><code>val objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>SpatialRDD objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", False)\n</code></pre> <p>The details CRS information can be found on EPSG.io</p>"},{"location":"tutorial/rdd/#write-a-spatial-range-query","title":"Write a Spatial Range Query","text":"<p>A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that have specified relationship with the query window.</p> <p>Assume you now have a SpatialRDD (typed or generic). You can use the following code to issue a Spatial Range Query on it.</p> <p>spatialPredicate can be set to <code>SpatialPredicate.INTERSECTS</code> to return all geometries intersect with query window. Supported spatial predicates are:</p> <ul> <li><code>CONTAINS</code>: geometry is completely inside the query window</li> <li><code>INTERSECTS</code>: geometry have at least one point in common with the query window</li> <li><code>WITHIN</code>: geometry is completely within the query window (no touching edges)</li> <li><code>COVERS</code>: query window has no point outside of the geometry</li> <li><code>COVERED_BY</code>: geometry has no point outside of the query window</li> <li><code>OVERLAPS</code>: geometry and the query window spatially overlap</li> <li><code>CROSSES</code>: geometry and the query window spatially cross</li> <li><code>TOUCHES</code>: the only points shared between geometry and the query window are on the boundary of geometry and the query window</li> <li><code>EQUALS</code>: geometry and the query window are spatially equal</li> </ul> <p>Note</p> <p>Spatial range query is equivalent with a SELECT query with spatial predicate as search condition in Spatial SQL. An example query is as follows: <pre><code>SELECT *\nFROM checkin\nWHERE ST_Intersects(checkin.location, queryWindow)\n</code></pre></p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nval usingIndex = false\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nboolean usingIndex = false\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.spark import Envelope\nfrom sedona.spark import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQuery.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\n</code></pre> <p>Note</p> <p>Sedona Python users: Please use RangeQueryRaw from the same module if you want to avoid jvm python serde while converting to Spatial DataFrame. It takes the same parameters as RangeQuery but returns reference to jvm rdd which can be converted to dataframe without python - jvm serde using Adapter.</p> <p>Example: <pre><code>from sedona.spark import Envelope\nfrom sedona.spark import RangeQueryRaw\nfrom sedona.spark import Adapter\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = (\n    False  ## Only return gemeotries fully covered by the window\n)\nusing_index = False\nquery_result = RangeQueryRaw.SpatialRangeQuery(\n    spatial_rdd, range_query_window, consider_boundary_intersection, using_index\n)\ngdf = StructuredAdapter.toDf(query_result, spark, [\"col1\", ..., \"coln\"])\n</code></pre></p>"},{"location":"tutorial/rdd/#range-query-window","title":"Range query window","text":"<p>Besides the rectangle (Envelope) type range query window, Sedona range query window can be Point/Polygon/LineString.</p> <p>The code to create a point, linestring (4 vertices) and polygon (4 vertices) is as follows:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nval polygonObject = geometryFactory.createPolygon(coordinates)\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nval linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nGeometryFactory geometryFactory = new GeometryFactory()\nCoordinate[] coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nPolygon polygonObject = geometryFactory.createPolygon(coordinates)\n\nGeometryFactory geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nLineString linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <p>A Shapely geometry can be used as a query window. To create shapely geometries, please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes","title":"Use spatial indexes","text":"<p>Sedona provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, Sedona will build a local tree index on each of the SpatialRDD partition.</p> <p>To utilize a spatial index in a spatial range query, use the following code:</p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.spark import Envelope\nfrom sedona.spark import IndexType\nfrom sedona.spark import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by the window\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\n\nquery_result = RangeQuery.SpatialRangeQuery(\n    spatial_rdd,\n    range_query_window,\n    consider_boundary_intersection,\n    using_index\n)\n</code></pre> <p>Tip</p> <p>Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.</p>"},{"location":"tutorial/rdd/#output-format","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial range query is another SpatialRDD.</p> <p>The output format of the spatial range query is another RDD which consists of GeoData objects.</p> <p>SpatialRangeQuery result can be used as RDD with map or other spark RDD functions. Also it can be used as Python objects when using collect method. Example:</p> <pre><code>query_result.map(lambda x: x.geom.length).collect()\n</code></pre> <pre><code>[\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]\n</code></pre> <p>Or transformed to GeoPandas GeoDataFrame</p> <pre><code>import geopandas as gpd\ngpd.GeoDataFrame(\n    query_result.map(lambda x: [x.geom, x.userData]).collect(),\n    columns=[\"geom\", \"user_data\"],\n    geometry=\"geom\"\n)\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-knn-query","title":"Write a Spatial KNN Query","text":"<p>A spatial K Nearest Neighbor query takes as input a K, a query point and a SpatialRDD and finds the K geometries in the RDD which are the closest to the query point.</p> <p>Assume you now have a SpatialRDD (typed or generic). You can use the following code to issue a Spatial KNN Query on it.</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\nval usingIndex = false\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nint K = 1000 // K Nearest Neighbors\nboolean usingIndex = false\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.spark import KNNQuery\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 1000 ## K Nearest Neighbors\nusing_index = False\nresult = KNNQuery.SpatialKnnQuery(object_rdd, point, k, using_index)\n</code></pre> <p>Note</p> <p>Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL <pre><code>SELECT ck.name, ck.rating, ST_Distance(ck.location, myLocation) AS distance\nFROM checkins ck\nORDER BY distance DESC\nLIMIT 5\n</code></pre></p>"},{"location":"tutorial/rdd/#query-center-geometry","title":"Query center geometry","text":"<p>Besides the Point type, Sedona KNN query center can be Polygon and LineString.</p> Scala/JavaPython <p>To learn how to create Polygon and LineString object, see Range query window.</p> <p>To create Polygon or Linestring object please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes_1","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial KNN query, use the following code:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.spark import KNNQuery\nfrom sedona.spark import IndexType\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 5 ## K Nearest Neighbors\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.RTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\nresult = KNNQuery.SpatialKnnQuery(spatial_rdd, point, k, using_index)\n</code></pre> <p>Warning</p> <p>Only R-Tree index supports Spatial KNN query</p>"},{"location":"tutorial/rdd/#output-format_1","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.</p> <p>The output format of the spatial KNN query is a list of GeoData objects. The list has K GeoData objects.</p> <p>Example: <pre><code>&gt;&gt; result\n\n[GeoData, GeoData, GeoData, GeoData, GeoData]\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-spatial-join-query","title":"Write a Spatial Join Query","text":"<p>A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue a Spatial Join Query on them.</p> ScalaJavaPython <pre><code>val spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>SpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.spark import GridType\nfrom sedona.spark import JoinQuery\n\nconsider_boundary_intersection = False ## Only return geometries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nobject_rdd.analyze()\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nresult = JoinQuery.SpatialJoinQuery(object_rdd, query_window_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Note</p> <p>Spatial join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom);\n</code></pre> Find the superheroes in each city</p>"},{"location":"tutorial/rdd/#use-spatial-partitioning","title":"Use spatial partitioning","text":"<p>Sedona spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.</p> <p>If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.</p> Scala/JavaPython <pre><code>objectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n</code></pre> <pre><code>object_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n</code></pre> <p>Or</p> Scala/JavaPython <pre><code>queryWindowRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRDD.spatialPartitioning(queryWindowRDD.getPartitioner)\n</code></pre> <pre><code>query_window_rdd.spatialPartitioning(GridType.KDBTREE)\nobject_rdd.spatialPartitioning(query_window_rdd.getPartitioner())\n</code></pre>"},{"location":"tutorial/rdd/#use-spatial-indexes_2","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial join query, use the following code:</p> ScalaJavaPython <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nval usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nboolean buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nboolean usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.spark import GridType\nfrom sedona.spark import IndexType\nfrom sedona.spark import JoinQuery\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nbuild_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\nusing_index = True\nquery_window_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nresult = JoinQuery.SpatialJoinQueryFlat(object_rdd, query_window_rdd, using_index, True)\n</code></pre> <p>The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.</p>"},{"location":"tutorial/rdd/#output-format_2","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.</p> <pre><code>Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n</code></pre> <p>Each object on the left is covered/intersected by the object on the right.</p> <p>Result for this query is RDD which holds two GeoData objects within list of lists. Example: <pre><code>result.collect()\n</code></pre></p> <pre><code>[[GeoData, GeoData], [GeoData, GeoData] ...]\n</code></pre> <p>It is possible to do some RDD operation on result data ex. Getting polygon centroid. <pre><code>result.map(lambda x: x[0].geom.centroid).collect()\n</code></pre></p> <p>Note</p> <p>Sedona Python users: Please use JoinQueryRaw from the same module for methods</p> <ul> <li> <p>spatialJoin</p> </li> <li> <p>DistanceJoinQueryFlat</p> </li> <li> <p>SpatialJoinQueryFlat</p> </li> </ul> <p>For better performance while converting to dataframe with adapter. That approach allows to avoid costly serialization between Python and jvm and in result operating on python object instead of native geometries.</p> <p>Example: <pre><code>from sedona.spark import CircleRDD\nfrom sedona.spark import GridType\nfrom sedona.spark import JoinQueryRaw\nfrom sedona.spark import StructuredAdapter\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1)  ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQueryRaw.DistanceJoinQueryFlat(\n    spatial_rdd, circle_rdd, using_index, consider_boundary_intersection\n)\n\ngdf = StructuredAdapter.toDf(\n    result, [\"left_col1\", ..., \"lefcoln\"], [\"rightcol1\", ..., \"rightcol2\"], spark\n)\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-distance-join-query","title":"Write a Distance Join Query","text":"<p>Warning</p> <p>RDD distance joins are only reliable for points. For other geometry types, please use Spatial SQL.</p> <p>A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained here.</p> <p>If you don't want to transform your data and are ok with sacrificing the query accuracy, you can use an approximate degree value for distance. Please use this calculator.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue a Distance Join Query on them.</p> ScalaJavaPython <pre><code>objectRddA.analyze()\n\nval circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nval result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRddA.analyze()\n\nCircleRDD circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nboolean usingIndex = false\n\nJavaPairRDD result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.spark import CircleRDD\nfrom sedona.spark import GridType\nfrom sedona.spark import JoinQuery\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQuery.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Distance join can only accept <code>COVERED_BY</code> and <code>INTERSECTS</code> as spatial predicates. The rest part of the join query is same as the spatial join query.</p> <p>The details of spatial partitioning in join query is here.</p> <p>The details of using spatial indexes in join query is here.</p> <p>The output format of the distance join query is here.</p> <p>Note</p> <p>Distance join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Distance(city.geom, superhero.geom) &lt;= 10;\n</code></pre> Find the superheroes within 10 miles of each city</p>"},{"location":"tutorial/rdd/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-not-indexed","title":"Save an SpatialRDD (not indexed)","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> Scala/JavaPython <pre><code>objectRDD.rawSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <pre><code>object_rdd.rawJvmSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <p>Note</p> <p>Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-indexed","title":"Save an SpatialRDD (indexed)","text":"<p>Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.</p> <p>Use the following code to save an SpatialRDD as a distributed object file:</p> <pre><code>objectRDD.indexedRawRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed","title":"Save an SpatialRDD (spatialPartitioned W/O indexed)","text":"<p>A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!</p>"},{"location":"tutorial/rdd/#reload-a-saved-spatialrdd","title":"Reload a saved SpatialRDD","text":"<p>You can easily reload an SpatialRDD that has been saved to a distributed object file. Use the following code to reload the SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.rawSpatialRDD = sc.objectFile[Geometry](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.rawSpatialRDD = sc.objectFile&lt;Geometry&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = load_spatial_rdd_from_disc(sc, \"hdfs://PATH\", GeoType.GEOMETRY)\n</code></pre> <p>Use the following code to reload the indexed SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.indexedRawRDD = sc.objectFile[SpatialIndex](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.indexedRawRDD = sc.objectFile&lt;SpatialIndex&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = SpatialRDD()\nsaved_rdd.indexedRawRDD = load_spatial_index_rdd_from_disc(sc, \"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/sql-pure-sql/","title":"Pure SQL environment","text":"<p>Starting from Sedona v1.0.1, you can use Sedona in a pure Spark SQL environment. The example code is written in SQL.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/sql-pure-sql/#initiate-session","title":"Initiate Session","text":"<p>Start <code>spark-sql</code> as following (replace <code>&lt;VERSION&gt;</code> with actual version like <code>1.8.0</code>):</p> <p>Run spark-sql with Apache Sedona</p> Spark 3.3+ and Scala 2.12 <pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:1.8.0-33.1 \\\n  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n  --conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n  --conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> <p>Please replace the <code>3.3</code> in artifact names with the corresponding major.minor version of Spark.</p> <p>This will register all Sedona types, functions and optimizations in SedonaSQL and SedonaViz.</p>"},{"location":"tutorial/sql-pure-sql/#load-data","title":"Load data","text":"<p>Let use data from <code>examples/sql</code>. To load data from CSV file we need to execute two commands:</p> <p>Use the following code to load the data and create a raw DataFrame:</p> <pre><code>CREATE TABLE IF NOT EXISTS pointraw (_c0 string, _c1 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testpoint.csv';\n\nCREATE TABLE IF NOT EXISTS polygonraw (_c0 string, _c1 string, _c2 string, _c3 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testenvelope.csv';\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#transform-the-data","title":"Transform the data","text":"<p>We need to transform our point and polygon data into respective types:</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointdata AS\n  SELECT ST_Point(cast(pointraw._c0 as Decimal(24,20)), cast(pointraw._c1 as Decimal(24,20))) AS pointshape\n  FROM pointraw;\n\nCREATE OR REPLACE TEMP VIEW polygondata AS\n  select ST_PolygonFromEnvelope(cast(polygonraw._c0 as Decimal(24,20)),\n        cast(polygonraw._c1 as Decimal(24,20)), cast(polygonraw._c2 as Decimal(24,20)),\n        cast(polygonraw._c3 as Decimal(24,20))) AS polygonshape\n  FROM polygonraw;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#work-with-data","title":"Work with data","text":"<p>For example, let join polygon and test data:</p> <pre><code>SELECT * from polygondata, pointdata\nWHERE ST_Contains(polygondata.polygonshape, pointdata.pointshape)\n      AND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondata.polygonshape)\nLIMIT 5;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#geometry-data-type-support","title":"<code>GEOMETRY</code> data type support","text":"<p>Sedona has a Spark SQL parser extension to support <code>GEOMETRY</code> data type in DDL statements. For example, you can specify a schema with a geometry column when creating the table:</p> <pre><code>CREATE TABLE geom_table (id STRING, version INT, geometry GEOMETRY)\nUSING geoparquet\nLOCATION '/path/to/geoparquet_geom_table';\n\nSELECT * FROM geom_table LIMIT 10;\n</code></pre> <p>The SQL parser extension is enabled by default. If you find it conflicting with other extensions and want to disable it, please specify <code>--conf spark.sedona.enableParserExtension=false</code> when starting <code>spark-sql</code>.</p>"},{"location":"tutorial/sql/","title":"Spatial DataFrame / SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL.</p> <p>Note</p> <p>Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example county data (i.e., <code>county_small.tsv</code>) in Sedona GitHub repo.</p>"},{"location":"tutorial/sql/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example.</li> </ol>"},{"location":"tutorial/sql/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by AWS EMR/Databricks/Microsoft Fabric, please skip this step.</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestJava\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName()) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.8.0,'\n           'org.datasyslab:geotools-wrapper:1.8.0-33.1'). \\\n    getOrCreate()\n</code></pre> If you are using a different Spark version, please replace the <code>3.3</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.8.0</code>.</p>"},{"location":"tutorial/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by AWS EMR/Databricks/Microsoft Fabric, please call <code>sedona = SedonaContext.create(spark)</code> instead.</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/sql/#load-data-from-text-files","title":"Load data from text files","text":"<p>Assume we have a WKT file, namely <code>usa-county.tsv</code>, at Path <code>/Download/usa-county.tsv</code> as follows:</p> <pre><code>POLYGON (..., ...)  Cuming County\nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n</code></pre> <p>The file may have many other columns.</p>"},{"location":"tutorial/sql/#load-the-raw-dataframe","title":"Load the raw DataFrame","text":"<p>Use the following code to load the data and create a raw DataFrame:</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will be like this:</p> <pre><code>|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre>"},{"location":"tutorial/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <pre><code>SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n</code></pre> <p>You can select many other attributes to compose this <code>spatialdDf</code>. The output will be something like this:</p> <pre><code>|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre> <p>Although it looks same with the input, but actually the type of column countyshape has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>spatialDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/sql/#load-geojson-data","title":"Load GeoJSON Data","text":"<p>Since <code>v1.6.1</code>, Sedona supports reading GeoJSON files using the <code>geojson</code> data source. It is designed to handle JSON files that use GeoJSON format for their geometries.</p> <p>Set the <code>multiLine</code> option to <code>True</code> to read multiline GeoJSON files.</p> PythonScalaJava <pre><code>df = (\n    sedona.read.format(\"geojson\")\n    .option(\"multiLine\", \"true\")\n    .load(\"PATH/TO/MYFILE.json\")\n    .selectExpr(\"explode(features) as features\")  # Explode the envelope\n    .select(\"features.*\")  # Unpack the features struct\n    .withColumn(\"prop0\", f.expr(\"properties['prop0']\"))\n    .drop(\"properties\")\n    .drop(\"type\")\n)\n\ndf.show()\ndf.printSchema()\n</code></pre> <pre><code>val df = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"PATH/TO/MYFILE.json\")\nval parsedDf = df.selectExpr(\"explode(features) as features\").select(\"features.*\")\n        .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\nparsedDf.show()\nparsedDf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"PATH/TO/MYFILE.json\")\n .selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n .select(\"features.*\") // Unpack the features struct.\n .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show();\ndf.printSchema();\n</code></pre> <p>See this page for more information on loading GeoJSON files.</p>"},{"location":"tutorial/sql/#load-shapefile","title":"Load Shapefile","text":"<p>Since v<code>1.7.0</code>, Sedona supports loading Shapefile as a DataFrame.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = sedona.read.format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <p>The input path can be a directory containing one or multiple shapefiles, or path to a <code>.shp</code> file.</p> <p>See this page for more information on loading Shapefiles.</p>"},{"location":"tutorial/sql/#load-geoparquet","title":"Load GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports loading GeoParquet file. Sedona will infer geometry fields using the \"geo\" metadata in GeoParquet files.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- pop_est: long (nullable = true)\n |-- continent: string (nullable = true)\n |-- name: string (nullable = true)\n |-- iso_a3: string (nullable = true)\n |-- gdp_md_est: double (nullable = true)\n |-- geometry: geometry (nullable = true)\n</code></pre> <p>Sedona supports spatial predicate push-down for GeoParquet files, please refer to the SedonaSQL query optimizer documentation for details.</p> <p>GeoParquet file reader can also be used to read legacy Parquet files written by Apache Sedona 1.3.1-incubating or earlier. Please refer to Reading Legacy Parquet Files for details.</p> <p>See this page for more information on loading GeoParquet.</p>"},{"location":"tutorial/sql/#load-data-from-stac-catalog","title":"Load data from STAC catalog","text":"<p>Sedona STAC data source allows you to read data from a SpatioTemporal Asset Catalog (STAC) API. The data source supports reading STAC items and collections.</p> <p>You can load a STAC collection from a s3 collection file object:</p> <pre><code>df = sedona.read.format(\"stac\").load(\n    \"s3a://example.com/stac_bucket/stac_collection.json\"\n)\n</code></pre> <p>You can also load a STAC collection from an HTTP/HTTPS endpoint:</p> <pre><code>df = sedona.read.format(\"stac\").load(\n    \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\"\n)\n</code></pre> <p>The STAC data source supports predicate pushdown for spatial and temporal filters. The data source can push down spatial and temporal filters to the underlying data source to reduce the amount of data that needs to be read.</p> <p>See this page for more information on loading data from STAC.</p>"},{"location":"tutorial/sql/#load-data-from-jdbc-data-sources","title":"Load data from JDBC data sources","text":"<p>The 'query' option in Spark SQL's JDBC data source can be used to convert geometry columns to a format that Sedona can interpret. This should work for most spatial JDBC data sources. For Postgis there is no need to add a query to convert geometry types since it's already using EWKB as it's wire format.</p> ScalaJavaPython <pre><code>// For any JDBC data source, including Postgis.\nval df = sedona.read.format(\"jdbc\")\n    // Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nval df = sedona.read.format(\"jdbc\")\n    // Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code>// For any JDBC data source, including Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n    // Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n    // Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code># For any JDBC data source, including Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n\n# This is a simplified version that works for Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n</code></pre>"},{"location":"tutorial/sql/#load-geopackage","title":"Load GeoPackage","text":"<p>Since v1.7.0, Sedona supports loading Geopackage file format as a DataFrame.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>df = sedona.read.format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <p>See this page for more information on loading GeoPackage.</p>"},{"location":"tutorial/sql/#load-osm-pbf","title":"Load OSM PBF","text":"<p>Since v1.7.1, Sedona supports loading OSM PBF file format as a DataFrame.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"osmpbf\").load(\"/path/to/osmpbf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"osmpbf\").load(\"/path/to/osmpbf\")\n</code></pre> <pre><code>df = sedona.read.format(\"osmpbf\").load(\"/path/to/osmpbf\")\n</code></pre> <p>OSM PBF files can contain nodes, ways, and relations. Currently Sedona support Nodes, DenseNodes, Ways and Relations. When you load the data you get a DataFrame with the following schema.</p> <pre><code>root\n |-- id: long (nullable = true)\n |-- kind: string (nullable = true)\n |-- location: struct (nullable = true)\n |    |-- longitude: double (nullable = true)\n |    |-- latitude: double (nullable = true)\n |-- tags: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- refs: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- ref_roles: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- ref_types: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre> <p>Where:</p> <ul> <li><code>id</code> is the unique identifier of the object.</li> <li><code>kind</code> is the type of the object, it can be <code>node</code>, <code>way</code> or <code>relation</code>.</li> <li><code>location</code> is the location of the object, it contains the <code>longitude</code> and <code>latitude</code> of the object.</li> <li><code>tags</code> is a map of key-value pairs that represent the tags of the object.</li> <li><code>refs</code> is an array of the references of the object.</li> <li><code>ref_roles</code> is an array of the roles of the references.</li> <li><code>ref_types</code> is an array of the types of the references.</li> </ul> <p>The dataframe for ways might look like this for nodes:</p> <pre><code>+---------+----+--------------------+--------------------+----+---------+---------+\n|       id|kind|            location|                tags|refs|ref_roles|ref_types|\n+---------+----+--------------------+--------------------+----+---------+---------+\n|248675410|node|{21.0884952545166...|{tactile_paving -...|NULL|     NULL|     NULL|\n|260821820|node|{21.0191555023193...|{created_by -&gt; JOSM}|NULL|     NULL|     NULL|\n|349189665|node|{22.1437530517578...|{source -&gt; http:/...|NULL|     NULL|     NULL|\n|353366899|node|{22.9787712097167...|{source -&gt; http:/...|NULL|     NULL|     NULL|\n|359460224|node|{22.4816703796386...|{source -&gt; http:/...|NULL|     NULL|     NULL|\n+---------+----+--------------------+--------------------+----+---------+---------+\nonly showing top 5 rows\n</code></pre> <p>and for way</p> <pre><code>+-------+----+--------+--------------------+--------------------+---------+---------+\n|     id|kind|location|                tags|                refs|ref_roles|ref_types|\n+-------+----+--------+--------------------+--------------------+---------+---------+\n|4307329| way|    NULL|{junction -&gt; roun...|[2448759046, 7093...|     NULL|     NULL|\n|4307330| way|    NULL|{surface -&gt; aspha...|[26063923, 260639...|     NULL|     NULL|\n|4308966| way|    NULL|{sidewalk -&gt; sepa...|[3387797238, 9252...|     NULL|     NULL|\n|4308968| way|    NULL|{surface -&gt; pavin...|[26083890, 744724...|     NULL|     NULL|\n|4308969| way|    NULL|{cycleway:both -&gt;...|[9526831176, 1218...|     NULL|     NULL|\n+-------+----+--------+--------------------+--------------------+---------+---------+\n</code></pre> <p>and for relation</p> <pre><code>+-----+--------+--------+--------------------+--------------------+--------------------+--------------------+\n|   id|    kind|location|                tags|                refs|           ref_roles|           ref_types|\n+-----+--------+--------+--------------------+--------------------+--------------------+--------------------+\n|28124|relation|    NULL|{official_name -&gt;...|[26382394, 26259985]|      [inner, outer]|          [WAY, WAY]|\n|28488|relation|    NULL|  {type -&gt; junction}|[26409253, 303249...|[roundabout, roun...|[WAY, WAY, WAY, WAY]|\n|32939|relation|    NULL|{ref -&gt; E 67, rou...|[140673970, 14067...|        [, , , , , ]|[WAY, WAY, RELATI...|\n|34387|relation|    NULL|{note -&gt; rz\u0105d III...|[209161000, 52154...|[main_stream, mai...|[WAY, WAY, WAY, W...|\n|34392|relation|    NULL|{distance -&gt; 1047...|[150033976, 25076...|[main_stream, mai...|[WAY, WAY, WAY, W...|\n+-----+--------+--------+--------------------+--------------------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>For more details, please read the <code>ST_Transform</code> section in Sedona API References.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\nFROM spatialdf\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>The coordinates of polygons have been changed. The output will be like this:</p> <pre><code>+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n</code></pre>"},{"location":"tutorial/sql/#cluster-with-dbscan","title":"Cluster with DBSCAN","text":"<p>Sedona provides an implementation of the DBSCAN algorithm to cluster spatial data.</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has an additional column added containing the unique identifier of the cluster that record is a member of and a boolean column indicating if the record is a core point.</p> <p>The first parameter is the dataframe, the next two are the epsilon and min_points parameters of the DBSCAN algorithm.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.clustering.DBSCAN.dbscan\n\ndbscan(df, 0.1, 5).show()\n</code></pre> <pre><code>import org.apache.sedona.stats.clustering.DBSCAN;\n\nDBSCAN.dbscan(df, 0.1, 5).show();\n</code></pre> <pre><code>from sedona.spark.stats import dbscan\n\ndbscan(df, 0.1, 5).show()\n</code></pre> <p>The output will look like this:</p> <pre><code>+----------------+---+------+-------+\n|        geometry| id|isCore|cluster|\n+----------------+---+------+-------+\n|   POINT (2.5 4)|  3| false|      1|\n|     POINT (3 4)|  2| false|      1|\n|     POINT (3 5)|  5| false|      1|\n|     POINT (1 3)|  9|  true|      0|\n| POINT (2.5 4.5)|  7|  true|      1|\n|     POINT (1 2)|  1|  true|      0|\n| POINT (1.5 2.5)|  4|  true|      0|\n| POINT (1.2 2.5)|  8|  true|      0|\n|   POINT (1 2.5)| 11|  true|      0|\n|     POINT (1 5)| 10| false|     -1|\n|     POINT (5 6)| 12| false|     -1|\n|POINT (12.8 4.5)|  6| false|     -1|\n|     POINT (4 3)| 13| false|     -1|\n+----------------+---+------+-------+\n</code></pre> <p>See this page for more information on the DBSCAN algorithm.</p>"},{"location":"tutorial/sql/#calculate-the-local-outlier-factor-lof","title":"Calculate the Local Outlier Factor (LOF)","text":"<p>Sedona provides an implementation of the Local Outlier Factor algorithm to identify anomalous data.</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has an additional column added containing the local outlier factor.</p> <p>The first parameter is the dataframe, the next is the number of nearest neighbors to consider use in calculating the score.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.outlierDetection.LocalOutlierFactor.localOutlierFactor\n\nlocalOutlierFactor(df, 20).show()\n</code></pre> <pre><code>import org.apache.sedona.stats.outlierDetection.LocalOutlierFactor;\n\nLocalOutlierFactor.localOutlierFactor(df, 20).show();\n</code></pre> <pre><code>from sedona.spark.stats import local_outlier_factor\n\nlocal_outlier_factor(df, 20).show()\n</code></pre> <p>The output will look like this:</p> <pre><code>+--------------------+------------------+\n|            geometry|               lof|\n+--------------------+------------------+\n|POINT (-2.0231305...| 0.952098153363662|\n|POINT (-2.0346944...|0.9975325496668104|\n|POINT (-2.2040074...|1.0825843906411081|\n|POINT (1.61573501...|1.7367129352162634|\n|POINT (-2.1176324...|1.5714144683150393|\n|POINT (-2.2349759...|0.9167275845938276|\n|POINT (1.65470192...| 1.046231536764447|\n|POINT (0.62624112...|1.1988700676990034|\n|POINT (2.01746261...|1.1060219481067417|\n|POINT (-2.0483857...|1.0775553430145446|\n|POINT (2.43969463...|1.1129132178576646|\n|POINT (-2.2425480...| 1.104108012697006|\n|POINT (-2.7859235...|  2.86371824574529|\n|POINT (-1.9738858...|1.0398822680356794|\n|POINT (2.00153403...| 0.927409656346015|\n|POINT (2.06422812...|0.9222203762264445|\n|POINT (-1.7533819...|1.0273650471626696|\n|POINT (-2.2030766...| 0.964744555830738|\n|POINT (-1.8509857...|1.0375927869698574|\n|POINT (2.10849080...|1.0753419197322656|\n+--------------------+------------------+\n</code></pre>"},{"location":"tutorial/sql/#perform-getis-ord-gi-hot-spot-analysis","title":"Perform Getis-Ord Gi(*) Hot Spot Analysis","text":"<p>Sedona provides an implementation of the Gi and Gi* algorithms to identify local hotspots in spatial data</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has additional columns added containing G statistic, E[G], V[G], the Z score, and the p-value.</p> <p>Using Gi involves first generating the neighbors list for each record, then calling the g_local function.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.Weighting.addBinaryDistanceBandColumn\nimport org.apache.sedona.stats.hotspotDetection.GetisOrd.gLocal\n\nval distanceRadius = 1.0\nval weightedDf = addBinaryDistanceBandColumn(df, distanceRadius)\ngLocal(weightedDf, \"val\").show()\n</code></pre> <pre><code>import org.apache.sedona.stats.Weighting;\nimport org.apache.sedona.stats.hotspotDetection.GetisOrd;\nimport org.apache.spark.sql.DataFrame;\n\ndouble distanceRadius = 1.0;\nDataFrame weightedDf = Weighting.addBinaryDistanceBandColumn(df, distanceRadius);\nGetisOrd.gLocal(weightedDf, \"val\").show();\n</code></pre> <pre><code>from sedona.spark.stats import add_binary_distance_band_column\nfrom sedona.spark.stats import g_local\n\ndistance_radius = 1.0\nweighted_df = addBinaryDistanceBandColumn(df, distance_radius)\ng_local(weightedDf, \"val\").show()\n</code></pre> <p>The output will look like this:</p> <pre>\n<code>\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n|   geometry|val|             weights|                  G|                 EG|                  VG|                   Z|                   P|\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n|POINT (2 2)|0.9|[{{POINT (2 3), 1...| 0.4488188976377953|0.45454545454545453| 0.00356321373799772|-0.09593402008347063|  0.4617864875295957|\n|POINT (2 3)|1.2|[{{POINT (2 2), 0...|0.35433070866141736|0.36363636363636365|0.003325666155464539|-0.16136436037034918|  0.4359032175415549|\n|POINT (3 3)|1.2|[{{POINT (2 3), 1...|0.28346456692913385| 0.2727272727272727|0.002850570990398176| 0.20110780337013057| 0.42030714022155924|\n|POINT (3 2)|1.2|[{{POINT (2 2), 0...| 0.4488188976377953|0.45454545454545453| 0.00356321373799772|-0.09593402008347063|  0.4617864875295957|\n|POINT (3 1)|1.2|[{{POINT (3 2), 3...| 0.3622047244094489| 0.2727272727272727|0.002850570990398176|  1.6758983614177538| 0.04687905137429871|\n|POINT (2 1)|2.2|[{{POINT (2 2), 0...| 0.4330708661417323|0.36363636363636365|0.003325666155464539|  1.2040263812249166| 0.11428969105925013|\n|POINT (1 1)|1.2|[{{POINT (2 1), 5...| 0.2834645669291339| 0.2727272727272727|0.002850570990398176|  0.2011078033701316|  0.4203071402215588|\n|POINT (1 2)|0.2|[{{POINT (2 2), 0...|0.35433070866141736|0.45454545454545453| 0.00356321373799772|   -1.67884535146075|0.046591093685710794|\n|POINT (1 3)|1.2|[{{POINT (2 3), 1...| 0.2047244094488189| 0.2727272727272727|0.002850570990398176| -1.2736827546774914| 0.10138793530151635|\n|POINT (0 2)|1.0|[{{POINT (1 2), 7...|0.09448818897637795|0.18181818181818182|0.002137928242798632| -1.8887168824332323|0.029464887612748458|\n|POINT (4 2)|1.2|[{{POINT (3 2), 3...| 0.1889763779527559|0.18181818181818182|0.002137928242798632| 0.15481285921583854| 0.43848442662481324|\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n</code>\n</pre>"},{"location":"tutorial/sql/#run-spatial-queries","title":"Run spatial queries","text":"<p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>SELECT *\nFROM spatialdf\nWHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n</code></pre> <p>Note</p> <p>Read SedonaSQL constructor API to learn how to create a Geometry type query window</p>"},{"location":"tutorial/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\nFROM spatialdf\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/sql/#join-query","title":"Join query","text":"<p>The details of a join query is available here Join query.</p>"},{"location":"tutorial/sql/#knn-join-query","title":"KNN join query","text":"<p>The details of a KNN join query is available here KNN join query.</p>"},{"location":"tutorial/sql/#other-queries","title":"Other queries","text":"<p>There are lots of other functions can be combined with these queries. Please read SedonaSQL functions and SedonaSQL aggregate functions.</p>"},{"location":"tutorial/sql/#visualize-query-results","title":"Visualize query results","text":"<p>Sedona provides <code>SedonaPyDeck</code> and <code>SedonaKepler</code> wrappers, both of which expose APIs to create interactive map visualizations from SedonaDataFrames in a Jupyter environment.</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler expect the default geometry order to be lon-lat. If your dataframe has geometries in the lat-lon order, please check out ST_FlipCoordinates</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler are designed to work with SedonaDataFrames containing only 1 geometry column. Passing dataframes with multiple geometry columns will cause errors.</p>"},{"location":"tutorial/sql/#sedonapydeck","title":"SedonaPyDeck","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaPyDeck.</p> <p>SedonaPyDeck exposes APIs to create interactive map visualizations using pydeck based on deck.gl</p> <p>Note</p> <p>To use SedonaPyDeck, install sedona with the <code>pydeck-map</code> extra: <pre><code>pip install apache-sedona[pydeck-map]\n</code></pre></p> <p>The following tutorial showcases the various maps that can be created using SedonaPyDeck, the datasets used to create these maps are publicly available.</p> <p>Each API exposed by SedonaPyDeck offers customization via optional arguments, details on all possible arguments can be found in the API docs of SedonaPyDeck.</p>"},{"location":"tutorial/sql/#creating-a-choropleth-map-using-sedonapydeck","title":"Creating a Choropleth map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a <code>create_choropleth_map</code> API which can be used to visualize a choropleth map out of the passed SedonaDataFrame containing polygons with an observation:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_choropleth_map(df=groupedresult, plot_col=\"AirportCount\")\n</code></pre> <p>Note</p> <p><code>plot_col</code> is a required argument informing SedonaPyDeck of the column name used to render the choropleth effect.</p> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p>"},{"location":"tutorial/sql/#creating-a-geometry-map-using-sedonapydeck","title":"Creating a Geometry map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_geometry_map API which can be used to visualize a passed SedonaDataFrame containing any type of geometries:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_geometry_map(df_building, elevation_col=\"height\")\n</code></pre> <p></p> <p>Tip</p> <p><code>elevation_col</code> is an optional argument which can be used to render a 3D map. Pass the column with 'elevation' values for the geometries here.</p>"},{"location":"tutorial/sql/#creating-a-scatterplot-map-using-sedonapydeck","title":"Creating a Scatterplot map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_scatterplot_map API which can be used to visualize a scatterplot out of the passed SedonaDataFrame containing points:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_scatterplot_map(df=crimes_df)\n</code></pre> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#creating-a-heatmap-using-sedonapydeck","title":"Creating a heatmap using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_heatmap API which can be used to visualize a heatmap out of the passed SedonaDataFrame containing points:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_heatmap(df=crimes_df)\n</code></pre> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#sedonakepler","title":"SedonaKepler","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaKepler.</p> <p>SedonaKepler exposes APIs to create interactive and customizable map visualizations using KeplerGl.</p> <p>Note</p> <p>To use SedonaKepler, install sedona with the <code>kepler-map</code> extra: <pre><code>pip install apache-sedona[kepler-map]\n</code></pre></p> <p>This tutorial showcases how simple it is to instantly visualize geospatial data using SedonaKepler.</p> <p>Example (referenced from an example notebook via the binder):</p> <pre><code>SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\n</code></pre> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p> <p>Details on all the APIs available by SedonaKepler are listed in the SedonaKepler API docs</p>"},{"location":"tutorial/sql/#create-a-user-defined-function-udf","title":"Create a User-Defined Function (UDF)","text":"<p>User-Defined Functions (UDFs) are user-created procedures that can perform operations on a single row of information. To cover almost all use cases, we will showcase 4 types of UDFs for a better understanding of how to use geometry with UDFs. Sedona's serializer deserializes the SQL geometry type to JTS Geometry (Scala/Java) or Shapely Geometry (Python). You can implement any custom logic using the rich ecosystem around these two libraries.</p>"},{"location":"tutorial/sql/#geometry-to-primitive","title":"Geometry to primitive","text":"<p>This UDF example takes a geometry type input and returns a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef lengthPoly(geom: Geometry): Double = {\n    geom.getLength\n}\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly _)\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_lengthPoly\",\n        (UDF1&lt;Geometry, Double&gt;) Geometry::getLength,\n        DataTypes.DoubleType);\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>from sedona.spark.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef lengthPoly(geom: GeometryType()):\n    return geom.length\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly, DoubleType())\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|udf_lengthPoly(geom)|\n+--------------------+\n|   3.414213562373095|\n+--------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-to-geometry","title":"Geometry to Geometry","text":"<p>This UDF example takes a geometry type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferFixed(geom: Geometry): Geometry = {\n    geom.buffer(5.5)\n}\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed _)\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_bufferFixed\",\n        (UDF1&lt;Geometry, Geometry&gt;) geom -&gt;\n            geom.buffer(5.5),\n        new GeometryUDT());\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>from sedona.spark import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferFixed(geom: GeometryType()):\n    return geom.buffer(5.5)\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed, GeometryType())\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                             udf_bufferFixed(geom)|\n+--------------------------------------------------+\n|POLYGON ((1 -4.5, -0.0729967710887076 -4.394319...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry","title":"Geometry, primitive to geometry","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferIt(geom: Geometry, distance: Double): Geometry = {\n    geom.buffer(distance)\n}\n\nsedona.udf.register(\"udf_buffer\", bufferIt _)\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_buffer\",\n        (UDF2&lt;Geometry, Double, Geometry&gt;) Geometry::buffer,\n        new GeometryUDT());\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>from sedona.spark import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferIt(geom: GeometryType(), distance: DoubleType()):\n    return geom.buffer(distance)\n\nsedona.udf.register(\"udf_buffer\", bufferIt, GeometryType())\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                        udf_buffer(geom, distance)|\n+--------------------------------------------------+\n|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry-primitive","title":"Geometry, primitive to Geometry, primitive","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type and a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.api.java.UDF2\n\nval schemaUDF = StructType(Array(\n    StructField(\"buffed\", GeometryUDT),\n    StructField(\"length\", DoubleType)\n))\n\nval udf_bufferLength = udf(\n    new UDF2[Geometry, Double, (Geometry, Double)] {\n        def call(geom: Geometry, distance: Double): (Geometry, Double) = {\n            val buffed = geom.buffer(distance)\n            val length = geom.getLength\n            (buffed, length)\n        }\n    }, schemaUDF)\n\nsedona.udf.register(\"udf_bufferLength\", udf_bufferLength)\n\ndata.withColumn(\"bufferLength\", expr(\"udf_bufferLengths(geom, distance)\"))\n    .select(\"geom\", \"distance\", \"bufferLength.*\")\n    .show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.StructType;\nimport scala.Tuple2;\n\nStructType schemaUDF = new StructType()\n            .add(\"buffedGeom\", new GeometryUDT())\n            .add(\"length\", DataTypes.DoubleType);\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\"udf_bufferLength\",\n            (UDF2&lt;Geometry, Double, Tuple2&lt;Geometry, Double&gt;&gt;) (geom, distance) -&gt; {\n                Geometry buffed = geom.buffer(distance);\n                Double length = buffed.getLength();\n                return new Tuple2&lt;&gt;(buffed, length);\n            },\n            schemaUDF);\n\ndf.withColumn(\"bufferLength\", functions.expr(\"udf_bufferLength(geom, distance)\"))\n            .select(\"geom\", \"distance\", \"bufferLength.*\")\n            .show();\n</code></pre> <pre><code>from sedona.spark import GeometryType\nfrom pyspark.sql.types import *\n\nschemaUDF = StructType([\n    StructField(\"buffed\", GeometryType()),\n    StructField(\"length\", DoubleType())\n])\n\ndef bufferAndLength(geom: GeometryType(), distance: DoubleType()):\n    buffed = geom.buffer(distance)\n    length = buffed.length\n    return [buffed, length]\n\nsedona.udf.register(\"udf_bufferLength\", bufferAndLength, schemaUDF)\n\ndf.withColumn(\"bufferLength\", expr(\"udf_bufferLength(geom, buffer)\")) \\\n            .select(\"geom\", \"buffer\", \"bufferLength.*\") \\\n            .show()\n</code></pre> <p>Output:</p> <pre><code>+------------------------------+--------+--------------------------------------------------+-----------------+\n|                          geom|distance|                                        buffedGeom|           length|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n|POLYGON ((1 1, 1 2, 2 1, 1 1))|    10.0|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|66.14518337329191|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n</code></pre>"},{"location":"tutorial/sql/#spatial-vectorized-udfs-python-only","title":"Spatial vectorized udfs (Python only)","text":"<p>By default when you create the user defined functions in Python, the UDFs are not vectorized. This means that the UDFs are called row by row which can be slow. To speed up the UDFs, you can use the <code>vectorized</code> UDF which will be called in a batch mode using Apache Arrow.</p> <p>To create a vectorized UDF please use the decorator sedona_vectorized_udf. Currently supports only the scalar UDFs. Vectorized UDFs are way faster than the normal UDFs. It might be even 2x faster than the normal UDFs.</p> <p>Note</p> <p>When you use geometry as an input type, please include the BaseGeometry type, like Point from shapely or geopandas GeoSeries, when you use GEO_SERIES\u00a0vectorized\u00a0udf. That's how Sedona infers the type and knows if the data should be cast.</p> <p>Decorator signature looks as follows:</p> <pre><code>def sedona_vectorized_udf(\n    udf_type: SedonaUDFType = SedonaUDFType.SHAPELY_SCALAR, return_type: DataType\n): ...\n</code></pre> <p>where udf_type is the type of the UDF function, currently supported are:</p> <ul> <li>SHAPELY_SCALAR</li> <li>GEO_SERIES</li> </ul> <p>The main difference is what input data you get in the function Let's analyze the two examples below, that creates buffers from a given geometry.</p>"},{"location":"tutorial/sql/#shapely-scalar-udf","title":"Shapely scalar UDF","text":"<pre><code>import shapely.geometry.base as b\nfrom sedona.spark import sedona_vectorized_udf\n\n\n@sedona_vectorized_udf(return_type=GeometryType())\ndef vectorized_buffer(geom: b.BaseGeometry) -&gt; b.BaseGeometry:\n    return geom.buffer(0.1)\n</code></pre>"},{"location":"tutorial/sql/#geoseries-udf","title":"GeoSeries UDF","text":"<pre><code>import geopandas as gpd\nfrom sedona.spark import sedona_vectorized_udf, SedonaUDFType\nfrom sedona.spark import GeometryType\n\n\n@sedona_vectorized_udf(udf_type=SedonaUDFType.GEO_SERIES, return_type=GeometryType())\ndef vectorized_geo_series_buffer(series: gpd.GeoSeries) -&gt; gpd.GeoSeries:\n    buffered = series.buffer(0.1)\n\n    return buffered\n</code></pre> <p>To call the UDFs you can use the following code:</p> <pre><code># Shapely scalar UDF\ndf.withColumn(\"buffered\", vectorized_buffer(df.geom)).show()\n\n# GeoSeries UDF\ndf.withColumn(\"buffered\", vectorized_geo_series_buffer(df.geom)).show()\n</code></pre>"},{"location":"tutorial/sql/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.</p> <p>Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:</p> <pre><code>SELECT ST_AsText(countyshape)\nFROM polygondf\n</code></pre>"},{"location":"tutorial/sql/#save-geojson","title":"Save GeoJSON","text":"<p>Since <code>v1.6.1</code>, the GeoJSON data source in Sedona can be used to save a Spatial DataFrame to a single-line JSON file, with geometries written in GeoJSON format.</p> <pre><code>df.write.format(\"geojson\").save(\"YOUR/PATH.json\")\n</code></pre> <p>See this page for more information on writing to GeoJSON.</p>"},{"location":"tutorial/sql/#save-geoparquet","title":"Save GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:</p> <pre><code>df.write.format(\"geoparquet\").save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>See this page for more information on writing to GeoParquet.</p>"},{"location":"tutorial/sql/#save-to-postgis","title":"Save to Postgis","text":"<p>Unfortunately, the Spark SQL JDBC data source doesn't support creating geometry types in PostGIS using the 'createTableColumnTypes' option. Only the Spark built-in types are recognized. This means that you'll need to manage your PostGIS schema separately from Spark. One way to do this is to create the table with the correct geometry column before writing data to it with Spark. Alternatively, you can write your data to the table using Spark and then manually alter the column to be a geometry type afterward.</p> <p>Postgis uses EWKB to serialize geometries. If you convert your geometries to EWKB format in Sedona you don't have to do any additional conversion in Postgis.</p> <pre><code>my_postgis_db# create table my_table (id int8, geom geometry);\n\ndf.withColumn(\"geom\", expr(\"ST_AsEWKB(geom)\")\n    .write.format(\"jdbc\")\n    .option(\"truncate\",\"true\") // Don't let Spark recreate the table.\n    // Other options.\n    .save()\n\n// If you didn't create the table before writing you can change the type afterward.\nmy_postgis_db# alter table my_table alter column geom type geometry;\n</code></pre>"},{"location":"tutorial/sql/#convert-between-dataframe-and-spatialrdd","title":"Convert between DataFrame and SpatialRDD","text":""},{"location":"tutorial/sql/#dataframe-to-spatialrdd","title":"DataFrame to SpatialRDD","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD.</p> ScalaJavaPython <pre><code>var spatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>SpatialRDD spatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>from sedona.spark import StructuredAdapter\n\nspatialRDD = StructuredAdapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <p>\"usacounty\" is the name of the geometry column. It is an optional parameter. If you don't provide it, the first geometry column will be used.</p>"},{"location":"tutorial/sql/#spatialrdd-to-dataframe","title":"SpatialRDD to DataFrame","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialDf = StructuredAdapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>Dataset&lt;Row&gt; spatialDf = StructuredAdapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>from sedona.spark import StructuredAdapter\n\nspatialDf = StructuredAdapter.toDf(spatialRDD, sedona)\n</code></pre>"},{"location":"tutorial/sql/#spatialrdd-to-dataframe-with-spatial-partitioning","title":"SpatialRDD to DataFrame with spatial partitioning","text":"<p>By default, <code>StructuredAdapter.toDf()</code> does not preserve spatial partitions because doing so may introduce duplicate features for most types of spatial data. These duplicates are introduced on purpose to ensure correctness when performing a spatial join; however, when using Sedona to prepare a dataset for distribution this is not typically desired.</p> <p>You can use <code>StructuredAdapter</code> and the <code>spatialRDD.spatialPartitioningWithoutDuplicates</code> function to obtain a Sedona DataFrame that is spatially partitioned without duplicates. This is especially useful for generating balanced GeoParquet files while preserving spatial proximity within files, which is crucial for optimizing filter pushdown performance in GeoParquet files.</p> ScalaJavaPython <pre><code>spatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE)\n// Specify the desired number of partitions as 10, though the actual number may vary\n// spatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE, 10)\nvar spatialDf = StructuredAdapter.toSpatialPartitionedDf(spatialRDD, sedona)\n</code></pre> <pre><code>spatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE)\n// Specify the desired number of partitions as 10, though the actual number may vary\n// spatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE, 10)\nDataset&lt;Row&gt; spatialDf = StructuredAdapter.toSpatialPartitionedDf(spatialRDD, sedona)\n</code></pre> <pre><code>from sedona.spark import StructuredAdapter\n\nspatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE)\n# Specify the desired number of partitions as 10, though the actual number may vary\n# spatialRDD.spatialPartitioningWithoutDuplicates(GridType.KDBTREE, 10)\nspatialDf = StructuredAdapter.toSpatialPartitionedDf(spatialRDD, sedona)\n</code></pre>"},{"location":"tutorial/sql/#spatialpairrdd-to-dataframe","title":"SpatialPairRDD to DataFrame","text":"<p>PairRDD is the result of a spatial join query or distance join query. SedonaSQL DataFrame-RDD Adapter can convert the result to a DataFrame. But you need to provide the schema of the left and right RDDs.</p> ScalaJavaPython <pre><code>var joinResultDf = StructuredAdapter.toDf(joinResultPairRDD, leftDf.schema, rightDf.schema, sedona)\n</code></pre> <pre><code>Dataset joinResultDf = StructuredAdapter.toDf(joinResultPairRDD, leftDf.schema, rightDf.schema, sedona);\n</code></pre> <pre><code>from sedona.spark import StructuredAdapter\n\njoinResultDf = StructuredAdapter.pairRddToDf(result_pair_rdd, leftDf.schema, rightDf.schema, spark)\n</code></pre>"},{"location":"tutorial/storing-blobs-in-parquet/","title":"Storing large raster geometries in Parquet files","text":""},{"location":"tutorial/storing-blobs-in-parquet/#storing-large-raster-geometries-in-parquet-files","title":"Storing large raster geometries in Parquet files","text":"<p>Warning</p> <p>Always convert the raster geometries to a well known format with the RS_AsXXX functions before saving them. It is possible to save the raw bytes of the raster geometries, but they will be stored in an internal Sedona format that is not guaranteed to be stable across versions.</p> <p>The default settings in Spark are not well suited for storing large binaries like raster geometries. It is very much worth the time to tune and benchmark your settings. Writing large binaries with the default settings will result in poorly structured Parquet files that are very expensive to read. Some basic tuning can increase the read performance by several magnitudes.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#background","title":"Background","text":"<p>Parquet files are divided into one or several row groups. Each column in a row group is stored in a column chunk. Each column chunk is further divided into pages. A page is conceptually an indivisible unit in terms of compression and encoding. The default size for a page is 1 MB. Data is buffered until the page is full and then written to disk. The frequency of checks of the page size limit will be between <code>parquet.page.size.row.check.min</code> and <code>parquet.page.size.row.check.max</code> (default between 100 and 10000 rows).</p> <p>If you write 5 MB image files to Parquet with the default setting the first page size check will happen after 100 rows. You will end up with pages of 500 MB instead of 1 MB. Reading such a file will require a lot of memory and will be slow.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#reading-poorly-structured-parquet-files","title":"Reading poorly structured Parquet files","text":"<p>Especially snappy compressed files are sensitive to oversized pages. More performant options are no compression or zstd compression. You can set <code>spark.buffer.size</code> to a value larger than the default of 64k to improve read performance. Increasing <code>spark.buffer.size</code> might add an io penalty for other columns in the Parquet file.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#writing-better-structured-parquet-files-for-blobs","title":"Writing better structured Parquet files for blobs","text":"<p>Ideally you want to write Parquet files with a sane page size to get better and more consistent read performance across different clients. Since version 1.12.0 of parquet-hadoop, bundled with Spark 3.2, you can add Hadoop properties for controlling page size checks. Better values for writing blobs are:</p> <pre><code>spark.sql.parquet.compression.codec=zstd\nspark.hadoop.parquet.page.size.row.check.min=2\nspark.hadoop.parquet.page.size.row.check.max=10\n</code></pre> <p>Zstd performs better than snappy in general. Even more so for large pages. The first page size check will happen after 2 rows. If the page is not full after 2 rows the next check will happen after another 2-10 rows, depending on the size of the two rows already written.</p> <p>Spark will set Hadoop properties from Spark properties prefixed with \"spark.hadoop.\". For a full list of Parquet Hadoop properties see: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md</p>"},{"location":"tutorial/viz-gallery/","title":"Gallery","text":""},{"location":"tutorial/viz/","title":"Scala/Java","text":"<p>The page outlines the steps to visualize spatial data using SedonaViz. The example code is written in Scala but also works for Java.</p> <p>SedonaViz provides native support for general cartographic design by extending Sedona to process large-scale spatial data. It can visualize Spatial RDD and Spatial Queries and render super high resolution image in parallel.</p> <p>SedonaViz offers Map Visualization SQL. This gives users a more flexible way to design beautiful map visualization effects including scatter plots and heat maps. SedonaViz RDD API is also available.</p> <p>Note</p> <p>All SedonaViz SQL/DataFrame APIs are explained in SedonaViz API. Please see Viz example project</p>"},{"location":"tutorial/viz/#why-scalable-map-visualization","title":"Why scalable map visualization?","text":"<p>Data visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.</p> <p>SedonaViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.</p>"},{"location":"tutorial/viz/#visualize-spatialrdd","title":"Visualize SpatialRDD","text":"<p>This tutorial mainly focuses on explaining SQL/DataFrame API. SedonaViz RDD example can be found in Please see Viz example project</p>"},{"location":"tutorial/viz/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Apache Spark core, Apache SparkSQL, Sedona-core, Sedona-SQL, Sedona-Viz</li> </ol>"},{"location":"tutorial/viz/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val config = SedonaContext.builder()\n        .config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n        .master(\"local[*]\") // Delete this if run in cluster mode\n        .appName(\"Sedona Viz\") // Change this to a proper name\n        .getOrCreate()\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> <pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.getOrCreate()\n</code></pre>"},{"location":"tutorial/viz/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val sedona = SedonaContext.create(config)\nSedonaVizRegistrator.registerAll(sedona)\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/viz/#create-spatial-dataframe","title":"Create Spatial DataFrame","text":"<p>There is a DataFrame as follows:</p> <pre><code>+----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|\n</code></pre> <p>You first need to create a Geometry type column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointtable AS\nSELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as shape\nFROM pointtable\n</code></pre> <p>As you know, Sedona provides many different methods to load various spatial data formats. Please read Write a Spatial DataFrame application.</p>"},{"location":"tutorial/viz/#generate-a-single-image","title":"Generate a single image","text":"<p>In most cases, you just want to see a single image out of your spatial dataset.</p>"},{"location":"tutorial/viz/#pixelize-spatial-objects","title":"Pixelize spatial objects","text":"<p>To put spatial objects on a map image, you first need to convert them to pixels.</p> <p>First, compute the spatial boundary of this column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW boundtable AS\nSELECT ST_Envelope_Aggr(shape) as bound FROM pointtable\n</code></pre> <p>Then use ST_Pixelize to convert them to pixels.</p> <p>This example is for Sedona before v1.0.1. ST_Pixelize extends Generator, so it can directly flatten the array without the explode function.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable)) AS pixel\n</code></pre> <p>This example is for Sedona on and after v1.0.1. ST_Pixelize returns an array of pixels. You need to use explode to flatten it.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW explode(ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable))) AS pixel\n</code></pre> <p>This will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.</p> <p>Warning</p> <p>We highly suggest that you should use ST_Transform to transform coordinates to a visualization-specific coordinate system such as epsg:3857, otherwise you map may look distorted.</p>"},{"location":"tutorial/viz/#aggregate-pixels","title":"Aggregate pixels","text":"<p>Many objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, count(*) as weight\nFROM pixels\nGROUP BY pixel\n</code></pre> <p>The weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.</p>"},{"location":"tutorial/viz/#colorize-pixels","title":"Colorize pixels","text":"<p>Run the following command to assign colors for pixels based on their weights.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, ST_Colorize(weight, (SELECT max(weight) FROM pixelaggregates)) as color\nFROM pixelaggregates\n</code></pre> <p>Please read ST_Colorize for a detailed API description.</p>"},{"location":"tutorial/viz/#render-the-image","title":"Render the image","text":"<p>Use ST_Render to plot all pixels on a single image.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\nFROM pixelaggregates\n</code></pre> <p>This DataFrame will contain an Image type column which has only one image.</p>"},{"location":"tutorial/viz/#store-the-image-on-disk","title":"Store the image on disk","text":"<p>Fetch the image from the previous DataFrame</p> <pre><code>var image = sedona.table(\"images\").take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage\n</code></pre> <p>Use Sedona Viz ImageGenerator to store this image on disk.</p> <pre><code>var imageGenerator = new ImageGenerator\nimageGenerator.SaveRasterImageAsLocalFile(image, System.getProperty(\"user.dir\")+\"/target/points\", ImageType.PNG)\n</code></pre>"},{"location":"tutorial/viz/#generate-map-tiles","title":"Generate map tiles","text":"<p>If you are a map professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.</p>"},{"location":"tutorial/viz/#pixelization-and-pixel-aggregation","title":"Pixelization and pixel aggregation","text":"<p>Please first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution, such as 1000*1000. Note that, each dimension should be divisible by 2^zoom-level</p>"},{"location":"tutorial/viz/#create-tile-name","title":"Create tile name","text":"<p>Run the following command to compute the tile name for every pixels</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, weight, ST_TileName(pixel, 3) AS pid\nFROM pixelaggregates\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#colorize-pixels_1","title":"Colorize pixels","text":"<p>Use the same command explained in single image generation to assign colors.</p>"},{"location":"tutorial/viz/#render-map-tiles","title":"Render map tiles","text":"<p>You now need to group pixels by tiles and then render map tile images in parallel.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color, 3) AS image\nFROM pixelaggregates\nGROUP BY pid\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#store-map-tiles-on-disk","title":"Store map tiles on disk","text":"<p>You can use the same commands in single image generation to fetch all map tiles and store them one by one.</p>"},{"location":"tutorial/zeppelin/","title":"Use Apache Zeppelin","text":"<p>Sedona provides a Helium visualization plugin tailored for Apache Zeppelin. This finally bridges the gap between Sedona and Zeppelin. Please read Install Sedona-Zeppelin to learn how to install this plugin in Zeppelin.</p> <p>Sedona-Zeppelin equips two approaches to visualize spatial data in Zeppelin. The first approach uses Zeppelin to plot all spatial objects on the map. The second one leverages SedonaViz to generate map images and overlay them on maps.</p>"},{"location":"tutorial/zeppelin/#small-scale-without-sedonaviz","title":"Small-scale without SedonaViz","text":"<p>Danger</p> <p>Zeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read SedonaViz solution.</p> <p>You can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW wktpoint AS\n    |SELECT ST_AsText(shape) as geom\n    |FROM pointtable\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data</p> <pre><code>%sql\nSELECT *\nFROM wktpoint\n</code></pre> <p>Select the geometry column to visualize:</p> <p></p>"},{"location":"tutorial/zeppelin/#large-scale-with-sedonaviz","title":"Large-scale with SedonaViz","text":"<p>SedonaViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read How to use SedonaViz.</p> <p>You can use Sedona-Zeppelin to ask Zeppelin to overlay SedonaViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).</p> <p>First, encode images of SedonaViz DataFrame in Zeppelin Spark notebook Scala paragraph,</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data</p> <pre><code>%sql\nSELECT *, 'I am the map center!'\nFROM images\n</code></pre> <p>Select the image and its geospatial boundary:</p> <p></p>"},{"location":"tutorial/zeppelin/#zeppelin-spark-notebook-demo","title":"Zeppelin Spark notebook demo","text":"<p>We provide a full Zeppelin Spark notebook which demonstrates all functions. Please download Sedona-Zeppelin notebook template and test data - arealm.csv.</p> <p>You need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.</p>"},{"location":"tutorial/concepts/clustering-algorithms/","title":"Clustering Algorithms","text":""},{"location":"tutorial/concepts/clustering-algorithms/#apache-sedona-clustering-algorithms-with-apache-spark","title":"Apache Sedona Clustering Algorithms with Apache Spark","text":"<p>Clustering algorithms group similar data points into \u201cclusters.\u201d  Apache Sedona can run clustering algorithms on large geometric datasets.</p> <p>Note that the term cluster is overloaded here:</p> <ul> <li>A computation cluster is a network of computers that work together to execute the algorithm</li> <li>A clustering algorithm divides data points into different \u201cclusters\u201d</li> </ul> <p>This page uses \u201ccluster\u201d to refer to the output of a clustering algorithm.</p>"},{"location":"tutorial/concepts/clustering-algorithms/#clustering-with-dbscan-and-spark","title":"Clustering with DBSCAN and Spark","text":"<p>This page explains how to use Apache Sedona to perform density-based spatial clustering of applications with noise (\u201cDBSCAN\u201d).</p> <p>This algorithm groups geometric objects in high-density areas as clusters and marks points in low-density areas as outliers.</p> <p>Let\u2019s look at a scatter plot of points to visualize a data set that can be clustered.</p> <p></p> <p>Here\u2019s how the DBSCAN algorithm clusters the points:</p> <p></p> <ul> <li>5 points are in cluster 0</li> <li>4 points are in cluster 1</li> <li>4 points are outliers</li> </ul> <p>Let\u2019s create a Spark DataFrame with this data and then run the clustering with Sedona.  Here\u2019s how to construct the DataFrame:</p> <pre><code>df = (\n    sedona.createDataFrame(\n        [\n            (1, 8.0, 2.0),\n            (2, 2.6, 4.0),\n            (3, 2.5, 4.0),\n            (4, 8.5, 2.5),\n            (5, 2.8, 4.3),\n            (6, 12.8, 4.5),\n            (7, 2.5, 4.2),\n            (8, 8.2, 2.5),\n            (9, 8.0, 3.0),\n            (10, 1.0, 5.0),\n            (11, 8.0, 2.5),\n            (12, 5.0, 6.0),\n            (13, 4.0, 3.0),\n        ],\n        [\"id\", \"x\", \"y\"],\n    )\n).withColumn(\"point\", ST_Point(col(\"x\"), col(\"y\")))\n</code></pre> <p>Here are the contents of the DataFrame:</p> <pre><code>+---+----+---+----------------+\n| id|   x|  y|           point|\n+---+----+---+----------------+\n|  1| 8.0|2.0|     POINT (8 2)|\n|  2| 2.6|4.0|   POINT (2.6 4)|\n|  3| 2.5|4.0|   POINT (2.5 4)|\n|  4| 8.5|2.5| POINT (8.5 2.5)|\n|  5| 2.8|4.3| POINT (2.8 4.3)|\n|  6|12.8|4.5|POINT (12.8 4.5)|\n|  7| 2.5|4.2| POINT (2.5 4.2)|\n|  8| 8.2|2.5| POINT (8.2 2.5)|\n|  9| 8.0|3.0|     POINT (8 3)|\n| 10| 1.0|5.0|     POINT (1 5)|\n| 11| 8.0|2.5|   POINT (8 2.5)|\n| 12| 5.0|6.0|     POINT (5 6)|\n| 13| 4.0|3.0|     POINT (4 3)|\n+---+----+---+----------------+\n</code></pre> <p>Here\u2019s how to run the DBSCAN algorithm:</p> <pre><code>from sedona.spark.stats import dbscan\n\ndbscan(df, 1.0, 3).orderBy(\"id\").show()\n</code></pre> <p>Here are the results of the computation:</p> <pre><code>+---+----+---+----------------+------+-------+\n| id|   x|  y|           point|isCore|cluster|\n+---+----+---+----------------+------+-------+\n|  1| 8.0|2.0|     POINT (8 2)|  true|      0|\n|  2| 2.6|4.0|   POINT (2.6 4)|  true|      1|\n|  3| 2.5|4.0|   POINT (2.5 4)|  true|      1|\n|  4| 8.5|2.5| POINT (8.5 2.5)|  true|      0|\n|  5| 2.8|4.3| POINT (2.8 4.3)|  true|      1|\n|  6|12.8|4.5|POINT (12.8 4.5)| false|     -1|\n|  7| 2.5|4.2| POINT (2.5 4.2)|  true|      1|\n|  8| 8.2|2.5| POINT (8.2 2.5)|  true|      0|\n|  9| 8.0|3.0|     POINT (8 3)|  true|      0|\n| 10| 1.0|5.0|     POINT (1 5)| false|     -1|\n| 11| 8.0|2.5|   POINT (8 2.5)|  true|      0|\n| 12| 5.0|6.0|     POINT (5 6)| false|     -1|\n| 13| 4.0|3.0|     POINT (4 3)| false|     -1|\n+---+----+---+----------------+------+-------+\n</code></pre> <p>You can see the <code>cluster</code> column that indicates the grouping of the geometric object.</p> <p>To run this operation, you must set the Spark checkpoint directory. The checkpoint directory is a temporary cache in durable storage where the query's intermediate results are written.</p> <p>Here is how you can set the checkpoint directory:</p> <pre><code>sedona.sparkContext.setCheckpointDir(myPath)\n</code></pre> <p><code>myPath</code> needs to be accessible to all executors. A local path is a good option on a local machine. When available, the HDFS is likely the best choice. Some runtime environments may allow or require block storage paths (e.g., Amazon S3, Google Cloud Storage). Depending on your environment, some runtime environments may already set the Spark checkpoint directory, so this step may not be necessary.</p>"},{"location":"tutorial/concepts/distance-spark/","title":"Distance","text":""},{"location":"tutorial/concepts/distance-spark/#compute-distance-with-sedona-and-apache-spark","title":"Compute distance with Sedona and Apache Spark","text":"<p>This post explains how to compute the distance between two points or geometric objects using Apache Sedona and Apache Spark.</p> <p>You will learn how to compute the distance on a two-dimensional Cartesian plane and how to calculate distance for geospatial data, taking into account the curvature of the Earth.</p> <p>Let\u2019s start with an example on how to compute the distance between two points in a two-dimensional Cartesian plane.</p>"},{"location":"tutorial/concepts/distance-spark/#distance-between-two-points-with-spark-and-sedona","title":"Distance between two points with Spark and Sedona","text":"<p>Suppose you have four points and would like to compute the distance between <code>point_a</code> and <code>point_b</code> and the distance between <code>point_c</code> and <code>point_d</code>.</p> <p></p> <p>Let\u2019s create a DataFrame with these points.</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (Point(2, 3), Point(6, 4)),\n        (Point(6, 2), Point(9, 2)),\n    ],\n    [\"start\", \"end\"],\n)\n</code></pre> <p>The <code>start</code> and <code>end</code> columns both have the <code>geometry</code> type.</p> <p>Now use the <code>ST_Distance</code> function to compute the distance between the points.</p> <pre><code>df.withColumn(\"distance\", ST_Distance(col(\"start\"), col(\"end\"))).show()\n</code></pre> <p>Here are the results:</p> <pre><code>+-----------+-----------+-----------------+\n|      start|        end|         distance|\n+-----------+-----------+-----------------+\n|POINT (2 3)|POINT (6 4)|4.123105625617661|\n|POINT (6 2)|POINT (9 2)|              3.0|\n+-----------+-----------+-----------------+\n</code></pre> <p>The <code>ST_Distance</code> function makes it relatively straightforward to compute the distance between points on a two-dimensional plane.</p>"},{"location":"tutorial/concepts/distance-spark/#distance-between-two-longitudelatitude-points-with-spark-and-sedona","title":"Distance between two longitude/latitude points with Spark and Sedona","text":"<p>Let\u2019s create two longitude/latitude points and compute the distance between them.  Start by creating a DataFrame with the longitude and latitude values.</p> <pre><code>seattle = Point(-122.335167, 47.608013)\nnew_york = Point(-73.935242, 40.730610)\nsydney = Point(151.2, -33.9)\ndf = sedona.createDataFrame(\n    [\n        (seattle, new_york),\n        (seattle, sydney),\n    ],\n    [\"place1\", \"place2\"],\n)\n</code></pre> <p>Let\u2019s compute the distance between these points now:</p> <pre><code>df.withColumn(\n    \"st_distance_sphere\", ST_DistanceSphere(col(\"place1\"), col(\"place2\"))\n).show()\n</code></pre> <p>Here are the results:</p> <pre><code>+--------------------+--------------------+--------------------+\n|              place1|              place2|  st_distance_sphere|\n+--------------------+--------------------+--------------------+\n|POINT (-122.33516...|POINT (-73.935242...|  3870075.7867602874|\n|POINT (-122.33516...| POINT (151.2 -33.9)|1.2473172370818963E7|\n+--------------------+--------------------+--------------------+\n</code></pre> <p>We use the <code>ST_DistanceSphere</code> function to calculate the distance, taking into account the Earth's curvature.  The function returns the distance in meters.</p> <p>Let\u2019s see how to compute the distance between two points with a spheroid model of the Earth.</p>"},{"location":"tutorial/concepts/distance-spark/#compute-distance-between-points-with-a-spheroid-with-spark-and-sedona","title":"Compute distance between points with a spheroid with Spark and Sedona","text":"<p>Let\u2019s use the same DataFrame from the previous section, but compute the distance using a spheroid model of the world.</p> <pre><code>res = df.withColumn(\n    \"st_distance_spheroid\", ST_DistanceSpheroid(col(\"place1\"), col(\"place2\"))\n)\nres.select(\"place1_name\", \"place2_name\", \"st_distance_spheroid\").show()\n</code></pre> <p>Here are the results:</p> <pre><code>+-----------+-----------+--------------------+\n|place1_name|place2_name|st_distance_spheroid|\n+-----------+-----------+--------------------+\n|    seattle|   new_york|  3880173.4858397646|\n|    seattle|     sydney|1.2456531875384018E7|\n+-----------+-----------+--------------------+\n</code></pre> <p>The <code>ST_DistanceSpheroid</code> function returns the meters between the two locations.  The spheroid distance computation yields similar results to those obtained when you model the Earth as a sphere.  Expect the spheroid function to return results that are slightly more accurate.</p>"},{"location":"tutorial/concepts/distance-spark/#distance-between-two-geometric-objects-with-spark-and-sedona","title":"Distance between two geometric objects with Spark and Sedona","text":"<p>Let\u2019s take a look at how to compute the distance between a linestring and a polygon.  Suppose you have the following objects:</p> <p></p> <p>The distance between two polygons is the minimum Euclidean distance between any two points.</p> <p>Let\u2019s compute the distance:</p> <pre><code>res = df.withColumn(\"distance\", ST_Distance(col(\"geom1\"), col(\"geom2\")))\n</code></pre> <p>Now, take a look at the results:</p> <pre><code>+---+---+--------+\n|id1|id2|distance|\n+---+---+--------+\n|a  |b  |2.0     |\n+---+---+--------+\n</code></pre> <p>You can readily see the minimum distance between the two polygons in the graph.</p>"},{"location":"tutorial/concepts/distance-spark/#three-dimensional-minimum-cartesian-distance","title":"Three-dimensional minimum Cartesian distance","text":"<p>Let\u2019s take a look at how to compute the distance between two points, factoring in the elevation of the points.</p> <p>We will examine the distance between someone standing on top of the Empire State Building and someone at sea level.</p> <p>Let\u2019s create the DataFrame:</p> <pre><code>empire_state_ground = Point(-73.9857, 40.7484, 0)\nempire_state_top = Point(-73.9857, 40.7484, 380)\ndf = sedona.createDataFrame(\n    [\n        (empire_state_ground, empire_state_top),\n    ],\n    [\"point_a\", \"point_b\"],\n)\n</code></pre> <p>Now compute the distance and the 3D distance between the points:</p> <pre><code>res = df.withColumn(\"distance\", ST_Distance(col(\"point_a\"), col(\"point_b\"))).withColumn(\n    \"3d_distance\", ST_3DDistance(col(\"point_a\"), col(\"point_b\"))\n)\n</code></pre> <p>Take a look at the results:</p> <pre><code>+--------------------+--------------------+--------+-----------+\n|             point_a|             point_b|distance|3d_distance|\n+--------------------+--------------------+--------+-----------+\n|POINT (-73.9857 4...|POINT (-73.9857 4...|     0.0|      380.0|\n+--------------------+--------------------+--------+-----------+\n</code></pre> <p><code>ST_Distance</code> does not factor in the elevation of the point.  <code>ST_3DDistance</code> factors in the elevation when measuring the distance.</p>"},{"location":"tutorial/concepts/distance-spark/#compute-frechet-distance-with-spark-and-sedona","title":"Compute Frechet distance with Spark and Sedona","text":"<p>Let\u2019s create a Sedona DataFrame with the following linestrings:</p> <p></p> <p>Here\u2019s how to create the Sedona DataFrame:</p> <pre><code>a = LineString([(1, 1), (1, 3), (2, 4)])\nb = LineString([(1.1, 1), (1.1, 3), (3, 4)])\nc = LineString([(7, 1), (7, 3), (6, 4)])\ndf = sedona.createDataFrame(\n    [\n        (a, \"a\", b, \"b\"),\n        (a, \"a\", c, \"c\"),\n    ],\n    [\"geometry1\", \"geometry1_id\", \"geometry2\", \"geometry2_id\"],\n)\n</code></pre> <p>Compute the Frechet distance:</p> <pre><code>res = df.withColumn(\n    \"frechet_distance\", ST_FrechetDistance(col(\"geometry1\"), col(\"geometry2\"))\n)\n</code></pre> <p>Now view the results:</p> <pre><code>res.select(\"geometry1_id\", \"geometry2_id\", \"frechet_distance\").show()\n\n+------------+------------+----------------+\n|geometry1_id|geometry2_id|frechet_distance|\n+------------+------------+----------------+\n|           a|           b|             1.0|\n|           a|           c|             6.0|\n+------------+------------+----------------+\n</code></pre> <p>This image visualizes the distances so you have a better intuition for the algorithm:</p> <p></p>"},{"location":"tutorial/concepts/distance-spark/#compute-the-max-distance-between-geometries-with-spark-and-sedona","title":"Compute the max distance between geometries with Spark and Sedona","text":"<p>Suppose you have the following geometric objects:</p> <p></p> <p>Here\u2019s how to compute the max distance between some of these geometries.  Run the computations:</p> <pre><code>res = df.withColumn(\"max_distance\", ST_MaxDistance(col(\"geom1\"), col(\"geom2\")))\n</code></pre> <p>Now view the results:</p> <pre><code>res.select(\"id1\", \"id2\", \"max_distance\").show(truncate=False)\n\n+---+---+-----------------+\n|id1|id2|max_distance     |\n+---+---+-----------------+\n|a  |b  |8.246211251235321|\n|a  |c  |7.615773105863909|\n+---+---+-----------------+\n</code></pre> <p>You can easily find the maximum distance between two geometric objects.</p>"},{"location":"tutorial/concepts/distance-spark/#conclusion","title":"Conclusion","text":"<p>Sedona enables you to perform various types of distance computations.  It also allows you to compute distance based on different models of the Earth and more complex distance computations, like distance factoring in elevation.</p> <p>Ensure you use the distance function that best suits your analysis.</p>"},{"location":"tutorial/concepts/spatial-joins/","title":"Spatial Joins","text":""},{"location":"tutorial/concepts/spatial-joins/#apache-sedona-spatial-joins-with-spark","title":"Apache Sedona Spatial Joins with Spark","text":"<p>This post explains how to perform spatial joins with Apache Sedona. You will learn about the different types of spatial joins and how to run them efficiently.</p> <p>This page provides basic examples that clearly illustrate the key conceptual points of spatial joins.  It also elaborates on spatial join concepts for real-world-sized datasets and highlights key performance enhancements.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-within-using-spark","title":"Spatial join within using Spark","text":"<p>Look at the following graph containing three points and two polygons.  <code>point_b</code> is within <code>polygon_y</code>, <code>point_c</code> is within <code>polygon_x</code>, and <code>point_a</code> isn\u2019t within any polygon.</p> <p></p> <p>The <code>points</code> table contains points and the <code>polygons</code> table includes polygons.</p> <p>Here\u2019s how you can run the SQL for this query:</p> <pre><code>SELECT\n    points.id as point_id,\n    polygons.id as polygon_id\nFROM points\nJOIN polygons ON ST_Within(points.geometry, polygons.geometry);  \n</code></pre> <p>Here\u2019s the result:</p> <pre><code>+--------+----------+\n|point_id|polygon_id|\n+--------+----------+\n|       b|         y|\n|       c|         x|\n+--------+----------+\n</code></pre> <p><code>point_a</code> is not in the resulting DataFrame because it\u2019s not within any polygon.</p> <p>It\u2019s easier to see that the <code>polygon_id</code> is <code>NULL</code> for <code>point_a</code> with a <code>LEFT JOIN</code>:</p> <pre><code>SELECT\n    points.id as point_id,\n    polygons.id as polygon_id\nFROM points\nLEFT JOIN polygons ON ST_Within(points.geometry, polygons.geometry);  \n</code></pre> <p>Here\u2019s the output:</p> <pre><code>+--------+----------+\n|point_id|polygon_id|\n+--------+----------+\n|       a|      NULL|\n|       b|         y|\n|       c|         x|\n+--------+----------+\n</code></pre> <p>The <code>polygon_id</code> is <code>NULL</code> for <code>point_a</code> because it is not within any polygon.</p> <p>In production applications, you typically use <code>JOIN</code>. This post uses <code>LEFT JOIN</code> to illustrate the rows that do not match in the join.</p> <p>This previous code snipped used the <code>ST_Within</code> predicate, which is closely related to the <code>ST_Contains</code> method.  They\u2019re the same but the parameter order is swapped.  Here\u2019s how you can get the same result with <code>ST_Contains</code>:</p> <pre><code>SELECT\n    points.id as point_id,\n    polygons.id as polygon_id\nFROM points\nLEFT JOIN polygons ON ST_Contains(polygons.geometry, points.geometry);  \n</code></pre> <p>Here\u2019s the same result:</p> <pre><code>+--------+----------+\n|point_id|polygon_id|\n+--------+----------+\n|       a|      NULL|\n|       b|         y|\n|       c|         x|\n+--------+----------+\n</code></pre>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-crosses-with-spark","title":"Spatial join crosses with Spark","text":"<p>Look at the following graph containing one polygon and two lines.  <code>line_a</code> and <code>line_b</code> cross <code>polygon_x</code>.  <code>line_c</code> does not cross <code>polygon_x</code>.</p> <p></p> <p>Here\u2019s the SQL query to run this spatial join:</p> <pre><code>SELECT\n    lines.id as line_id,\n    polygons.id as polygon_id\nFROM lines\nLEFT JOIN polygons ON ST_Crosses(lines.geometry, polygons.geometry);  \n</code></pre> <p>Here is the result:</p> <pre><code>+-------+----------+\n|line_id|polygon_id|\n+-------+----------+\n|      a|         x|\n|      b|         x|\n|      c|      NULL|\n+-------+----------+\n</code></pre> <p>A spatial join with <code>ST_Crosses</code> lets us identify the lines that cross the polygon.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-with-touches-using-spark","title":"Spatial join with touches using Spark","text":"<p>Suppose you have a polygon and two lines.  <code>line_a</code> does not touch the polygon, and <code>line_b</code> does touch the polygon.  See the following diagram:</p> <p></p> <p>Let\u2019s create <code>table_a</code> with the lines and <code>table_b</code> with the polygon and then join them.</p> <p>Here is the content of the polygons table:</p> <pre><code>+---+-----------------------------------+\n|id |geometry                           |\n+---+-----------------------------------+\n|x  |POLYGON ((6 2, 6 4, 8 4, 8 2, 6 2))|\n+---+-----------------------------------+\n</code></pre> <p>Here is the content of the lines table:</p> <pre><code>+---+----------------------+\n|id |geometry              |\n+---+----------------------+\n|a  |LINESTRING (2 4, 4 0) |\n|b  |LINESTRING (6 0, 10 4)|\n+---+----------------------+\n</code></pre> <p>Here\u2019s a join that matches any touching values:</p> <pre><code>sedona.sql(\n    \"\"\"\nSELECT\n    lines.id as line_id,\n    polygons.id as polygon_id\nFROM lines\nLEFT JOIN polygons ON ST_Touches(lines.geometry, polygons.geometry);  \n\"\"\"\n).show()\n</code></pre> <p>Here\u2019s the result of the join:</p> <pre><code>+-------+----------+\n|line_id|polygon_id|\n+-------+----------+\n|      a|      NULL|\n|      b|         x|\n+-------+----------+\n</code></pre> <p>Now, let\u2019s look at running a join to see if points are within a polygon.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-overlaps-with-spark","title":"Spatial join overlaps with Spark","text":"<p>The following diagram shows two polygons and a few shapes.  <code>polygon_a</code> overlaps <code>polygon_x</code>.  Neither <code>line_b</code>, <code>line_c</code>, or <code>point_d</code> overlap with <code>polygon_y</code> or <code>polygon_x</code>.</p> <p></p> <p>Here is the SQL query to run this spatial join:</p> <pre><code>SELECT\n    shapes.id as shape_id,\n    polygons.id as polygon_id\nFROM shapes\nLEFT JOIN polygons ON ST_Overlaps(shapes.geometry, polygons.geometry);\n</code></pre> <p>Here is the result:</p> <pre><code>+--------+----------+\n|shape_id|polygon_id|\n+--------+----------+\n|       a|         x|\n|       b|      NULL|\n|       c|      NULL|\n|       d|      NULL|\n+--------+----------+\n</code></pre>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-k-nearest-neighbors-knn-spatial-join-with-spark","title":"Spatial join K-nearest neighbors (KNN spatial join) with Spark","text":"<p>Suppose you have tables with addresses and coffee shop locations.  You\u2019d like to find the two nearest coffee shops to each address.</p> <p>Here are the <code>addresses</code> with <code>latitude</code> and <code>longitude</code> coordinates:</p> <pre><code>+---+---------+--------+\n| id|longitude|latitude|\n+---+---------+--------+\n| a1|      2.0|     3.0|\n| a2|      5.0|     5.0|\n| a3|      7.0|     2.0|\n+---+---------+--------+\n</code></pre> <p>Here are the <code>coffee_shops</code> with <code>latitude</code> and <code>longitude</code>:</p> <pre><code>+---+---------+--------+\n| id|longitude|latitude|\n+---+---------+--------+\n| c1|      1.0|     4.0|\n| c2|      3.0|     5.0|\n| c3|      5.0|     1.0|\n| c4|      8.0|     4.0|\n+---+---------+--------+\n</code></pre> <p>Here\u2019s how to compute the two nearest coffee shops to each address:</p> <pre><code>SELECT\n    addresses.id AS address_id,\n    coffee_shops.id AS coffee_shop_id\nFROM addresses\nJOIN coffee_shops\nON ST_KNN(addresses.geometry, coffee_shops.geometry, 2)\n</code></pre> <p>Here is the result:</p> <pre><code>+----------+--------------+\n|address_id|coffee_shop_id|\n+----------+--------------+\n|        a1|            c1|\n|        a1|            c2|\n|        a2|            c2|\n|        a2|            c4|\n|        a3|            c3|\n|        a3|            c4|\n+----------+--------------+\n</code></pre> <p>Here\u2019s a visualization of the results:</p> <p></p> <p>You can easily see the coffee shops that are closest to each address.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-distance-join-with-spark","title":"Spatial distance join with Spark","text":"<p>Look at the following graph, which shows a point and different transit stations. Let\u2019s perform a spatial join to find all the transit stations within 2.5 units of the point.</p> <p></p> <p>We can see that <code>t2</code> and <code>t3</code> are within 2.5 units from the point.</p> <p>Here is the points table:</p> <pre><code>+---+-------------+\n| id|     geometry|\n+---+-------------+\n| p1|POINT (4.5 3)|\n+---+-------------+\n</code></pre> <p>And here is the transit table:</p> <pre><code>+---+-----------+\n| id|   geometry|\n+---+-----------+\n| t1|POINT (1 4)|\n| t2|POINT (3 4)|\n| t3|POINT (5 2)|\n| t4|POINT (8 4)|\n+---+-----------+\n</code></pre> <p>Let\u2019s perform a distance join to find all the transit stations that are within 2.5 units of the point:</p> <pre><code>SELECT\n    points.id AS point_id,\n    transit.id AS transit_id\nFROM points\nJOIN transit\nON ST_DWithin(points.geometry, transit.geometry, 2.5)\n</code></pre> <p>Here are the results:</p> <pre><code>+--------+----------+\n|point_id|transit_id|\n+--------+----------+\n|      p1|        t2|\n|      p1|        t3|\n+--------+----------+\n</code></pre> <p>Sedona is an excellent tool for finding locations within a certain distance from a point.</p> <p>Sedona uses the Euclidean distance between two objects so the distance unit has the same CRS of the original coordinates. To directly operate on WGS84 coordinates with meter distance, you should use <code>ST_DistanceSphere</code>, <code>ST_DistanceSpheroid</code>, or <code>ST_DWithnin(useSpheroid = true)</code>.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-range-join-with-spark","title":"Spatial range join with Spark","text":"<p>All joins triggered by <code>ST_Intersects</code>, <code>ST_Contains</code>, <code>ST_Within</code>, <code>ST_DWithin</code>, <code>ST_Touches</code>, and <code>ST_Crosses</code> are considered a range join.  This section illustrates another range join, but we've already covered several range joins on this page.</p> <p>Suppose you have a table with cities and another table with restaurants.  You want to identify all the restaurants in a given city.  See the following diagram for some sample data.</p> <p></p> <p>Three restaurants are within the city boundary, and one is outside the city.</p> <p>Here is the cities table:</p> <pre><code>+-----+--------------------+\n|   id|            geometry|\n+-----+--------------------+\n|city1|POLYGON ((1 1, 1 ...|\n+-----+--------------------+\n</code></pre> <p>And here is the <code>restaurants</code> table:</p> <pre><code>+---+-----------+\n| id|   geometry|\n+---+-----------+\n| r1|POINT (2 2)|\n| r2|POINT (3 3)|\n| r3|POINT (4 4)|\n| r4|POINT (6 6)|\n+---+-----------+\n</code></pre> <p>Here\u2019s how to execute the range join:</p> <pre><code>SELECT\n    cities.id AS city_id,\n    restaurants.id AS restaurant_id\nFROM cities\nJOIN restaurants\nON ST_Intersects(restaurants.geometry, cities.geometry)\n</code></pre> <p>Here are the results:</p> <pre><code>+-------+-------------+\n|city_id|restaurant_id|\n+-------+-------------+\n|  city1|           r1|\n|  city1|           r2|\n|  city1|           r3|\n+-------+-------------+\n</code></pre> <p>Range joins are helpful in many practical applications.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-join-optimizations-for-sedona-and-apache-spark","title":"Spatial join optimizations for Sedona and Apache Spark","text":"<p>You can optimize spatial joins by using better file formats, indexing your data, or optimizing your queries.</p> <p>For example, suppose you\u2019re performing a spatial join with two wide tables stored in GeoJSON files and don\u2019t need all the output columns.  GeoJSON files are row-oriented and don\u2019t support column pruning.  You can switch the data from GeoJSON to a column-oriented file format like GeoParquet to take advantage of column pruning, a vital performance enhancement.</p> <p>See this section on additional query enhancements here to learn more.</p>"},{"location":"tutorial/concepts/spatial-joins/#spatial-broadcast-joins","title":"Spatial broadcast joins","text":"<p>Sedona can run on a single node or many nodes in a cluster.  </p> <p>As you can imagine, joining two datasets can be slower when your data is on different machines in a cluster.  That requires data shuffling, which can be slow.</p> <p>If one of the tables is small, you can broadcast it, which copies it to all the machines in the cluster.  Broadcasting can make the join much faster.</p> <p>You should generally only broadcast DataFrames that are relatively small, see here for more information.  </p> <p>Sedona will automatically broadcast tables smaller than the threshold; see here for more details.</p>"},{"location":"tutorial/concepts/spatial-joins/#conclusion","title":"Conclusion","text":"<p>Apache Sedona supports a variety of spatial joins.</p> <p>Spatial joins are one of the strengths of the Sedona engine.  Other engines can struggle with memory issues for spatial joins.  Sedona is capable of performing spatial joins on massive datasets.</p>"},{"location":"tutorial/files/csv-geometry-sedona-spark/","title":"CSV","text":""},{"location":"tutorial/files/csv-geometry-sedona-spark/#apache-sedona-csv-with-geometry-using-spark","title":"Apache Sedona CSV with geometry using Spark","text":"<p>This post shows how to read and write CSV files with geometry columns using Sedona and Spark.</p> <p>You will learn about the advantages and disadvantages of the CSV file format for storing geometric data.</p> <p>Let\u2019s start by seeing how to write CSV files with geometric data.</p>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#write-csv-with-geometry-using-sedona-and-spark","title":"Write CSV with geometry using Sedona and Spark","text":"<p>Let\u2019s start by creating a DataFrame with Sedona and Spark:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"a\", \"LINESTRING(2.0 5.0,6.0 1.0)\"),\n        (\"b\", \"POINT(1.0 2.0)\"),\n        (\"c\", \"POLYGON((7.0 1.0,7.0 3.0,9.0 3.0,7.0 1.0))\"),\n    ],\n    [\"id\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\n</code></pre> <p>Here are the contents of the DataFrame:</p> <pre><code>+---+------------------------------+\n|id |geometry                      |\n+---+------------------------------+\n|a  |LINESTRING (2 5, 6 1)         |\n|b  |POINT (1 2)                   |\n|c  |POLYGON ((7 1, 7 3, 9 3, 7 1))|\n+---+------------------------------+\n</code></pre> <p>Now write the DataFrame to a CSV file:</p> <pre><code>df = df.withColumn(\"geom_wkt\", ST_AsText(col(\"geometry\"))).drop(\"geometry\")\ndf.repartition(1).write.option(\"header\", True).format(\"csv\").mode(\"overwrite\").save(\n    \"/tmp/my_csvs\"\n)\n</code></pre> <p>Notice that we\u2019re using <code>repartition(1)</code> to output the DataFrame as a single file.  It\u2019s usually better to output many files in parallel, making the write operation faster.  We\u2019re just writing to a single file for the simplicity of this example.</p> <p>Here are the contents of the CSV file:</p> <pre><code>id,geom_wkt\na,\"LINESTRING (2 5, 6 1)\"\nb,POINT (1 2)\nc,\"POLYGON ((7 1, 7 3, 9 3, 7 1))\"\n</code></pre> <p>This file stores the <code>geom_wkt</code> column as plain text, making it easily human-readable.  It follows a standard format, so any engine that knows how to parse WKT can read the column.</p>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#read-csv-with-geometry-using-sedona-and-spark","title":"Read CSV with geometry using Sedona and Spark","text":"<p>Now read the CSV file into a DataFrame:</p> <pre><code>df = (\n    sedona.read.option(\"header\", True)\n    .format(\"CSV\")\n    .load(\"/tmp/my_csvs\")\n    .withColumn(\"geometry\", ST_GeomFromText(col(\"geom_wkt\")))\n    .drop(\"geom_wkt\")\n)\n</code></pre> <p>This file stores the <code>geom_wkt</code> column as text.  When you read the data, you must convert it to a geometry column with the <code>ST_GeomFromText</code> function.  Here are the contents of the DataFrame:</p> <pre><code>+---+------------------------------+\n|id |geometry                      |\n+---+------------------------------+\n|a  |LINESTRING (2 5, 6 1)         |\n|b  |POINT (1 2)                   |\n|c  |POLYGON ((7 1, 7 3, 9 3, 7 1))|\n+---+------------------------------+\n</code></pre> <p>Verify that the schema is correct:</p> <pre><code>root\n |-- id: string (nullable = true)\n |-- geometry: geometry (nullable = true)\n</code></pre>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#readwrite-csv-files-with-extended-well-known-text-ewkt","title":"Read/write CSV files with Extended Well-Known Text (EWKT)","text":"<p>Let\u2019s see how to write the DataFrame to CSV with EWKT.  Start by adding the SRID to the geometry column.</p> <pre><code>df = df.withColumn(\"geometry\", ST_SetSRID(col(\"geometry\"), 4326))\n</code></pre> <p>Now write out the DataFrame with an EWKT column:</p> <pre><code>df = df.withColumn(\"geom_ewkt\", ST_AsEWKT(col(\"geometry\"))).drop(\"geometry\")\ndf.repartition(1).write.option(\"header\", True).format(\"csv\").mode(\"overwrite\").save(\n    \"/tmp/my_ewkt_csvs\"\n)\n</code></pre> <p>Here are the contents of the CSV file:</p> <pre><code>id,geom_ewkt\na,\"SRID=4326;LINESTRING (2 5, 6 1)\"\nb,SRID=4326;POINT (1 2)\nc,\"SRID=4326;POLYGON ((7 1, 7 3, 9 3, 7 1))\"\n</code></pre> <p>Here\u2019s how to read the CSV file with an EWKT column into a Sedona DataFrame:</p> <pre><code>df = (\n    sedona.read.option(\"header\", True)\n    .format(\"csv\")\n    .load(\"/tmp/my_ewkt_csvs\")\n    .withColumn(\"geometry\", ST_GeomFromEWKT(col(\"geom_ewkt\")))\n    .drop(\"geom_ewkt\")\n)\n</code></pre> <p>Here are the contents of the DataFrame:</p> <pre><code>+---+------------------------------+\n|id |geometry                      |\n+---+------------------------------+\n|a  |LINESTRING (2 5, 6 1)         |\n|b  |POINT (1 2)                   |\n|c  |POLYGON ((7 1, 7 3, 9 3, 7 1))|\n+---+------------------------------+\n</code></pre> <p>You don\u2019t see the SRID when printing the Sedona DataFrame, but this metadata is maintained internally.</p>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#advantages-of-csv-for-data-with-geometry","title":"Advantages of CSV for data with geometry","text":"<p>There are a few advantages of using CSV with geometry data:</p> <ul> <li>Many engines support CSV</li> <li>It\u2019s human-readable</li> <li>The \u201cextended\u201d format saves CRS information</li> <li>The standard has withstood the test of time</li> </ul> <p>But CSV also has lots of disadvantages.</p>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#disadvantages-of-csv-for-datasets-with-geometry","title":"Disadvantages of CSV for datasets with geometry","text":"<p>Here are the disadvantages of storing geometric data in CSV files:</p> <ul> <li>CSV is a row-oriented file format, so engines can\u2019t cherry-pick individual columns while reading data.  Column-oriented files allow for column pruning, an important performance feature.</li> <li>CSV\u2019s row-oriented nature makes it harder to compress files.</li> <li>CSV files don\u2019t contain the schema of the data so engines need to either infer the schema or users need to manually specify it when reading the data.  Inferring the schema is error-prone, and manually specifying the schema is tedious.</li> <li>CSV doesn\u2019t store row-group metadata, so row-group skipping isn\u2019t possible.</li> <li>CSV doesn\u2019t store file-level metadata, so file skipping isn\u2019t possible.</li> <li>When SRID metadata is tracked, it\u2019s written on every line of the CSV file, which unnecessarily takes up a lot of space because CSVs don\u2019t support file-level metadata.</li> </ul>"},{"location":"tutorial/files/csv-geometry-sedona-spark/#conclusion","title":"Conclusion","text":"<p>Spark and Sedona support the CSV file format for geometric data, but it generally is slow and should only be used when necessary.</p> <p>If you\u2019re building a geospatial data lake, GeoParquet is almost always a better alternative.</p> <p>And if you\u2019re building a geospatial data lakehouse, then Iceberg is a good option.</p>"},{"location":"tutorial/files/geojson-sedona-spark/","title":"GeoJSON","text":""},{"location":"tutorial/files/geojson-sedona-spark/#apache-sedona-geojson-with-spark","title":"Apache Sedona GeoJSON with Spark","text":"<p>This page shows how to read/write single-line GeoJSON files and multiline GeoJSON files with Apache Sedona and Spark.  </p> <p>The post concludes with a summary of the benefits and drawbacks of the GeoJSON file format for spatial analyses.</p> <p>GeoJSON is based on JSON and supports the following types:</p> <ul> <li>Point</li> <li>LineString</li> <li>Polygon</li> <li>MultiPoint</li> <li>MultiLineString</li> <li>MultiPolygon</li> </ul> <p>See here for more details about the GeoJSON format specification.</p>"},{"location":"tutorial/files/geojson-sedona-spark/#read-multiline-geojson-files-with-sedona-and-spark","title":"Read multiline GeoJSON files with Sedona and Spark","text":"<p>Here\u2019s how to read a multiline GeoJSON file with Sedona:</p> <pre><code>df = (\n    sedona.read.format(\"geojson\")\n    .option(\"multiLine\", \"true\")\n    .load(\"data/multiline_geojson.json\")\n    .selectExpr(\"explode(features) as features\")\n    .select(\"features.*\")\n    .withColumn(\"prop0\", expr(\"properties['prop0']\"))\n    .drop(\"properties\")\n    .drop(\"type\")\n)\ndf.show(truncate=False)\n</code></pre> <p>Here\u2019s the output:</p> <pre><code>+---------------------------------------------+------+\n|geometry                                     |prop0 |\n+---------------------------------------------+------+\n|POINT (102 0.5)                              |value0|\n|LINESTRING (102 0, 103 1, 104 0, 105 1)      |value1|\n|POLYGON ((100 0, 101 0, 101 1, 100 1, 100 0))|value2|\n+---------------------------------------------+------+\n</code></pre> <p>The multiline GeoJSON file contains a point, a linestring, and a polygon.  Let\u2019s inspect the content of the file:</p> <pre><code>{ \"type\": \"FeatureCollection\",\n    \"features\": [\n      { \"type\": \"Feature\",\n        \"geometry\": {\"type\": \"Point\", \"coordinates\": [102.0, 0.5]},\n        \"properties\": {\"prop0\": \"value0\"}\n        },\n      { \"type\": \"Feature\",\n        \"geometry\": {\n          \"type\": \"LineString\",\n          \"coordinates\": [\n            [102.0, 0.0], [103.0, 1.0], [104.0, 0.0], [105.0, 1.0]\n            ]\n          },\n        \"properties\": {\n          \"prop0\": \"value1\",\n          \"prop1\": 0.0\n          }\n        },\n      { \"type\": \"Feature\",\n         \"geometry\": {\n           \"type\": \"Polygon\",\n           \"coordinates\": [\n             [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0],\n               [100.0, 1.0], [100.0, 0.0] ]\n             ]\n         },\n         \"properties\": {\n           \"prop0\": \"value2\",\n           \"prop1\": {\"this\": \"that\"}\n           }\n         }\n       ]\n}\n</code></pre> <p>Notice how the data is modeled as a <code>FeatureCollection</code>.  Each feature has a geometry type, geometry coordinates, and properties.</p> <p>You can also read many multiline GeoJSON files.  Suppose you have the following GeoJSON files:</p> <pre><code>many_geojsons/\n  file1.json\n  file2.json\n</code></pre> <p>Here's how you can read many GeoJSON files:</p> <pre><code>df = (\n    sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"data/many_geojsons\")\n)\n</code></pre> <p>You just need to pass the directory that contains the JSON files.</p> <p>Multiline GeoJSON is nicely formatted for humans but inefficient for machines. It\u2019s better to store all the JSON data in a single line.</p>"},{"location":"tutorial/files/geojson-sedona-spark/#read-single-line-geojson-files-with-sedona-and-spark","title":"Read single-line GeoJSON files with Sedona and Spark","text":"<p>Here\u2019s how to read single-line GeoJSON files with Sedona:</p> <pre><code>df = (\n    sedona.read.format(\"geojson\")\n    .load(\"data/singleline_geojson.json\")\n    .withColumn(\"prop0\", expr(\"properties['prop0']\"))\n    .drop(\"properties\")\n    .drop(\"type\")\n)\ndf.show(truncate=False)\n</code></pre> <p>Here\u2019s the result:</p> <pre><code>+---------------------------------------------+------+  \n|geometry                                     |prop0 |\n+---------------------------------------------+------+\n|POINT (102 0.5)                              |value0|\n|LINESTRING (102 0, 103 1, 104 0, 105 1)      |value1|\n|POLYGON ((100 0, 101 0, 101 1, 100 1, 100 0))|value2|\n+---------------------------------------------+------+\n</code></pre> <p>Here\u2019s the data:</p> <pre><code>{\"type\":\"Feature\",\"geometry\":{\"type\":\"Point\",\"coordinates\":[102.0,0.5]},\"properties\":{\"prop0\":\"value0\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"LineString\",\"coordinates\":[[102.0,0.0],[103.0,1.0],[104.0,0.0],[105.0,1.0]]},\"properties\":{\"prop0\":\"value1\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[100.0,0.0],[101.0,0.0],[101.0,1.0],[100.0,1.0],[100.0,0.0]]]},\"properties\":{\"prop0\":\"value2\"}}\n</code></pre> <p>Notice how the multi-line GeoJSON uses a <code>FeatureCollection</code> whereas each single-line GeoJSON row uses a different <code>Feature</code>.</p> <p>Single-line GeoJSON files are better because they\u2019re splittable by query engines.</p> <p>Now, let's see how to create GeoJSON files with Sedona by writing out DataFrames.</p>"},{"location":"tutorial/files/geojson-sedona-spark/#write-to-geojson-with-sedona-and-spark","title":"Write to GeoJSON with Sedona and Spark","text":"<p>Let\u2019s create a Sedona DataFrame and then write it out to GeoJSON files:</p> <pre><code>df = sedona.createDataFrame([\n    (\"a\", 'LINESTRING(2.0 5.0,6.0 1.0)'),\n    (\"b\", 'LINESTRING(7.0 4.0,9.0 2.0)'),\n    (\"c\", 'LINESTRING(1.0 3.0,3.0 1.0)'),\n], [\"id\", \"geometry\"])\nactual = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\nactual.write.format(\"geojson\").mode(\"overwrite\").save(\"/tmp/a_thing\")\n</code></pre> <p>Here are the files that get written:</p> <pre><code>a_thing/\n  _SUCCESS\n  part-00000-856044c5-ae35-4306-bf7a-ae9c3cb25434-c000.json\n  part-00003-856044c5-ae35-4306-bf7a-ae9c3cb25434-c000.json\n  part-00007-856044c5-ae35-4306-bf7a-ae9c3cb25434-c000.json\n  part-00011-856044c5-ae35-4306-bf7a-ae9c3cb25434-c000.json\n</code></pre> <p>Sedona writes multiple GeoJSON files in parallel, which is faster than writing a single file.</p> <p>Note that the DataFrame must contain at least one column with geometry type for the write operation to work. Sedona will use the following rules to determine which column to use as the geometry:</p> <ol> <li>If there's a column named \"geometry\" with geometry type, Sedona will use this column</li> <li>Otherwise, Sedona will use the first geometry column found in the root schema</li> </ol> <p>You can also manually specify which geometry column to use with the \"geometry.column\" option:</p> <pre><code>df.write.format(\"geojson\").option(\"geometry.column\", \"geometry\").save(\"/tmp/a_thing\")\n</code></pre> <p>Now let\u2019s read these GeoJSON files into a DataFrame:</p> <pre><code>df = sedona.read.format(\"geojson\").load(\"/tmp/a_thing\")\ndf.show(truncate=False)\n</code></pre> <pre><code>+---------------------+----------+-------+\n|geometry             |properties|type   |\n+---------------------+----------+-------+\n|LINESTRING (1 3, 3 1)|{c}       |Feature|\n|LINESTRING (2 5, 6 1)|{a}       |Feature|\n|LINESTRING (7 4, 9 2)|{b}       |Feature|\n+---------------------+----------+-------+\n</code></pre>"},{"location":"tutorial/files/geojson-sedona-spark/#benefits-of-the-geojson-file-format","title":"Benefits of the GeoJSON file format","text":"<p>The GeoJSON file format has many advantages:</p> <ul> <li>It is human-readable</li> <li>It can be output in multiple files, which allows for faster I/O for parallel processing engines.</li> <li>Many engines support GeoJSON / JSON files.</li> </ul> <p>However, GeoJSON has many downsides, making it a suboptimal choice for storing geospatial data.</p>"},{"location":"tutorial/files/geojson-sedona-spark/#limitations-of-the-geojson-file-format","title":"Limitations of the GeoJSON file format","text":"<p>The GeoJSON format has many limitations that can make it a slow option for spatial data lakes:</p> <ul> <li>A GeoJSON object may have a CRS, but it's optional, so this critical data can be lost.</li> <li>It\u2019s a row-oriented file format, so performance optimizations like column pruning aren\u2019t available (column-oriented file formats, like GeoParquet, can take advantage of this optimization).</li> <li>It does not store metadata information on row groups, so row-group filtering isn\u2019t possible (row-group filtering is a Parquet performance optimization).</li> <li>The schema is not specified in the footer, so it needs to be manually written or inferred.</li> <li>The GeoJSON specification requires a specific structure that can be rigid for certain types of datasets.</li> <li>You can only build GeoJSON data lakes.  You can\u2019t use GeoJSON to build data lakehouses.</li> </ul>"},{"location":"tutorial/files/geojson-sedona-spark/#conclusion","title":"Conclusion","text":"<p>GeoJSON is a common file format in spatial data analyses, and it\u2019s convenient that Apache Sedona offers full read and write capabilities.</p> <p>GeoJSON is well-supported and human-readable, but it\u2019s pretty slow compared to formats like GeoParquet.  It\u2019s generally best to use GeoParquet or Iceberg for spatial data analyses because the performance is much better.</p>"},{"location":"tutorial/files/geopackage-sedona-spark/","title":"GeoPackage","text":""},{"location":"tutorial/files/geopackage-sedona-spark/#apache-sedona-geopackage-with-spark","title":"Apache Sedona GeoPackage with Spark","text":"<p>This page shows how to read GeoPackage files with Apache Sedona and Spark.</p> <p>You\u2019ll learn about the advantages and disadvantages of the GeoPackage file format and how to use them in production settings.</p> <p>Let\u2019s start by creating a GeoPackage file and then reading it.</p>"},{"location":"tutorial/files/geopackage-sedona-spark/#reading-a-geopackage-file-with-sedona-and-spark","title":"Reading a GeoPackage file with Sedona and Spark","text":"<p>Let\u2019s create a GeoPackage file with a few rows of data.</p> <p>Start by creating a GeoPandas DataFrame:</p> <pre><code>point1 = Point(0, 0)\npoint2 = Point(1, 1)\npolygon1 = Polygon([(5, 5), (6, 6), (7, 5), (6, 4)])\n\ndata = {\n    \"name\": [\"Point A\", \"Point B\", \"Polygon A\"],\n    \"value\": [10, 20, 30],\n    \"geometry\": [point1, point2, polygon1],\n}\ngdf = gpd.GeoDataFrame(data, geometry=\"geometry\")\n</code></pre> <p>Now write the GeoPandas DataFrame to a GeoPackage file:</p> <pre><code>gdf.to_file(\"/tmp/my_file.gpkg\", layer=\"my_layer\", driver=\"GPKG\")\n</code></pre> <p>GeoPandas knows to write this to a GeoPackage file because the code sets the driver to <code>GPKG</code>.</p> <p>You can think of the layer as the table name.</p> <p>Now let\u2019s read the GeoPackage file Apache Sedona and Spark:</p> <pre><code>df = (\n    sedona.read.format(\"geopackage\")\n    .option(\"tableName\", \"my_layer\")\n    .load(\"/tmp/my_file.gpkg\")\n)\ndf.show()\n</code></pre> <p>Here are the contents of the DataFrame:</p> <pre><code>+---+--------------------+---------+-----+\n|fid|                geom|     name|value|\n+---+--------------------+---------+-----+\n|  1|         POINT (0 0)|  Point A|   10|\n|  2|         POINT (1 1)|  Point B|   20|\n|  3|POLYGON ((5 5, 6 ...|Polygon A|   30|\n+---+--------------------+---------+-----+\n</code></pre> <p>The geometry column can contain many different geometric objects like points, polygons, and many more.</p> <p>You can also see the metadata of the GeoPackage file:</p> <pre><code>df = (\n    sedona.read.format(\"geopackage\")\n    .option(\"showMetadata\", \"true\")\n    .load(\"/tmp/my_file.gpkg\")\n)\ndf.show()\n</code></pre> <p>Here are the contents:</p> <pre><code>+----------+---------+----------+-----------+--------------------+-----+-----+-----+-----+------+\n|table_name|data_type|identifier|description|         last_change|min_x|min_y|max_x|max_y|srs_id|\n+----------+---------+----------+-----------+--------------------+-----+-----+-----+-----+------+\n|  my_layer| features|  my_layer|           |2025-02-25 06:28:...|  0.0|  0.0|  7.0|  6.0| 99999|\n+----------+---------+----------+-----------+--------------------+-----+-----+-----+-----+------+\n</code></pre>"},{"location":"tutorial/files/geopackage-sedona-spark/#reading-many-geopackage-files-with-sedona-and-spark","title":"Reading many GeoPackage files with Sedona and Spark","text":"<p>You can also read many GeoPackage files with Sedona.  Suppose you have the following GeoPackage files:</p> <pre><code>gpkgs/\n  my_file1.gpkg\n  my_file2.gpkg\n</code></pre> <p>Here\u2019s how you can read all the files:</p> <pre><code>df = sedona.read.format(\"geopackage\").option(\"tableName\", \"my_layer\").load(\"/tmp/gpkgs\")\ndf.show()\n</code></pre> <p>Here are the results:</p> <pre><code>+---+--------------------+---------+-----+\n|fid|                geom|     name|value|\n+---+--------------------+---------+-----+\n|  1|         POINT (5 5)|  Point C|   30|\n|  2|POLYGON ((5 5, 6 ...|Polygon A|   40|\n|  1|         POINT (0 0)|  Point A|   10|\n|  2|         POINT (1 1)|  Point B|   20|\n+---+--------------------+---------+-----+\n</code></pre> <p>You just need to supply the directory containing the GeoPackage files, and Sedona can read all of them into a DataFrame.</p> <p>Sedona is an excellent option for analyzing many GeoPackage files because it can read and process them in parallel.</p>"},{"location":"tutorial/files/geopackage-sedona-spark/#load-raster-data-stored-in-geopackage-files","title":"Load raster data stored in GeoPackage files","text":"<p>You can also load data from raster tables in the GeoPackage file. To load raster data, you can use the following code.</p> <pre><code>df = (\n    sedona.read.format(\"geopackage\")\n    .option(\"tableName\", \"raster_table\")\n    .load(\"/path/to/geopackage\")\n)\n</code></pre> <p>Here are the contents of the DataFrame:</p> <pre><code>+---+----------+-----------+--------+--------------------+\n| id|zoom_level|tile_column|tile_row|           tile_data|\n+---+----------+-----------+--------+--------------------+\n|  1|        11|        428|     778|GridCoverage2D[\"c...|\n|  2|        11|        429|     778|GridCoverage2D[\"c...|\n|  3|        11|        428|     779|GridCoverage2D[\"c...|\n|  4|        11|        429|     779|GridCoverage2D[\"c...|\n|  5|        11|        427|     777|GridCoverage2D[\"c...|\n+---+----------+-----------+--------+--------------------+\n</code></pre> <p>Known limitations (v1.7.0):</p> <ul> <li>webp rasters are not supported</li> <li>ewkb geometries are not supported</li> <li>filtering based on geometries envelopes are not supported</li> </ul> <p>All points above should be resolved soon; stay tuned!</p>"},{"location":"tutorial/files/geopackage-sedona-spark/#advantages-of-the-geopackage-file-format","title":"Advantages of the GeoPackage file format","text":"<p>The GeoPackage file format has many advantages:</p> <ul> <li>Any engine can support GeoPackage because it\u2019s an open format.</li> <li>It\u2019s mutable, unlike many other formats.</li> <li>It saves CRS information, unlike some other formats.</li> <li>It can store spatial and raster data.</li> <li>It can be read by many engines like GeoPandas, Sedona, and SQLite, of course.</li> </ul> <p>However, the GeoPackage format also has many downsides.</p>"},{"location":"tutorial/files/geopackage-sedona-spark/#disadvantages-of-geopackage","title":"Disadvantages of GeoPackage","text":"<p>The GeoPackage file format has the following disadvantages:</p> <ul> <li>It\u2019s row-oriented, so it can\u2019t take advantage of column pruning like columnar file formats.</li> <li>It does not support multi-engine concurrency transactions.</li> <li>SQLite transactions are supported, but building reliable transactions with other engines would be hard.</li> <li>All engines do not fully support it.</li> </ul>"},{"location":"tutorial/files/geopackage-sedona-spark/#conclusion","title":"Conclusion","text":"<p>GeoPackage is a solid file format if you\u2019re using SQLite.</p> <p>It\u2019s excellent that Sedona can read GeoPackage files created by SQLite analyses. This allows you to read GeoPackage files in parallel and analyze massive datasets. You can also run Sedona on a cluster.</p> <p>If you don\u2019t already use GeoPackage, you should probably use file formats like GeoParquet or Iceberg.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/","title":"GeoParquet","text":""},{"location":"tutorial/files/geoparquet-sedona-spark/#apache-sedona-geoparquet-with-spark","title":"Apache Sedona GeoParquet with Spark","text":"<p>This page explains how to build spatial data lakes with Apache Sedona and GeoParquet.</p> <p>GeoParquet offers many advantages for spatial data sets:</p> <ul> <li>Built-in support for geometry columns</li> <li>GeoParquet's columnar nature allows for column pruning, which speeds up queries that use only a subset of columns.</li> <li>Embedded statistics enable readers to skip reading entire chunks of a file in some queries.</li> <li>Storing bounding-box metadata (\u201cbbox\u201d) for each row group allows for row-group filtering.</li> <li>Columnar file formats are easy to compress.</li> <li>You don\u2019t need to infer or manually specify the schema as you would have to with GeoJSON or CSV because GeoParquet includes it in the footer.</li> </ul> <p>Let's see how to write a Sedona DataFrame to GeoParquet files.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#write-sedona-dataframe-to-geoparquet-with-spark","title":"Write Sedona DataFrame to GeoParquet with Spark","text":"<p>Start by creating a Sedona DataFrame:</p> <pre><code>df = sedona.createDataFrame(\n    [\n        (\"a\", \"LINESTRING(2.0 5.0,6.0 1.0)\"),\n        (\"b\", \"LINESTRING(7.0 4.0,9.0 2.0)\"),\n        (\"c\", \"LINESTRING(1.0 3.0,3.0 1.0)\"),\n    ],\n    [\"id\", \"geometry\"],\n)\ndf = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geometry\")))\n</code></pre> <p>Now write the DataFrame to GeoParquet files:</p> <pre><code>df.write.format(\"geoparquet\").mode(\"overwrite\").save(\"/tmp/somewhere\")\n</code></pre> <p>Here are the files created in storage:</p> <pre><code>somewhere/\n  _SUCCESS\n  part-00000-1c13be9e-6d4c-401e-89d8-739000ad3aba-c000.snappy.parquet\n  part-00003-1c13be9e-6d4c-401e-89d8-739000ad3aba-c000.snappy.parquet\n  part-00007-1c13be9e-6d4c-401e-89d8-739000ad3aba-c000.snappy.parquet\n  part-00011-1c13be9e-6d4c-401e-89d8-739000ad3aba-c000.snappy.parquet\n</code></pre> <p>Sedona writes many Parquet files in parallel because that's much faster than writing a single file.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#read-geoparquet-files-into-sedona-dataframe-with-spark","title":"Read GeoParquet files into Sedona DataFrame with Spark","text":"<p>Let's read the GeoParquet files into a Sedona DataFrame:</p> <pre><code>df = sedona.read.format(\"geoparquet\").load(\"/tmp/somewhere\")\ndf.show(truncate=False)\n</code></pre> <p>Here are the results:</p> <pre><code>+---+---------------------+\n|id |geometry             |\n+---+---------------------+\n|a  |LINESTRING (2 5, 6 1)|\n|b  |LINESTRING (7 4, 9 2)|\n|c  |LINESTRING (1 3, 3 1)|\n+---+---------------------+\n</code></pre> <p>Here's how Sedona executes this query under the hood:</p> <ol> <li>It fetches the schema from the footer of a GeoParquet file, so no schema inference is needed.</li> <li>It collects the bbox metadata for each file and sees if any data can be skipped.</li> <li>It runs the query, skipping any columns that are not required.</li> </ol> <p>Sedona queries on GeoParquet files can often be executed faster and more reliably than other file formats, like CSV.  CSV files don't store the schema in the file footer, don't support column pruning, and don't have row-group metadata for data skipping.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#inspect-geoparquet-metadata","title":"Inspect GeoParquet metadata","text":"<p>Since v<code>1.5.1</code>, Sedona provides a Spark SQL data source <code>\"geoparquet.metadata\"</code> for inspecting GeoParquet metadata. The resulting dataframe contains the \"geo\" metadata for each input file.</p> Scala <pre><code>val df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> Java <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> Python <pre><code>df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- path: string (nullable = true)\n |-- version: string (nullable = true)\n |-- primary_column: string (nullable = true)\n |-- columns: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- encoding: string (nullable = true)\n |    |    |-- geometry_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- bbox: array (nullable = true)\n |    |    |    |-- element: double (containsNull = true)\n |    |    |-- crs: string (nullable = true)\n</code></pre> <p>The GeoParquet footer stores the following data for each column:</p> <ul> <li>geometry_type: type of geometric object like point or polygon</li> <li>bbox: bounding box of objects in file</li> <li>crs: Coordinate Reference System</li> <li>covering: a struct column of xmin, ymin, xmax, ymax that provides the row group statistics for GeoParquet 1.1 files</li> </ul> <p>The rest of the Parquet metadata is stored in the Parquet footer, right where it is stored for other Parquet files too.</p> <p>If the input Parquet file does not have GeoParquet metadata, the values of <code>version</code>, <code>primary_column</code> and <code>columns</code> fields of the resulting dataframe will be <code>null</code>.</p> <p><code>geoparquet.metadata</code> only supports reading GeoParquet specific metadata. Users can use G-Research/spark-extension to read comprehensive metadata of generic Parquet files.</p> <p>Let\u2019s check out the content of the metadata and the schema:</p> <pre><code>df.show(truncate=False)\n\n+-----------------------------+-------------------------------------------------------------------+\n|path                         |columns                                                            |\n+-----------------------------+-------------------------------------------------------------------+\n|file:/part-00003-1c13.parquet|{geometry -&gt; {WKB, [LineString], [2.0, 1.0, 6.0, 5.0], null, NULL}}|\n|file:/part-00007-1c13.parquet|{geometry -&gt; {WKB, [LineString], [7.0, 2.0, 9.0, 4.0], null, NULL}}|\n|file:/part-00011-1c13.parquet|{geometry -&gt; {WKB, [LineString], [1.0, 1.0, 3.0, 3.0], null, NULL}}|\n|file:/part-00000-1c13.parquet|{geometry -&gt; {WKB, [], [0.0, 0.0, 0.0, 0.0], null, NULL}}          |\n+-----------------------------+-------------------------------------------------------------------+\n</code></pre> <p>The <code>columns</code> column contains bounding box information on each file in the GeoParquet data lake.  These bbox values are useful for skipping files or row-groups.  More will come on bbox metadata in the following section.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#write-geoparquet-with-crs-metadata","title":"Write GeoParquet with CRS Metadata","text":"<p>Since v<code>1.5.1</code>, Sedona supports writing GeoParquet files with custom GeoParquet spec version and crs. The default GeoParquet spec version is <code>1.0.0</code> and the default crs is <code>null</code>. You can specify the GeoParquet spec version and crs as follows:</p> <pre><code>val projjson = \"{...}\" // PROJJSON string for all geometry columns\ndf.write.format(\"geoparquet\")\n    .option(\"geoparquet.version\", \"1.0.0\")\n    .option(\"geoparquet.crs\", projjson)\n    .save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>If you have multiple geometry columns written to the GeoParquet file, you can specify the CRS for each column. For example, <code>g0</code> and <code>g1</code> are two geometry columns in the DataFrame <code>df</code>, and you want to specify the CRS for each column as follows:</p> <pre><code>val projjson_g0 = \"{...}\" // PROJJSON string for g0\nval projjson_g1 = \"{...}\" // PROJJSON string for g1\ndf.write.format(\"geoparquet\")\n    .option(\"geoparquet.version\", \"1.0.0\")\n    .option(\"geoparquet.crs.g0\", projjson_g0)\n    .option(\"geoparquet.crs.g1\", projjson_g1)\n    .save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>The value of <code>geoparquet.crs</code> and <code>geoparquet.crs.&lt;column_name&gt;</code> can be one of the following:</p> <ul> <li><code>\"null\"</code>: Explicitly setting <code>crs</code> field to <code>null</code>. This is the default behavior.</li> <li><code>\"\"</code> (empty string): Omit the <code>crs</code> field. This implies that the CRS is OGC:CRS84 for CRS-aware implementations.</li> <li><code>\"{...}\"</code> (PROJJSON string): The <code>crs</code> field will be set as the PROJJSON object representing the Coordinate Reference System (CRS) of the geometry. You can find the PROJJSON string of a specific CRS from here: https://epsg.io/ (click the JSON option at the bottom of the page). You can also customize your PROJJSON string as needed.</li> </ul> <p>Please note that Sedona currently cannot set/get a projjson string to/from a CRS. Its geoparquet reader will ignore the projjson metadata and you will have to set your CRS via <code>ST_SetSRID</code> after reading the file. Its geoparquet writer will not leverage the SRID field of a geometry so you will have to always set the <code>geoparquet.crs</code> option manually when writing the file, if you want to write a meaningful CRS field.</p> <p>Due to the same reason, Sedona geoparquet reader and writer do NOT check the axis order (lon/lat or lat/lon) and assume they are handled by the users themselves when writing / reading the files. You can always use <code>ST_FlipCoordinates</code> to swap the axis order of your geometries.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#save-geoparquet-with-covering-metadata","title":"Save GeoParquet with Covering Metadata","text":"<p>Since <code>v1.6.1</code>, Sedona supports writing the <code>covering</code> field to geometry column metadata. The <code>covering</code> field specifies a bounding box column to help accelerate spatial data retrieval. The bounding box column should be a top-level struct column containing <code>xmin</code>, <code>ymin</code>, <code>xmax</code>, <code>ymax</code> columns. If the DataFrame you are writing contains such columns, you can specify <code>.option(\"geoparquet.covering.&lt;geometryColumnName&gt;\", \"&lt;coveringColumnName&gt;\")</code> option to write <code>covering</code> metadata to GeoParquet files:</p> <pre><code>df.write.format(\"geoparquet\")\n    .option(\"geoparquet.covering.geometry\", \"bbox\")\n    .save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre> <p>If the DataFrame has only one geometry column, you can simply specify the <code>geoparquet.covering</code> option and omit the geometry column name:</p> <pre><code>df.write.format(\"geoparquet\")\n    .option(\"geoparquet.covering\", \"bbox\")\n    .save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre> <p>If the DataFrame does not have a covering column, you can construct one using Sedona's SQL functions:</p> <pre><code>val df_bbox = df.withColumn(\"bbox\", expr(\"struct(ST_XMin(geometry) AS xmin, ST_YMin(geometry) AS ymin, ST_XMax(geometry) AS xmax, ST_YMax(geometry) AS ymax)\"))\ndf_bbox.write.format(\"geoparquet\").option(\"geoparquet.covering.geometry\", \"bbox\").save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre>"},{"location":"tutorial/files/geoparquet-sedona-spark/#sort-then-save-geoparquet","title":"Sort then Save GeoParquet","text":"<p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre> <p>Let's look closer at how Sedona uses the GeoParquet bbox metadata to optimize queries.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#how-sedona-uses-geoparquet-bounding-box-bbox-metadata-with-spark","title":"How Sedona uses GeoParquet bounding box (bbox) metadata with Spark","text":"<p>The bounding box metadata specifies the area covered by geometric shapes in a given file.  Suppose you query points in a region not covered by the bounding box for a given file.  The engine can skip that entire file when executing the query because it\u2019s known that it does not cover any relevant data.</p> <p>Skipping entire files of data can massively improve performance.  The more data that\u2019s skipped, the faster the query will run.</p> <p>Let\u2019s look at an example of a dataset with points and three bounding boxes.</p> <p></p> <p>Now, let\u2019s apply a spatial filter to read points within a particular area:</p> <p></p> <p>Here is the query:</p> <pre><code>my_shape = \"POLYGON((4.0 3.5, 4.0 6.0, 8.0 6.0, 8.0 4.5, 4.0 3.5))\"\n\nres = sedona.sql(\n    f\"\"\"\nselect *\nfrom points\nwhere st_intersects(geometry, ST_GeomFromWKT('{my_shape}'))\n\"\"\"\n)\nres.show(truncate=False)\n</code></pre> <p>Here are the results:</p> <pre><code>+---+-----------+\n|id |geometry   |\n+---+-----------+\n|e  |POINT (5 4)|\n|f  |POINT (6 4)|\n|g  |POINT (6 5)|\n+---+-----------+\n</code></pre> <p>We don\u2019t need the data in bounding boxes 1 or 3 to run this query.  We just need points in bounding box 2.  File skipping lets us read one file instead of three for this query.  Here are the bounding boxes for the files in the GeoParquet data lake:</p> <pre><code>+--------------------------------------------------------------+\n|columns                                                       |\n+--------------------------------------------------------------+\n|{geometry -&gt; {WKB, [Point], [2.0, 1.0, 3.0, 3.0], null, NULL}}|\n|{geometry -&gt; {WKB, [Point], [7.0, 1.0, 8.0, 2.0], null, NULL}}|\n|{geometry -&gt; {WKB, [Point], [5.0, 4.0, 6.0, 5.0], null, NULL}}|\n+--------------------------------------------------------------+\n</code></pre> <p>Skipping entire files can result in massive performance gains.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#advantages-of-geoparquet-files","title":"Advantages of GeoParquet files","text":"<p>As previously mentioned, GeoParquet files have many advantages over row-oriented files like CSV or GeoJSON:</p> <ul> <li>Column pruning</li> <li>Row group filtering</li> <li>Schema in the footer</li> <li>Good compressibility because of the columnar data layout</li> </ul> <p>These are huge advantages but don\u2019t offer the complete feature set that spatial data practitioners need.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#limitations-of-geoparquet-files","title":"Limitations of GeoParquet files","text":"<p>Parquet data lakes and GeoParquet data lakes face many of the same limitations as other data lakes:</p> <ul> <li>They don\u2019t support reliable transactions</li> <li>Some Data Manipulation Language (\u201cDML\u201d) operations (e.g., update, delete) are not supported</li> <li>No concurrency protection</li> <li>Poor performance compared to databases for certain operations</li> </ul> <p>To overcome these limitations, the data community has migrated to open table formats like Iceberg, Delta Lake, and Hudi.</p> <p>Iceberg recently rolled out support for geometry and geography columns.  This allows data practitioners to get the best features of open table formats and the benefits of Parquet files, where the data is stored.  Iceberg overcomes the limitations of GeoParquet data lakes and allows for reliable transactions and easy access to common operations like updates and deletes.</p>"},{"location":"tutorial/files/geoparquet-sedona-spark/#conclusion","title":"Conclusion","text":"<p>GeoParquet is a significant innovation for the spatial data community.  It provides geo engineers with embedded schemas, performance optimizations, and native support for geometry columns.</p> <p>Spatial data engineers can also migrate to Iceberg now that it supports all these positive features of GeoParquet, and even more.  Iceberg provides many useful open table features and is almost always a better option than vanilla GeoParquet (except for single file datasets that will never change or for compatibility with other engines).</p> <p>Switching from legacy file formats like Shapefile, GeoPackage, CSV, GeoJSON to high-performance file formats like GeoParquet/Iceberg should immediately speed up your computational workflows.</p>"},{"location":"tutorial/files/shapefiles-sedona-spark/","title":"Shapefiles","text":""},{"location":"tutorial/files/shapefiles-sedona-spark/#shapefiles-with-apache-sedona-and-spark","title":"Shapefiles with Apache Sedona and Spark","text":"<p>This post explains how to read Shapefiles with Apache Sedona and Spark.</p> <p>A Shapefile is \u201can Esri vector data storage format for storing the location, shape, and attributes of geographic features.\u201d  The Shapefile format is proprietary, but the spec is open.</p> <p>Shapefiles have many limitations but are extensively used, so it\u2019s beneficial that they are readable by Sedona.</p> <p>Let\u2019s look at how to read Shapefiles with Sedona and Spark.</p>"},{"location":"tutorial/files/shapefiles-sedona-spark/#read-shapefiles-with-sedona-and-spark","title":"Read Shapefiles with Sedona and Spark","text":"<p>Let\u2019s start by creating a Shapefile with GeoPandas and Shapely:</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import Point\n\npoint1 = Point(0, 0)\npoint2 = Point(1, 1)\n\ndata = {\"name\": [\"Point A\", \"Point B\"], \"value\": [10, 20], \"geometry\": [point1, point2]}\n\ngdf = gpd.GeoDataFrame(data, geometry=\"geometry\")\ngdf.to_file(\"/tmp/my_geodata.shp\")\n</code></pre> <p>Here are the files that are output:</p> <pre><code>/tmp/\n  my_geodata.cpg\n  my_geodata.dbf\n  my_geodata.shp\n  my_geodata.shx\n</code></pre> <p>Shapefiles are not stored in a single file.  They contain data in many different files.</p> <p>Here\u2019s how to read a Shapefile into a Sedona DataFrame powered by Spark:</p> <pre><code>df = sedona.read.format(\"shapefile\").load(\"/tmp/my_geodata.shp\")\ndf.show()\n</code></pre> <pre><code>+-----------+-------+-----+\n|   geometry|   name|value|\n+-----------+-------+-----+\n|POINT (0 0)|Point A|   10|\n|POINT (1 1)|Point B|   20|\n+-----------+-------+-----+\n</code></pre> <p>You can also see the unique record number for each row in the Shapefile as follows:</p> <pre><code>df = (\n    sedona.read.format(\"shapefile\")\n    .option(\"key.name\", \"FID\")\n    .load(\"/tmp/my_geodata.shp\")\n)\n</code></pre> <pre><code>+-----------+---+-------+-----+\n|   geometry|FID|   name|value|\n+-----------+---+-------+-----+\n|POINT (0 0)|  1|Point A|   10|\n|POINT (1 1)|  2|Point B|   20|\n+-----------+---+-------+-----+\n</code></pre> <p>The name of the geometry column is geometry by default. You can change the name of the geometry column using the <code>geometry.name</code> option. Suppose one of the non-spatial attributes is named \"geometry\", <code>geometry.name</code> must be configured to avoid conflict.</p> <pre><code>df = (\n    sedona.read.format(\"shapefile\")\n    .option(\"geometry.name\", \"geom\")\n    .load(\"/path/to/shapefile\")\n)\n</code></pre> <p>The character encoding of string attributes are inferred from the <code>.cpg</code> file. If you see garbled values in string fields, you can manually specify the correct charset using the <code>charset</code> option. For example:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").option(\"charset\", \"UTF-8\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").option(\"charset\", \"UTF-8\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = (\n    sedona.read.format(\"shapefile\")\n    .option(\"charset\", \"UTF-8\")\n    .load(\"/path/to/shapefile\")\n)\n</code></pre> <p>Let\u2019s see how to load many Shapefiles into a Sedona DataFrame.</p>"},{"location":"tutorial/files/shapefiles-sedona-spark/#load-many-shapefiles-with-sedona","title":"Load many Shapefiles with Sedona","text":"<p>Suppose you have a directory with many Shapefiles as follows:</p> <pre><code>/tmp/shapefiles/\n  file1.cpg\n  file1.dbf\n  file1.shp\n  file1.shx\n  file2.cpg\n  file2.dbf\n  file2.shp\n  file2.shx\n</code></pre> <p>The directory contains two <code>.shp</code> files and other supporting files.</p> <p>Here\u2019s how to load many Shapefiles into a Sedona DataFrame:</p> <pre><code>df = sedona.read.format(\"shapefile\").load(\"/tmp/shapefiles\")\ndf.show()\n</code></pre> <pre><code>+-----------+-------+-----+\n|   geometry|   name|value|\n+-----------+-------+-----+\n|POINT (0 0)|Point A|   10|\n|POINT (1 1)|Point B|   20|\n|POINT (2 2)|Point C|   10|\n|POINT (3 3)|Point D|   20|\n+-----------+-------+-----+\n</code></pre> <p>You can just pass the directory where the Shapefiles are stored, and the Sedona reader will pick them up.</p> <p>The input path can be a directory containing one or multiple Shapefiles or a path to a <code>.shp</code> file.</p> <ul> <li>All shapefiles directly under the directory will be loaded when the input path is a directory. If you want to load all shapefiles in subdirectories, please specify <code>.option(\"recursiveFileLookup\", \"true\")</code>.</li> <li>The shapefile will be loaded when the input path is a .shp file. Sedona will look for sibling files (.dbf, .shx, etc.) with the same main file name and load them automatically.</li> </ul>"},{"location":"tutorial/files/shapefiles-sedona-spark/#advantages-of-shapefiles","title":"Advantages of Shapefiles","text":"<p>Shapefiles are deeply integrated into the Esri ecosystem and extensively used in many services.</p> <p>You can output a Shapefile from Esri and then read it with another engine like Sedona.</p> <p>However, Esri created the Shapefile format in the early 1990s, so it has many limitations.</p>"},{"location":"tutorial/files/shapefiles-sedona-spark/#limitations-of-shapefiles","title":"Limitations of Shapefiles","text":"<p>Here are some of the disadvantages of Shapefiles:</p> <ul> <li>Don\u2019t support complex geometries</li> <li>They don\u2019t support NULL values</li> <li>They round numbers</li> <li>Bad Unicode support</li> <li>Don\u2019t allow for long field names</li> <li>2GB file size limit</li> <li>Spatial indexes are slower compared to alternatives</li> <li>Unable to store datetimes</li> </ul> <p>See this page for more information on the limitations of Shapefiles.</p> <p>Due to these limitations, other options are worth investigating.</p>"},{"location":"tutorial/files/shapefiles-sedona-spark/#shapefile-alternatives","title":"Shapefile alternatives","text":"<p>There are a variety of other file formats that are good for geometric data:</p> <ul> <li>Iceberg</li> <li>GeoParquet</li> <li>FlatGeoBuf</li> <li>GeoPackage</li> <li>GeoJSON</li> <li>CSV</li> <li>GeoTIFF</li> </ul>"},{"location":"tutorial/files/shapefiles-sedona-spark/#why-sedona-does-not-support-shapefile-writes","title":"Why Sedona does not support Shapefile writes","text":"<p>Sedona does not write Shapefiles for two main reasons:</p> <ol> <li>Each Shapefile is a collection of files, which is hard for distributed systems to write.</li> <li>A Shapefile has a hard 2 GB size limit, which isn\u2019t large enough for some spatial data.</li> </ol>"},{"location":"tutorial/files/shapefiles-sedona-spark/#conclusion","title":"Conclusion","text":"<p>Shapefiles are a legacy file format still used in many production applications. However, they have many limitations and aren\u2019t the best option in a modern data pipeline unless you need compatibility with legacy systems.</p>"},{"location":"tutorial/files/stac-sedona-spark/","title":"STAC catalog","text":""},{"location":"tutorial/files/stac-sedona-spark/#stac-catalog-with-apache-sedona-and-spark","title":"STAC catalog with Apache Sedona and Spark","text":"<p>The STAC data source allows you to read data from a SpatioTemporal Asset Catalog (STAC) API. The data source supports reading STAC items and collections.</p>"},{"location":"tutorial/files/stac-sedona-spark/#usage","title":"Usage","text":"<p>To use the STAC data source, you can load a STAC catalog into a Sedona DataFrame using the stac format. The path can be either a local STAC collection JSON file or an HTTP/HTTPS endpoint to retrieve the collection JSON file.</p> <p>You can load a STAC collection from a local collection file:</p> <pre><code>df = sedona.read.format(\"stac\").load(\"/user/stac_collection.json\")\ndf.printSchema()\ndf.show()\n</code></pre> <p>You can load a STAC collection from a s3 collection file object:</p> <pre><code>df = sedona.read.format(\"stac\").load(\n    \"s3a://example.com/stac_bucket/stac_collection.json\"\n)\ndf.printSchema()\ndf.show()\n</code></pre> <p>You can also load a STAC collection from an HTTP/HTTPS endpoint:</p> <pre><code>df = sedona.read.format(\"stac\").load(\n    \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\"\n)\ndf.printSchema()\ndf.show()\n</code></pre> <p>output:</p> <pre><code>root\n |-- stac_version: string (nullable = false)\n |-- stac_extensions: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- type: string (nullable = false)\n |-- id: string (nullable = false)\n |-- bbox: array (nullable = true)\n |    |-- element: double (containsNull = true)\n |-- geometry: geometry (nullable = true)\n |-- title: string (nullable = true)\n |-- description: string (nullable = true)\n |-- datetime: timestamp (nullable = true)\n |-- start_datetime: timestamp (nullable = true)\n |-- end_datetime: timestamp (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- updated: timestamp (nullable = true)\n |-- platform: string (nullable = true)\n |-- instruments: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- constellation: string (nullable = true)\n |-- mission: string (nullable = true)\n |-- grid:code: string (nullable = true)\n |-- gsd: double (nullable = true)\n |-- collection: string (nullable = true)\n |-- links: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- rel: string (nullable = true)\n |    |    |-- href: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |-- assets: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- href: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- roles: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n\n+------------+--------------------+-------+--------------------+--------------------+--------------------+-----+-----------+--------------------+--------------+------------+--------------------+--------------------+-----------+-----------+-------------+-------+----+--------------------+--------------------+--------------------+\n|stac_version|     stac_extensions|   type|                  id|                bbox|            geometry|title|description|            datetime|start_datetime|end_datetime|             created|             updated|   platform|instruments|constellation|mission| gsd|          collection|               links|              assets|\n+------------+--------------------+-------+--------------------+--------------------+--------------------+-----+-----------+--------------------+--------------+------------+--------------------+--------------------+-----------+-----------+-------------+-------+----+--------------------+--------------------+--------------------+\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NYC_202212...|[-55.202493, 1.71...|POLYGON ((-55.201...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-01 21:13:...|2024-05-01 21:13:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NZC_202212...|[-54.30394, 1.719...|POLYGON ((-54.302...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:39:...|2024-05-03 00:39:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T22NBH_202212...|[-53.698196, 2.63...|POLYGON ((-53.698...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:26:...|2024-05-03 00:26:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NYD_202212...|[-55.201423, 2.62...|POLYGON ((-55.199...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-01 21:10:...|2024-05-01 21:10:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NZD_202212...|[-54.302336, 2.62...|POLYGON ((-54.299...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:12:...|2024-05-03 00:12:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T22NBJ_202212...|[-53.700535, 2.63...|POLYGON ((-53.700...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:30:...|2024-05-03 00:30:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NYE_202212...|[-55.199906, 3.52...|POLYGON ((-55.197...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-01 21:24:...|2024-05-01 21:24:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NZE_202212...|[-54.300062, 3.52...|POLYGON ((-54.296...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:14:...|2024-05-03 00:14:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T22NBK_202212...|[-53.703548, 3.52...|POLYGON ((-53.703...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-03 00:32:...|2024-05-03 00:32:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n|       1.0.0|[https://stac-ext...|Feature|S2B_T21NYF_202212...|[-55.197941, 4.42...|POLYGON ((-55.195...| NULL|       NULL|2022-12-05 14:11:...|          NULL|        NULL|2024-05-01 21:43:...|2024-05-01 21:43:...|sentinel-2b|      [msi]|   sentinel-2|   NULL|NULL|sentinel-2-pre-c1...|[{self, https://e...|{red -&gt; {https://...|\n+------------+--------------------+-------+--------------------+--------------------+--------------------+-----+-----------+--------------------+--------------+------------+--------------------+--------------------+-----------+-----------+-------------+-------+----+--------------------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#filter-pushdown","title":"Filter Pushdown","text":"<p>The STAC data source supports predicate pushdown for spatial and temporal filters. The data source can push down spatial and temporal filters to the underlying data source to reduce the amount of data that needs to be read.</p>"},{"location":"tutorial/files/stac-sedona-spark/#spatial-filter-pushdown","title":"Spatial Filter Pushdown","text":"<p>Spatial filter pushdown allows the data source to apply spatial predicates (e.g., st_intersects) directly at the data source level, reducing the amount of data transferred and processed.</p>"},{"location":"tutorial/files/stac-sedona-spark/#temporal-filter-pushdown","title":"Temporal Filter Pushdown","text":"<p>Temporal filter pushdown allows the data source to apply temporal predicates (e.g., BETWEEN, &gt;=, &lt;=) directly at the data source level, similarly reducing the amount of data transferred and processed.</p>"},{"location":"tutorial/files/stac-sedona-spark/#examples","title":"Examples","text":"<p>Here are some examples demonstrating how to query a STAC data source that is loaded into a table named <code>STAC_TABLE</code>.</p>"},{"location":"tutorial/files/stac-sedona-spark/#sql-select-without-filters","title":"SQL Select Without Filters","text":"<pre><code>SELECT id, datetime as dt, geometry, bbox FROM STAC_TABLE\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#sql-select-with-temporal-filter","title":"SQL Select With Temporal Filter","text":"<pre><code>  SELECT id, datetime as dt, geometry, bbox\n  FROM STAC_TABLE\n  WHERE datetime BETWEEN '2020-01-01' AND '2020-12-13'\n</code></pre> <p>In this example, the data source will push down the temporal filter to the underlying data source.</p>"},{"location":"tutorial/files/stac-sedona-spark/#sql-select-with-spatial-filter","title":"SQL Select With Spatial Filter","text":"<pre><code>  SELECT id, geometry\n  FROM STAC_TABLE\n  WHERE st_intersects(ST_GeomFromText('POLYGON((17 10, 18 10, 18 11, 17 11, 17 10))'), geometry)\n</code></pre> <p>In this example, the data source will push down the spatial filter to the underlying data source.</p>"},{"location":"tutorial/files/stac-sedona-spark/#sedona-configuration-for-stac-reader","title":"Sedona Configuration for STAC Reader","text":"<p>When using the STAC reader in Sedona, several configuration options can be set to control the behavior of the reader. These configurations are typically set in a <code>Map[String, String]</code> and passed to the reader. Below are the key sedona configuration options:</p> <ul> <li> <p>spark.sedona.stac.load.maxPartitionItemFiles: This option specifies the maximum number of item files that can be included in a single partition. It helps in controlling the size of partitions. The default value is set to -1, meaning the system will automatically determine the number of item files per partition.</p> </li> <li> <p>spark.sedona.stac.load.numPartitions: This option sets the number of partitions to be created for the STAC data. It allows for better control over data distribution and parallel processing. The default value is set to -1, meaning the system will automatically determine the number of item files per partition.</p> </li> </ul> <p>Below are reader options that can be set to control the behavior of the STAC reader:</p> <ul> <li> <p>itemsLimitMax: This option specifies the maximum number of items to be loaded from the STAC collection. It helps in limiting the amount of data processed. The default value is set to -1, meaning all items will be loaded.</p> </li> <li> <p>itemsLoadProcessReportThreshold: This option specifies the threshold for reporting the progress of item loading. It helps in monitoring the progress of the loading process. The default value is set to 1000000, meaning the progress will be reported every 1,000,000 items loaded.</p> </li> <li> <p>itemsLimitPerRequest: This option specifies the maximum number of items to be requested in a single API call. It helps in controlling the size of each request. The default value is set to 10.</p> </li> <li> <p>headers: This option specifies HTTP headers to include in STAC API requests. It should be a JSON-encoded string containing a dictionary of header key-value pairs. This is useful for authentication and custom headers. Example: <code>{\"Authorization\": \"Basic &lt;base64_credentials&gt;\"}</code></p> </li> </ul> <p>These configurations can be combined into a single <code>Map[String, String]</code> and passed to the STAC reader as shown below:</p> <pre><code>  def defaultSparkConfig: Map[String, String] = Map(\n    \"spark.sedona.stac.load.maxPartitionItemFiles\" -&gt; \"100\",\n    \"spark.sedona.stac.load.numPartitions\" -&gt; \"10\",\n    \"spark.sedona.stac.load.itemsLimitMax\" -&gt; \"20\")\n\n  val sparkSession: SparkSession = {\n    val builder = SedonaContext\n            .builder()\n            .master(\"local[*]\")\n    defaultSparkConfig.foreach { case (key, value) =&gt; builder.config(key, value) }\n    builder.getOrCreate()\n  }\n\n df = sedona.read\n      .format(\"stac\")\n      .option(\"itemsLimitMax\", \"100\")\n      .option(\"itemsLoadProcessReportThreshold\", \"2000000\")\n      .option(\"itemsLimitPerRequest\", \"100\")\n      .load(\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\")\n</code></pre> <p>These options above provide fine-grained control over how the STAC data is read and processed in Sedona.</p>"},{"location":"tutorial/files/stac-sedona-spark/#python-api","title":"Python API","text":"<p>The Python API allows you to interact with a SpatioTemporal Asset Catalog (STAC) API using the Client class. This class provides methods to open a connection to a STAC API, retrieve collections, and search for items with various filters.</p>"},{"location":"tutorial/files/stac-sedona-spark/#sample-code","title":"Sample Code","text":""},{"location":"tutorial/files/stac-sedona-spark/#initialize-the-client","title":"Initialize the Client","text":"<pre><code>from sedona.spark.stac import Client\n\n# Initialize the client\nclient = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#search-items-on-a-collection-within-a-year","title":"Search Items on a Collection Within a Year","text":"<pre><code>items = client.search(\n    collection_id=\"aster-l1t\", datetime=\"2020\", return_dataframe=False\n)\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#search-items-on-a-collection-within-a-month-and-max-items","title":"Search Items on a Collection Within a Month and Max Items","text":"<pre><code>items = client.search(\n    collection_id=\"aster-l1t\", datetime=\"2020-05\", return_dataframe=False, max_items=5\n)\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#search-items-with-bounding-box-and-interval","title":"Search Items with Bounding Box and Interval","text":"<pre><code>items = client.search(\n    collection_id=\"aster-l1t\",\n    ids=[\"AST_L1T_00312272006020322_20150518201805\"],\n    bbox=[-180.0, -90.0, 180.0, 90.0],\n    datetime=[\"2006-01-01T00:00:00Z\", \"2007-01-01T00:00:00Z\"],\n    return_dataframe=False,\n)\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#search-multiple-items-with-multiple-bounding-boxes","title":"Search Multiple Items with Multiple Bounding Boxes","text":"<pre><code>bbox_list = [[-180.0, -90.0, 180.0, 90.0], [-100.0, -50.0, 100.0, 50.0]]\nitems = client.search(collection_id=\"aster-l1t\", bbox=bbox_list, return_dataframe=False)\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#search-items-and-get-dataframe-as-return-with-multiple-intervals","title":"Search Items and Get DataFrame as Return with Multiple Intervals","text":"<pre><code>interval_list = [\n    [\"2020-01-01T00:00:00Z\", \"2020-06-01T00:00:00Z\"],\n    [\"2020-07-01T00:00:00Z\", \"2021-01-01T00:00:00Z\"],\n]\ndf = client.search(\n    collection_id=\"aster-l1t\", datetime=interval_list, return_dataframe=True\n)\ndf.show()\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#save-items-in-dataframe-to-geoparquet-with-both-bounding-boxes-and-intervals","title":"Save Items in DataFrame to GeoParquet with Both Bounding Boxes and Intervals","text":"<pre><code># Save items in DataFrame to GeoParquet with both bounding boxes and intervals\nclient.get_collection(\"aster-l1t\").save_to_geoparquet(\n    output_path=\"/path/to/output\", bbox=bbox_list, datetime=\"2020-05\"\n)\n</code></pre> <p>These examples demonstrate how to use the Client class to search for items in a STAC collection with various filters and return the results as either an iterator of PyStacItem objects or a Spark DataFrame.</p>"},{"location":"tutorial/files/stac-sedona-spark/#authentication","title":"Authentication","text":"<p>Many STAC services require authentication to access their data. The STAC client supports multiple authentication methods including HTTP Basic Authentication, Bearer Token Authentication, and custom headers.</p>"},{"location":"tutorial/files/stac-sedona-spark/#basic-authentication","title":"Basic Authentication","text":"<p>Basic authentication is commonly used with API keys or username/password combinations. Many services (like Planet Labs) use API keys as the username with an empty password.</p> <pre><code>from sedona.spark.stac import Client\n\n# Example 1: Using an API key (common pattern)\nclient = Client.open(\"https://api.example.com/stac/v1\")\nclient.with_basic_auth(\"your_api_key_here\", \"\")\n\n# Search for items with authentication\ndf = client.search(collection_id=\"example-collection\", max_items=10)\ndf.show()\n\n# Example 2: Using username and password\nclient = Client.open(\"https://api.example.com/stac/v1\")\nclient.with_basic_auth(\"username\", \"password\")\n\ndf = client.search(collection_id=\"example-collection\", max_items=10)\ndf.show()\n\n# Example 3: Method chaining\ndf = (\n    Client.open(\"https://api.example.com/stac/v1\")\n    .with_basic_auth(\"your_api_key\", \"\")\n    .search(collection_id=\"example-collection\", max_items=10)\n)\ndf.show()\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<p>Bearer token authentication is used with OAuth2 tokens and JWT tokens. Note that some services may only support specific authentication methods.</p> <pre><code>from sedona.spark.stac import Client\n\n# Using a bearer token\nclient = Client.open(\"https://api.example.com/stac/v1\")\nclient.with_bearer_token(\"your_access_token_here\")\n\ndf = client.search(collection_id=\"example-collection\", max_items=10)\ndf.show()\n\n# Method chaining\ndf = (\n    Client.open(\"https://api.example.com/stac/v1\")\n    .with_bearer_token(\"your_token\")\n    .search(collection_id=\"example-collection\", max_items=10)\n)\ndf.show()\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#custom-headers","title":"Custom Headers","text":"<p>You can also pass custom headers directly when creating the client, which is useful for services with non-standard authentication requirements.</p> <pre><code>from sedona.spark.stac import Client\n\n# Using custom headers\nheaders = {\"Authorization\": \"Bearer your_token_here\", \"X-Custom-Header\": \"custom_value\"}\nclient = Client.open(\"https://api.example.com/stac/v1\", headers=headers)\n\ndf = client.search(collection_id=\"example-collection\", max_items=10)\ndf.show()\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#authentication-with-scala-datasource","title":"Authentication with Scala DataSource","text":"<p>When using the STAC data source directly in Scala or through Spark SQL, you can pass authentication headers as a JSON-encoded option:</p> <pre><code>import json\nfrom pyspark.sql import SparkSession\n\n# Prepare authentication headers\nheaders = {\"Authorization\": \"Basic &lt;base64_encoded_credentials&gt;\"}\nheaders_json = json.dumps(headers)\n\n# Load STAC data with authentication\ndf = (\n    spark.read.format(\"stac\")\n    .option(\"headers\", headers_json)\n    .load(\"https://api.example.com/stac/v1/collections/example-collection\")\n)\n\ndf.show()\n</code></pre> <pre><code>// Scala example\nval headersJson = \"\"\"{\"Authorization\":\"Basic &lt;base64_encoded_credentials&gt;\"}\"\"\"\n\nval df = sparkSession.read\n  .format(\"stac\")\n  .option(\"headers\", headersJson)\n  .load(\"https://api.example.com/stac/v1/collections/example-collection\")\n\ndf.show()\n</code></pre>"},{"location":"tutorial/files/stac-sedona-spark/#important-notes","title":"Important Notes","text":"<ul> <li>Authentication methods are mutually exclusive: Setting a new authentication method will overwrite any previously set Authorization header, but other custom headers remain unchanged.</li> <li>Headers are propagated: Headers set on the Client are automatically passed to all collection and item requests.</li> <li>Service-specific requirements: Different STAC services may require different authentication methods. For example, Planet Labs requires Basic Authentication rather than Bearer tokens for collection access.</li> <li>Backward compatibility: All authentication parameters are optional. Existing code that accesses public STAC services without authentication will continue to work unchanged.</li> </ul>"},{"location":"tutorial/files/stac-sedona-spark/#methods","title":"Methods","text":"<p><code>open(url: str, headers: Optional[dict] = None) -&gt; Client</code> Opens a connection to the specified STAC API URL.</p> <p>Parameters:</p> <ul> <li><code>url</code> (str): The URL of the STAC API to connect to. Example: <code>\"https://planetarycomputer.microsoft.com/api/stac/v1\"</code></li> <li><code>headers</code> (Optional[dict]): Optional dictionary of HTTP headers for authentication or custom headers. Example: <code>{\"Authorization\": \"Bearer token123\"}</code></li> </ul> <p>Returns:</p> <ul> <li><code>Client</code>: An instance of the <code>Client</code> class connected to the specified URL.</li> </ul> <p><code>with_basic_auth(username: str, password: str) -&gt; Client</code> Adds HTTP Basic Authentication to the client.</p> <p>This method encodes the username and password using Base64 and adds the appropriate Authorization header for HTTP Basic Authentication.</p> <p>Parameters:</p> <ul> <li><code>username</code> (str): The username for authentication. For API keys, this is typically the API key itself. Example: <code>\"your_api_key\"</code></li> <li><code>password</code> (str): The password for authentication. For API keys, this is often left empty. Example: <code>\"\"</code></li> </ul> <p>Returns:</p> <ul> <li><code>Client</code>: Returns self for method chaining.</li> </ul> <p><code>with_bearer_token(token: str) -&gt; Client</code> Adds Bearer Token Authentication to the client.</p> <p>This method adds the appropriate Authorization header for Bearer Token authentication, commonly used with OAuth2 and API tokens.</p> <p>Parameters:</p> <ul> <li><code>token</code> (str): The bearer token for authentication. Example: <code>\"your_access_token_here\"</code></li> </ul> <p>Returns:</p> <ul> <li><code>Client</code>: Returns self for method chaining.</li> </ul> <p><code>get_collection(collection_id: str) -&gt; CollectionClient</code> Retrieves a collection client for the specified collection ID.</p> <p>Parameters:</p> <ul> <li><code>collection_id</code> (str): The ID of the collection to retrieve. Example: <code>\"aster-l1t\"</code></li> </ul> <p>Returns:</p> <ul> <li><code>CollectionClient</code>: An instance of the <code>CollectionClient</code> class for the specified collection.</li> </ul> <p><code>search(*ids: Union[str, list], collection_id: str, bbox: Optional[list] = None, datetime: Optional[Union[str, datetime.datetime, list]] = None, max_items: Optional[int] = None, return_dataframe: bool = True) -&gt; Union[Iterator[PyStacItem], DataFrame]</code> Searches for items in the specified collection with optional filters.</p> <p>Parameters:</p> <ul> <li><code>ids</code> (Union[str, list]): A variable number of item IDs to filter the items. Example: <code>\"item_id1\"</code> or <code>[\"item_id1\", \"item_id2\"]</code></li> <li><code>collection_id</code> (str): The ID of the collection to search in. Example: <code>\"aster-l1t\"</code></li> <li><code>bbox</code> (Optional[list]): A list of bounding boxes for filtering the items, represented as <code>[min_lon, min_lat, max_lon, max_lat]</code>. Example: <code>[[ -180.0, -90.0, 180.0, 90.0 ]]</code></li> <li><code>datetime</code> (Optional[Union[str, datetime.datetime, list]]): A single datetime, RFC 3339-compliant timestamp, or a list of date-time ranges. Example: <code>\"2020-01-01T00:00:00Z\"</code>, <code>datetime.datetime(2020, 1, 1)</code>, <code>[[\"2020-01-01T00:00:00Z\", \"2021-01-01T00:00:00Z\"]]</code></li> <li><code>max_items</code> (Optional[int]): The maximum number of items to return. Example: <code>100</code></li> <li><code>return_dataframe</code> (bool): If <code>True</code> (default), return the result as a Spark DataFrame instead of an iterator of <code>PyStacItem</code> objects. Example: <code>True</code></li> </ul> <p>Returns:</p> <ul> <li>Union[Iterator[PyStacItem], DataFrame]: An iterator of <code>PyStacItem</code> objects or a Spark DataFrame that matches the specified filters.</li> </ul>"},{"location":"tutorial/files/stac-sedona-spark/#references","title":"References","text":"<ul> <li> <p>STAC Specification: https://stacspec.org/</p> </li> <li> <p>STAC Browser: https://github.com/radiantearth/stac-browser</p> </li> <li> <p>STAC YouTube Video: https://www.youtube.com/watch?v=stac-video</p> </li> </ul>"},{"location":"tutorial/flink/pyflink-sql/","title":"Spatial SQL app (PyFlink)","text":"<p>To set up the PyFlink with Apache Sedona, please follow the guide. PyFlink When you finish it, you can run the following code to test if everything works.</p> <pre><code>from sedona.flink import SedonaContext\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import EnvironmentSettings, StreamTableEnvironment\n\nstream_env = StreamExecutionEnvironment.get_execution_environment()\nflink_settings = EnvironmentSettings.in_streaming_mode()\ntable_env = SedonaContext.create(stream_env, flink_settings)\n\ntable_env.sql_query(\"SELECT ST_Point(1.0, 2.0)\").execute()\n</code></pre> <p>PyFlink does not expose the possibility of transforming Scala's own user-defined types (UDT) to Python UDT. So, when you want to collect the result in Python, you need to use functions like <code>ST_AsText</code> or <code>ST_ASBinary</code> to convert the result to a string or binary.</p> <pre><code>from shapely.wkb import loads\n\ntable_env.sql_query(\"SELECT ST_ASBinary(ST_Point(1.0, 2.0))\").execute().collect()\n\n[loads(bytes(el[0])) for el in result]\n</code></pre> <pre><code>[&lt;POINT (1 2)&gt;]\n</code></pre> <p>Similar with User Defined Scalar functions</p> <pre><code>from pyflink.table.udf import ScalarFunction, udf\nfrom shapely.wkb import loads\n\n\nclass Buffer(ScalarFunction):\n    def eval(self, s):\n        geom = loads(s)\n        return geom.buffer(1).wkb\n\n\ntable_env.create_temporary_function(\n    \"ST_BufferPython\", udf(Buffer(), result_type=\"Binary\")\n)\n\nbuffer_table = table_env.sql_query(\n    \"SELECT ST_BufferPython(ST_ASBinary(ST_Point(1.0, 2.0))) AS buffer\"\n)\n</code></pre> <p>For more SQL examples please follow the FlinkSQL section FlinkSQL.</p>"},{"location":"tutorial/flink/sql/","title":"Spatial SQL app (Flink)","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL. The example code is written in Java but also works for Scala.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> <pre><code>Table myTable = tableEnv.sqlQuery(\"YOUR_SQL\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/flink/sql/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Flink dependencies in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol>"},{"location":"tutorial/flink/sql/#initiate-stream-environment","title":"Initiate Stream Environment","text":"<p>Use the following code to initiate your <code>StreamExecutionEnvironment</code> at the beginning:</p> <pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);\n</code></pre>"},{"location":"tutorial/flink/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after your <code>StreamExecutionEnvironment</code> and <code>StreamTableEnvironment</code> declaration</p> <p>Sedona &gt;= 1.4.1</p> <pre><code>StreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaFlinkRegistrator.registerType(env);\nSedonaFlinkRegistrator.registerFunc(tableEnv);\n</code></pre> <p>Warning</p> <p>Sedona has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.</p> <p>This function will register Sedona User Defined Type and User Defined Function</p>"},{"location":"tutorial/flink/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <p>Assume you have a Flink Table <code>tbl</code> like this:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>You can create a Table with a Geometry type column as follows:</p> <pre><code>sedona.createTemporaryView(\"myTable\", tbl)\nTable geomTbl = sedona.sqlQuery(\"SELECT ST_GeomFromWKT(geom_polygon) as geom_polygon, name_polygon FROM myTable\")\ngeomTbl.execute().print()\n</code></pre> <p>The output will be:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>Although it looks same with the input, actually the type of column geom_polygon has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>geomTbl.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>(\n  `geom_polygon` RAW('org.locationtech.jts.geom.Geometry', '...'),\n  `name_polygon` STRING\n)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/flink/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>Table geomTbl3857 = sedona.sqlQuery(\"SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS geom_polygon, name_polygon FROM myTable\")\ngeomTbl3857.execute().print()\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>Note</p> <p>Read SedonaSQL ST_Transform API to learn different spatial query predicates.</p> <p>For example, a Table that has coordinates in the US will become like this.</p> <p>Before the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                     geom_point |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I |                POINT (32 -118) |                          point |\n| +I |                POINT (33 -117) |                          point |\n| +I |                POINT (34 -116) |                          point |\n| +I |                POINT (35 -115) |                          point |\n| +I |                POINT (36 -114) |                          point |\n| +I |                POINT (37 -113) |                          point |\n| +I |                POINT (38 -112) |                          point |\n| +I |                POINT (39 -111) |                          point |\n| +I |                POINT (40 -110) |                          point |\n| +I |                POINT (41 -109) |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                            _c0 |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I | POINT (-13135699.91360628 3... |                          point |\n| +I | POINT (-13024380.422813008 ... |                          point |\n| +I | POINT (-12913060.932019735 ... |                          point |\n| +I | POINT (-12801741.44122646 4... |                          point |\n| +I | POINT (-12690421.950433187 ... |                          point |\n| +I | POINT (-12579102.459639912 ... |                          point |\n| +I | POINT (-12467782.96884664 4... |                          point |\n| +I | POINT (-12356463.478053367 ... |                          point |\n| +I | POINT (-12245143.987260092 ... |                          point |\n| +I | POINT (-12133824.496466817 ... |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/flink/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects and so on to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>geomTable = sedona.sqlQuery(\n  \"\n    SELECT *\n    FROM spatialdf\n    WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n  \")\ngeomTable.execute().print()\n</code></pre> <p>Note</p> <p>Read SedonaSQL Predicate API to learn different spatial query predicates.</p>"},{"location":"tutorial/flink/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>geomTable = sedona.sqlQuery(\n  \"\n    SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n    FROM geomTable\n    ORDER BY distance DESC\n    LIMIT 5\n  \")\ngeomTable.execute().print()\n</code></pre>"},{"location":"tutorial/flink/sql/#join-query","title":"Join query","text":"<p>This equi-join leverages Flink's internal equi-join algorithm. You can opt to skip the Sedona refinement step  by sacrificing query accuracy. A running example is in SQL example project.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/flink/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM rights\n</code></pre>"},{"location":"tutorial/flink/sql/#2-explode-id-array","title":"2. Explode id array","text":"<p>The produced S2 ids are arrays of integers. We need to explode these Ids to multiple rows, so later we can join two tables by ids.</p> <pre><code>SELECT id, geom, name, cellId\nFROM lefts CROSS JOIN UNNEST(lefts.idarray) AS tmpTbl1(cellId)\n</code></pre> <pre><code>SELECT id, geom, name, cellId\nFROM rights CROSS JOIN UNNEST(rights.idarray) AS tmpTbl2(cellId)\n</code></pre>"},{"location":"tutorial/flink/sql/#3-perform-equi-join","title":"3. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/flink/sql/#4-optional-refine-the-result","title":"4. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query as the query in Step 3.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/flink/sql/#5-optional-de-duplicate","title":"5. Optional: De-duplicate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, FIRST_VALUE(lcs_geom), FIRST_VALUE(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>FIRST_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem, and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/flink/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>For example. run this query first on the left table before Step 1.</p> <pre><code>SELECT id, ST_Buffer(geom, DISTANCE), name\nFROM lefts\n</code></pre> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You will have to estimate the corresponding degrees based on your meter values. Please use this calculator.</p>"},{"location":"tutorial/flink/sql/#convert-spatial-table-to-spatial-datastream","title":"Convert Spatial Table to Spatial DataStream","text":""},{"location":"tutorial/flink/sql/#get-datastream","title":"Get DataStream","text":"<p>Use TableEnv's toDataStream function</p> <pre><code>DataStream&lt;Row&gt; geomStream = sedona.toDataStream(geomTable)\n</code></pre>"},{"location":"tutorial/flink/sql/#retrieve-geometries","title":"Retrieve Geometries","text":"<p>Then get the Geometry from each Row object using Map</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n            @Override\n            public Geometry map(Row value) throws Exception {\n                return (Geometry) value.getField(0);\n            }\n        });\ngeometries.print();\n</code></pre> <p>The output will be</p> <pre><code>14&gt; POLYGON ((1.5 1.5, 1.5 2.5, 2.5 2.5, 2.5 1.5, 1.5 1.5))\n2&gt; POLYGON ((5.5 5.5, 5.5 6.5, 6.5 6.5, 6.5 5.5, 5.5 5.5))\n5&gt; POLYGON ((8.5 8.5, 8.5 9.5, 9.5 9.5, 9.5 8.5, 8.5 8.5))\n16&gt; POLYGON ((3.5 3.5, 3.5 4.5, 4.5 4.5, 4.5 3.5, 3.5 3.5))\n12&gt; POLYGON ((-0.5 -0.5, -0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5 -0.5))\n13&gt; POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))\n15&gt; POLYGON ((2.5 2.5, 2.5 3.5, 3.5 3.5, 3.5 2.5, 2.5 2.5))\n3&gt; POLYGON ((6.5 6.5, 6.5 7.5, 7.5 7.5, 7.5 6.5, 6.5 6.5))\n1&gt; POLYGON ((4.5 4.5, 4.5 5.5, 5.5 5.5, 5.5 4.5, 4.5 4.5))\n4&gt; POLYGON ((7.5 7.5, 7.5 8.5, 8.5 8.5, 8.5 7.5, 7.5 7.5))\n</code></pre>"},{"location":"tutorial/flink/sql/#store-non-spatial-attributes-in-geometries","title":"Store non-spatial attributes in Geometries","text":"<p>You can concatenate other non-spatial attributes and store them in Geometry's <code>userData</code> field, so you can recover them later on. <code>userData</code> field can be any object type.</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n            @Override\n            public Geometry map(Row value) throws Exception {\n                Geometry geom = (Geometry) value.getField(0);\n                geom.setUserData(value.getField(1));\n                return geom;\n            }\n        });\ngeometries.print();\n</code></pre> <p>The <code>print</code> command will not print out <code>userData</code> field. But you can get it this way:</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\ngeometries.map(new MapFunction&lt;Geometry, String&gt;() {\n            @Override\n            public String map(Geometry value) throws Exception\n            {\n                return (String) value.getUserData();\n            }\n        }).print();\n</code></pre> <p>The output will be</p> <pre><code>13&gt; polygon9\n6&gt; polygon2\n10&gt; polygon6\n11&gt; polygon7\n5&gt; polygon1\n12&gt; polygon8\n8&gt; polygon4\n4&gt; polygon0\n7&gt; polygon3\n9&gt; polygon5\n</code></pre>"},{"location":"tutorial/flink/sql/#convert-spatial-datastream-to-spatial-table","title":"Convert Spatial DataStream to Spatial Table","text":""},{"location":"tutorial/flink/sql/#create-geometries-using-sedona-formatutils","title":"Create Geometries using Sedona FormatUtils","text":"<ul> <li>Create a Geometry from a WKT string</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                FormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\n                return formatUtils.readGeometry(value);\n            }\n        })\n</code></pre> <ul> <li>Create a Point from a String <code>1.1, 2.2</code>. Use <code>,</code> as the delimiter.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                FormatUtils&lt;Geometry&gt; formatUtils = new FormatUtils(\",\", false, GeometryType.POINT);\n                return formatUtils.readGeometry(value);\n            }\n        })\n</code></pre> <ul> <li>Create a Polygon from a String <code>1.1, 1.1, 10.1, 10.1</code>. This is a rectangle with (1.1, 1.1) and (10.1, 10.1) as their min/max corners.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.GeometryFactory;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                  // Write some code to get four double type values: minX, minY, maxX, maxY\n                  ...\n                Coordinate[] coordinates = new Coordinate[5];\n                coordinates[0] = new Coordinate(minX, minY);\n                coordinates[1] = new Coordinate(minX, maxY);\n                coordinates[2] = new Coordinate(maxX, maxY);\n                coordinates[3] = new Coordinate(maxX, minY);\n                coordinates[4] = coordinates[0];\n                GeometryFactory geometryFactory = new GeometryFactory();\n                return geometryFactory.createPolygon(coordinates);\n            }\n        })\n</code></pre>"},{"location":"tutorial/flink/sql/#create-row-objects","title":"Create Row objects","text":"<p>Put a geometry in a Flink Row to a <code>geomStream</code>. Note that you can put other attributes in Row as well. This example uses a constant value <code>myName</code> for all geometries.</p> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\nimport org.apache.flink.types.Row;\n\nDataStream&lt;Row&gt; geomStream = text.map(new MapFunction&lt;String, Row&gt;() {\n            @Override\n            public Row map(String value) throws Exception\n            {\n                FormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\n                return Row.of(formatUtils.readGeometry(value), \"myName\");\n            }\n        })\n</code></pre>"},{"location":"tutorial/flink/sql/#get-spatial-table","title":"Get Spatial Table","text":"<p>Use TableEnv's fromDataStream function, with two column names <code>geom</code> and <code>geom_name</code>.</p> <pre><code>Table geomTable = sedona.fromDataStream(geomStream, \"geom\", \"geom_name\")\n</code></pre>"},{"location":"tutorial/snowflake/sql/","title":"Spatial SQL app (Snowflake)","text":"<p>After the installation done, you can start using Sedona functions. Please log in to Snowflake again using the user that has the privilege to access the database.</p> <p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"tutorial/snowflake/sql/#accessing-sedona-functions-in-snowflake","title":"Accessing Sedona functions in snowflake","text":"<p>First make sure to point to the correct functions. By default, the Sedona functions should be accessible in db.schema <code>SEDONASNOW.SEDONA</code>.</p> <pre><code>USE DATABASE SEDONASNOW;\n</code></pre> <p>Alternatively, use another database and specify the fully qualified name e.g. <code>SEDONASNOW.SEDONA.ST_GeomFromText</code></p>"},{"location":"tutorial/snowflake/sql/#create-a-sample-table","title":"Create a sample table","text":"<p>Let's create a <code>city_tbl</code> that contains the locations and names of cities. Each location is a WKT string.</p> <pre><code>CREATE OR REPLACE TABLE city_tbl (wkt STRING, city_name STRING);\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.33 47.61)', 'Seattle');\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.42 37.76)', 'San Francisco');\n</code></pre> <p>Then we can show the content of this table:</p> <pre><code>SELECT *\nFROM city_tbl;\n</code></pre> <p>Output:</p> <pre><code>WKT CITY_NAME\nPOINT (-122.33 47.61)   Seattle\nPOINT (-122.42 37.76)   San Francisco\n</code></pre>"},{"location":"tutorial/snowflake/sql/#create-a-geometrygeography-column","title":"Create a Geometry/Geography column","text":"<p>All geometrical operations in SedonaSQL are on Geometry/Geography type objects. Therefore, before any kind of queries, you need to create a Geometry/Geography type column on the table.</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT SEDONA.ST_GeomFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>The <code>geom</code> column Table <code>city_tbl_geom</code> is now in a <code>Binary</code> type and data in this column is in a format that can be understood by Sedona. The output of this query will show geometries in WKB binary format like this:</p> <pre><code>GEOM CITY_NAME\n010100000085eb51b81e955ec0ae47e17a14ce4740  Seattle\n01010000007b14ae47e19a5ec0e17a14ae47e14240  San Francisco\n</code></pre> <p>To view the content of this column in a human-readable format, you can use <code>SEDONA.ST_AsText</code>. For example,</p> <pre><code>SELECT SEDONA.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Alternatively, you can also create Snowflake native Geometry and Geography type columns. For example, you can create a Snowflake native Geometry type column as follows (note the function has no <code>SEDONA</code> prefix):</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT ST_GeometryFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>The following code creates a Snowflake native Geography type column (note the function has no <code>SEDONA</code> prefix):</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT ST_GeographyFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#check-the-lonlat-order","title":"Check the lon/lat order","text":"<p>In SedonaSnow <code>v1.4.1</code> and before, we use lat/lon order in the following functions:</p> <ul> <li>SEDONA.ST_Transform</li> <li>SEDONA.ST_DistanceSphere</li> <li>SEDONA.ST_DistanceSpheroid</li> </ul> <p>We use <code>lon/lat</code> order in the following functions:</p> <ul> <li>SEDONA.ST_GeomFromGeoHash</li> <li>SEDONA.ST_GeoHash</li> <li>SEDONA.ST_S2CellIDs</li> </ul> <p>In Sedona <code>v1.5.0</code> and above, all functions will be fixed to lon/lat order.</p> <p>If your original data is not in the order you want, you need to flip the coordinate using <code>SEDONA.ST_FlipCoordinates(geom: Geometry)</code>.</p> <p>The sample data used above is in lon/lat order, we can flip the coordinates as follows:</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT SEDONA.ST_FlipCoordinates(geom) AS geom, city_name\nFROM city_tbl_geom\n</code></pre> <p>If we show the content of this table, it is now in lat/lon order:</p> <pre><code>SELECT SEDONA.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Output:</p> <pre><code>GEOM    CITY_NAME\nPOINT (47.61 -122.33)   Seattle\nPOINT (37.76 -122.42)   San Francisco\n</code></pre>"},{"location":"tutorial/snowflake/sql/#save-as-an-ordinary-column","title":"Save as an ordinary column","text":"<p>To save a table to some permanent storage, you can simply convert each geometry in the Geometry type column back to a plain String and save it anywhere you want.</p> <p>Use the following code to convert the Geometry column in a table back to a WKT string column:</p> <pre><code>SELECT SEDONA.ST_AsText(geom)\nFROM city_tbl_geom\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to save the Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use <code>SEDONA.ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string)</code></p> <p>The first EPSG code EPSG:4326 in <code>SEDONA.ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>SEDONA.ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>SEDONA.ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io.</p> <p>Note</p> <p>This function follows lon/order in 1.5.0+ and lat/lon order in 1.4.1 and before. You can use <code>SEDONA.ST_FlipCoordinates</code> to swap X and Y.</p> <p>We can transform our sample data as follows</p> <pre><code>SELECT SEDONA.ST_AsText(SEDONA.ST_Transform(geom, 'epsg:4326', 'epsg:3857')), city_name\nFROM city_tbl_geom\n</code></pre> <p>The output will be like this:</p> <pre><code>POINT (6042216.250411431 -13617713.308741156)  Seattle\nPOINT (4545577.120361927 -13627732.06291255)  San Francisco\n</code></pre> <p><code>SEDONA.ST_Transform</code> also supports the CRS string in OGC WKT format. For example, the following query generates the same output but with a OGC WKT CRS string.</p> <pre><code>SELECT SEDONA.ST_AsText(SEDONA.ST_Transform(geom, 'epsg:4326', 'PROJCS[\"WGS 84 / Pseudo-Mercator\",\n     GEOGCS[\"WGS 84\",\n         DATUM[\"WGS_1984\",\n             SPHEROID[\"WGS 84\",6378137,298.257223563,\n                 AUTHORITY[\"EPSG\",\"7030\"]],\n             AUTHORITY[\"EPSG\",\"6326\"]],\n         PRIMEM[\"Greenwich\",0,\n             AUTHORITY[\"EPSG\",\"8901\"]],\n         UNIT[\"degree\",0.0174532925199433,\n             AUTHORITY[\"EPSG\",\"9122\"]],\n         AUTHORITY[\"EPSG\",\"4326\"]],\n     PROJECTION[\"Mercator_1SP\"],\n     PARAMETER[\"central_meridian\",0],\n     PARAMETER[\"scale_factor\",1],\n     PARAMETER[\"false_easting\",0],\n     PARAMETER[\"false_northing\",0],\n     UNIT[\"metre\",1,\n         AUTHORITY[\"EPSG\",\"9001\"]],\n     AXIS[\"Easting\",EAST],\n     AXIS[\"Northing\",NORTH],\n     EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n     AUTHORITY[\"EPSG\",\"3857\"]]')), city_name\nFROM city_tbl_geom\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-query","title":"Range query","text":"<p>Use SEDONA.ST_Contains, SEDONA.ST_Intersects, SEDONA.ST_Within to run a range query over a single column.</p> <p>The following example finds all geometries that are within the given polygon:</p> <pre><code>SELECT *\nFROM city_tbl_geom\nWHERE SEDONA.ST_Contains(SEDONA.ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), geom)\n</code></pre> <p>Note</p> <p>Read SedonaSQL API to learn how to create a Geometry type query window.</p>"},{"location":"tutorial/snowflake/sql/#knn-query","title":"KNN query","text":"<p>Use SEDONA.ST_Distance, SEDONA.ST_DistanceSphere, ST_DistanceSpheroid to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given point.</p> <pre><code>SELECT geom, SEDONA.ST_Distance(SEDONA.ST_Point(1.0, 1.0), geom) AS distance\nFROM city_tbl_geom\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-join-query","title":"Range join query","text":"<p>Warning</p> <p>Sedona range join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions + native <code>Geography</code> type to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate.</p> <p>Example:</p> <p>Create the illustrative tables:</p> <pre><code>CREATE OR REPLACE TABLE polygondf AS\nSELECT SEDONA.ST_GeomFromText('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))') polygonshape;\n</code></pre> <pre><code>CREATE OR REPLACE TABLE pointdf AS\nSELECT SEDONA.ST_GeomFromText('POINT(0.5 0.5)') pointshape;\n</code></pre> <p>Now run the following queries with different spatial predicates:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE SEDONA.ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE SEDONA.ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE SEDONA.ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre> <p>The corresponding (faster) spatial joins use only native Snowflake functions:</p> <pre><code>WITH polygondf AS (\n    SELECT ST_GeomFromText('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))') polygonshape\n),\npointdf AS (\n    SELECT ST_GeomFromText('POINT(0.5 0.5)') pointshape\n)\nSELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <p>This also work natively with other predicates like <code>ST_Intersects</code> and <code>ST_Within</code>.</p>"},{"location":"tutorial/snowflake/sql/#distance-join","title":"Distance join","text":"<p>Warning</p> <p>Sedona distance join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions + native <code>Geography</code> type to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>SEDONA.ST_Distance</code>, <code>SEDONA.ST_HausdorffDistance</code>, <code>SEDONA.ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>SEDONA.ST_DistanceSpheroid</code> and <code>SEDONA.ST_DistanceSphere</code>. Snowflake only natively supports <code>ST_Distance</code>.</p> <p>First create two new illustrative tables with a single geometry.</p> <pre><code>CREATE OR REPLACE TABLE pointdf2 AS\nSELECT SEDONA.ST_GeomFromText('POINT(0 0)') pointshape;\n</code></pre> <pre><code>CREATE OR REPLACE TABLE polygondf2 AS\nSELECT SEDONA.ST_GeomFromText('POLYGON((0.5 0.5, 0.5 1, 1 1, 1 0.5, 0.5 0.5))') polygonshape;\n</code></pre> <p>Examples for planar Euclidean distances:</p> <p>The usual l2 Euclidean distance between <code>POINT(0 0)</code> and <code>POINT(0.5 0.5)</code> points is the square root of 0.5, so the following returns the single pair of points.</p> <pre><code>SELECT *\nFROM pointdf, pointdf2\nWHERE SEDONA.ST_Distance(pointdf.pointshape,pointdf2.pointshape) &lt;= sqrt(0.5)\n</code></pre> <p>The l2 distance between two polygons that are not disjoint is 0, so the following returns the single polygon pair</p> <pre><code>SELECT *\nFROM polygondf, polygondf2\nWHERE SEDONA.ST_Distance(polygondf.polygonshape,polygondf2.polygonshape) &lt;= 0\n</code></pre> <p>This is not necessarily the case of the Hausdorff or Fr\u00e9chet distances. The following queries return no rows:</p> <pre><code>SELECT *\nFROM polygondf, polygondf2\nWHERE SEDONA.ST_HausdorffDistance(polygondf.polygonshape, polygondf2.polygonshape, 0.3) &lt;= 0\n</code></pre> <pre><code>SELECT *\nFROM polygondf, polygondf2\nWHERE SEDONA.ST_FrechetDistance(polygondf.polygonshape, polygondf2.polygonshape)  &lt;= 0\n</code></pre> <p>Note that only the Hausdorff distance takes in a third <code>densityFraction</code> parameter in (0,1] with smaller values giving more accurate results.</p> <p>Warning</p> <p>If you use planar Euclidean distance functions like <code>SEDONA.ST_Distance</code>, <code>SEDONA.ST_HausdorffDistance</code> or <code>SEDONA.ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See SEDONA.ST_Transform. If you don't want to transform your data, please consider using <code>SEDONA.ST_DistanceSpheroid</code> or <code>SEDONA.ST_DistanceSphere</code>.</p> <p>For instance, the following returns roughly 78.45 kilometers, since the geometries are assumed to be in degrees of longitude and latitude, even if no CRS was explicitly set.</p> <pre><code>SELECT SEDONA.ST_DistanceSpheroid(pointdf.pointshape,pointdf2.pointshape)\nFROM pointdf, pointdf2\n</code></pre> <p>As the following query illustrates, running Snowflake's native <code>ST_DISTANCE</code> on <code>GEOGRAPHY</code> objects will return a result similar to <code>SEDONA.ST_DistanceSphere</code> and <code>SEDONA.ST_DistanceSpheroid</code>:</p> <pre><code>SELECT\n    SEDONA.ST_DistanceSphere(\n        pointdf.pointshape,\n        pointdf2.pointshape\n    ) SEDONA_ST_DistanceSphere,\n    --\n    SEDONA.ST_DistanceSpheroid(\n        pointdf.pointshape,\n        pointdf2.pointshape\n    ) SEDONA_ST_DistanceSpheroid,\n    --\n    ST_DISTANCE(\n        TO_GEOGRAPHY(SEDONA.st_astext(pointdf.pointshape)),\n        TO_GEOGRAPHY(SEDONA.st_astext(pointdf2.pointshape) )\n    ) SFKL_ST_DISTANCE\nFROM pointdf, pointdf2\n</code></pre> <p>Output:</p> SEDONA_ST_DISTANCESPHERE SEDONA_ST_DISTANCESPHEROID SFKL_ST_DISTANCE 78626.28640698 78451.248031239 78626.311089506"},{"location":"tutorial/snowflake/sql/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>You can use Sedona's built-in Google S2 functions to perform an approximate equi-join. This algorithm leverages Snowflake's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy. Sedona's S2 functions are a nice complement to Snowflake's native H3 and Geohash functions.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/snowflake/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use SEDONA.ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT * FROM lefts, TABLE(FLATTEN(SEDONA.ST_S2CellIDs(lefts.geom, 15))) s1\n</code></pre> <pre><code>SELECT * FROM rights, TABLE(FLATTEN(SEDONA.ST_S2CellIDs(rights.geom, 15))) s2\n</code></pre>"},{"location":"tutorial/snowflake/sql/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id AS lcs_id, lcs.geom AS lcs_geom, lcs.name AS lcs_name, rcs.id AS rcs_id, rcs.geom AS rcs_geom, rcs.name AS rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/snowflake/sql/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id AS lcs_id, lcs.geom AS lcs_geom, lcs.name AS lcs_name, rcs.id AS rcs_id, rcs.geom AS rcs_geom, rcs.name AS rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/snowflake/sql/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the <code>Flatten</code> function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, ANY_VALUE(lcs_geom), ANY_VALUE(lcs_name), ANY_VALUE(rcs_geom), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>ANY_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, ANY_VALUE(lcs_name), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem, and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/snowflake/sql/#5-putting-it-all-together","title":"5. Putting it all together","text":"<p>The following queries creates 2 regions of interests and 5 different spatial queries in the form of bounding boxes in longitude, latitude. It then computes S2 coverings of both tables and merges on the S2 cell ID. A spatial filter can be applied to the original geometries to guarantee the exactness of the join.</p> <p>The following example uses Snowflake's native <code>ST_GeogFromText</code> (treated as geographies), but also works with <code>ST_GeomFromText</code>. In both cases, note <code>SEDONA.ST_S2CellIDs</code> takes in a Snowflake <code>GEOGRAPHY</code> or <code>GEOMETRY</code> object along with an integer S2 precision.</p> <pre><code>-- lng/lat format: no need to flip\n-- lefts are area of interest (AOI)\nwith lefts AS (\n  SELECT index AS poly_id_left, value AS wkt FROM TABLE(\n      SPLIT_TO_TABLE (\n          'POLYGON ((-74.64966372842101805 44.92318068906040196, -73.05513946490677313 44.92318068906040196, -73.05513946490677313 45.9127817308399031, -74.64966372842101805 45.9127817308399031, -74.64966372842101805 44.92318068906040196))|POLYGON ((-71.72125014775386376 46.58534825561803672, -70.72763860520016976 46.58534825561803672, -70.72763860520016976 47.26007441306773416, -71.72125014775386376 47.26007441306773416, -71.72125014775386376 46.58534825561803672))', '|'\n          )\n      )\n\n),\n-- rights are queries\n-- 1st polygon intersects both AOI\n-- 2nd polygon contains both AOI\n-- 3rd polygon is contained by AOI 1\n-- 4th polygon touches AOI 1\n-- 5th polygon is disjoint from both\nrights AS (\n  SELECT index AS poly_id_right, value AS wkt FROM TABLE(\n      SPLIT_TO_TABLE (\n          'POLYGON ((-73.51160030163114811 45.54300066590783302, -71.31736631503980561 45.54300066590783302, -71.31736631503980561 46.82515071005796869, -73.51160030163114811 46.82515071005796869, -73.51160030163114811 45.54300066590783302))|POLYGON ((-75.26522832 44.23585380, -69.77500453509208 44.23585380, -69.77500453509208 47.59180373699041411, -75.26522832 47.59180373699041411, -75.26522832 44.23585380))|POLYGON ((-73.82001807503861812 45.0759163125215423, -73.34011383205873358 45.0759163125215423, -73.34011383205873358 45.35768613572972185, -73.82001807503861812 45.35768613572972185, -73.82001807503861812 45.0759163125215423))|POLYGON ((-74.64966372842101805 44.92318068906040196, -75.26522832 44.92318068906040196, -75.26522832 44.23585380, -74.64966372842101805 44.23585380, -74.64966372842101805 44.92318068906040196))|POLYGON ((-71.65791191749059408 44.19369241163229844, -70.2497216546401404 44.19369241163229844, -70.2497216546401404 44.96946189775351144, -71.65791191749059408 44.96946189775351144, -71.65791191749059408 44.19369241163229844))', '|'\n          )\n      )\n),\n-- compute s2 cells covering the polyons for both tables\n-- S2 discretizes the polygons, hence higher resolution yields more accurate queries, at the cost of increased computation\n-- both tables need to be discretized at the same level - in this case 10\nlefts_s2 AS (\n    SELECT * FROM lefts, TABLE(FLATTEN(SEDONA.ST_S2CellIDs(ST_GeogFromText(lefts.wkt), 10)))\n),\nrights_s2 AS (\n    SELECT * FROM rights, TABLE(FLATTEN(SEDONA.ST_S2CellIDs(ST_GeogFromText(rights.wkt), 10)))\n)\n-- merge on s2 index (int) and group by to retrieve the original polygons rather than the s2 cells\n-- add the spatial predicate to be exact and omit if speed is more important\n-- expect all queries except the 5th to match\n-- expected result: 1 (touches) + 1 (contained) + 2 (intersect both) + 2 (contains both) = 6 rows total\n-- AOI 1 (poly_id_left 1) should be present 4 times and AOI 2 (poly_id_left 2) should be present 2 times\nSELECT rights_s2.wkt ,  lefts_s2.wkt, LISTAGG(DISTINCT poly_id_right::TEXT) poly_id_right, LISTAGG(DISTINCT poly_id_left::TEXT) poly_id_left\nFROM lefts_s2,rights_s2\nWHERE rights_s2.value = lefts_s2.value AND NOT ST_DISJOINT(ST_GeogFromText(rights_s2.wkt) , ST_GeogFromText( lefts_s2.wkt ) )\nGROUP BY rights_s2.wkt ,  lefts_s2.wkt\n</code></pre> <p>In this case, omitting <code>not ST_DISJOINT(ST_GeogFromText(rights_s2.wkt) , ST_GeogFromText( lefts_s2.wkt ) )</code> yields the same results.</p>"},{"location":"tutorial/snowflake/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>SEDONA.ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>SEDONA.ST_Buffer</code> will make them become circles with a radius of <code>distance</code>. Note that Snowflake does not implement a native <code>ST_Buffer</code> function.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives. Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, SEDONA.ST_Buffer(geom, METER_DISTANCE/111000.0) AS buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"tutorial/snowflake/sql/#functions-that-are-only-available-in-sedona","title":"Functions that are only available in Sedona","text":"<p>Sedona implements over 200 geospatial vector and raster functions, which are much more than what Snowflake native functions offer. For example:</p> <ul> <li>SEDONA.ST_3DDistance</li> <li>SEDONA.ST_Force2D</li> <li>SEDONA.ST_GeometryN</li> <li>SEDONA.ST_MakeValid</li> <li>SEDONA.ST_Multi</li> <li>SEDONA.ST_NumGeometries</li> <li>SEDONA.ST_ReducePrecision</li> <li>SEDONA.ST_SubdivideExplode</li> </ul> <p>You can click the links above to learn more about these functions. More functions can be found in SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#interoperate-with-snowflake-native-functions","title":"Interoperate with Snowflake native functions","text":"<p>Sedona can interoperate with Snowflake native functions seamlessly. There are two ways to do this:</p> <ul> <li>Use <code>Sedona functions</code> to create a Geometry column, then use Snowflake native functions and Sedona functions to query it.</li> <li>Use <code>Snowflake native functions</code> to create a Geometry/Geography column, then use Snowflake native functions and Sedona functions to query it.</li> </ul> <p>Now we will show you how to do this.</p>"},{"location":"tutorial/snowflake/sql/#geometries-created-by-sedona-geometry-constructors","title":"Geometries created by Sedona Geometry constructors","text":"<p>In this case, Sedona uses EWKB type as the input/output type for geometry. If you have datasets of built-in Snowflake GEOMETRY/GEOGRAPHY type, you can easily transform them into EWKB through this function.</p>"},{"location":"tutorial/snowflake/sql/#from-snowflake-native-functions-to-sedona-functions","title":"From Snowflake native functions to Sedona functions","text":"<p>In this example, <code>SEDONA.ST_X</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AsEWKB</code> are Snowflake native functions.</p> <pre><code>SELECT SEDONA.ST_X(ST_AsEWKB(ST_GeometryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'))) FROM {{ geometry_table }};\n</code></pre>"},{"location":"tutorial/snowflake/sql/#from-sedona-functions-to-snowflake-native-functions","title":"From Sedona functions to Snowflake native functions","text":"<p>In this example, <code>SEDONA.ST_GeomFromText</code> is a Sedona function, <code>ST_AREA</code> and <code>to_geometry</code> are Snowflake native functions.</p> <pre><code>SELECT ST_AREA(to_geometry(SEDONA.ST_GeomFromText('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')));\n</code></pre>"},{"location":"tutorial/snowflake/sql/#pros","title":"Pros:","text":"<p>Sedona geometry constructors are more powerful than Snowflake native functions. It has the following advantages:</p> <ul> <li>Sedona offers more constructors especially for 3D (XYZ) geometries, but Snowflake native functions don't.</li> <li>WKB serialization is more efficient. If you need to use multiple Sedona functions, it is more efficient to use this method, which might bring in 2X performance improvement.</li> <li>SRID information of geometries is preserved. The method below will lose SRID information.</li> </ul>"},{"location":"tutorial/snowflake/sql/#geometry-geography-created-by-snowflake-geometry-geography-constructors","title":"Geometry / Geography created by Snowflake Geometry / Geography constructors","text":"<p>In this case, Sedona uses Snowflake native GEOMETRY/GEOGRAPHY type as the input/output type for geometry. The serialization format is GeoJSON string.</p> <pre><code>SELECT ST_AREA(SEDONA.ST_Buffer(ST_GeometryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'), 1));\n</code></pre> <p>In this example, <code>SEDONA.ST_Buffer</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AREA</code> are Snowflake native functions.</p> <p>As you can see, you can use Sedona functions and Snowflake native functions together without explicit conversion.</p>"},{"location":"tutorial/snowflake/sql/#pros_1","title":"Pros:","text":"<ul> <li>You don't need to convert the geometry type, which is more convenient.</li> </ul> <p>Note that: Snowflake natively serializes Geometry type data to GeoJSON String and sends to UDF as input. GeoJSON spec does not include SRID. So the SRID information will be lost if you mix-match Snowflake functions and Sedona functions directly without using <code>WKB</code>.</p> <p>In the example below, the SRID=4326 information is lost.</p> <pre><code>SELECT ST_AsEWKT(SEDONA.ST_SetSRID(ST_GeometryFromWKT('POINT(1 2)'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=0;POINT(1 2)\n</code></pre>"},{"location":"tutorial/snowflake/sql/#known-issues","title":"Known issues","text":"<ol> <li>Sedona Snowflake doesn't support <code>M</code> dimension due to the limitation of WKB serialization. Sedona Spark and Sedona Flink support XYZM because it uses our in-house serialization format. Although Sedona Snowflake has functions related to <code>M</code> dimension, all <code>M</code> values will be ignored.</li> <li>Sedona H3 functions are not supported because Snowflake does not allow embedded C code in UDF.</li> <li>All User Defined Table Functions only work with geometries created by Sedona constructors due to Snowflake current limitation <code>Data type GEOMETRY is not supported in non-SQL UDTF return type</code>. This includes:<ul> <li>SEDONA.ST_MinimumBoundingRadius</li> <li>SEDONA.ST_Intersection_Aggr</li> <li>SEDONA.ST_SubDivideExplode</li> <li>SEDONA.ST_Envelope_Aggr</li> <li>SEDONA.ST_Union_Aggr</li> <li>SEDONA.ST_Collect</li> <li>SEDONA.ST_Dump</li> </ul> </li> <li>Only Sedona ST functions are available in Snowflake. Raster functions (RS functions) are not available in Snowflake yet.</li> </ol>"},{"location":"blog/archive/2025/","title":"December 2025","text":""}]}