{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#12022024-sedona-170-released-it-introduces-a-new-join-type-named-knn-join-a-new-statistics-module-called-geostats-dataframe-based-readers-for-shapefiles-and-geopackage-and-many-new-st-functions","title":"12/02/2024: Sedona 1.7.0 released. It introduces a new join type named KNN Join, a new statistics module called GeoStats, DataFrame based readers for Shapefiles and GeoPackage, and many new ST functions.","text":""},{"location":"#08242024-sedona-161-released-it-adds-a-native-dataframe-based-geojson-reader-and-writer-48-new-st-functions-geoparquet-110-covering-column-and-improves-the-error-handling-of-st-functions","title":"08/24/2024: Sedona 1.6.1 released. It adds a native DataFrame based GeoJSON reader and writer, 48 new ST functions, GeoParquet 1.1.0 covering column, and improves the error handling of ST functions.","text":""},{"location":"#05172024-sedona-160-released-it-provides-enhanced-support-for-geography-type-rasterio-and-numpy-udf-on-raster-type-shapely-20-udf-on-geometry-type-and-many-more","title":"05/17/2024: Sedona 1.6.0 released. It provides enhanced support for geography type, RasterIO and NumPy UDF on raster type, Shapely 2.0 UDF on geometry type, and many more!","text":""},{"location":"#05072024-sedona-152-released-this-is-a-maintenance-release-that-only-includes-bug-fixes-and-minor-improvements-we-strongly-recommend-15x-users-to-upgrade-to-152","title":"05/07/2024: Sedona 1.5.2 released. This is a maintenance release that only includes bug fixes and minor improvements. We strongly recommend 1.5.X users to upgrade to 1.5.2.","text":""},{"location":"download/","title":"Download","text":""},{"location":"download/#github-repository","title":"GitHub repository","text":"<p>Latest source code: GitHub repository</p> <p>Old GeoSpark releases: GitHub releases</p> <p>Automatically generated binary JARs (per each Master branch commit): GitHub Action</p>"},{"location":"download/#verify-the-integrity","title":"Verify the integrity","text":"<p>Public keys</p> <p>Instructions</p>"},{"location":"download/#versions","title":"Versions","text":""},{"location":"download/#170","title":"1.7.0","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#161","title":"1.6.1","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#153","title":"1.5.3","text":"Download from ASF Checksum Signature Source code src sha512 asc Binary bin sha512 asc"},{"location":"download/#past-releases","title":"Past releases","text":"<p>Past Sedona releases are archived and can be found here: Apache archive (on and after 1.2.1-incubating).</p>"},{"location":"download/#security","title":"Security","text":"<p>For security issues, please refer to https://www.apache.org/security/</p>"},{"location":"api/java-api/","title":"Scala/Java doc","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/python-api/","title":"Python api","text":"<p>Will be available soon.</p>"},{"location":"api/flink/Aggregator/","title":"Aggregator (Flink)","text":""},{"location":"api/flink/Aggregator/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Aggregator/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A. All inputs must be polygons.</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/flink/Constructor/","title":"Constructor (Flink)","text":""},{"location":"api/flink/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML1 and GML2. GML3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4) AS geom\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. Alias of  ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt: String)</code></p> <p><code>ST_GeomFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre> <p>Format: <code>ST_GeomFromWKB (Wkb: Bytes)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKB(polygontable._c0) AS polygonshape\nFROM polygontable\n</code></pre>"},{"location":"api/flink/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT</p> <p>Format: <code>ST_GeomFromWKT (Wkt: String)</code></p> <p><code>ST_GeomFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a LineString from Text</p> <p>Format: <code>ST_LineFromText (Text: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromText('Linestring(1 2, 3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter (Optional). Alias of  ST_LineFromText</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/flink/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/flink/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Text and Optional SRID</p> <p>Format:</p> <p><code>ST_MLineFromText (Wkt: String)</code></p> <p><code>ST_MLineFromText (Wkt: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/flink/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Text and Optional SRID</p> <p>Format:</p> <p><code>ST_MPolyFromText (Wkt: String)</code></p> <p><code>ST_MPolyFromText (Wkt: String, Srid: Integer)</code></p> <p>Since: <code>1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/flink/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/flink/Constructor/#st_makepointm","title":"ST_MakePointM","text":"<p>Introduction: Creates a point with X, Y, and M coordinate. Use ST_MakePoint to make points with XY, XYZ, or XYZM coordinates.</p> <p>Format: <code>ST_MakePointM(x: Double, y: Double, m: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePointM(1, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>Point M(1 2 3)\n</code></pre>"},{"location":"api/flink/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointm","title":"ST_PointM","text":"<p>Introduction: Construct a Point from X, Y and M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointM (X: Double, Y: Double, M: Double)</code></p> <p><code>ST_PointM (X: Double, Y: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointM(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 0 3.4567)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointzm","title":"ST_PointZM","text":"<p>Introduction: Construct a Point from X, Y, Z, M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZM(1.2345, 2.3456, 3.4567, 100))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 3.4567, 100)\n</code></pre>"},{"location":"api/flink/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/flink/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/flink/Function/","title":"Function (Flink)","text":""},{"location":"api/flink/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Result:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/flink/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\n                     ST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output:</p> <pre><code>1.7320508075688772\n</code></pre>"},{"location":"api/flink/Function/#st_addmeasure","title":"ST_AddMeasure","text":"<p>Introduction: Computes a new geometry with measure (M) values linearly interpolated between start and end points. For geometries lacking M dimensions, M values are added. Existing M values are overwritten by the new values. Applies only to LineString and MultiLineString inputs.</p> <p>Format: <code>ST_AddMeasure(geom: Geometry, measureStart: Double, measureEnd: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_AddMeasure(\n        ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)')\n))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 0 10, 1 0 16, 2 0 22, 3 0 28, 4 0 34, 5 0 40)\n</code></pre>"},{"location":"api/flink/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: Return Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/flink/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Examples:</p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/flink/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/flink/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Compute and return the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code></p> <p>Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code></p> <p>Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/flink/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>10\n</code></pre>"},{"location":"api/flink/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/flink/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/flink/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry.</p> <p>The type parameter (Since: <code>v1.6.1</code>) takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A: Geometry)</code></p> <p><code>ST_AsGeoJSON (A: Geometry, type: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n    \"type\":\"Feature\",\n    \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n    }\n  }\n  ]\n}\n</code></pre>"},{"location":"api/flink/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/flink/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/flink/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/flink/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/flink/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON ((1 1, 0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/flink/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>Spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : The option <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/flink/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(ST_Collect(smallDf, bigDf)) AS geom\nFROM smallDf, bigDf\n</code></pre> <p>Input: <code>MULTILINESTRING((0 0, 10 0, 10 10, 0 10, 0 0),(10 10, 20 10, 20 20, 10 20, 10 10))</code></p> <p>Output: <code>MULTIPOLYGON(((0 0,0 10,10 10,10 0,0 0)),((10 10,10 20,20 20,20 10,10 10)))</code></p>"},{"location":"api/flink/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/flink/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/flink/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Collect(\n    ST_GeomFromText('POINT(21.427834 52.042576573)'),\n    ST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Collect(\n    Array(\n        ST_GeomFromText('POINT(21.427834 52.042576573)'),\n        ST_GeomFromText('POINT(45.342524 56.342354355)')\n    )\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/flink/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/flink/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/flink/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Result:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/flink/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/flink/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Distance</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/flink/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/flink/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/flink/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/flink/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/flink/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/flink/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/flink/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/flink/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a LINESTRING representing the exterior ring (shell) of a POLYGON. Returns NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/flink/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/flink/Function/#st_force3dm","title":"ST_Force3DM","text":"<p>Introduction: Forces the geometry into XYM mode. Retains any existing M coordinate, but removes the Z coordinate if present. Assigns a default M value of 0.0 if <code>mValue</code> is not specified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds M for in the WKT.</p> <p>Format: <code>ST_Force3DM(geometry: Geometry, mValue: Double = 0.0)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('POLYGON M((0 0 3,0 5 3,5 0 3,0 0 3),(1 1 3,3 1 3,1 3 3,1 1 3))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON M((0 0 3, 0 5 3, 5 0 3, 0 0 3), (1 1 3, 3 1 3, 1 3 3, 1 1 3))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING Z(0 1 3,1 0 3,2 0 3)'), 5))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 5, 1 0 5, 2 0 5)\n</code></pre>"},{"location":"api/flink/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/flink/Function/#st_force4d","title":"ST_Force4D","text":"<p>Introduction: Converts the input geometry to 4D XYZM representation. Retains original Z and M values if present. Assigning 0.0 defaults if <code>mValue</code> and <code>zValue</code> aren't specified. The output contains X, Y, Z, and M coordinates. For geometries already in 4D form, the function returns the original geometry unmodified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format:</p> <p><code>ST_Force4D(geom: Geometry, zValue: Double, mValue: Double)</code></p> <p><code>ST_Force4D(geom: Geometry</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 5, 10))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ZM((0 0 2 10, 0 5 2 10, 5 0 2 10, 0 0 2 10), (1 1 2 10, 3 1 2 10, 1 3 2 10, 1 1 2 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 3, 1))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING ZM(0 1 3 1, 1 0 3 1, 2 0 3 1)\n</code></pre>"},{"location":"api/flink/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/flink/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/flink/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/flink/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/flink/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/flink/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/flink/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/flink/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LINESTRING\n</code></pre>"},{"location":"api/flink/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[1])\n</code></pre> <p>Output:</p> <pre><code>+----+----------------------+\n| op |               EXPR$0 |\n+----+----------------------+\n| +I |                   78 |\n+----+----------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/flink/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/flink/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: true)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre> <p>Output:</p> <pre><code>+----+--------------------------------+\n| op |                         EXPR$0 |\n+----+--------------------------------+\n| +I | [605547539457900543, 605547... |\n+----+--------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, false), ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[1], 1, true)\n</code></pre> <p>Output:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                         EXPR$0 |                         EXPR$1 |\n+----+--------------------------------+--------------------------------+\n| +I | [614552609325318143, 614552... | [614552597293957119, 614552... |\n+----+--------------------------------+--------------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: Return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Converts an array of Uber H3 cell indices into an array of Polygon geometries, where each polygon represents a hexagonal H3 cell.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((1.0057629565405093 1.9984665139177547, 1.0037116327309097 2.0018325249140068, 0.999727799357053 2.001163270465665, 0.9977951427833316 1.997128228393235, 0.9998461908217928 1.993762152933182, 1.0038301712104316 1.9944311839965523, 1.0057629565405093 1.9984665139177547))]\n</code></pre>"},{"location":"api/flink/Function/#st_hasm","title":"ST_HasM","text":"<p>Introduction: Checks for the presence of M coordinate values representing measures or linear references. Returns true if the input geometry includes an M coordinate, false otherwise.</p> <p>Format: <code>ST_HasM(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasM(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/flink/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/flink/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/flink/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINEARRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_interpolatepoint","title":"ST_InterpolatePoint","text":"<p>Introduction: Returns the interpolated measure value of a linear measured LineString at the point closest to the specified point.</p> <p>Note</p> <p>Make sure that both geometries have the same SRID, otherwise the function will throw an IllegalArgumentException.</p> <p>Format: <code>ST_InterpolatePoint(linestringM: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InterpolatePoint(\n    ST_GeomFromWKT(\"LINESTRING M (0 0 0, 2 0 2, 4 0 4)\"),\n    ST_GeomFromWKT(\"POINT (1 1)\")\n    )\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/flink/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersection(\n    ST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\n    ST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/flink/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/flink/Function/#st_isvalidtrajectory","title":"ST_IsValidTrajectory","text":"<p>Introduction: This function checks if a geometry is a valid trajectory representation. For a trajectory to be considered valid, it must be a LineString that includes measure (M) values. The key requirement is that the M values increase from one vertex to the next as you move along the line.</p> <p>Format: <code>ST_IsValidTrajectory(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 2)')\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 0)')\n)\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/flink/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Length2D(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/flink/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/flink/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/flink/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output:</p> <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre>"},{"location":"api/flink/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/flink/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/flink/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/flink/Function/#st_locatealong","title":"ST_LocateAlong","text":"<p>Introduction: This function computes Point or MultiPoint geometries representing locations along a measured input geometry (LineString or MultiLineString) corresponding to the provided measure value(s). Polygonal geometry inputs are not supported. The output points lie directly on the input line at the specified measure positions.</p> <p>Additionally, an optional <code>offset</code> parameter can shift the resulting points left or right from the input line. A positive offset displaces the points to the left side, while a negative value offsets them to the right side by the given distance.</p> <p>This allows identifying precise locations along a measured linear geometry based on supplied measure values, with the ability to offset the output points if needed.</p> <p>Format:</p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double, offset: Double)</code></p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LocateAlong(\n        ST_GeomFromText('LINESTRING M (10 30 1, 50 50 1, 30 110 2, 70 90 2, 180 140 3, 130 190 3)')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT M((30 110 2), (50 100 2), (70 90 2))\n</code></pre>"},{"location":"api/flink/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre>"},{"location":"api/flink/Function/#st_m","title":"ST_M","text":"<p>Introduction: Returns M Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_M(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_M(ST_MakePoint(1, 2, 3, 4))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_mmax","title":"ST_MMax","text":"<p>Introduction: Returns M maxima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMax(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MMax(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_mmin","title":"ST_MMin","text":"<p>Introduction: Returns M minima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMin(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MMin(\n        ST_GeomFromWKT('LINESTRING ZM(1 1 1 1, 2 2 2 2, 3 3 3 3, -1 -1 -1 -1)')\n)\n</code></pre> <p>Output:</p> <pre><code>-1.0\n</code></pre>"},{"location":"api/flink/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/flink/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/flink/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre>"},{"location":"api/flink/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/flink/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/flink/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/flink/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments is 48.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/flink/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/flink/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/flink/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format: <code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Returns the number of points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/flink/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/flink/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: Returns number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/flink/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString.</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(1 2, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/flink/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointN(df.geometry, 2)\nFROM df\n</code></pre> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), 2</code></p> <p>Output: <code>POINT (1 2)</code></p> <p>Input: <code>LINESTRING(0 0, 1 2, 2 4, 3 6), -2</code></p> <p>Output: <code>POINT (2 4)</code></p> <p>Input: <code>CIRCULARSTRING(1 1, 1 2, 2 4, 3 6, 1 2, 1 1), -1</code></p> <p>Output: <code>POINT (1 1)</code></p>"},{"location":"api/flink/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_PointOnSurface(df.geometry)\nFROM df\n</code></pre> <ol> <li>Input: <code>POINT (0 5)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5, 0 10)</code></li> </ol> <p>Output: <code>POINT (0 5)</code></p> <ol> <li>Input: <code>POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))</code></li> </ol> <p>Output: <code>POINT (2.5 2.5)</code></p> <ol> <li>Input: <code>LINESTRING(0 5 1, 0 0 1, 0 10 2)</code></li> </ol> <p>Output: <code>POINT Z(0 0 1)</code></p>"},{"location":"api/flink/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/flink/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/flink/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/flink/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/flink/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/flink/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/flink/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: Return Linestring with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/flink/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre>"},{"location":"api/flink/Function/#st_s2togeom","title":"ST_S2ToGeom","text":"<p>Introduction: Returns an array of Polygons for the corresponding S2 cell IDs.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_S2ToGeom(cellIds: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_S2ToGeom(array(11540474045136890))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((-36.609392788630245 -38.169532607255846, -36.609392706252954 -38.169532607255846, -36.609392706252954 -38.169532507473015, -36.609392788630245 -38.169532507473015, -36.609392788630245 -38.169532607255846))]\n</code></pre>"},{"location":"api/flink/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/flink/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/flink/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/flink/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/flink/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/flink/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/flink/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.5.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>Example:</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/flink/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/flink/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n           ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/flink/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/flink/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre>"},{"location":"api/flink/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/flink/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since v1.3.1.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will force the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, it will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, [Optional] lenientMode: Boolean)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/flink/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it.</p> <p>If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/flink/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/flink/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/flink/Function/#st_union","title":"ST_Union","text":"<p>Introduction:</p> <p>Variant 1: Return the union of geometry A and B.</p> <p>Variant 2: This function accepts an array of Geometry objects and returns the geometric union of all geometries in the input array. If the polygons within the input array do not share common boundaries, the ST_Union result will be a MultiPolygon geometry.</p> <p>Format:</p> <p><code>ST_Union (A: Geometry, B: Geometry)</code></p> <p><code>ST_Union (geoms: Array(Geometry))</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Union(\n    Array(\n        ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'),\n        ST_GeomFromWKT('POLYGON ((-2 1, 2 1, 2 4, -2 4, -2 1))')\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 3, 3 3, 3 -3, -3 -3, -3 3, -2 3, -2 4, 2 4, 2 3))\n</code></pre>"},{"location":"api/flink/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/flink/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/flink/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/flink/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/flink/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output :</p> <pre><code>2\n</code></pre>"},{"location":"api/flink/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/flink/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/flink/Function/#st_zmflag","title":"ST_Zmflag","text":"<p>Introduction: Returns a code indicating the Z and M coordinate dimensions present in the input geometry.</p> <p>Values are: 0 = 2D, 1 = 3D-M, 2 = 3D-Z, 3 = 4D.</p> <p>Format: <code>ST_Zmflag(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('LINESTRING Z(1 2 3, 4 5 6)')\n)\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('POINT ZM(1 2 3 4)')\n)\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/flink/Overview/","title":"Overview (Flink)","text":""},{"location":"api/flink/Overview/#introduction","title":"Introduction","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Please read the programming guide: Sedona with Flink SQL app.</p> <p>Sedona includes SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> </ul> </li> <li>Aggregator: Return a single aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> </ul> </li> </ul>"},{"location":"api/flink/Predicate/","title":"Predicate (Flink)","text":""},{"location":"api/flink/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example:</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'.</p> <p>If <code>useSpheroid</code> is passed true, ST_DWithin uses Sedona's ST_DistanceSpheroid to check the spheroid distance between the centroids of two geometries. The unit of the distance in this case is meter.</p> <p>If <code>useSpheroid</code> is passed false, ST_DWithin uses Euclidean distance and the unit of the distance is the same as the CRS of the geometries. To obtain the correct result, please consider using ST_Transform to put data in an appropriate CRS.</p> <p>If useSpheroid is not given, it defaults to false</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double, useSpheroid: Optional(Boolean) = false)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <pre><code>Check for distance between New York and Seattle (&lt; 4000 km)\n</code></pre> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT(-122.335167 47.608013), ST_GeomFromWKT(-73.935242 40.730610), 4000000, true)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is within B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/flink/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/flink/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>Example:</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/","title":"Aggregate Function (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(envelope)\nFROM src_tbl,\n     TABLE(sedona.ST_Envelope_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(intersected)\nFROM src_tbl,\n     TABLE(sedona.ST_Intersection_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 1, 1 1, 1 0.5, 0.5 0.5, 0.5 1))\n</code></pre>"},{"location":"api/snowflake/vector-data/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A:geometryColumn)</code></p> <p>SQL example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))') AS geom\n)\nSELECT sedona.ST_AsText(unioned)\nFROM src_tbl,\n     TABLE(sedona.ST_Union_Aggr(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 1, 0.5 1, 0.5 1.5, 1.5 1.5, 1.5 0.5, 1 0.5, 1 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/","title":"Constructor (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML1 and GML2. GML3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: string, precision: int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format: <code>ST_GeomFromText (Wkt:string)</code> <code>ST_GeomFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format: <code>ST_GeomFromWKB (Wkb:string)</code> <code>ST_GeomFromWKB (Wkb:binary)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL example:</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_GeomFromWKT (Wkt:string)</code> <code>ST_GeomFromWKT (Wkt:string, srid:integer)</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt:string)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MLineFromText (Wkt:string)</code> <code>ST_MLineFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format: <code>ST_MPolyFromText (Wkt:string)</code> <code>ST_MPolyFromText (Wkt:string, srid:integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X:decimal, Y:decimal, Z:decimal, M:decimal)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X:decimal, Y:decimal)</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ or ST_PointZM to create 3D points.</p> <p>SQL example:</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code></p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointz_1","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal)</code></p> <p>Format: <code>ST_PointZ (X:decimal, Y:decimal, Z:decimal, srid:integer)</code></p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format: <code>ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal)</code></p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/snowflake/vector-data/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text:string, Delimiter:char)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/","title":"Function (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_3DDistance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry, position: integer)</code></p> <p>Format: <code>ST_AddPoint(geom: geometry, point: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText('Linestring(0 0, 1 1, 1 0)'), ST_GeomFromText('Point(21 52)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException.</p> <p>Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Area(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Format: <code>ST_AreaSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((35 34, 30 28, 34 25, 35 34))'))\n</code></pre> <p>Output: <code>201824850811.76245</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsBinary(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKB (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsEWKB(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID</p> <p>Format: <code>ST_AsEWKT (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsEWKT(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>The type parameter takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A:geometry)</code></p> <p><code>ST_AsGeoJSON (A:geometry, type: String)</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n      \"type\":\"Feature\",\n      \"geometry\": {\n          \"type\":\"Polygon\",\n          \"coordinates\":[\n            [[1.0,1.0],\n              [8.0,1.0],\n              [8.0,8.0],\n              [1.0,8.0],\n              [1.0,1.0]]\n          ]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsGML(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsKML(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry</p> <p>Format: <code>ST_AsText (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output: <code>3.141592653589793</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Boundary(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output: <code>LINESTRING (1 1, 0 0, -1 1, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>Spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : The option <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/snowflake/vector-data/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_BuildArea(\n    ST_GeomFromText('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Centroid(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction:</p> <p>Build an appropriate <code>Geometry</code>, <code>MultiGeometry</code>, or <code>GeometryCollection</code> to contain the <code>Geometry</code>s in it. For example:</p> <ul> <li>If <code>geomList</code> contains a single <code>Polygon</code>, the <code>Polygon</code> is returned.</li> <li>If <code>geomList</code> contains several <code>Polygon</code>s, a <code>MultiPolygon</code> is returned.</li> <li>If <code>geomList</code> contains some <code>Polygon</code>s and some <code>LineString</code>s, a <code>GeometryCollection</code> is returned.</li> <li>If <code>geomList</code> is empty, an empty <code>GeometryCollection</code> is returned.</li> </ul> <p>Note that this method does not \"flatten\" Geometries in the input, and hence if any MultiGeometries are contained in the input, a GeometryCollection containing them will be returned.</p> <p>Format</p> <p><code>ST_Collect(*geom: geometry)</code></p> <p>Example:</p> <pre><code>WITH src_tbl AS (\n    SELECT sedona.ST_GeomFromText('POINT (40 10)') AS geom\n    UNION\n    SELECT sedona.ST_GeomFromText('LINESTRING (0 5, 0 10)') AS geom\n)\nSELECT sedona.ST_AsText(collection)\nFROM src_tbl,\n     TABLE(sedona.ST_Collect(src_tbl.geom) OVER (PARTITION BY 1));\n</code></pre> <p>Result:</p> <pre><code>GEOMETRYCOLLECTION (POINT (40 10), LINESTRING (0 5, 0 10))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format: <code>ST_CollectionExtract (A:geometry)</code></p> <p>Format: <code>ST_CollectionExtract (A:geometry, type:Int)</code></p> <p>Example:</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float)</code></p> <p>Format: <code>ST_ConcaveHull (A:geometry, pctConvex:float, allowHoles:Boolean)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ConcaveHull(polygondf.countyshape, pctConvex)`\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ConvexHull(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/snowflake/vector-data/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Distance(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSphere (A:geometry)</code></p> <p>SQL example 1:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre> <p>Output: <code>543796.9506134904</code></p> <p>SQL example 2:</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'), 6378137.0)\n</code></pre> <p>Output: <code>544405.4459192449</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Format: <code>ST_DistanceSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (51.3168 -0.56)'), ST_GeomFromWKT('POINT (55.9533 -3.1883)'))\n</code></pre> <p>Output: <code>544430.9411996207</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: This function takes a GeometryCollection/Multi Geometry object and returns a set of geometries containing the individual geometries that make up the input geometry. The function is useful for breaking down a GeometryCollection/Multi Geometry into its constituent geometries.</p> <p>Format: <code>ST_Dump(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT sedona.ST_AsText(geom)\nFROM table(sedona.ST_Dump(sedona.ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))')));\n</code></pre> <p>Output:</p> <pre><code>POINT (10 40)\nPOINT (40 30)\nPOINT (20 20)\nPOINT (30 10)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns a MultiPoint geometry which consists of individual points that compose the input line string.</p> <p>Format: <code>ST_DumpPoints(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output: <code>MultiPoint ((0 0), (0 1), (1 1), (1 0), (0 0))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output: <code>POINT(160 170)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: <code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_FlipCoordinates(df.geometry)\nFROM df\n</code></pre> <p>Input: <code>POINT (1 2)</code></p> <p>Output: <code>POINT (2 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))                   |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry, zValue)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Force3D(geometry) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 0, 1 2 0, 2 1 0)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <pre><code>SELECT ST_Force3D(geometry, 2.3) AS geom\n</code></pre> <p>Input: <code>LINESTRING(0 1, 1 2, 2 1)</code></p> <p>Output: <code>LINESTRING Z(0 1 2.3, 1 2 2.3, 2 1 2.3)</code></p> <p>Input: <code>POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Output: <code>POLYGON Z((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))</code></p> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: geometry, precision: int)</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geohash                      |\n+-----------------------------+\n|u3r0p                        |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer, failIfNotConverged: boolean)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float, maxIter: integer)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry, tolerance: float)</code></p> <p>Format: <code>ST_GeometricMedian(geom: geometry)</code></p> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Example:</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: geometry, n: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output: <code>POINT (3 4)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeometryType(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: geometry, n: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output: <code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Intersection(polygondf.countyshape, polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsEmpty(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText('LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)'))\n</code></pre> <p>Output: <code>true</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_IsSimple(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvaliddetail","title":"ST_IsValidDetail","text":"<p>Introduction: Returns a row, containing a boolean <code>valid</code> stating if a geometry is valid, a string <code>reason</code> stating why it is invalid and a geometry <code>location</code> pointing out where it is invalid.</p> <p>This function is a combination of ST_IsValid and ST_IsValidReason.</p> <p>The flags parameter is a bitfield with the following options:</p> <ul> <li>0: Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Format:</p> <pre><code>SELECT valid, reason, Sedonm.ST_AsText(location) AS location\nFROM table(Sedona.ST_IsValidDetail(geom: Geometry, flag: Integer))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT valid, reason, Sedonm.ST_AsText(location) AS location\n     FROM table(Sedona.ST_IsValidDetail(Sedona.ST_GeomFromWKT('POLYGON ((30 10, 40 40, 20 40, 30 10, 10 20, 30 10))'), 0))\n</code></pre> <p>Output:</p> <pre><code>+-----+---------------------------------------------------------+-------------+\n|valid|reason                                                   |location     |\n+-----+---------------------------------------------------------+-------------+\n|false|Ring Self-intersection at or near point (30.0, 10.0, NaN)|POINT (30 10)|\n+-----+---------------------------------------------------------+-------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length (A:geometry)</p> <p>SQL example:</p> <pre><code>SELECT ST_Length(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>SQL example:</p> <pre><code>SELECT ST_Length2D(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lat/lon order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Warning</p> <p>This function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (10 40, 40 30, 20 20, 30 10)                        |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: geometry, fraction: Double)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2) as Interpolated\n</code></pre> <p>Output:</p> <pre><code>+-----------------------------------------+\n|Interpolated                             |\n+-----------------------------------------+\n|POINT (51.5974135047432 76.5974135047432)|\n+-----------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A:geometry)</code></p> <pre><code>SELECT ST_LineMerge(geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format: <code>ST_LineSubstring (geom: geometry, startfraction: Double, endfraction: Double)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666) as Substring\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------------------------------------+\n|Substring                                                                                       |\n+------------------------------------------------------------------------------------------------+\n|LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)|\n+------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: Geometry)</code> This Geometry must be a GeometryCollection of the geometry types listed above.</p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes. The holes must be a MultiLinestring.</p> <p>Format: <code>ST_MakePolygon(geom: geometry, holes: &lt;geometry&gt;)</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT\n    ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ST_GeomFromText('MultiLINESTRING((6 2, 8 2, 8 1, 6 1, 6 2))')\n    ) AS polygon\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------+\n|polygon                                                         |\n+----------------------------------------------------------------+\n|POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))|\n+----------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format: <code>ST_MakeValid (A:geometry)</code></p> <p>Format: <code>ST_MakeValid (A:geometry, keepCollapsed:Boolean)</code></p> <p>SQL example:</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different. Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/snowflake/vector-data/Function/#st_maximuminscribedcircle","title":"ST_MaximumInscribedCircle","text":"<p>Introduction: Finds the largest circle that is contained within a (multi)polygon, or which does not overlap any lines and points. Returns a row with fields:</p> <ul> <li><code>center</code> - center point of the circle</li> <li><code>nearest</code> - nearest point from the center of the circle</li> <li><code>radius</code> - radius of the circle</li> </ul> <p>For polygonal geometries, the function inscribes the circle within the boundary rings, treating internal rings as additional constraints. When processing linear and point inputs, the algorithm inscribes the circle within the convex hull of the input, utilizing the input lines and points as additional boundary constraints.</p> <p>Format: <code>ST_MaximumInscribedCircle(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT Sedona.ST_AsText(center) AS center, Sedona.ST_AsText(nearest) AS nearest, radius  FROM table(\n    SELECT ST_MaximumIncribedCircle(ST_GeomFromWKT('POLYGON ((62.11 19.68, 60.79 17.20, 61.30 15.96, 62.11 16.08, 65.93 16.95, 66.20 20.61, 63.08 21.43, 64.48 18.70, 62.11 19.68))'))\n)\n</code></pre> <p>Output:</p> <pre><code>+---------------------------------------------+-------------------------------------------+------------------+\n|center                                       |nearest                                    |radius            |\n+---------------------------------------------+-------------------------------------------+------------------+\n|POINT (62.794975585937514 17.774780273437496)|POINT (63.36773534817729 19.15992378007859)|1.4988916836219184|\n+---------------------------------------------+-------------------------------------------+------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingCircle(geom: geometry, [Optional] quadrantSegments:int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns two columns containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT sedona.ST_AsText(center), radius\nFROM table(sedona.ST_MinimumBoundingRadius(sedona.ST_GeomFromText('POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))')))\n</code></pre> <p>Result:</p> <pre><code>POINT (0.5 0.5), 0.7071067811865476\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format</p> <p><code>ST_Multi(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Multi(\n    ST_GeomFromText('POINT(1 1)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT (1 1)                                               |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: geometry)</code></p> <p>SQL example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output: <code>3</code></p> <p>SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format</p> <p><code>ST_Normalize(geom: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))'))) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------------+\n|geom                               |\n+-----------------------------------+\n|POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))|\n+-----------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A:geometry)</code></p> <pre><code>SELECT ST_NPoints(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: geometry)</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A:geometry)</code></p> <pre><code>SELECT ST_NumGeometries(df.geometry)\nFROM df\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output: <code>1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Format: <code>ST_NumPoints(geom: geometry)</code></p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown.</p> <p>SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre> <p>Output: <code>3</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: geometry, n: integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_PointN(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'), 2) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|POINT (1 2)                                                    |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A:geometry)</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format: <code>ST_RemovePoint(geom: geometry, position: integer)</code></p> <p>Format: <code>ST_RemovePoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'), 1)\n</code></pre> <p>Output: <code>LINESTRING(0 0, 1 0)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(\n    ST_Reverse(ST_GeomFromText('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|LINESTRING (3 6, 2 4, 1 2, 0 0)                                |\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: geometry, level: Int)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------------------------------------------------------------------+\n|st_s2cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)                                                                 |\n+------------------------------------------------------------------------------------------------------------------------------+\n|[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]|\n+------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: geometry, index: integer, point: geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)')) AS geom\n</code></pre> <p>Result:</p> <pre><code>+--------------------------+\n|geom                      |\n+--------------------------+\n|LINESTRING (0 0, 0 1, 1 0)|\n+--------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A:geometry, srid: Integer)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_SetSRID(polygondf.countyshape, 3021)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Format: <code>ST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)</code></p> <pre><code>SELECT ST_SimplifyPreserveTopology(polygondf.countyshape, 10.0)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n            ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Format: <code>ST_Split (input: geometry, blade: geometry)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Split(\n    ST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\n    ST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre> <p>Output: <code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_SRID(polygondf.countyshape)\nFROM polygondf\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output: <code>POINT(100 150)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns a multi-geometry divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: geometry, maxVertices: int)</code></p> <p>SQL example:</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_SubDivide(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((0 0, 5 5), (5 5, 10 10), (10 10, 21 21), (21 21, 60 60), (60 60, 85 85), (85 85, 100 100), (100 100, 120 120))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of a multi-geometry.</p> <p>Format: <code>SELECT SEDONA.ST_AsText(GEOM) FROM table(SEDONA.ST_SubDivideExplode(geom: geometry, maxVertices: int))</code></p> <p>Example:</p> <p>Query:</p> <pre><code>SELECT Sedona.ST_AsText(GEOM)\nFROM table(Sedona.ST_SubDivideExplode(Sedona.ST_GeomFromText('LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)'), 5));\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Result:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available.</p> <p>Note</p> <p>By default, this function uses lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>Note</p> <p>If ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.</p> <p>Format: <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string ,[Optional] DisableError)</code></p> <p>SQL example (simple):</p> <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857')\nFROM polygondf\n</code></pre> <p>SQL example (with optional parameters):</p> <pre><code>SELECT ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857', false)\nFROM polygondf\n</code></pre> <p>Note</p> <p>The detailed EPSG information can be searched on EPSG.io.</p>"},{"location":"api/snowflake/vector-data/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format: <code>ST_Translate(geometry: geometry, deltaX: deltaX, deltaY: deltaY, deltaZ: deltaZ)</code></p> <p>Example:</p> <p>Input: <code>ST_Translate(GEOMETRYCOLLECTION(MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0)), ((1 2, 3 4, 3 5, 1 2))), POINT(1, 1, 1), LINESTRING EMPTY), 2, 2, 3)</code></p> <p>Output: <code>GEOMETRYCOLLECTION(MULTIPOLYGON (((3 2, 3 3, 4 3, 4 2, 3 2)), ((3 4, 5 6, 5 7, 3 4))), POINT(3, 3, 4), LINESTRING EMPTY)</code></p> <p>Input: <code>ST_Translate(POINT(1, 3, 2), 1, 2)</code></p> <p>Output: <code>POINT(2, 5, 2)</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_union","title":"ST_Union","text":"<p>Introduction: Return the union of geometry A and B</p> <p>Format: <code>ST_Union (A:geometry, B:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>SQL Example:</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/snowflake/vector-data/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output: <code>0.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMax(df.geometry) AS xmax\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>2</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A:geometry)</code></p> <p>Example:</p> <pre><code>SELECT ST_XMin(df.geometry) AS xmin\nFROM df\n</code></pre> <p>Input: <code>POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))</code></p> <p>Output: <code>-1</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output: <code>25.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: 2</p>"},{"location":"api/snowflake/vector-data/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output : 0</p>"},{"location":"api/snowflake/vector-data/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output: <code>11.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output: <code>1.0</code></p>"},{"location":"api/snowflake/vector-data/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output: <code>4.0</code></p>"},{"location":"api/snowflake/vector-data/Overview/","title":"Overview (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows.</p> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates</li> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> <li>Function: Execute a function on the given column or columns</li> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> <li>Aggregate function: Return the aggregated value on the given column</li> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> <li>Predicate: Execute a logic judgement on the given columns and return true or false</li> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul>"},{"location":"api/snowflake/vector-data/Predicate/","title":"Predicate (Snowflake)","text":"<p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"api/snowflake/vector-data/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Crosses(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Disjoinnt(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'. This function essentially checks if the shortest distance between the envelope of the two geometries is &lt;= the provided distance.</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Equals(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>SQL example 1:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output: <code>true</code></p> <p>SQL example 2:</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output: <code>false</code></p>"},{"location":"api/snowflake/vector-data/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM geom\nWHERE ST_Overlaps(geom.geom_a, geom.geom_b)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A:geometry, B:geometry)</code></p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Touches(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_Covers(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n</code></pre>"},{"location":"api/snowflake/vector-data/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A:geometry, B:geometry)</code></p> <p>SQL example:</p> <pre><code>SELECT *\nFROM pointdf\nWHERE ST_CoveredBy(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))\n</code></pre>"},{"location":"api/sql/AggregateFunction/","title":"Aggregate function","text":""},{"location":"api/sql/AggregateFunction/#st_envelope_aggr","title":"ST_Envelope_Aggr","text":"<p>Introduction: Return the entire envelope boundary of all geometries in A</p> <p>Format: <code>ST_Envelope_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Envelope_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.1 101.1, 1.1 120.1, 20.1 120.1, 20.1 101.1, 1.1 101.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_intersection_aggr","title":"ST_Intersection_Aggr","text":"<p>Introduction: Return the polygon intersection of all polygons in A</p> <p>Format: <code>ST_Intersection_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersection_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/AggregateFunction/#st_union_aggr","title":"ST_Union_Aggr","text":"<p>Introduction: Return the polygon union of all polygons in A</p> <p>Format: <code>ST_Union_Aggr (A: geometryColumn)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union_Aggr(ST_GeomFromText('MULTIPOINT(1.1 101.1,2.1 102.1,3.1 103.1,4.1 104.1,5.1 105.1,6.1 106.1,7.1 107.1,8.1 108.1,9.1 109.1,10.1 110.1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((1.1 101.1), (2.1 102.1), (3.1 103.1), (4.1 104.1), (5.1 105.1), (6.1 106.1), (7.1 107.1), (8.1 108.1), (9.1 109.1), (10.1 110.1))\n</code></pre>"},{"location":"api/sql/Constructor/","title":"Constructor","text":""},{"location":"api/sql/Constructor/#st_geomcollfromtext","title":"ST_GeomCollFromText","text":"<p>Introduction: Constructs a GeometryCollection from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>GEOMETRYCOLLECTION</code>.</p> <p>Format:</p> <p><code>ST_GeomCollFromText (Wkt: String)</code></p> <p><code>ST_GeomCollFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeomCollFromText('GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))')\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (50 50), LINESTRING (20 30, 40 60, 80 90), POLYGON ((30 10, 40 20, 30 20, 30 10), (35 15, 45 15, 40 25, 35 15)))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromewkb","title":"ST_GeomFromEWKB","text":"<p>Introduction: Construct a Geometry from EWKB string or Binary. This function is an alias of ST_GeomFromWKB.</p> <p>Format:</p> <p><code>ST_GeomFromEWKB (Wkb: String)</code></p> <p><code>ST_GeomFromEWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromEWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromEWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromewkt","title":"ST_GeomFromEWKT","text":"<p>Introduction: Construct a Geometry from OGC Extended WKT</p> <p>Format: <code>ST_GeomFromEWKT (EWkt: String)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_AsText(ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgml","title":"ST_GeomFromGML","text":"<p>Introduction: Construct a Geometry from GML.</p> <p>Note</p> <p>This function only supports GML 1 and GML 2. GML 3 is not supported.</p> <p>Format: <code>ST_GeomFromGML (gml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromGML('\n    &lt;gml:LineString srsName=\"EPSG:4269\"&gt;\n        &lt;gml:coordinates&gt;\n            -71.16028,42.258729\n            -71.160837,42.259112\n            -71.161143,42.25932\n        &lt;/gml:coordinates&gt;\n    &lt;/gml:LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.16028 42.258729, -71.160837 42.259112, -71.161143 42.25932)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeohash","title":"ST_GeomFromGeoHash","text":"<p>Introduction: Create Geometry from geohash string and optional precision</p> <p>Format: <code>ST_GeomFromGeoHash(geohash: String, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.703125 0.87890625, 0.703125 1.0546875, 1.0546875 1.0546875, 1.0546875 0.87890625, 0.703125 0.87890625))\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON","text":"<p>Note</p> <p>This method is not recommended. Please use Sedona GeoJSON data source to read GeoJSON files.</p> <p>Introduction: Construct a Geometry from GeoJson</p> <p>Format: <code>ST_GeomFromGeoJSON (GeoJson: String)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Feature\",\n   \"properties\":{\n      \"STATEFP\":\"01\",\n      \"COUNTYFP\":\"077\",\n      \"TRACTCE\":\"011501\",\n      \"BLKGRPCE\":\"5\",\n      \"AFFGEOID\":\"1500000US010770115015\",\n      \"GEOID\":\"010770115015\",\n      \"NAME\":\"5\",\n      \"LSAD\":\"BG\",\n      \"ALAND\":6844991,\n      \"AWATER\":32636\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [-87.621765, 34.873444],\n            [-87.617535, 34.873369],\n            [-87.62119, 34.85053],\n            [-87.62144, 34.865379],\n            [-87.621765, 34.873444]\n         ]\n      ]\n   }\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromGeoJSON('{\n   \"type\":\"Polygon\",\n   \"coordinates\":[\n      [\n         [-87.621765, 34.873444],\n         [-87.617535, 34.873369],\n         [-87.62119, 34.85053],\n         [-87.62144, 34.865379],\n         [-87.621765, 34.873444]\n      ]\n   ]\n}')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\n</code></pre> <p>Warning</p> <p>The way that SedonaSQL reads GeoJSON is different from that in SparkSQL</p>"},{"location":"api/sql/Constructor/#st_geomfromkml","title":"ST_GeomFromKML","text":"<p>Introduction: Construct a Geometry from KML.</p> <p>Format: <code>ST_GeomFromKML (kml: String)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_GeomFromKML('\n    &lt;LineString&gt;\n        &lt;coordinates&gt;\n            -71.1663,42.2614\n            -71.1667,42.2616\n        &lt;/coordinates&gt;\n    &lt;/LineString&gt;\n')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-71.1663 42.2614, -71.1667 42.2616)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromtext","title":"ST_GeomFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeomFromText (Wkt: String)</code></p> <p><code>ST_GeomFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkb","title":"ST_GeomFromWKB","text":"<p>Introduction: Construct a Geometry from WKB string or Binary. This function also supports EWKB format.</p> <p>Format:</p> <p><code>ST_GeomFromWKB (Wkb: String)</code></p> <p><code>ST_GeomFromWKB (Wkb: Binary)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_asEWKT(ST_GeomFromWKB('01010000a0e6100000000000000000f03f000000000000f03f000000000000f03f'))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT Z(1 1 1)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geomfromwkt","title":"ST_GeomFromWKT","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_GeomFromWKT (Wkt: String)</code></p> <p><code>ST_GeomFromWKT (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>The optional srid parameter was added in <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeomFromWKT('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_geometryfromtext","title":"ST_GeometryFromText","text":"<p>Introduction: Construct a Geometry from WKT. If SRID is not set, it defaults to 0 (unknown). Alias of ST_GeomFromWKT</p> <p>Format:</p> <p><code>ST_GeometryFromText (Wkt: String)</code></p> <p><code>ST_GeometryFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryFromText('POINT(40.7128 -74.0060)')\n</code></pre> <p>Output:</p> <pre><code>POINT(40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromtext","title":"ST_LineFromText","text":"<p>Introduction: Construct a Line from Wkt text</p> <p>Format: <code>ST_LineFromText (Wkt: String)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineFromText('LINESTRING(1 2,3 4)')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 2, 3 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linefromwkb","title":"ST_LineFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LineFromWKB (Wkb: String)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary)</code></p> <p><code>ST_LineFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LineFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LineFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromtext","title":"ST_LineStringFromText","text":"<p>Introduction: Construct a LineString from Text, delimited by Delimiter</p> <p>Format: <code>ST_LineStringFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineStringFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794', ',')\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794)\n</code></pre>"},{"location":"api/sql/Constructor/#st_linestringfromwkb","title":"ST_LinestringFromWKB","text":"<p>Introduction: Construct a LineString geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format and it is an alias of ST_LineFromWKB.</p> <p>Note</p> <p>Returns null if geometry is not of type LineString.</p> <p>Format:</p> <p><code>ST_LinestringFromWKB (Wkb: String)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary)</code></p> <p><code>ST_LinestringFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_LinestringFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_LinestringFromWKB([01 02 00 00 00 02 00 00 00 00 00 00 00 84 D6 00 C0 00 00 00 00 80 B5 D6 BF 00 00 00 60 E1 EF F7 BF 00 00 00 80 07 5D E5 BF])\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-2.1047439575195312 -0.354827880859375, -1.49606454372406 -0.6676061153411865)\n</code></pre>"},{"location":"api/sql/Constructor/#st_makeenvelope","title":"ST_MakeEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY, and an optional SRID.</p> <p>Format:</p> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)\n</code></pre> <pre><code>ST_MakeEnvelope(MinX: Double, MinY: Double, MaxX: Double, MaxY: Double, srid: Integer)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakeEnvelope(1.234, 2.234, 3.345, 3.345, 4236)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mlinefromtext","title":"ST_MLineFromText","text":"<p>Introduction: Construct a MultiLineString from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MLineFromText (Wkt: String)</code></p> <p><code>ST_MLineFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MLineFromText('MULTILINESTRING((1 2, 3 4), (4 5, 6 7))')\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((1 2, 3 4), (4 5, 6 7))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mpointfromtext","title":"ST_MPointFromText","text":"<p>Introduction: Constructs a MultiPoint from the WKT with the given SRID. If SRID is not provided then it defaults to 0. It returns <code>null</code> if the WKT is not a <code>MULTIPOINT</code>.</p> <p>Format:</p> <p><code>ST_MPointFromText (Wkt: String)</code></p> <p><code>ST_MPointFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MPointFromText('MULTIPOINT ((10 10), (20 20), (30 30))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((10 10), (20 20), (30 30))\n</code></pre>"},{"location":"api/sql/Constructor/#st_mpolyfromtext","title":"ST_MPolyFromText","text":"<p>Introduction: Construct a MultiPolygon from Wkt. If srid is not set, it defaults to 0 (unknown).</p> <p>Format:</p> <p><code>ST_MPolyFromText (Wkt: String)</code></p> <p><code>ST_MPolyFromText (Wkt: String, srid: Integer)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MPolyFromText('MULTIPOLYGON(((0 0 1,20 0 1,20 20 1,0 20 1,0 0 1),(5 5 3,5 7 3,7 7 3,7 5 3,5 5 3)))')\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((0 0, 20 0, 20 20, 0 20, 0 0), (5 5, 5 7, 7 7, 7 5, 5 5)))\n</code></pre>"},{"location":"api/sql/Constructor/#st_makepoint","title":"ST_MakePoint","text":"<p>Introduction: Creates a 2D, 3D Z or 4D ZM Point geometry. Use ST_MakePointM to make points with XYM coordinates. Z and M values are optional.</p> <p>Format: <code>ST_MakePoint (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456));\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567));\n</code></pre> <p>Output:</p> <pre><code>POINT Z (1.2345 2.3456 3.4567)\n</code></pre> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.2345, 2.3456, 3.4567, 4));\n</code></pre> <p>Output:</p> <pre><code>POINT ZM (1.2345 2.3456 3.4567 4)\n</code></pre>"},{"location":"api/sql/Constructor/#st_makepointm","title":"ST_MakePointM","text":"<p>Introduction: Creates a point with X, Y, and M coordinate. Use ST_MakePoint to make points with XY, XYZ, or XYZM coordinates.</p> <p>Format: <code>ST_MakePointM(x: Double, y: Double, m: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_MakePointM(1, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>Point M(1 2 3)\n</code></pre>"},{"location":"api/sql/Constructor/#st_point","title":"ST_Point","text":"<p>Introduction: Construct a Point from X and Y</p> <p>Format: <code>ST_Point (X: Double, Y: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>In <code>v1.4.0</code> an optional Z parameter was removed to be more consistent with other spatial SQL implementations. If you are upgrading from an older version of Sedona - please use ST_PointZ to create 3D points.</p> <p>SQL Example</p> <pre><code>SELECT ST_Point(double(1.2345), 2.3456)\n</code></pre> <p>Output:</p> <pre><code>POINT (1.2345 2.3456)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromgeohash","title":"ST_PointFromGeoHash","text":"<p>Introduction: Generates a Point geometry representing the center of the GeoHash cell defined by the input string. If <code>precision</code> is not specified, the full GeoHash precision is used. Providing a <code>precision</code> value limits the GeoHash characters used to determine the Point coordinates.</p> <p>Format: <code>ST_PointFromGeoHash(geoHash: String, precision: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromGeoHash('s00twy01mt', 4)\n</code></pre> <p>Output:</p> <pre><code>POINT (0.87890625 0.966796875)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromtext","title":"ST_PointFromText","text":"<p>Introduction: Construct a Point from Text, delimited by Delimiter</p> <p>Format: <code>ST_PointFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointFromText('40.7128,-74.0060', ',')\n</code></pre> <p>Output:</p> <pre><code>POINT (40.7128 -74.006)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointz","title":"ST_PointZ","text":"<p>Introduction: Construct a Point from X, Y and Z and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z coordinate.</p> <p>Format:</p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double)</code></p> <p><code>ST_PointZ (X: Double, Y: Double, Z: Double, srid: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZ(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT Z(1.2345 2.3456 3.4567)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointm","title":"ST_PointM","text":"<p>Introduction: Construct a Point from X, Y and M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointM (X: Double, Y: Double, M: Double)</code></p> <p><code>ST_PointM (X: Double, Y: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_AsEWKT(ST_PointM(1.2345, 2.3456, 3.4567))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 0 3.4567)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointzm","title":"ST_PointZM","text":"<p>Introduction: Construct a Point from X, Y, Z, M and an optional srid. If srid is not set, it defaults to 0 (unknown). Must use ST_AsEWKT function to print the Z and M coordinates.</p> <p>Format:</p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double)</code></p> <p><code>ST_PointZM (X: Double, Y: Double, Z: Double, M: Double, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_PointZM(1.2345, 2.3456, 3.4567, 100))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1.2345 2.3456 3.4567, 100)\n</code></pre>"},{"location":"api/sql/Constructor/#st_pointfromwkb","title":"ST_PointFromWKB","text":"<p>Introduction: Construct a Point geometry from WKB string or Binary and an optional SRID. This function also supports EWKB format.</p> <p>Note</p> <p>Returns null if geometry is not of type Point.</p> <p>Format:</p> <p><code>ST_PointFromWKB (Wkb: String)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary)</code></p> <p><code>ST_PointFromWKB (Wkb: String, srid: Integer)</code></p> <p><code>ST_PointFromWKB (Wkb: Binary, srid: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>Example:</p> <pre><code>SELECT ST_PointFromWKB([01 01 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 2e 40])\n</code></pre> <p>Output:</p> <pre><code>POINT (10 15)\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromenvelope","title":"ST_PolygonFromEnvelope","text":"<p>Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY.</p> <p>Format:</p> <p><code>ST_PolygonFromEnvelope (MinX: Double, MinY: Double, MaxX: Double, MaxY: Double)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PolygonFromEnvelope(double(1.234),double(2.234),double(3.345),double(3.345))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((1.234 2.234, 1.234 3.345, 3.345 3.345, 3.345 2.234, 1.234 2.234))\n</code></pre>"},{"location":"api/sql/Constructor/#st_polygonfromtext","title":"ST_PolygonFromText","text":"<p>Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed</p> <p>Format: <code>ST_PolygonFromText (Text: String, Delimiter: Char)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PolygonFromText('-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969', ',')\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-74.0428197 40.6867969, -74.0421975 40.6921336, -74.050802 40.6912794, -74.0428197 40.6867969))\n</code></pre>"},{"location":"api/sql/DataFrameAPI/","title":"DataFrame Style functions","text":"<p>Sedona SQL functions can be used in a DataFrame style API similar to Spark functions.</p> <p>The following objects contain the exposed functions: <code>org.apache.spark.sql.sedona_sql.expressions.st_functions</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_constructors</code>, <code>org.apache.spark.sql.sedona_sql.expressions.st_predicates</code>, and <code>org.apache.spark.sql.sedona_sql.expressions.st_aggregates</code>.</p> <p>Every function can take all <code>Column</code> arguments. Additionally, overloaded forms can commonly take a mix of <code>String</code> and other Scala types (such as <code>Double</code>) as arguments.</p> <p>In general the following rules apply (although check the documentation of specific functions for any exceptions):</p> ScalaPython <ol> <li>Every function returns a <code>Column</code> so that it can be used interchangeably with Spark functions as well as <code>DataFrame</code> methods such as <code>DataFrame.select</code> or <code>DataFrame.join</code>.</li> <li>Every function has a form that takes all <code>Column</code> arguments. These are the most versatile of the forms.</li> <li>Most functions have a form that takes a mix of <code>String</code> arguments with other Scala types.</li> </ol> <ol> <li><code>Column</code> type arguments are passed straight through and are always accepted.</li> <li><code>str</code> type arguments are always assumed to be names of columns and are wrapped in a <code>Column</code> to support that. If an actual string literal needs to be passed then it will need to be wrapped in a <code>Column</code> using <code>pyspark.sql.functions.lit</code>.</li> <li>Any other types of arguments are checked on a per function basis. Generally, arguments that could reasonably support a python native type are accepted and passed through.   4. Shapely <code>Geometry</code> objects are not currently accepted in any of the functions.</li> </ol> <p>The exact mixture of argument types allowed is function specific. However, in these instances, all <code>String</code> arguments are assumed to be the names of columns and will be wrapped in a <code>Column</code> automatically. Non-<code>String</code> arguments are assumed to be literals that are passed to the sedona function. If you need to pass a <code>String</code> literal then you should use the all <code>Column</code> form of the sedona function and wrap the <code>String</code> literal in a <code>Column</code> with the <code>lit</code> Spark function.</p> <p>A short example of using this API (uses the <code>array_min</code> and <code>array_max</code> Spark functions):</p> ScalaPython <pre><code>val values_df = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\nval min_value = array_min(\"values\")\nval max_value = array_max(\"values\")\nval point_df = values_df.select(ST_Point(min_value, max_value).as(\"point\"))\n</code></pre> <pre><code>from pyspark.sql import functions as f\n\nfrom sedona.spark import *\n\ndf = spark.sql(\"SELECT array(0.0, 1.0, 2.0) AS values\")\n\nmin_value = f.array_min(\"values\")\nmax_value = f.array_max(\"values\")\n\ndf = df.select(ST_Point(min_value, max_value).alias(\"point\"))\n</code></pre> <p>The above code will generate the following dataframe:</p> <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (0 2)|\n+-----------+\n</code></pre> <p>Some functions will take native python values and infer them as literals. For example:</p> <pre><code>from sedona.spark import *\n\ndf = df.select(ST_Point(1.0, 3.0).alias(\"point\"))\n</code></pre> <p>This will generate a dataframe with a constant point in a column:</p> <pre><code>+-----------+\n|point      |\n+-----------+\n|POINT (1 3)|\n+-----------+\n</code></pre>"},{"location":"api/sql/Function/","title":"Function","text":""},{"location":"api/sql/Function/#geometrytype","title":"GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. Eg: 'LINESTRING', 'POLYGON', 'MULTIPOINT', etc. This function also indicates if the geometry is measured, by returning a string of the form 'POINTM'.</p> <p>Format: <code>GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n LINESTRING\n</code></pre> <pre><code>SELECT GeometryType(ST_GeomFromText('POINTM(0 0 1)'));\n</code></pre> <p>Output:</p> <pre><code> geometrytype\n--------------\n POINTM\n</code></pre>"},{"location":"api/sql/Function/#st_3ddistance","title":"ST_3DDistance","text":"<p>Introduction: Return the 3-dimensional minimum cartesian distance between A and B</p> <p>Format: <code>ST_3DDistance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_3DDistance(ST_GeomFromText(\"POINT Z (0 0 -5)\"),\n                     ST_GeomFromText(\"POINT Z(1  1 -6\"))\n</code></pre> <p>Output:</p> <pre><code>1.7320508075688772\n</code></pre>"},{"location":"api/sql/Function/#st_addmeasure","title":"ST_AddMeasure","text":"<p>Introduction: Computes a new geometry with measure (M) values linearly interpolated between start and end points. For geometries lacking M dimensions, M values are added. Existing M values are overwritten by the new values. Applies only to LineString and MultiLineString inputs.</p> <p>Format: <code>ST_AddMeasure(geom: Geometry, measureStart: Double, measureEnd: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_AddMeasure(\n        ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)')\n))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 0 10, 1 0 16, 2 0 22, 3 0 28, 4 0 34, 5 0 40)\n</code></pre>"},{"location":"api/sql/Function/#st_addpoint","title":"ST_AddPoint","text":"<p>Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.</p> <p>Format:</p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry, position: Integer)</code></p> <p><code>ST_AddPoint(geom: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AddPoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"), 1)\n\nSELECT ST_AddPoint(ST_GeomFromText(\"Linestring(0 0, 1 1, 1 0)\"), ST_GeomFromText(\"Point(21 52)\"))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n</code></pre>"},{"location":"api/sql/Function/#st_affine","title":"ST_Affine","text":"<p>Introduction: Apply an affine transformation to the given geometry.</p> <p>ST_Affine has 2 overloaded signatures:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <p>Based on the invoked function, the following transformation is applied:</p> <p><code>x = a * x + b * y + c * z + xOff OR x = a * x + b * y + xOff</code></p> <p><code>y = d * x + e * y + f * z + yOff OR y = d * x + e * y + yOff</code></p> <p><code>z = g * x + f * y + i * z + zOff OR z = g * x + f * y + zOff</code></p> <p>If the given geometry is empty, the result is also empty.</p> <p>Format:</p> <p><code>ST_Affine(geometry, a, b, c, d, e, f, g, h, i, xOff, yOff, zOff)</code></p> <p><code>ST_Affine(geometry, a, b, d, e, xOff, yOff)</code></p> <pre><code>ST_Affine(geometry, 1, 2, 4, 1, 1, 2, 3, 2, 5, 4, 8, 3)\n</code></pre> <p>Input: <code>LINESTRING EMPTY</code></p> <p>Output: <code>LINESTRING EMPTY</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((9 11 11, 11 12 13, 18 16 23, 9 11 11))</code></p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5))</code></p> <p>Output: <code>POLYGON((5 9, 7 10, 8 11, 6 10, 5 9), (6 9.5, 6.5 9.75, 7 10.25, 6.5 10, 6 9.5))</code></p> <pre><code>ST_Affine(geometry, 1, 2, 1, 2, 1, 2)\n</code></pre> <p>Input: <code>POLYGON EMPTY</code></p> <p>Output: <code>POLYGON EMPTY</code></p> <p>Input: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((1 0, 1 1, 2 1, 2 0, 1 0), (1 0.5, 1 0.75, 1.5 0.75, 1.5 0.5, 1 0.5)), ((5 0, 5 5, 7 5, 7 0, 5 0))), POINT (10 10))</code></p> <p>Output: <code>GEOMETRYCOLLECTION (MULTIPOLYGON (((2 3, 4 5, 5 6, 3 4, 2 3), (3 4, 3.5 4.5, 4 5, 3.5 4.5, 3 4)), ((6 7, 16 17, 18 19, 8 9, 6 7))), POINT (31 32))</code></p> <p>Input: <code>POLYGON ((1 0 1, 1 1 1, 2 2 2, 1 0 1))</code></p> <p>Output: <code>POLYGON Z((2 3 1, 4 5 1, 7 8 2, 2 3 1))</code></p>"},{"location":"api/sql/Function/#st_labelpoint","title":"ST_LabelPoint","text":"<p>Introduction: <code>ST_LabelPoint</code> computes and returns a label point for a given polygon or geometry collection. The label point is chosen to be sufficiently far from boundaries of the geometry. For a regular Polygon this will be the centroid.</p> <p>The algorithm is derived from Tippecanoe\u2019s <code>polygon_to_anchor</code>, an approximate solution for label point generation, designed to be faster than optimal algorithms like <code>polylabel</code>. It searches for a \u201cgood enough\u201d label point within a limited number of iterations. For geometry collections, only the largest Polygon by area is considered. While <code>ST_Centroid</code> is a fast algorithm to calculate the center of mass of a (Multi)Polygon, it may place the point outside of the Polygon or near a boundary for concave shapes, polygons with holes, or MultiPolygons.</p> <p><code>ST_LabelPoint</code> takes up to 3 arguments,</p> <ul> <li><code>geometry</code>: input geometry (e.g., a Polygon or GeometryCollection) for which the anchor point is to be calculated.</li> <li><code>gridResolution</code> (Optional, default is 16): Controls the resolution of the search grid for refining the label point. A higher resolution increases the grid density, providing a higher chance of finding a good enough result at the cost of runtime. For example, a gridResolution of 16 divides the bounding box of the polygon into a 16x16 grid.</li> <li><code>goodnessThreshold</code> (Optional, default is 0.2): Determines the minimum acceptable \u201cgoodness\u201d value for the anchor point. Higher thresholds prioritize points farther from boundaries but may require more computation.</li> </ul> <p>Note</p> <ul> <li><code>ST_LabelPoint</code> throws an <code>IllegalArgumentException</code> if the input geometry has an area of zero or less.</li> <li>Holes within polygons are respected. Points within a hole are given a goodness of 0.</li> <li>For GeometryCollections, only the largest polygon by area is considered.</li> </ul> <p>Tip</p> <ul> <li>Use <code>ST_LabelPoint</code> for tasks such as label placement, identifying representative points for polygons, or other spatial analyses where an internal reference point is preferred but not required. If intersection of the point and the original geometry is required, use of an algorithm like <code>polylabel</code> should be considered.</li> <li><code>ST_LabelPoint</code> offers a faster, approximate solution for label point generation, making it ideal for large datasets or real-time applications.</li> </ul> <p>Format:</p> <pre><code>ST_LabelPoint(geometry: Geometry)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer)\n</code></pre> <pre><code>ST_LabelPoint(geometry: Geometry, gridResolution: Integer, goodnessThreshold: Double)\n</code></pre> <p>Since: <code>v1.7.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON((0 0, 4 0, 4 4, 0 4, 0 0))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 2)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('GEOMETRYCOLLECTION(POLYGON ((-112.840785 33.435962, -112.840785 33.708284, -112.409597 33.708284, -112.409597 33.435962, -112.840785 33.435962)), POLYGON ((-112.309264 33.398167, -112.309264 33.746007, -111.787444 33.746007, -111.787444 33.398167, -112.309264 33.398167)))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (-112.04835399999999 33.57208699999999)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_LabelPoint(ST_GeomFromWKT('POLYGON ((-112.654072 33.114485, -112.313516 33.653431, -111.63515 33.314399, -111.497829 33.874913, -111.692825 33.431378, -112.376684 33.788215, -112.654072 33.114485))', 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POINT (-112.0722602222832 33.53914975012836)\n</code></pre>"},{"location":"api/sql/Function/#st_angle","title":"ST_Angle","text":"<p>Introduction: Computes and returns the angle between two vectors represented by the provided points or linestrings.</p> <p>There are three variants possible for ST_Angle:</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry, point4: Geometry)</code> Computes the angle formed by vectors represented by point1 - point2 and point3 - point4</p> <p><code>ST_Angle(point1: Geometry, point2: Geometry, point3: Geometry)</code> Computes the angle formed by vectors represented by point2 - point1 and point2 - point3</p> <p><code>ST_Angle(line1: Geometry, line2: Geometry)</code> Computes the angle formed by vectors S1 - E1 and S2 - E2, where S and E denote start and end points respectively</p> <p>Note</p> <p>If any other geometry type is provided, ST_Angle throws an IllegalArgumentException. Additionally, if any of the provided geometry is empty, ST_Angle throws an IllegalArgumentException.</p> <p>Note</p> <p>If a 3D geometry is provided, ST_Angle computes the angle ignoring the z ordinate, equivalent to calling ST_Angle for corresponding 2D geometries.</p> <p>Tip</p> <p>ST_Angle returns the angle in radian between 0 and 2\\Pi. To convert the angle to degrees, use ST_Degrees.</p> <p>Format: <code>ST_Angle(p1, p2, p3, p4) | ST_Angle(p1, p2, p3) | ST_Angle(line1, line2)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT(0 0)'), ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT(1 0)'), ST_GeomFromWKT('POINT(6 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.4048917862850834\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('POINT (1 1)'), ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT(3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Angle(ST_GeomFromWKT('LINESTRING (0 0, 1 1)'), ST_GeomFromWKT('LINESTRING (0 0, 3 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.19739555984988044\n</code></pre>"},{"location":"api/sql/Function/#st_area","title":"ST_Area","text":"<p>Introduction: Return the area of A</p> <p>Format: <code>ST_Area (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Area(ST_GeomFromText(\"POLYGON(0 0, 0 10, 10 10, 0 10, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>10\n</code></pre>"},{"location":"api/sql/Function/#st_areaspheroid","title":"ST_AreaSpheroid","text":"<p>Introduction: Return the geodesic area of A using WGS84 spheroid. Unit is square meter. Works better for large geometries (country level) compared to <code>ST_Area</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Area(geography, use_spheroid=true)</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_AreaSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AreaSpheroid(ST_GeomFromWKT('Polygon ((34 35, 28 30, 25 34, 34 35))'))\n</code></pre> <p>Output:</p> <pre><code>201824850811.76245\n</code></pre>"},{"location":"api/sql/Function/#st_asbinary","title":"ST_AsBinary","text":"<p>Introduction: Return the Well-Known Binary representation of a geometry</p> <p>Format: <code>ST_AsBinary (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsBinary(ST_GeomFromWKT('POINT (1 1)'))\n</code></pre> <p>Output:</p> <pre><code>0101000000000000000000f87f000000000000f87f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkb","title":"ST_AsEWKB","text":"<p>Introduction: Return the Extended Well-Known Binary representation of a geometry. EWKB is an extended version of WKB which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKB format is produced. See ST_SetSRID It will ignore the M coordinate if present.</p> <p>Format: <code>ST_AsEWKB (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKB(ST_SetSrid(ST_GeomFromWKT('POINT (1 1)'), 3021))\n</code></pre> <p>Output:</p> <pre><code>0101000020cd0b0000000000000000f03f000000000000f03f\n</code></pre>"},{"location":"api/sql/Function/#st_asewkt","title":"ST_AsEWKT","text":"<p>Introduction: Return the Extended Well-Known Text representation of a geometry. EWKT is an extended version of WKT which includes the SRID of the geometry. The format originated in PostGIS but is supported by many GIS tools. If the geometry is lacking SRID a WKT format is produced. See ST_SetSRID It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsEWKT (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_SetSrid(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_asgeojson","title":"ST_AsGeoJSON","text":"<p>Note</p> <p>This method is not recommended. Please use Sedona GeoJSON data source to write GeoJSON files.</p> <p>Introduction: Return the GeoJSON string representation of a geometry</p> <p>The type parameter (Since: <code>v1.6.1</code>) takes the following options -</p> <ul> <li>\"Simple\" (default): Returns a simple GeoJSON geometry.</li> <li>\"Feature\": Wraps the geometry in a GeoJSON Feature.</li> <li>\"FeatureCollection\": Wraps the Feature in a GeoJSON FeatureCollection.</li> </ul> <p>Format:</p> <p><code>ST_AsGeoJSON (A: Geometry)</code></p> <p><code>ST_AsGeoJSON (A: Geometry, type: String)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example (Simple GeoJSON):</p> <pre><code>SELECT ST_AsGeoJSON(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{\n  \"type\":\"Polygon\",\n  \"coordinates\":[\n    [[1.0,1.0],\n      [8.0,1.0],\n      [8.0,8.0],\n      [1.0,8.0],\n      [1.0,1.0]]\n  ]\n}\n</code></pre> <p>SQL Example (Feature GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"Feature\",\n  \"geometry\": {\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n        [[1.0,1.0],\n          [8.0,1.0],\n          [8.0,8.0],\n          [1.0,8.0],\n          [1.0,1.0]]\n      ]\n  }\n}\n</code></pre> <p>SQL Example (FeatureCollection GeoJSON):</p> <p>Output:</p> <pre><code>{\n  \"type\":\"FeatureCollection\",\n  \"features\": [{\n      \"type\":\"Feature\",\n      \"geometry\": {\n          \"type\":\"Polygon\",\n          \"coordinates\":[\n            [[1.0,1.0],\n              [8.0,1.0],\n              [8.0,8.0],\n              [1.0,8.0],\n              [1.0,1.0]]\n          ]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/sql/Function/#st_asgml","title":"ST_AsGML","text":"<p>Introduction: Return the GML string representation of a geometry</p> <p>Format: <code>ST_AsGML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsGML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_ashexewkb","title":"ST_AsHEXEWKB","text":"<p>Introduction: This function returns the input geometry encoded to a text representation in HEXEWKB format. The HEXEWKB encoding can use either little-endian (NDR) or big-endian (XDR) byte ordering. If no encoding is explicitly specified, the function defaults to using the little-endian (NDR) format.</p> <p>Format: <code>ST_AsHEXEWKB(geom: Geometry, endian: String = NDR)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('POINT(1 2)'), 'XDR')\n</code></pre> <p>Output:</p> <pre><code>00000000013FF00000000000004000000000000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsHEXEWKB(ST_GeomFromWKT('LINESTRING (30 20, 20 25, 20 15, 30 20)'))\n</code></pre> <p>Output:</p> <pre><code>0102000000040000000000000000003E4000000000000034400000000000003440000000000000394000000000000034400000000000002E400000000000003E400000000000003440\n</code></pre>"},{"location":"api/sql/Function/#st_askml","title":"ST_AsKML","text":"<p>Introduction: Return the KML string representation of a geometry</p> <p>Format: <code>ST_AsKML (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsKML(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0,1.0 8.0,1.0 8.0,8.0 1.0,8.0 1.0,1.0\n</code></pre>"},{"location":"api/sql/Function/#st_astext","title":"ST_AsText","text":"<p>Introduction: Return the Well-Known Text string representation of a geometry. It will support M coordinate if present since v1.5.0.</p> <p>Format: <code>ST_AsText (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_SetSRID(ST_Point(1.0,1.0), 3021))\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_MakePointM(1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT M(1 1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_MakePoint(1.0, 1.0, 1.0, 1.0))\n</code></pre> <p>Output:</p> <pre><code>POINT ZM(1 1 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_azimuth","title":"ST_Azimuth","text":"<p>Introduction: Returns Azimuth for two given points in radians null otherwise.</p> <p>Format: <code>ST_Azimuth(pointA: Point, pointB: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Azimuth(ST_POINT(0.0, 25.0), ST_POINT(0.0, 0.0))\n</code></pre> <p>Output:</p> <pre><code>3.141592653589793\n</code></pre>"},{"location":"api/sql/Function/#st_bestsrid","title":"ST_BestSRID","text":"<p>Introduction: Returns the estimated most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location. It evaluates the geometry's bounding envelope and selects an SRID that optimally represents the geometry on the Earth's surface. The function prioritizes Universal Transverse Mercator (UTM), Lambert Azimuthal Equal Area (LAEA), or falls back to the Mercator projection. The function takes a WGS84 geometry and must be in lon/lat order.</p> <ul> <li>For geometries in the Arctic or Antarctic regions, the Lambert Azimuthal Equal Area projection is used.</li> <li>For geometries that fit within a single UTM zone and do not cross the International Date Line (IDL), a corresponding UTM SRID is chosen.</li> <li>In cases where none of the above conditions are met, the function defaults to the Mercator projection.</li> <li>For Geometries that cross the IDL, <code>ST_BestSRID</code> defaults the SRID to Mercator. Currently, <code>ST_BestSRID</code> does not handle geometries crossing the IDL.</li> </ul> <p>Warning</p> <p><code>ST_BestSRID</code> is designed to estimate a suitable SRID from a set of approximately 125 EPSG codes and works best for geometries that fit within the UTM zones. It should not be solely relied upon to determine the most accurate SRID, especially for specialized or high-precision spatial requirements.</p> <p>Format: <code>ST_BestSRID(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_BestSRID(ST_GeomFromWKT('POLYGON((-73.9980 40.7265, -73.9970 40.7265, -73.9970 40.7255, -73.9980 40.7255, -73.9980 40.7265))'))\n</code></pre> <p>Output:</p> <pre><code>32618\n</code></pre>"},{"location":"api/sql/Function/#st_boundary","title":"ST_Boundary","text":"<p>Introduction: Returns the closure of the combinatorial boundary of this Geometry.</p> <p>Format: <code>ST_Boundary(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Boundary(ST_GeomFromWKT('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 0 0, -1 1, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_boundingdiagonal","title":"ST_BoundingDiagonal","text":"<p>Introduction: Returns a linestring spanning minimum and maximum values of each dimension of the given geometry's coordinates as its start and end point respectively. If an empty geometry is provided, the returned LineString is also empty. If a single vertex (POINT) is provided, the returned LineString has both the start and end points same as the points coordinates</p> <p>Format: <code>ST_BoundingDiagonal(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_BoundingDiagonal(ST_GeomFromWKT(geom))\n</code></pre> <p>Input: <code>POLYGON ((1 1 1, 3 3 3, 0 1 4, 4 4 0, 1 1 1))</code></p> <p>Output: <code>LINESTRING Z(0 1 1, 4 4 4)</code></p> <p>Input: <code>POINT (10 10)</code></p> <p>Output: <code>LINESTRING (10 10, 10 10)</code></p> <p>Input: <code>GEOMETRYCOLLECTION(POLYGON ((5 5 5, -1 2 3, -1 -1 0, 5 5 5)), POINT (10 3 3))</code></p> <p>Output: <code>LINESTRING Z(-1 -1 0, 10 5 5)</code></p>"},{"location":"api/sql/Function/#st_buffer","title":"ST_Buffer","text":"<p>Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance. The function supports both Planar/Euclidean and Spheroidal/Geodesic buffering (Since v1.6.0). Spheroidal buffer also supports geometries crossing the International Date Line (IDL).</p> <p>Mode of buffer calculation (Since: <code>v1.6.0</code>):</p> <p>The optional third parameter, <code>useSpheroid</code>, controls the mode of buffer calculation.</p> <ul> <li>Planar Buffering (default): When <code>useSpheroid</code> is false, <code>ST_Buffer</code> performs standard planar buffering based on the provided parameters.</li> <li>Spheroidal Buffering:<ul> <li>When <code>useSpheroid</code> is set to true, the function returns the spheroidal buffer polygon for more accurate representation over the Earth. In this mode, the unit of the buffer distance is interpreted as meters.</li> <li>ST_Buffer first determines the most appropriate Spatial Reference Identifier (SRID) for a given geometry, based on its spatial extent and location, using <code>ST_BestSRID</code>.</li> <li>The geometry is then transformed from its original SRID to the selected SRID. If the input geometry does not have a set SRID, <code>ST_Buffer</code> defaults to using WGS 84 (SRID 4326) as its original SRID.</li> <li>The standard planar buffer operation is then applied in this coordinate system.</li> <li>Finally, the buffered geometry is transformed back to its original SRID, or to WGS 84 if the original SRID was not set.</li> </ul> </li> </ul> <p>Note</p> <p>As of now, spheroidal buffering only supports lon/lat coordinate systems and will throw an <code>IllegalArgumentException</code> for input geometries in meter based coordinate systems.</p> <p>Note</p> <p>Spheroidal buffering may not produce accurate output buffer for input geometries larger than a UTM zone.</p> <p>Buffer Style Parameters:</p> <p>The optional forth parameter controls the buffer accuracy and style. Buffer accuracy is specified by the number of line segments approximating a quarter circle, with a default of 8 segments. Buffer style can be set by providing blank-separated key=value pairs in a list format.</p> <ul> <li><code>quad_segs=#</code> : Number of line segments utilized to approximate a quarter circle (default is 8).</li> <li><code>endcap=round|flat|square</code> : End cap style (default is <code>round</code>). <code>butt</code> is an accepted synonym for <code>flat</code>.</li> <li><code>join=round|mitre|bevel</code> : Join style (default is <code>round</code>). <code>miter</code> is an accepted synonym for <code>mitre</code>.</li> <li><code>mitre_limit=#.#</code> : mitre ratio limit and it only affects mitred join style. <code>miter_limit</code> is an accepted synonym for <code>mitre_limit</code>.</li> <li><code>side=both|left|right</code> : The option <code>left</code> or <code>right</code> enables a single-sided buffer operation on the geometry, with the buffered side aligned according to the direction of the line. This functionality is specific to LINESTRING geometry and has no impact on POINT or POLYGON geometries. By default, square end caps are applied.</li> </ul> <p>Note</p> <p><code>ST_Buffer</code> throws an <code>IllegalArgumentException</code> if the correct format, parameters, or options are not provided.</p> <p>Format:</p> <pre><code>ST_Buffer (A: Geometry, buffer: Double)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean)\n</code></pre> <pre><code>ST_Buffer (A: Geometry, buffer: Double, useSpheroid: Boolean, bufferStyleParameters: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10)\nSELECT ST_Buffer(ST_GeomFromWKT('POINT(0 0)'), 10, false, 'quad_segs=2')\n</code></pre> <p>Output:</p> <p> </p> <p>8 Segments \u2002 2 Segments</p> <p>SQL Example:</p> <pre><code>SELECT ST_Buffer(ST_GeomFromWKT('LINESTRING(0 0, 50 70, 100 100)'), 10, false, 'side=left')\n</code></pre> <p>Output:</p> <p> </p> <p>Original Linestring \u2003 Left side buffed Linestring</p>"},{"location":"api/sql/Function/#st_buildarea","title":"ST_BuildArea","text":"<p>Introduction: Returns the areal geometry formed by the constituent linework of the input geometry.</p> <p>Format: <code>ST_BuildArea (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_BuildArea(\n    ST_GeomFromWKT('MULTILINESTRING((0 0, 20 0, 20 20, 0 20, 0 0),(2 2, 18 2, 18 18, 2 18, 2 2))')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|geom                                                                        |\n+----------------------------------------------------------------------------+\n|POLYGON((0 0,0 20,20 20,20 0,0 0),(2 2,18 2,18 18,2 18,2 2))                |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_centroid","title":"ST_Centroid","text":"<p>Introduction: Return the centroid point of A</p> <p>Format: <code>ST_Centroid (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Centroid(ST_GeomFromWKT('MULTIPOINT(-1  0, -1 2, 7 8, 9 8, 10 6)'))\n</code></pre> <p>Output:</p> <pre><code>POINT (4.8 4.8)\n</code></pre>"},{"location":"api/sql/Function/#st_closestpoint","title":"ST_ClosestPoint","text":"<p>Introduction: Returns the 2-dimensional point on geom1 that is closest to geom2. This is the first point of the shortest line between the geometries. If using 3D geometries, the Z coordinates will be ignored. If you have a 3D Geometry, you may prefer to use ST_3DClosestPoint. It will throw an exception indicates illegal argument if one of the params is an empty geometry.</p> <p>Format: <code>ST_ClosestPoint(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_ClosestPoint(g1, g2)) As ptwkt;\n</code></pre> <p>Input: <code>g1: POINT (160 40), g2: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190)</code></p> <p>Output: <code>POINT(160 40)</code></p> <p>Input: <code>g1: LINESTRING (10 30, 50 50, 30 110, 70 90, 180 140, 130 190), g2: POINT (160 40)</code></p> <p>Output: <code>POINT(125.75342465753425 115.34246575342466)</code></p> <p>Input: <code>g1: 'POLYGON ((190 150, 20 10, 160 70, 190 150))', g2: ST_Buffer('POINT(80 160)', 30)</code></p> <p>Output: <code>POINT(131.59149149528952 101.89887534906197)</code></p>"},{"location":"api/sql/Function/#st_collect","title":"ST_Collect","text":"<p>Introduction: Returns MultiGeometry object based on geometry column/s or array with geometries</p> <p>Format:</p> <p><code>ST_Collect(*geom: Geometry)</code></p> <p><code>ST_Collect(geom: ARRAY[Geometry])</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Collect(\n    ST_GeomFromText('POINT(21.427834 52.042576573)'),\n    ST_GeomFromText('POINT(45.342524 56.342354355)')\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Collect(\n    Array(\n        ST_GeomFromText('POINT(21.427834 52.042576573)'),\n        ST_GeomFromText('POINT(45.342524 56.342354355)')\n    )\n) AS geom\n</code></pre> <p>Result:</p> <pre><code>+---------------------------------------------------------------+\n|geom                                                           |\n+---------------------------------------------------------------+\n|MULTIPOINT ((21.427834 52.042576573), (45.342524 56.342354355))|\n+---------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_collectionextract","title":"ST_CollectionExtract","text":"<p>Introduction: Returns a homogeneous multi-geometry from a given geometry collection.</p> <p>The type numbers are:</p> <ol> <li>POINT</li> <li>LINESTRING</li> <li>POLYGON</li> </ol> <p>If the type parameter is omitted a multi-geometry of the highest dimension is returned.</p> <p>Format:</p> <p><code>ST_CollectionExtract (A: Geometry)</code></p> <p><code>ST_CollectionExtract (A: Geometry, type: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>WITH test_data as (\n    ST_GeomFromText(\n        'GEOMETRYCOLLECTION(POINT(40 10), POLYGON((0 0, 0 5, 5 5, 5 0, 0 0)))'\n    ) as geom\n)\nSELECT ST_CollectionExtract(geom) as c1, ST_CollectionExtract(geom, 1) as c2\nFROM test_data\n</code></pre> <p>Result:</p> <pre><code>+----------------------------------------------------------------------------+\n|c1                                        |c2                               |\n+----------------------------------------------------------------------------+\n|MULTIPOLYGON(((0 0, 0 5, 5 5, 5 0, 0 0))) |MULTIPOINT(40 10)                |              |\n+----------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_concavehull","title":"ST_ConcaveHull","text":"<p>Introduction: Return the Concave Hull of polygon A, with alpha set to pctConvex[0, 1] in the Delaunay Triangulation method, the concave hull will not contain a hole unless allowHoles is set to true</p> <p>Format:</p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double)</code></p> <p><code>ST_ConcaveHull (A: Geometry, pctConvex: Double, allowHoles: Boolean)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ConcaveHull(ST_GeomFromWKT('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((125 100, 20 40, 50 60, 175 150, 125 100))\n</code></pre>"},{"location":"api/sql/Function/#st_convexhull","title":"ST_ConvexHull","text":"<p>Introduction: Return the Convex Hull of polygon A</p> <p>Format: <code>ST_ConvexHull (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ConvexHull(ST_GeomFromText('POLYGON((175 150, 20 40, 50 60, 125 100, 175 150))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 40, 175 150, 125 100, 20 40))\n</code></pre>"},{"location":"api/sql/Function/#st_coorddim","title":"ST_CoordDim","text":"<p>Introduction: Returns the coordinate dimensions of the geometry. It is an alias of <code>ST_NDims</code>.</p> <p>Format: <code>ST_CoordDim(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>Spark SQL Example with x, y, z coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromText('POINT(1 1 2'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL Example with x, y coordinate:</p> <pre><code>SELECT ST_CoordDim(ST_GeomFromWKT('POINT(3 7)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_crossesdateline","title":"ST_CrossesDateLine","text":"<p>Introduction: This function determines if a given geometry crosses the International Date Line. It operates by checking if the difference in longitude between any pair of consecutive points in the geometry exceeds 180 degrees. If such a difference is found, it is assumed that the geometry crosses the Date Line. It returns true if the geometry crosses the Date Line, and false otherwise.</p> <p>Note</p> <p>The function assumes that the provided geometry is in lon/lat coordinate reference system where longitude values range from -180 to 180 degrees.</p> <p>Note</p> <p>For multi-geometries (e.g., MultiPolygon, MultiLineString), this function will return true if any one of the geometries within the multi-geometry crosses the International Date Line.</p> <p>Format: <code>ST_CrossesDateLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_CrossesDateLine(ST_GeomFromWKT('LINESTRING(170 30, -170 30)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>Warning</p> <p>For geometries that span more than 180 degrees in longitude without actually crossing the Date Line, this function may still return true, indicating a crossing.</p>"},{"location":"api/sql/Function/#st_degrees","title":"ST_Degrees","text":"<p>Introduction: Convert an angle in radian to degrees.</p> <p>Format: <code>ST_Degrees(angleInRadian)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Degrees(0.19739555984988044)\n</code></pre> <p>Output:</p> <pre><code>11.309932474020195\n</code></pre>"},{"location":"api/sql/Function/#st_delaunaytriangles","title":"ST_DelaunayTriangles","text":"<p>Introduction: This function computes the Delaunay triangulation for the set of vertices in the input geometry. An optional <code>tolerance</code> parameter allows snapping nearby input vertices together prior to triangulation and can improve robustness in certain scenarios by handling near-coincident vertices. The default for  <code>tolerance</code> is 0. The Delaunay triangulation geometry is bounded by the convex hull of the input vertex set.</p> <p>The output geometry representation depends on the provided <code>flag</code>:</p> <ul> <li><code>0</code> - a GeometryCollection of triangular Polygons (default option)</li> <li><code>1</code> - a MultiLinestring of the edges of the triangulation</li> </ul> <p>Format:</p> <p><code>ST_DelaunayTriangles(geometry: Geometry)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double)</code></p> <p><code>ST_DelaunayTriangles(geometry: Geometry, tolerance: Double, flag: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DelaunayTriangles(\n        ST_GeomFromWKT('POLYGON ((10 10, 15 30, 20 25, 25 35, 30 20, 40 30, 50 10, 45 5, 35 15, 30 5, 25 15, 20 10, 15 20, 10 10))')\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((15 30, 10 10, 15 20, 15 30)), POLYGON ((15 30, 15 20, 20 25, 15 30)), POLYGON ((15 30, 20 25, 25 35, 15 30)), POLYGON ((25 35, 20 25, 30 20, 25 35)), POLYGON ((25 35, 30 20, 40 30, 25 35)), POLYGON ((40 30, 30 20, 35 15, 40 30)), POLYGON ((40 30, 35 15, 50 10, 40 30)), POLYGON ((50 10, 35 15, 45 5, 50 10)), POLYGON ((30 5, 45 5, 35 15, 30 5)), POLYGON ((30 5, 35 15, 25 15, 30 5)), POLYGON ((30 5, 25 15, 20 10, 30 5)), POLYGON ((30 5, 20 10, 10 10, 30 5)), POLYGON ((10 10, 20 10, 15 20, 10 10)), POLYGON ((15 20, 20 10, 25 15, 15 20)), POLYGON ((15 20, 25 15, 20 25, 15 20)), POLYGON ((20 25, 25 15, 30 20, 20 25)), POLYGON ((30 20, 25 15, 35 15, 30 20)))\n</code></pre>"},{"location":"api/sql/Function/#st_difference","title":"ST_Difference","text":"<p>Introduction: Return the difference between geometry A and B (return part of geometry A that does not intersect geometry B)</p> <p>Format: <code>ST_Difference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Difference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((0 -4, 4 -4, 4 4, 0 4, 0 -4))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 -3, -3 -3, -3 3, 0 3, 0 -3))\n</code></pre>"},{"location":"api/sql/Function/#st_dimension","title":"ST_Dimension","text":"<p>Introduction: Return the topological dimension of this Geometry object, which must be less than or equal to the coordinate dimension. OGC SPEC s2.1.1.1 - returns 0 for POINT, 1 for LINESTRING, 2 for POLYGON, and the largest dimension of the components of a GEOMETRYCOLLECTION. If the dimension is unknown (e.g. for an empty GEOMETRYCOLLECTION) 0 is returned.</p> <p>Format: <code>ST_Dimension (A: Geometry) | ST_Dimension (C: Geometrycollection)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Dimension('GEOMETRYCOLLECTION(LINESTRING(1 1,0 0),POINT(0 0))');\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_distance","title":"ST_Distance","text":"<p>Introduction: Return the Euclidean distance between A and B</p> <p>Format: <code>ST_Distance (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Distance(ST_GeomFromText('POINT(72 42)'), ST_GeomFromText('LINESTRING(-72 -42, 82 92)'))\n</code></pre> <p>Output:</p> <pre><code>31.155515639003543\n</code></pre>"},{"location":"api/sql/Function/#st_distancesphere","title":"ST_DistanceSphere","text":"<p>Introduction: Return the haversine / great-circle distance of A using a given earth radius (default radius: 6371008.0). Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=false)</code> and <code>ST_DistanceSphere</code> function and produces nearly identical results. It provides faster but less accurate result compared to <code>ST_DistanceSpheroid</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSphere (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>543796.9506134904\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSphere(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'), 6378137.0)\n</code></pre> <p>Output:</p> <pre><code>544405.4459192449\n</code></pre>"},{"location":"api/sql/Function/#st_distancespheroid","title":"ST_DistanceSpheroid","text":"<p>Introduction: Return the geodesic distance of A using WGS84 spheroid. Unit is meter. Compared to <code>ST_Distance</code> + <code>ST_Transform</code>, it works better for datasets that cover large regions such as continents or the entire planet. It is equivalent to PostGIS <code>ST_Distance(geography, use_spheroid=true)</code> and <code>ST_DistanceSpheroid</code> function and produces nearly identical results. It provides slower but more accurate result compared to <code>ST_DistanceSphere</code>.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon. For non-point data, we first take the centroids of both geometries and then compute the distance.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Format: <code>ST_DistanceSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DistanceSpheroid(ST_GeomFromWKT('POINT (-0.56 51.3168)'), ST_GeomFromWKT('POINT (-3.1883 55.9533)'))\n</code></pre> <p>Output:</p> <pre><code>544430.9411996207\n</code></pre>"},{"location":"api/sql/Function/#st_dump","title":"ST_Dump","text":"<p>Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry itself, if the geometry is collection or multi it returns record for each of collection components.</p> <p>Format: <code>ST_Dump(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Dump(ST_GeomFromText('MULTIPOINT ((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n</code></pre>"},{"location":"api/sql/Function/#st_dumppoints","title":"ST_DumpPoints","text":"<p>Introduction: Returns list of Points which geometry consists of.</p> <p>Format: <code>ST_DumpPoints(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DumpPoints(ST_GeomFromText('LINESTRING (0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n</code></pre>"},{"location":"api/sql/Function/#st_endpoint","title":"ST_EndPoint","text":"<p>Introduction: Returns last point of given linestring.</p> <p>Format: <code>ST_EndPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_EndPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(160 170)\n</code></pre>"},{"location":"api/sql/Function/#st_envelope","title":"ST_Envelope","text":"<p>Introduction: Return the envelope boundary of A</p> <p>Format: <code>ST_Envelope (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Envelope(ST_GeomFromWKT('LINESTRING(0 0, 1 3)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 1 3, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_expand","title":"ST_Expand","text":"<p>Introduction: Returns a geometry expanded from the bounding box of the input. The expansion can be specified in two ways:</p> <ol> <li>By individual axis using <code>deltaX</code>, <code>deltaY</code>, or <code>deltaZ</code> parameters.</li> <li>Uniformly across all axes using the <code>uniformDelta</code> parameter.</li> </ol> <p>Note</p> <p>Things to consider when using this function:</p> <ol> <li>The <code>uniformDelta</code> parameter expands Z dimensions for XYZ geometries; otherwise, it only affects XY dimensions.</li> <li>For XYZ geometries, specifying only <code>deltaX</code> and <code>deltaY</code> will preserve the original Z dimension.</li> <li>If the input geometry has an M dimension then using this function will drop the said M dimension.</li> </ol> <p>Format:</p> <p><code>ST_Expand(geometry: Geometry, uniformDelta: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double)</code></p> <p><code>ST_Expand(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Expand(\n        ST_GeomFromWKT('POLYGON Z((50 50 1, 50 80 2, 80 80 3, 80 50 2, 50 50 1))'),\n        10\n   )\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((40 40 -9, 40 90 -9, 90 90 13, 90 40 13, 40 40 -9))\n</code></pre>"},{"location":"api/sql/Function/#st_exteriorring","title":"ST_ExteriorRing","text":"<p>Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.</p> <p>Format: <code>ST_ExteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ExteriorRing(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_flipcoordinates","title":"ST_FlipCoordinates","text":"<p>Introduction: Returns a version of the given geometry with X and Y axis flipped.</p> <p>Format: <code>ST_FlipCoordinates(A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_FlipCoordinates(ST_GeomFromWKT(\"POINT (1 2)\"))\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre>"},{"location":"api/sql/Function/#st_force_2d","title":"ST_Force_2D","text":"<p>Introduction: Forces the geometries into a \"2-dimensional mode\" so that all output representations will only have the X and Y coordinates</p> <p>Format: <code>ST_Force_2D (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Force_2D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON((0 0,0 5,5 0,0 0),(1 1,3 1,1 3,1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_force3d","title":"ST_Force3D","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3D(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3D(ST_GeomFromText('LINESTRING EMPTY'), 3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING EMPTY\n</code></pre>"},{"location":"api/sql/Function/#st_force3dm","title":"ST_Force3DM","text":"<p>Introduction: Forces the geometry into XYM mode. Retains any existing M coordinate, but removes the Z coordinate if present. Assigns a default M value of 0.0 if <code>mValue</code> is not specified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds M for in the WKT.</p> <p>Format: <code>ST_Force3DM(geometry: Geometry, mValue: Double = 0.0)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('POLYGON M((0 0 3,0 5 3,5 0 3,0 0 3),(1 1 3,3 1 3,1 3 3,1 1 3))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON M((0 0 3, 0 5 3, 5 0 3, 0 0 3), (1 1 3, 3 1 3, 1 3 3, 1 1 3))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DM(ST_GeomFromText('LINESTRING Z(0 1 3,1 0 3,2 0 3)'), 5))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING M(0 1 5, 1 0 5, 2 0 5)\n</code></pre>"},{"location":"api/sql/Function/#st_force3dz","title":"ST_Force3DZ","text":"<p>Introduction: Forces the geometry into a 3-dimensional model so that all output representations will have X, Y and Z coordinates. An optionally given zValue is tacked onto the geometry if the geometry is 2-dimensional. Default value of zValue is 0.0 If the given geometry is 3-dimensional, no change is performed on it. If the given geometry is empty, no change is performed on it. This function is an alias for ST_Force3D.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format: <code>ST_Force3DZ(geometry: Geometry, zValue: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON Z((0 0 2, 0 5 2, 5 0 2, 0 0 2), (1 1 2, 3 1 2, 1 3 2, 1 1 2))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force3DZ(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 2.3))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING Z(0 1 2.3, 1 0 2.3, 2 0 2.3)\n</code></pre>"},{"location":"api/sql/Function/#st_force4d","title":"ST_Force4D","text":"<p>Introduction: Converts the input geometry to 4D XYZM representation. Retains original Z and M values if present. Assigning 0.0 defaults if <code>mValue</code> and <code>zValue</code> aren't specified. The output contains X, Y, Z, and M coordinates. For geometries already in 4D form, the function returns the original geometry unmodified.</p> <p>Note</p> <p>Example output is after calling ST_AsText() on returned geometry, which adds Z for in the WKT for 3D geometries</p> <p>Format:</p> <p><code>ST_Force4D(geom: Geometry, zValue: Double, mValue: Double)</code></p> <p><code>ST_Force4D(geom: Geometry</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('POLYGON((0 0 2,0 5 2,5 0 2,0 0 2),(1 1 2,3 1 2,1 3 2,1 1 2))'), 5, 10))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ZM((0 0 2 10, 0 5 2 10, 5 0 2 10, 0 0 2 10), (1 1 2 10, 3 1 2 10, 1 3 2 10, 1 1 2 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Force4D(ST_GeomFromText('LINESTRING(0 1,1 0,2 0)'), 3, 1))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING ZM(0 1 3 1, 1 0 3 1, 2 0 3 1)\n</code></pre>"},{"location":"api/sql/Function/#st_forcecollection","title":"ST_ForceCollection","text":"<p>Introduction: This function converts the input geometry into a GeometryCollection, regardless of the original geometry type. If the input is a multipart geometry, such as a MultiPolygon or MultiLineString, it will be decomposed into a GeometryCollection containing each individual Polygon or LineString element from the original multipart geometry.</p> <p>Format: <code>ST_ForceCollection(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ForceCollection(\n            ST_GeomFromWKT(\n                \"MULTIPOINT (30 10, 40 40, 20 20, 10 30, 10 10, 20 50)\"\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (30 10), POINT (40 40), POINT (20 20), POINT (10 30), POINT (10 10), POINT (20 50))\n</code></pre>"},{"location":"api/sql/Function/#st_forcepolygonccw","title":"ST_ForcePolygonCCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to counter-clockwise and interior rings to clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCCW(ST_GeomFromText('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_forcepolygoncw","title":"ST_ForcePolygonCW","text":"<p>Introduction: For (Multi)Polygon geometries, this function sets the exterior ring orientation to clockwise and interior rings to counter-clockwise orientation. Non-polygonal geometries are returned unchanged.</p> <p>Format: <code>ST_ForcePolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForcePolygonCW(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_forcerhr","title":"ST_ForceRHR","text":"<p>Introduction: Sets the orientation of polygon vertex orderings to follow the Right-Hand-Rule convention. The exterior ring will have a clockwise winding order, while any interior rings are oriented counter-clockwise. This ensures the area bounded by the polygon falls on the right-hand side relative to the ring directions. The function is an alias for ST_ForcePolygonCW.</p> <p>Format: <code>ST_ForceRHR(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_AsText(ST_ForceRHR(ST_GeomFromText('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35),(30 20, 20 15, 20 25, 30 20))')))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))\n</code></pre>"},{"location":"api/sql/Function/#st_frechetdistance","title":"ST_FrechetDistance","text":"<p>Introduction: Computes and returns discrete Frechet Distance between the given two geometries, based on Computing Discrete Frechet Distance</p> <p>If any of the geometries is empty, returns 0.0</p> <p>Format: <code>ST_FrechetDistance(g1: Geometry, g2: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_FrechetDistance(ST_GeomFromWKT('POINT (0 1)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'))\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre>"},{"location":"api/sql/Function/#st_generatepoints","title":"ST_GeneratePoints","text":"<p>Introduction: Generates a specified quantity of pseudo-random points within the boundaries of the provided polygonal geometry. When <code>seed</code> is either zero or not defined then output will be random.</p> <p>Format:</p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer, seed: Long = 0)</code></p> <p><code>ST_GeneratePoints(geom: Geometry, numPoints: Integer)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_GeneratePoints(\n        ST_GeomFromWKT('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))'), 4\n)\n</code></pre> <p>Output:</p> <p>Note</p> <p>Due to the pseudo-random nature of point generation, the output of this function will vary between executions and may not match any provided examples.</p> <pre><code>MULTIPOINT ((0.2393028905520183 0.9721563442837837), (0.3805848547053376 0.7546556656982678), (0.0950295778200995 0.2494334895495989), (0.4133520939987385 0.3447046312451945))\n</code></pre>"},{"location":"api/sql/Function/#st_geohash","title":"ST_GeoHash","text":"<p>Introduction: Returns GeoHash of the geometry with given precision</p> <p>Format: <code>ST_GeoHash(geom: Geometry, precision: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeoHash(ST_GeomFromText('POINT(21.427834 52.042576573)'), 5) AS geohash\n</code></pre> <p>Output:</p> <pre><code>u3r0p\n</code></pre>"},{"location":"api/sql/Function/#st_geometricmedian","title":"ST_GeometricMedian","text":"<p>Introduction: Computes the approximate geometric median of a MultiPoint geometry using the Weiszfeld algorithm. The geometric median provides a centrality measure that is less sensitive to outlier points than the centroid.</p> <p>The algorithm will iterate until the distance change between successive iterations is less than the supplied <code>tolerance</code> parameter. If this condition has not been met after <code>maxIter</code> iterations, the function will produce an error and exit, unless <code>failIfNotConverged</code> is set to <code>false</code>.</p> <p>If a <code>tolerance</code> value is not provided, a default <code>tolerance</code> value is <code>1e-6</code>.</p> <p>Format:</p> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer, failIfNotConverged: Boolean)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double, maxIter: Integer)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry, tolerance: Double)\n</code></pre> <pre><code>ST_GeometricMedian(geom: Geometry)\n</code></pre> <p>Default parameters: <code>tolerance: 1e-6, maxIter: 1000, failIfNotConverged: false</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometricMedian(ST_GeomFromWKT('MULTIPOINT((0 0), (1 1), (2 2), (200 200))'))\n</code></pre> <p>Output:</p> <pre><code>POINT (1.9761550281255005 1.9761550281255005)\n</code></pre>"},{"location":"api/sql/Function/#st_geometryn","title":"ST_GeometryN","text":"<p>Introduction: Return the 0-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON. Otherwise, return null</p> <p>Format: <code>ST_GeometryN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryN(ST_GeomFromText('MULTIPOINT((1 2), (3 4), (5 6), (8 9))'), 1)\n</code></pre> <p>Output:</p> <pre><code>POINT (3 4)\n</code></pre>"},{"location":"api/sql/Function/#st_geometrytype","title":"ST_GeometryType","text":"<p>Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.</p> <p>Format: <code>ST_GeometryType (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_GeometryType(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>ST_LINESTRING\n</code></pre>"},{"location":"api/sql/Function/#st_h3celldistance","title":"ST_H3CellDistance","text":"<p>Introduction: return result of h3 function gridDistance(cel1, cell2). As described by H3 documentation</p> <p>Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions.</p> <p>In this case, Sedona use in-house implementation of estimation the shortest path and return the size as distance.</p> <p>Format: <code>ST_H3CellDistance(cell1: Long, cell2: Long)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>select ST_H3CellDistance(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], ST_H3CellIDs(ST_GeomFromWKT('POINT(1.23 1.59)'), 8, true)[0])\n</code></pre> <p>Output:</p> <pre><code>+-----------------------------------------------------------------------------------------------------------------------------------------+\n|st_h3celldistance(st_h3cellids(st_geomfromwkt(POINT(1 2), 0), 8, true)[0], st_h3cellids(st_geomfromwkt(POINT(1.23 1.59), 0), 8, true)[0])|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                                                                                       78|\n+-----------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3cellids","title":"ST_H3CellIDs","text":"<p>Introduction: Cover the geometry by H3 cell IDs with the given resolution(level). To understand the cell statistics please refer to H3 Doc H3 native fill functions doesn't guarantee full coverage on the shapes.</p>"},{"location":"api/sql/Function/#cover-polygon","title":"Cover Polygon","text":"<p>When fullCover = false, for polygon sedona will use polygonToCells. This can't guarantee full coverage but will guarantee no false positive.</p> <p>When fullCover = true, sedona will add on extra traversal logic to guarantee full coverage on shapes. This will lead to redundancy but can guarantee full coverage.</p> <p>Choose the option according to your use case.</p>"},{"location":"api/sql/Function/#cover-linestring","title":"Cover LineString","text":"<p>For the lineString, sedona will call gridPathCells(https://h3geo.org/docs/api/traversal#gridpathcells) per segment. From H3's documentation</p> <p>This function may fail to find the line between two indexes, for example if they are very far apart. It may also fail when finding distances for indexes on opposite sides of a pentagon.</p> <p>When the <code>gridPathCells</code> function throw error, Sedona implemented in-house approximate implementation to generate the shortest path, which can cover the corner cases.</p> <p>Both functions can't guarantee full coverage. When the <code>fullCover = true</code>, we'll do extra cell traversal to guarantee full cover. In worst case, sedona will use MBR to guarantee the full coverage.</p> <p>If you seek to get the shortest path between cells, you can call this function with <code>fullCover = false</code></p> <p>Format: <code>ST_H3CellIDs(geom: geometry, level: Int, fullCover: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_H3CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6, true)\n</code></pre> <p>Output:</p> <pre><code>+-------------------------------------------------------------+\n|st_h3cellids(st_geomfromtext(LINESTRING(1 3 4, 5 6 7), 0), 6)|\n+-------------------------------------------------------------+\n|                                         [6055475394579005...|\n+-------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3kring","title":"ST_H3KRing","text":"<p>Introduction: return the result of H3 function gridDisk(cell, k).</p> <p>K means <code>the distance of the origin index</code>, <code>gridDisk(cell, k)</code> return cells with distance <code>&lt;=k</code> from the original cell.</p> <p><code>exactRing : Boolean</code>, when set to <code>true</code>, sedona will remove the result of <code>gridDisk(cell, k-1)</code> from the original results, means only keep the cells with distance exactly <code>k</code> from the original cell</p> <p>Format: <code>ST_H3KRing(cell: Long, k: Int, exactRing: Boolean)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true) cells union select ST_H3KRing(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, false) cells\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------------------------------------------------------------------------------------------------+\n|cells                                                                                                                                       |\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n|[614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]                    |\n|[614552609325318143, 614552597293957119, 614552609329512447, 614552609316929535, 614552609327415295, 614552609287569407, 614552597289762815]|\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_h3togeom","title":"ST_H3ToGeom","text":"<p>Introduction: Return the result of H3 function cellsToMultiPolygon(cells).</p> <p>Converts an array of Uber H3 cell indices into an array of Polygon geometries, where each polygon represents a hexagonal H3 cell.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_H3ToGeom(cells: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_H3ToGeom(ST_H3CellIDs(ST_GeomFromWKT('POINT(1 2)'), 8, true)[0], 1, true))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((1.0057629565405093 1.9984665139177547, 1.0037116327309097 2.0018325249140068, 0.999727799357053 2.001163270465665, 0.9977951427833316 1.997128228393235, 0.9998461908217928 1.993762152933182, 1.0038301712104316 1.9944311839965523, 1.0057629565405093 1.9984665139177547))]\n</code></pre>"},{"location":"api/sql/Function/#st_hasm","title":"ST_HasM","text":"<p>Introduction: Checks for the presence of M coordinate values representing measures or linear references. Returns true if the input geometry includes an M coordinate, false otherwise.</p> <p>Format: <code>ST_HasM(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasM(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/sql/Function/#st_hasz","title":"ST_HasZ","text":"<p>Introduction: Checks for the presence of Z coordinate values representing measures or linear references. Returns true if the input geometry includes an Z coordinate, false otherwise.</p> <p>Format: <code>ST_HasZ(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HasZ(\n        ST_GeomFromWKT('LINESTRING Z (30 10 5, 40 40 10, 20 40 15, 10 20 20)')\n)\n</code></pre> <p>Output:</p> <pre><code>True\n</code></pre>"},{"location":"api/sql/Function/#st_hausdorffdistance","title":"ST_HausdorffDistance","text":"<p>Introduction: Returns a discretized (and hence approximate) Hausdorff distance between the given 2 geometries. Optionally, a densityFraction parameter can be specified, which gives more accurate results by densifying segments before computing hausdorff distance between them. Each segment is broken down into equal-length subsegments whose ratio with segment length is closest to the given density fraction.</p> <p>Hence, the lower the densityFrac value, the more accurate is the computed hausdorff distance, and the more time it takes to compute it.</p> <p>If any of the geometry is empty, 0.0 is returned.</p> <p>Note</p> <p>Accepted range of densityFrac is (0.0, 1.0], if any other value is provided, ST_HausdorffDistance throws an IllegalArgumentException</p> <p>Note</p> <p>Even though the function accepts 3D geometry, the z ordinate is ignored and the computed hausdorff distance is equivalent to the geometries not having the z ordinate.</p> <p>Format: <code>ST_HausdorffDistance(g1: Geometry, g2: Geometry, densityFrac: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromWKT('POINT (0.0 1.0)'), ST_GeomFromWKT('LINESTRING (0 0, 1 0, 2 0, 3 0, 4 0, 5 0)'), 0.1)\n</code></pre> <p>Output:</p> <pre><code>5.0990195135927845\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_HausdorffDistance(ST_GeomFromText('POLYGON Z((1 0 1, 1 1 2, 2 1 5, 2 0 1, 1 0 1))'), ST_GeomFromText('POLYGON Z((4 0 4, 6 1 4, 6 4 9, 6 1 3, 4 0 4))'))\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Function/#st_interiorringn","title":"ST_InteriorRingN","text":"<p>Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range</p> <p>Format: <code>ST_InteriorRingN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InteriorRingN(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))'), 0)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_interpolatepoint","title":"ST_InterpolatePoint","text":"<p>Introduction: Returns the interpolated measure value of a linear measured LineString at the point closest to the specified point.</p> <p>Note</p> <p>Make sure that both geometries have the same SRID, otherwise the function will throw an IllegalArgumentException.</p> <p>Format: <code>ST_InterpolatePoint(linestringM: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_InterpolatePoint(\n    ST_GeomFromWKT(\"LINESTRING M (0 0 0, 2 0 2, 4 0 4)\"),\n    ST_GeomFromWKT(\"POINT (1 1)\")\n    )\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/sql/Function/#st_intersection","title":"ST_Intersection","text":"<p>Introduction: Return the intersection geometry of A and B</p> <p>Format: <code>ST_Intersection (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersection(\n    ST_GeomFromWKT(\"POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))\"),\n    ST_GeomFromWKT(\"POLYGON((2 2, 9 2, 9 9, 2 9, 2 2))\")\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 8, 8 8, 8 2, 2 2, 2 8))\n</code></pre>"},{"location":"api/sql/Function/#st_isclosed","title":"ST_IsClosed","text":"<p>Introduction: RETURNS true if the LINESTRING start and end point are the same.</p> <p>Format: <code>ST_IsClosed(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsClosed(ST_GeomFromText('LINESTRING(0 0, 1 1, 1 0)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_iscollection","title":"ST_IsCollection","text":"<p>Introduction: Returns <code>TRUE</code> if the geometry type of the input is a geometry collection type. Collection types are the following:</p> <ul> <li>GEOMETRYCOLLECTION</li> <li>MULTI{POINT, POLYGON, LINESTRING}</li> </ul> <p>Format: <code>ST_IsCollection(geom: Geometry)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('MULTIPOINT(0 0), (6 6)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_IsCollection(ST_GeomFromText('POINT(5 5)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isempty","title":"ST_IsEmpty","text":"<p>Introduction: Test if a geometry is empty geometry</p> <p>Format: <code>ST_IsEmpty (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsEmpty(ST_GeomFromWKT('POLYGON((0 0,0 1,1 1,1 0,0 0))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_ispolygonccw","title":"ST_IsPolygonCCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCCW(ST_GeomFromWKT('POLYGON ((20 35, 10 30, 10 10, 30 5, 45 20, 20 35), (30 20, 20 15, 20 25, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_ispolygoncw","title":"ST_IsPolygonCW","text":"<p>Introduction: Returns true if all polygonal components in the input geometry have their exterior rings oriented counter-clockwise and interior rings oriented clockwise.</p> <p>Format: <code>ST_IsPolygonCW(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsPolygonCW(ST_GeomFromWKT('POLYGON ((20 35, 45 20, 30 5, 10 10, 10 30, 20 35), (30 20, 20 25, 20 15, 30 20))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_isring","title":"ST_IsRing","text":"<p>Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.</p> <p>Format: <code>ST_IsRing(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsRing(ST_GeomFromText(\"LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\"))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_issimple","title":"ST_IsSimple","text":"<p>Introduction: Test if geometry's only self-intersections are at boundary points.</p> <p>Format: <code>ST_IsSimple (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_IsSimple(ST_GeomFromWKT('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Function/#st_isvalid","title":"ST_IsValid","text":"<p>Introduction: Test if a geometry is well-formed. The function can be invoked with just the geometry or with an additional flag (from <code>v1.5.1</code>). The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValid (A: Geometry)\n</code></pre> <pre><code>ST_IsValid (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValid(ST_GeomFromWKT('POLYGON((0 0, 10 0, 10 10, 0 10, 0 0), (15 15, 15 20, 20 20, 20 15, 15 15))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_isvaliddetail","title":"ST_IsValidDetail","text":"<p>Introduction: Returns a row, containing a boolean <code>valid</code> stating if a geometry is valid, a string <code>reason</code> stating why it is invalid and a geometry <code>location</code> pointing out where it is invalid.</p> <p>This function is a combination of ST_IsValid and ST_IsValidReason.</p> <p>The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidDetail(geom: Geometry)\n</code></pre> <pre><code>ST_IsValidDetail(geom: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidDetail(ST_GeomFromWKT('POLYGON ((30 10, 40 40, 20 40, 30 10, 10 20, 30 10))'))\n</code></pre> <p>Output:</p> <pre><code>+-----+---------------------------------------------------------+-------------+\n|valid|reason                                                   |location     |\n+-----+---------------------------------------------------------+-------------+\n|false|Ring Self-intersection at or near point (30.0, 10.0, NaN)|POINT (30 10)|\n+-----+---------------------------------------------------------+-------------+\n</code></pre>"},{"location":"api/sql/Function/#st_isvalidreason","title":"ST_IsValidReason","text":"<p>Introduction: Returns text stating if the geometry is valid. If not, it provides a reason why it is invalid. The function can be invoked with just the geometry or with an additional flag. The flag alters the validity checking behavior. The flags parameter is a bitfield with the following options:</p> <ul> <li>0 (default): Use usual OGC SFS (Simple Features Specification) validity semantics.</li> <li>1: \"ESRI flag\", Accepts certain self-touching rings as valid, which are considered invalid under OGC standards.</li> </ul> <p>Formats:</p> <pre><code>ST_IsValidReason (A: Geometry)\n</code></pre> <pre><code>ST_IsValidReason (A: Geometry, flag: Integer)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example for valid geometry:</p> <pre><code>SELECT ST_IsValidReason(ST_GeomFromWKT('POLYGON ((100 100, 100 300, 300 300, 300 100, 100 100))')) as validity_info\n</code></pre> <p>Output:</p> <pre><code>Valid Geometry\n</code></pre> <p>SQL Example for invalid geometries:</p> <pre><code>SELECT gid, ST_IsValidReason(geom) as validity_info\nFROM Geometry_table\nWHERE ST_IsValid(geom) = false\nORDER BY gid\n</code></pre> <p>Output:</p> <pre><code>gid  |                  validity_info\n-----+----------------------------------------------------\n5330 | Self-intersection at or near point (32.0, 5.0, NaN)\n5340 | Self-intersection at or near point (42.0, 5.0, NaN)\n5350 | Self-intersection at or near point (52.0, 5.0, NaN)\n</code></pre>"},{"location":"api/sql/Function/#st_isvalidtrajectory","title":"ST_IsValidTrajectory","text":"<p>Introduction: This function checks if a geometry is a valid trajectory representation. For a trajectory to be considered valid, it must be a LineString that includes measure (M) values. The key requirement is that the M values increase from one vertex to the next as you move along the line.</p> <p>Format: <code>ST_IsValidTrajectory(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 2)')\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_IsValidTrajectory(\n               ST_GeomFromText('LINESTRING M (0 0 1, 0 1 0)')\n)\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Function/#st_length","title":"ST_Length","text":"<p>Introduction: Returns the perimeter of A.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_Length (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Length(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/sql/Function/#st_length2d","title":"ST_Length2D","text":"<p>Introduction: Returns the perimeter of A. This function is an alias of ST_Length.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: ST_Length2D (A:geometry)</p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Length2D(ST_GeomFromWKT('LINESTRING(38 16,38 50,65 50,66 16,38 16)'))\n</code></pre> <p>Output:</p> <pre><code>123.0147027033899\n</code></pre>"},{"location":"api/sql/Function/#st_lengthspheroid","title":"ST_LengthSpheroid","text":"<p>Introduction: Return the geodesic perimeter of A using WGS84 spheroid. Unit is meter. Works better for large geometries (country level) compared to <code>ST_Length</code> + <code>ST_Transform</code>. It is equivalent to PostGIS <code>ST_Length(geography, use_spheroid=true)</code> and <code>ST_LengthSpheroid</code> function and produces nearly identical results.</p> <p>Geometry must be in EPSG:4326 (WGS84) projection and must be in lon/lat order. You can use ST_FlipCoordinates to swap lat and lon.</p> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Warning</p> <p>Since <code>v1.7.0</code>, this function only supports LineString, MultiLineString, and GeometryCollections containing linear geometries. Use ST_Perimeter for polygons.</p> <p>Format: <code>ST_LengthSpheroid (A: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LengthSpheroid(ST_GeomFromWKT('LINESTRING (0 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>222638.98158654713\n</code></pre>"},{"location":"api/sql/Function/#st_linefrommultipoint","title":"ST_LineFromMultiPoint","text":"<p>Introduction: Creates a LineString from a MultiPoint geometry.</p> <p>Format: <code>ST_LineFromMultiPoint (A: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineFromMultiPoint(ST_GeomFromText('MULTIPOINT((10 40), (40 30), (20 20), (30 10))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (10 40, 40 30, 20 20, 30 10)\n</code></pre>"},{"location":"api/sql/Function/#st_lineinterpolatepoint","title":"ST_LineInterpolatePoint","text":"<p>Introduction: Returns a point interpolated along a line. First argument must be a LINESTRING. Second argument is a Double between 0 and 1 representing fraction of total linestring length the point has to be located.</p> <p>Format: <code>ST_LineInterpolatePoint (geom: Geometry, fraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineInterpolatePoint(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.2)\n</code></pre> <p>Output:</p> <pre><code>POINT (51.5974135047432 76.5974135047432)\n</code></pre>"},{"location":"api/sql/Function/#st_linelocatepoint","title":"ST_LineLocatePoint","text":"<p>Introduction: Returns a double between 0 and 1, representing the location of the closest point on the LineString as a fraction of its total length. The first argument must be a LINESTRING, and the second argument is a POINT geometry.</p> <p>Format: <code>ST_LineLocatePoint(linestring: Geometry, point: Geometry)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LineLocatePoint(ST_GeomFromWKT('LINESTRING(0 0, 1 1, 2 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/sql/Function/#st_linemerge","title":"ST_LineMerge","text":"<p>Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.</p> <p>Note</p> <p>Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.</p> <p>Format: <code>ST_LineMerge (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineMerge(ST_GeomFromWKT('MULTILINESTRING ((-29 -27, -30 -29.7, -45 -33), (-45 -33, -46 -32))'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (-29 -27, -30 -29.7, -45 -33, -46 -32)\n</code></pre>"},{"location":"api/sql/Function/#st_linesubstring","title":"ST_LineSubstring","text":"<p>Introduction: Return a linestring being a substring of the input one starting and ending at the given fractions of total 2d length. Second and third arguments are Double values between 0 and 1. This only works with LINESTRINGs.</p> <p>Format:</p> <p><code>ST_LineSubstring (geom: Geometry, startfraction: Double, endfraction: Double)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_LineSubstring(ST_GeomFromWKT('LINESTRING(25 50, 100 125, 150 190)'), 0.333, 0.666)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (69.28469348539744 94.28469348539744, 100 125, 111.70035626068274 140.21046313888758)\n</code></pre>"},{"location":"api/sql/Function/#st_locatealong","title":"ST_LocateAlong","text":"<p>Introduction: This function computes Point or MultiPoint geometries representing locations along a measured input geometry (LineString or MultiLineString) corresponding to the provided measure value(s). Polygonal geometry inputs are not supported. The output points lie directly on the input line at the specified measure positions.</p> <p>Additionally, an optional <code>offset</code> parameter can shift the resulting points left or right from the input line. A positive offset displaces the points to the left side, while a negative value offsets them to the right side by the given distance.</p> <p>This allows identifying precise locations along a measured linear geometry based on supplied measure values, with the ability to offset the output points if needed.</p> <p>Format:</p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double, offset: Double)</code></p> <p><code>ST_LocateAlong(linear: Geometry, measure: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LocateAlong(\n        ST_GeomFromText('LINESTRING M (10 30 1, 50 50 1, 30 110 2, 70 90 2, 180 140 3, 130 190 3)')\n)\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT M((30 110 2), (50 100 2), (70 90 2))\n</code></pre>"},{"location":"api/sql/Function/#st_longestline","title":"ST_LongestLine","text":"<p>Introduction: Returns the LineString geometry representing the maximum distance between any two points from the input geometries.</p> <p>Format: <code>ST_LongestLine(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_LongestLine(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (40 40, 10 20)\n</code></pre>"},{"location":"api/sql/Function/#st_m","title":"ST_M","text":"<p>Introduction: Returns M Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_M(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_M(ST_MakePoint(1, 2, 3, 4))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_mmax","title":"ST_MMax","text":"<p>Introduction: Returns M maxima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMax(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MMax(\n        ST_GeomFromWKT('POLYGON ZM ((30 10 5 1, 40 40 10 2, 20 40 15 3, 10 20 20 4, 30 10 5 1))')\n)\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_mmin","title":"ST_MMin","text":"<p>Introduction: Returns M minima of the given geometry or null if there is no M coordinate.</p> <p>Format: <code>ST_MMin(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MMin(\n        ST_GeomFromWKT('LINESTRING ZM(1 1 1 1, 2 2 2 2, 3 3 3 3, -1 -1 -1 -1)')\n)\n</code></pre> <p>Output:</p> <pre><code>-1.0\n</code></pre>"},{"location":"api/sql/Function/#st_makeline","title":"ST_MakeLine","text":"<p>Introduction: Creates a LineString containing the points of Point, MultiPoint, or LineString geometries. Other geometry types cause an error.</p> <p>Format:</p> <p><code>ST_MakeLine(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_MakeLine(geoms: ARRAY[Geometry])</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_MakeLine(ST_Point(1,2), ST_Point(3,4)) );\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(1 2,3 4)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_MakeLine( 'LINESTRING(0 0, 1 1)', 'LINESTRING(2 2, 3 3)' ) );\n</code></pre> <p>Output:</p> <pre><code> LINESTRING(0 0,1 1,2 2,3 3)\n</code></pre>"},{"location":"api/sql/Function/#st_makepolygon","title":"ST_MakePolygon","text":"<p>Introduction: Function to convert closed linestring to polygon including holes</p> <p>Format: <code>ST_MakePolygon(geom: Geometry, holes: ARRAY[Geometry])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MakePolygon(\n        ST_GeomFromText('LINESTRING(7 -1, 7 6, 9 6, 9 1, 7 -1)'),\n        ARRAY(ST_GeomFromText('LINESTRING(6 2, 8 2, 8 1, 6 1, 6 2)'))\n    )\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((7 -1, 7 6, 9 6, 9 1, 7 -1), (6 2, 8 2, 8 1, 6 1, 6 2))\n</code></pre>"},{"location":"api/sql/Function/#st_makevalid","title":"ST_MakeValid","text":"<p>Introduction: Given an invalid geometry, create a valid representation of the geometry.</p> <p>Collapsed geometries are either converted to empty (keepCollapsed=true) or a valid geometry of lower dimension (keepCollapsed=false). Default is keepCollapsed=false.</p> <p>Format:</p> <p><code>ST_MakeValid (A: Geometry)</code></p> <p><code>ST_MakeValid (A: Geometry, keepCollapsed: Boolean)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>WITH linestring AS (\n    SELECT ST_GeomFromWKT('LINESTRING(1 1, 1 1)') AS geom\n) SELECT ST_MakeValid(geom), ST_MakeValid(geom, true) FROM linestring\n</code></pre> <p>Result:</p> <pre><code>+------------------+------------------------+\n|st_makevalid(geom)|st_makevalid(geom, true)|\n+------------------+------------------------+\n|  LINESTRING EMPTY|             POINT (1 1)|\n+------------------+------------------------+\n</code></pre> <p>Note</p> <p>In Sedona up to and including version 1.2 the behaviour of ST_MakeValid was different.</p> <p>Be sure to check you code when upgrading. The previous implementation only worked for (multi)polygons and had a different interpretation of the second, boolean, argument. It would also sometimes return multiple geometries for a single geometry input.</p>"},{"location":"api/sql/Function/#st_maximuminscribedcircle","title":"ST_MaximumInscribedCircle","text":"<p>Introduction: Finds the largest circle that is contained within a (multi)polygon, or which does not overlap any lines and points. Returns a row with fields:</p> <ul> <li><code>center</code> - center point of the circle</li> <li><code>nearest</code> - nearest point from the center of the circle</li> <li><code>radius</code> - radius of the circle</li> </ul> <p>For polygonal geometries, the function inscribes the circle within the boundary rings, treating internal rings as additional constraints. When processing linear and point inputs, the algorithm inscribes the circle within the convex hull of the input, utilizing the input lines and points as additional boundary constraints.</p> <p>Format: <code>ST_MaximumInscribedCircle(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT inscribedCircle.* FROM (\n    SELECT ST_MaximumIncribedCircle(ST_GeomFromWKT('POLYGON ((62.11 19.68, 60.79 17.20, 61.30 15.96, 62.11 16.08, 65.93 16.95, 66.20 20.61, 63.08 21.43, 64.48 18.70, 62.11 19.68))')) AS inscribedCircle\n)\n</code></pre> <p>Output:</p> <pre><code>+---------------------------------------------+-------------------------------------------+------------------+\n|center                                       |nearest                                    |radius            |\n+---------------------------------------------+-------------------------------------------+------------------+\n|POINT (62.794975585937514 17.774780273437496)|POINT (63.36773534817729 19.15992378007859)|1.4988916836219184|\n+---------------------------------------------+-------------------------------------------+------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_maxdistance","title":"ST_MaxDistance","text":"<p>Introduction: Calculates and returns the length value representing the maximum distance between any two points across the input geometries. This function is an alias for <code>ST_LongestDistance</code>.</p> <p>Format: <code>ST_MaxDistance(geom1: Geometry, geom2: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MaxDistance(\n        ST_GeomFromText(\"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\"),\n        ST_GeomFromText(\"POLYGON ((10 20, 30 30, 40 20, 30 10, 10 20))\")\n)\n</code></pre> <p>Output:</p> <pre><code>36.05551275463989\n</code></pre>"},{"location":"api/sql/Function/#st_minimumclearance","title":"ST_MinimumClearance","text":"<p>Introduction: The minimum clearance is a metric that quantifies a geometry's tolerance to changes in coordinate precision or vertex positions. It represents the maximum distance by which vertices can be adjusted without introducing invalidity to the geometry's structure. A larger minimum clearance value indicates greater robustness against such perturbations.</p> <p>For a geometry with a minimum clearance of <code>x</code>, the following conditions hold:</p> <ul> <li>No two distinct vertices are separated by a distance less than <code>x</code>.</li> <li>No vertex lies within a distance <code>x</code> from any line segment it is not an endpoint of.</li> </ul> <p>For geometries with no definable minimum clearance, such as single Point geometries or MultiPoint geometries where all points occupy the same location, the function returns <code>Double.MAX_VALUE</code>.</p> <p>Format: <code>ST_MinimumClearance(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumClearance(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>0.5\n</code></pre>"},{"location":"api/sql/Function/#st_minimumclearanceline","title":"ST_MinimumClearanceLine","text":"<p>Introduction: This function returns a two-point LineString geometry representing the minimum clearance distance of the input geometry. If the input geometry does not have a defined minimum clearance, such as for single Points or coincident MultiPoints, an empty LineString geometry is returned instead.</p> <p>Format: <code>ST_MinimumClearanceLine(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_MinimumClearanceLine(\n        ST_GeomFromWKT('POLYGON ((65 18, 62 16, 64.5 16, 62 14, 65 14, 65 18))')\n)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (64.5 16, 65 16)\n</code></pre>"},{"location":"api/sql/Function/#st_minimumboundingcircle","title":"ST_MinimumBoundingCircle","text":"<p>Introduction: Returns the smallest circle polygon that contains a geometry. The optional quadrantSegments parameter determines how many segments to use per quadrant and the default number of segments has been changed to 48 since v1.5.0.</p> <p>Format:</p> <p><code>ST_MinimumBoundingCircle(geom: Geometry, [Optional] quadrantSegments: Integer)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumBoundingCircle(ST_GeomFromWKT('LINESTRING(0 0, 0 1)'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0.5 0.5, 0.4997322937381828 0.4836404585891119, 0.4989294616193017 0.4672984353849285, 0.4975923633360985 0.4509914298352197, 0.4957224306869052 0.4347369038899742, 0.4933216660424395 0.4185522633027057, 0.4903926402016152 0.4024548389919359, 0.4869384896386668 0.3864618684828134, 0.4829629131445342 0.3705904774487396, 0.4784701678661044 0.3548576613727689, 0.4734650647475528 0.3392802673484192, 0.4679529633786629 0.3238749760393833, 0.4619397662556434 0.3086582838174551, 0.4554319124605879 0.2936464850978027, 0.4484363707663442 0.2788556548904993, 0.4409606321741775 0.2643016315870012, 0.4330127018922194 0.25, 0.4246010907632894 0.2359660746748161, 0.4157348061512726 0.2222148834901989, 0.4064233422958076 0.2087611515660989, 0.3966766701456176 0.1956192854956397, 0.3865052266813685 0.1828033579181773, 0.3759199037394887 0.1703270924499656, 0.3649320363489179 0.1582038489885644, 0.3535533905932738 0.1464466094067263, 0.3417961510114357 0.1350679636510822, 0.3296729075500345 0.1240800962605114, 0.3171966420818228 0.1134947733186316, 0.3043807145043603 0.1033233298543824, 0.2912388484339011 0.0935766577041924, 0.2777851165098012 0.0842651938487274, 0.264033925325184 0.0753989092367106, 0.2500000000000001 0.0669872981077807, 0.2356983684129989 0.0590393678258225, 0.2211443451095007 0.0515636292336559, 0.2063535149021975 0.0445680875394122, 0.1913417161825449 0.0380602337443566, 0.1761250239606168 0.0320470366213372, 0.1607197326515808 0.0265349352524472, 0.1451423386272312 0.0215298321338956, 0.1294095225512605 0.0170370868554659, 0.1135381315171867 0.0130615103613332, 0.0975451610080642 0.0096073597983848, 0.0814477366972944 0.0066783339575605, 0.0652630961100259 0.0042775693130948, 0.0490085701647804 0.0024076366639016, 0.0327015646150716 0.0010705383806983, 0.0163595414108882 0.0002677062618172, 0 0, -0.016359541410888 0.0002677062618172, -0.0327015646150715 0.0010705383806983, -0.0490085701647802 0.0024076366639015, -0.0652630961100257 0.0042775693130948, -0.0814477366972942 0.0066783339575605, -0.097545161008064 0.0096073597983847, -0.1135381315171866 0.0130615103613332, -0.1294095225512603 0.0170370868554658, -0.1451423386272311 0.0215298321338955, -0.1607197326515807 0.0265349352524472, -0.1761250239606166 0.0320470366213371, -0.1913417161825448 0.0380602337443566, -0.2063535149021973 0.044568087539412, -0.2211443451095006 0.0515636292336558, -0.2356983684129987 0.0590393678258224, -0.2499999999999999 0.0669872981077806, -0.264033925325184 0.0753989092367106, -0.277785116509801 0.0842651938487273, -0.291238848433901 0.0935766577041924, -0.3043807145043602 0.1033233298543823, -0.3171966420818227 0.1134947733186314, -0.3296729075500343 0.1240800962605111, -0.3417961510114356 0.1350679636510821, -0.3535533905932737 0.1464466094067262, -0.3649320363489177 0.1582038489885642, -0.3759199037394886 0.1703270924499655, -0.3865052266813683 0.1828033579181771, -0.3966766701456175 0.1956192854956396, -0.4064233422958076 0.2087611515660989, -0.4157348061512725 0.2222148834901987, -0.4246010907632894 0.235966074674816, -0.4330127018922192 0.2499999999999998, -0.4409606321741775 0.264301631587001, -0.4484363707663441 0.2788556548904991, -0.4554319124605878 0.2936464850978025, -0.4619397662556434 0.3086582838174551, -0.4679529633786628 0.3238749760393831, -0.4734650647475528 0.3392802673484191, -0.4784701678661044 0.3548576613727686, -0.4829629131445341 0.3705904774487395, -0.4869384896386668 0.3864618684828132, -0.4903926402016152 0.4024548389919357, -0.4933216660424395 0.4185522633027056, -0.4957224306869052 0.434736903889974, -0.4975923633360984 0.4509914298352196, -0.4989294616193017 0.4672984353849282, -0.4997322937381828 0.4836404585891118, -0.5 0.4999999999999999, -0.4997322937381828 0.5163595414108879, -0.4989294616193017 0.5327015646150715, -0.4975923633360985 0.5490085701647801, -0.4957224306869052 0.5652630961100257, -0.4933216660424395 0.5814477366972941, -0.4903926402016153 0.597545161008064, -0.4869384896386668 0.6135381315171865, -0.4829629131445342 0.6294095225512601, -0.4784701678661045 0.645142338627231, -0.4734650647475529 0.6607197326515806, -0.4679529633786629 0.6761250239606166, -0.4619397662556435 0.6913417161825446, -0.455431912460588 0.7063535149021972, -0.4484363707663442 0.7211443451095005, -0.4409606321741776 0.7356983684129986, -0.4330127018922194 0.7499999999999999, -0.4246010907632896 0.7640339253251838, -0.4157348061512727 0.777785116509801, -0.4064233422958078 0.7912388484339008, -0.3966766701456177 0.8043807145043602, -0.3865052266813686 0.8171966420818226, -0.3759199037394889 0.8296729075500342, -0.3649320363489179 0.8417961510114356, -0.353553390593274 0.8535533905932735, -0.3417961510114358 0.8649320363489177, -0.3296729075500345 0.8759199037394887, -0.317196642081823 0.8865052266813683, -0.3043807145043604 0.8966766701456175, -0.2912388484339011 0.9064233422958076, -0.2777851165098015 0.9157348061512725, -0.2640339253251843 0.9246010907632893, -0.2500000000000002 0.9330127018922192, -0.235698368412999 0.9409606321741775, -0.2211443451095007 0.9484363707663441, -0.2063535149021977 0.9554319124605877, -0.1913417161825452 0.9619397662556433, -0.176125023960617 0.9679529633786628, -0.1607197326515809 0.9734650647475528, -0.1451423386272312 0.9784701678661044, -0.1294095225512608 0.9829629131445341, -0.1135381315171869 0.9869384896386668, -0.0975451610080643 0.9903926402016152, -0.0814477366972945 0.9933216660424395, -0.0652630961100262 0.9957224306869051, -0.0490085701647807 0.9975923633360984, -0.0327015646150718 0.9989294616193017, -0.0163595414108883 0.9997322937381828, -0.0000000000000001 1, 0.0163595414108876 0.9997322937381828, 0.0327015646150712 0.9989294616193019, 0.04900857016478 0.9975923633360985, 0.0652630961100256 0.9957224306869052, 0.0814477366972943 0.9933216660424395, 0.0975451610080637 0.9903926402016153, 0.1135381315171863 0.9869384896386669, 0.1294095225512601 0.9829629131445342, 0.145142338627231 0.9784701678661045, 0.1607197326515807 0.9734650647475529, 0.1761250239606164 0.967952963378663, 0.1913417161825446 0.9619397662556435, 0.2063535149021972 0.955431912460588, 0.2211443451095005 0.9484363707663442, 0.2356983684129984 0.9409606321741777, 0.2499999999999997 0.9330127018922195, 0.2640339253251837 0.9246010907632896, 0.2777851165098009 0.9157348061512727, 0.291238848433901 0.9064233422958077, 0.3043807145043599 0.8966766701456179, 0.3171966420818225 0.8865052266813687, 0.3296729075500342 0.8759199037394889, 0.3417961510114355 0.8649320363489179, 0.3535533905932737 0.8535533905932738, 0.3649320363489175 0.841796151011436, 0.3759199037394885 0.8296729075500346, 0.3865052266813683 0.817196642081823, 0.3966766701456175 0.8043807145043604, 0.4064233422958076 0.7912388484339011, 0.4157348061512723 0.7777851165098015, 0.4246010907632893 0.7640339253251842, 0.4330127018922192 0.7500000000000002, 0.4409606321741774 0.735698368412999, 0.4484363707663439 0.7211443451095011, 0.4554319124605877 0.7063535149021978, 0.4619397662556433 0.6913417161825453, 0.4679529633786628 0.676125023960617, 0.4734650647475528 0.6607197326515809, 0.4784701678661043 0.6451423386272317, 0.482962913144534 0.6294095225512608, 0.4869384896386668 0.613538131517187, 0.4903926402016152 0.5975451610080643, 0.4933216660424395 0.5814477366972945, 0.4957224306869051 0.5652630961100262, 0.4975923633360984 0.5490085701647807, 0.4989294616193017 0.5327015646150718, 0.4997322937381828 0.5163595414108882, 0.5 0.5))\n</code></pre>"},{"location":"api/sql/Function/#st_minimumboundingradius","title":"ST_MinimumBoundingRadius","text":"<p>Introduction: Returns a struct containing the center point and radius of the smallest circle that contains a geometry.</p> <p>Format: <code>ST_MinimumBoundingRadius(geom: Geometry)</code></p> <p>Since: <code>v1.0.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_MinimumBoundingRadius(ST_GeomFromText('POLYGON((1 1,0 0, -1 1, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>{POINT (0 1), 1.0}\n</code></pre>"},{"location":"api/sql/Function/#st_multi","title":"ST_Multi","text":"<p>Introduction: Returns a MultiGeometry object based on the geometry input. ST_Multi is basically an alias for ST_Collect with one geometry.</p> <p>Format: <code>ST_Multi(geom: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Multi(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT (1 1)\n</code></pre>"},{"location":"api/sql/Function/#st_ndims","title":"ST_NDims","text":"<p>Introduction: Returns the coordinate dimension of the geometry.</p> <p>Format: <code>ST_NDims(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>Spark SQL example with z coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromEWKT('POINT(1 1 2)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Spark SQL example with x,y coordinate:</p> <pre><code>SELECT ST_NDims(ST_GeomFromText('POINT(1 1)'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_normalize","title":"ST_Normalize","text":"<p>Introduction: Returns the input geometry in its normalized form.</p> <p>Format:</p> <p><code>ST_Normalize(geom: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_Normalize(ST_GeomFromWKT('POLYGON((0 1, 1 1, 1 0, 0 0, 0 1))')))\n</code></pre> <p>Result:</p> <pre><code>POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_npoints","title":"ST_NPoints","text":"<p>Introduction: Return points of the geometry</p> <p>Format: <code>ST_NPoints (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NPoints(ST_GeomFromText('LINESTRING(77.29 29.07,77.42 29.26,77.27 29.31,77.29 29.07)'))\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Function/#st_nrings","title":"ST_NRings","text":"<p>Introduction: Returns the number of rings in a Polygon or MultiPolygon. Contrary to ST_NumInteriorRings, this function also takes into account the number of  exterior rings.</p> <p>This function returns 0 for an empty Polygon or MultiPolygon. If the geometry is not a Polygon or MultiPolygon, an IllegalArgument Exception is thrown.</p> <p>Format: <code>ST_NRings(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>Examples:</p> <p>Input: <code>POLYGON ((1 0, 1 1, 2 1, 2 0, 1 0))</code></p> <p>Output: <code>1</code></p> <p>Input: <code>'MULTIPOLYGON (((1 0, 1 6, 6 6, 6 0, 1 0), (2 1, 2 2, 3 2, 3 1, 2 1)), ((10 0, 10 6, 16 6, 16 0, 10 0), (12 1, 12 2, 13 2, 13 1, 12 1)))'</code></p> <p>Output: <code>4</code></p> <p>Input: <code>'POLYGON EMPTY'</code></p> <p>Output: <code>0</code></p> <p>Input: <code>'LINESTRING (1 0, 1 1, 2 1)'</code></p> <p>Output: <code>Unsupported geometry type: LineString, only Polygon or MultiPolygon geometries are supported.</code></p>"},{"location":"api/sql/Function/#st_numgeometries","title":"ST_NumGeometries","text":"<p>Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.</p> <p>Format: <code>ST_NumGeometries (A: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumGeometries(ST_GeomFromWKT('LINESTRING (-29 -27, -30 -29.7, -45 -33)'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorring","title":"ST_NumInteriorRing","text":"<p>Introduction: Returns number of interior rings of polygon geometries. It is an alias of ST_NumInteriorRings.</p> <p>Format: <code>ST_NumInteriorRing(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRing(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numinteriorrings","title":"ST_NumInteriorRings","text":"<p>Introduction: RETURNS number of interior rings of polygon geometries.</p> <p>Format: <code>ST_NumInteriorRings(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumInteriorRings(ST_GeomFromText('POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))'))\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Function/#st_numpoints","title":"ST_NumPoints","text":"<p>Introduction: Returns number of points in a LineString</p> <p>Note</p> <p>If any other geometry is provided as an argument, an IllegalArgumentException is thrown. Example: <code>SELECT ST_NumPoints(ST_GeomFromWKT('MULTIPOINT ((0 0), (1 1), (0 1), (2 2))'))</code></p> <p>Output: <code>IllegalArgumentException: Unsupported geometry type: MultiPoint, only LineString geometry is supported.</code></p> <p>Format: <code>ST_NumPoints(geom: Geometry)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_NumPoints(ST_GeomFromText('LINESTRING(0 1, 1 0, 2 0)'))\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Function/#st_perimeter","title":"ST_Perimeter","text":"<p>Introduction: This function calculates the 2D perimeter of a given geometry. It supports Polygon, MultiPolygon, and GeometryCollection geometries (as long as the GeometryCollection contains polygonal geometries). For other types, it returns 0. To measure lines, use ST_Length.</p> <p>To get the perimeter in meters, set <code>use_spheroid</code> to <code>true</code>. This calculates the geodesic perimeter using the WGS84 spheroid. When using <code>use_spheroid</code>, the <code>lenient</code> parameter defaults to true, assuming the geometry uses EPSG:4326. To throw an exception instead, set <code>lenient</code> to <code>false</code>.</p> <p>Format:</p> <p><code>ST_Perimeter(geom: Geometry)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean)</code></p> <p><code>ST_Perimeter(geom: Geometry, use_spheroid: Boolean, lenient: Boolean = True)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')\n)\n</code></pre> <p>Output:</p> <pre><code>20.0\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Perimeter(\n        ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))', 4326),\n        true, false\n)\n</code></pre> <p>Output:</p> <pre><code>2216860.5497177234\n</code></pre>"},{"location":"api/sql/Function/#st_pointn","title":"ST_PointN","text":"<p>Introduction: Return the Nth point in a single linestring or circular linestring in the geometry. Negative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.</p> <p>Format: <code>ST_PointN(geom: Geometry, n: Integer)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_PointN(ST_GeomFromText(\"LINESTRING(0 0, 1 2, 2 4, 3 6)\"), 2)\n</code></pre> <p>Result:</p> <pre><code>POINT (1 2)\n</code></pre>"},{"location":"api/sql/Function/#st_pointonsurface","title":"ST_PointOnSurface","text":"<p>Introduction: Returns a POINT guaranteed to lie on the surface.</p> <p>Format: <code>ST_PointOnSurface(A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>Examples:</p> <pre><code>SELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POINT(0 5)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5, 0 10)')));\n st_astext\n------------\n POINT(0 5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('POLYGON((0 0, 0 5, 5 5, 5 0, 0 0))')));\n   st_astext\n----------------\n POINT(2.5 2.5)\n\nSELECT ST_AsText(ST_PointOnSurface(ST_GeomFromText('LINESTRING(0 5 1, 0 0 1, 0 10 2)')));\n   st_astext\n----------------\n POINT Z(0 0 1)\n</code></pre>"},{"location":"api/sql/Function/#st_points","title":"ST_Points","text":"<p>Introduction: Returns a MultiPoint geometry consisting of all the coordinates of the input geometry. It preserves duplicate points as well as M and Z coordinates.</p> <p>Format: <code>ST_Points(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Points(ST_GeomFromEWKT('LINESTRING (2 4, 3 3, 4 2, 7 3)')));\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((2 4), (3 3), (4 2), (7,3))\n</code></pre>"},{"location":"api/sql/Function/#st_polygon","title":"ST_Polygon","text":"<p>Introduction: Function to create a polygon built from the given LineString and sets the spatial reference system from the srid</p> <p>Format: <code>ST_Polygon(geom: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText( ST_Polygon(ST_GeomFromEWKT('LINESTRING(75 29 1, 77 29 2, 77 29 3, 75 29 1)'), 4326) );\n</code></pre> <p>Output:</p> <pre><code>POLYGON((75 29 1, 77 29 2, 77 29 3, 75 29 1))\n</code></pre>"},{"location":"api/sql/Function/#st_polygonize","title":"ST_Polygonize","text":"<p>Introduction: Generates a GeometryCollection composed of polygons that are formed from the linework of an input GeometryCollection. When the input does not contain any linework that forms a polygon, the function will return an empty GeometryCollection.</p> <p>Note</p> <p><code>ST_Polygonize</code> function assumes that the input geometries form a valid and simple closed linestring that can be turned into a polygon. If the input geometries are not noded or do not form such linestrings, the resulting GeometryCollection may be empty or may not contain the expected polygons.</p> <p>Format: <code>ST_Polygonize(geom: Geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>Example:</p> <pre><code>SELECT ST_AsText(ST_Polygonize(ST_GeomFromEWKT('GEOMETRYCOLLECTION (LINESTRING (2 0, 2 1, 2 2), LINESTRING (2 2, 2 3, 2 4), LINESTRING (0 2, 1 2, 2 2), LINESTRING (2 2, 3 2, 4 2), LINESTRING (0 2, 1 3, 2 4), LINESTRING (2 4, 3 3, 4 2))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 2, 1 3, 2 4, 2 3, 2 2, 1 2, 0 2)), POLYGON ((2 2, 2 3, 2 4, 3 3, 4 2, 3 2, 2 2)))\n</code></pre>"},{"location":"api/sql/Function/#st_project","title":"ST_Project","text":"<p>Introduction: Calculates a new point location given a starting point, distance, and azimuth. The azimuth indicates the direction, expressed in radians, and is measured in a clockwise manner starting from true north. The system can handle azimuth values that are negative or exceed 2\u03c0 (360 degrees). The optional <code>lenient</code> parameter prevents an error if the input geometry is not a Point. Its default value is <code>false</code>.</p> <p>Format:</p> <pre><code>ST_Project(point: Geometry, distance: Double, azimuth: Double, lenient: Boolean = False)\n</code></pre> <pre><code>ST_Project(point: Geometry, distance: Double, Azimuth: Double)\n</code></pre> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Project(ST_GeomFromText('POINT (10 15)'), 100, radians(90))\n</code></pre> <p>Output:</p> <pre><code>POINT (110 14.999999999999975)\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Project(\n        ST_GeomFromText('POLYGON ((1 5, 1 1, 3 3, 5 3, 1 5))'),\n        25, radians(270), true)\n</code></pre> <p>Output:</p> <pre><code>POINT EMPTY\n</code></pre>"},{"location":"api/sql/Function/#st_reduceprecision","title":"ST_ReducePrecision","text":"<p>Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded. This function was called ST_PrecisionReduce in versions prior to v1.5.0.</p> <p>Format: <code>ST_ReducePrecision (A: Geometry, B: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ReducePrecision(ST_GeomFromWKT('Point(0.1234567890123456789 0.1234567890123456789)')\n    , 9)\n</code></pre> <p>The new coordinates will only have 9 decimal places.</p> <p>Output:</p> <pre><code>POINT (0.123456789 0.123456789)\n</code></pre>"},{"location":"api/sql/Function/#st_removepoint","title":"ST_RemovePoint","text":"<p>Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.</p> <p>Format:</p> <p><code>ST_RemovePoint(geom: Geometry, position: Integer)</code></p> <p><code>ST_RemovePoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_RemovePoint(ST_GeomFromText(\"LINESTRING(0 0, 1 1, 1 0)\"), 1)\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(0 0, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_removerepeatedpoints","title":"ST_RemoveRepeatedPoints","text":"<p>Introduction: This function eliminates consecutive duplicate points within a geometry, preserving endpoints of LineStrings. It operates on (Multi)LineStrings, (Multi)Polygons, and MultiPoints, processing GeometryCollection elements individually. When an optional 'tolerance' value is provided, vertices within that distance are also considered duplicates.</p> <p>Format:</p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry, tolerance: Double)</code></p> <p><code>ST_RemoveRepeatedPoints(geom: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('MULTIPOINT ((20 20), (10 10), (30 30), (40 40), (20 20), (30 30), (40 40))')\n       )\n</code></pre> <p>Output:</p> <pre><code>MULTIPOINT ((20 20), (10 10), (30 30), (40 40))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)')\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)\n</code></pre> <p>SQL Example: Each geometry within a collection is processed independently.</p> <pre><code>ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('GEOMETRYCOLLECTION (POINT (10 10), POINT(10 10), LINESTRING (20 20, 20 20, 30 30, 30 30), MULTIPOINT ((80 80), (90 90), (90 90), (100 100)))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POINT (10 10), POINT (10 10), LINESTRING (20 20, 30 30), MULTIPOINT ((80 80), (90 90), (100 100)))\n</code></pre> <p>SQL Example: Elimination of repeated points within a specified distance tolerance.</p> <pre><code>SELECT ST_RemoveRepeatedPoints(\n        ST_GeomFromWKT('LINESTRING (20 20, 10 10, 30 30, 40 40, 20 20, 30 30, 40 40)'),\n        20\n       )\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (20 20, 40 40, 20 20, 40 40)\n</code></pre>"},{"location":"api/sql/Function/#st_reverse","title":"ST_Reverse","text":"<p>Introduction: Return the geometry with vertex order reversed</p> <p>Format: <code>ST_Reverse (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Reverse(ST_GeomFromWKT('LINESTRING(0 0, 1 2, 2 4, 3 6)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (3 6, 2 4, 1 2, 0 0)\n</code></pre>"},{"location":"api/sql/Function/#st_rotate","title":"ST_Rotate","text":"<p>Introduction: Rotates a geometry by a specified angle in radians counter-clockwise around a given origin point. The origin for rotation can be specified as either a POINT geometry or x and y coordinates. If the origin is not specified, the geometry is rotated around POINT(0 0).</p> <p>Formats;</p> <p><code>ST_Rotate (geometry: Geometry, angle: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, originX: Double, originY: Double)</code></p> <p><code>ST_Rotate (geometry: Geometry, angle: Double, pointOrigin: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Rotate(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10, 0, 0)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 -0.5440211108893698, -0.2950504181870827 -1.383092639965822, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_rotatex","title":"ST_RotateX","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the X-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateX(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateX(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, 1 0, 1 -0.8390715290764524, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_rotatey","title":"ST_RotateY","text":"<p>Introduction: Performs a counter-clockwise rotation of the specified geometry around the Y-axis by the given angle measured in radians.</p> <p>Format: <code>ST_RotateY(geometry: Geometry, angle: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RotateY(ST_GeomFromEWKT('SRID=4326;POLYGON ((0 0, 1 0, 1 1, 0 0))'), 10)\n</code></pre> <p>Output:</p> <pre><code>SRID=4326;POLYGON ((0 0, -0.8390715290764524 0, -0.8390715290764524 1, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_s2cellids","title":"ST_S2CellIDs","text":"<p>Introduction: Cover the geometry with Google S2 Cells, return the corresponding cell IDs with the given level. The level indicates the size of cells. With a bigger level, the cells will be smaller, the coverage will be more accurate, but the result size will be exponentially increasing.</p> <p>Format: <code>ST_S2CellIDs(geom: Geometry, level: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_S2CellIDs(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'), 6)\n</code></pre> <p>Output:</p> <pre><code>[1159395429071192064, 1159958379024613376, 1160521328978034688, 1161084278931456000, 1170091478186196992, 1170654428139618304]\n</code></pre>"},{"location":"api/sql/Function/#st_s2togeom","title":"ST_S2ToGeom","text":"<p>Introduction: Returns an array of Polygons for the corresponding S2 cell IDs.</p> <p>Hint</p> <p>To convert a Polygon array to MultiPolygon, use ST_Collect. However, the result may be an invalid geometry. Apply ST_MakeValid to the <code>ST_Collect</code> output to ensure a valid MultiPolygon.</p> <p>An alternative approach to consolidate a Polygon array into a Polygon/MultiPolygon, use the ST_Union function.</p> <p>Format: <code>ST_S2ToGeom(cellIds: Array[Long])</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_S2ToGeom(array(11540474045136890))\n</code></pre> <p>Output:</p> <pre><code>[POLYGON ((-36.609392788630245 -38.169532607255846, -36.609392706252954 -38.169532607255846, -36.609392706252954 -38.169532507473015, -36.609392788630245 -38.169532507473015, -36.609392788630245 -38.169532607255846))]\n</code></pre>"},{"location":"api/sql/Function/#st_scale","title":"ST_Scale","text":"<p>Introduction: This function scales the geometry to a new size by multiplying the ordinates with the corresponding scaling factors provided as parameters <code>scaleX</code> and <code>scaleY</code>.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format: <code>ST_Scale(geometry: Geometry, scaleX: Double, scaleY: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       3, 2\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre>"},{"location":"api/sql/Function/#st_scalegeom","title":"ST_ScaleGeom","text":"<p>Introduction: This function scales the input geometry (<code>geometry</code>) to a new size. It does this by multiplying the coordinates of the input geometry with corresponding values from another geometry (<code>factor</code>) representing the scaling factors.</p> <p>To scale the geometry relative to a point other than the true origin (e.g., scaling a polygon in place using its centroid), you can use the three-geometry variant of this function. This variant requires an additional geometry (<code>origin</code>) representing the \"false origin\" for the scaling operation. If no <code>origin</code> is provided, the scaling occurs relative to the true origin, with all coordinates of the input geometry simply multiplied by the corresponding scale factors.</p> <p>Note</p> <p>This function is designed for scaling 2D geometries. While it currently doesn't support scaling the Z and M coordinates, it preserves these values during the scaling operation.</p> <p>Format:</p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry, origin: Geometry)</code></p> <p><code>ST_ScaleGeom(geometry: Geometry, factor: Geometry)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0, 0 3, 4.5 3, 4.5 0, 0 0))\n</code></pre> <p>SQL Example:</p> <pre><code>SELECT ST_Scale(\n        ST_GeomFromWKT('POLYGON ((0 0, 0 1.5, 1.5 1.5, 1.5 0, 0 0))'),\n       ST_Point(3, 2), ST_Point(1, 2)\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((-2 -2, -2 1, 2.5 1, 2.5 -2, -2 -2))\n</code></pre>"},{"location":"api/sql/Function/#st_setpoint","title":"ST_SetPoint","text":"<p>Introduction: Replace Nth point of linestring with given point. Index is 0-based. Negative index are counted backwards, e.g., -1 is last point.</p> <p>Format: <code>ST_SetPoint (linestring: Geometry, index: Integer, point: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SetPoint(ST_GeomFromText('LINESTRING (0 0, 0 1, 1 1)'), 2, ST_GeomFromText('POINT (1 0)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING (0 0, 0 1, 1 0)\n</code></pre>"},{"location":"api/sql/Function/#st_setsrid","title":"ST_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SetSRID (A: Geometry, srid: Integer)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsEWKT(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>SRID=3021;POLYGON ((1 1, 8 1, 8 8, 1 8, 1 1))\n</code></pre>"},{"location":"api/sql/Function/#st_shiftlongitude","title":"ST_ShiftLongitude","text":"<p>Introduction: Modifies longitude coordinates in geometries, shifting values between -180..0 degrees to 180..360 degrees and vice versa. This is useful for normalizing data across the International Date Line and standardizing coordinate ranges for visualization and spheroidal calculations.</p> <p>Note</p> <p>This function is only applicable to geometries that use lon/lat coordinate systems.</p> <p>Format: <code>ST_ShiftLongitude (geom: geometry)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL example:</p> <pre><code>SELECT ST_ShiftLongitude(ST_GeomFromText('LINESTRING(177 10, 179 10, -179 10, -177 10)'))\n</code></pre> <p>Output:</p> <pre><code>LINESTRING(177 10, 179 10, 181 10, 183 10)\n</code></pre>"},{"location":"api/sql/Function/#st_simplify","title":"ST_Simplify","text":"<p>Introduction: This function simplifies the input geometry by applying the Douglas-Peucker algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_Simplify(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.7.0</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_Simplify(ST_Buffer(ST_GeomFromWKT('POINT (0 2)'), 10), 1)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 2, 7.0710678118654755 -5.071067811865475, 0.0000000000000006 -8, -7.071067811865475 -5.0710678118654755, -10 1.9999999999999987, -7.071067811865477 9.071067811865476, -0.0000000000000018 12, 7.071067811865474 9.071067811865477, 10 2))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifypolygonhull","title":"ST_SimplifyPolygonHull","text":"<p>Introduction: This function computes a topology-preserving simplified hull, either outer or inner, for a polygonal geometry input. An outer hull fully encloses the original geometry, while an inner hull lies entirely within. The result maintains the same structure as the input, including handling of MultiPolygons and holes, represented as a polygonal geometry formed from a subset of vertices.</p> <p>Vertex reduction is governed by the <code>vertexFactor</code> parameter ranging from 0 to 1, with lower values yielding simpler outputs with fewer vertices and reduced concavity. For both hull types, a <code>vertexFactor</code> of 1.0 returns the original geometry. Specifically, for outer hulls, 0.0 computes the convex hull; for inner hulls, 0.0 produces a triangular geometry.</p> <p>The simplification algorithm iteratively removes concave corners containing the least area until reaching the target vertex count. It preserves topology by preventing edge crossings, ensuring the output is a valid polygonal geometry in all cases.</p> <p>Format:</p> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double, isOuter: Boolean = true)\n</code></pre> <pre><code>ST_SimplifyPolygonHull(geom: Geometry, vertexFactor: Double)\n</code></pre> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 40 40, 45 45, 60 50, 65 45, 80 25, 70 10, 30 10))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPolygonHull(\n        ST_GeomFromText('POLYGON ((30 10, 40 40, 45 45, 50 30, 55 25, 60 50, 65 45, 70 30, 75 20, 80 25, 70 10, 30 10))'),\n       0.4, false\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((30 10, 70 10, 60 50, 55 25, 30 10))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifypreservetopology","title":"ST_SimplifyPreserveTopology","text":"<p>Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input, and with the components having the same topological relationship.</p> <p>Since: <code>v1.0.0</code></p> <p>Format: <code>ST_SimplifyPreserveTopology (A: Geometry, distanceTolerance: Double)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyPreserveTopology(ST_GeomFromText('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 10)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 35 36, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/sql/Function/#st_simplifyvw","title":"ST_SimplifyVW","text":"<p>Introduction: This function simplifies the input geometry by applying the Visvalingam-Whyatt algorithm.</p> <p>Note</p> <p>The simplification may not preserve topology, potentially producing invalid geometries. Use ST_SimplifyPreserveTopology to retain valid topology after simplification.</p> <p>Format: <code>ST_SimplifyVW(geom: Geometry, tolerance: Double)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SimplifyVW(ST_GeomFromWKT('POLYGON((8 25, 28 22, 28 20, 15 11, 33 3, 56 30, 46 33,46 34, 47 44, 35 36, 45 33, 43 19, 29 21, 29 22,35 26, 24 39, 8 25))'), 80)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8 25, 28 22, 15 11, 33 3, 56 30, 47 44, 43 19, 24 39, 8 25))\n</code></pre>"},{"location":"api/sql/Function/#st_snap","title":"ST_Snap","text":"<p>Introduction: Snaps the vertices and segments of the <code>input</code> geometry to <code>reference</code> geometry within the specified <code>tolerance</code> distance. The <code>tolerance</code> parameter controls the maximum snap distance.</p> <p>If the minimum distance between the geometries exceeds the <code>tolerance</code>, the <code>input</code> geometry is returned unmodified. Adjusting the <code>tolerance</code> value allows tuning which vertices should snap to the <code>reference</code> and which remain untouched.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>ST_Snap(input: Geometry, reference: Geometry, tolerance: double)</code></p> <p>Input geometry:</p> <p></p> <p>SQL Example:</p> <pre><code>SELECT\n    ST_Snap(poly, line, ST_Distance(poly, line) * 1.01) AS polySnapped FROM (\n        SELECT ST_GeomFromWKT('POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.35 -6.62, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.50, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))') as poly,\n            ST_GeomFromWKT('LINESTRING (236880.53 -8.22, 236881.15 -7.68, 236880.69 -6.81)') as line\n)\n</code></pre> <p>Output:</p> <p></p> <pre><code>POLYGON ((236877.58 -6.61, 236878.29 -8.35, 236879.98 -8.33, 236879.72 -7.63, 236880.69 -6.81, 236877.58 -6.61), (236878.45 -7.01, 236878.43 -7.52, 236879.29 -7.5, 236878.63 -7.22, 236878.76 -6.89, 236878.45 -7.01))\n</code></pre>"},{"location":"api/sql/Function/#st_split","title":"ST_Split","text":"<p>Introduction: Split an input geometry by another geometry (called the blade). Linear (LineString or MultiLineString) geometry can be split by a Point, MultiPoint, LineString, MultiLineString, Polygon, or MultiPolygon. Polygonal (Polygon or MultiPolygon) geometry can be split by a LineString, MultiLineString, Polygon, or MultiPolygon. In either case, when a polygonal blade is used then the boundary of the blade is what is actually split by. ST_Split will always return either a MultiLineString or MultiPolygon even if they only contain a single geometry. Homogeneous GeometryCollections are treated as a multi-geometry of the type it contains. For example, if a GeometryCollection of only Point geometries is passed as a blade it is the same as passing a MultiPoint of the same geometries.</p> <p>Since: <code>v1.4.0</code></p> <p>Format: <code>ST_Split (input: Geometry, blade: Geometry)</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Split(\n    ST_GeomFromWKT('LINESTRING (0 0, 1.5 1.5, 2 2)'),\n    ST_GeomFromWKT('MULTIPOINT (0.5 0.5, 1 1)'))\n</code></pre> <p>Output:</p> <pre><code>MULTILINESTRING ((0 0, 0.5 0.5), (0.5 0.5, 1 1), (1 1, 1.5 1.5, 2 2))\n</code></pre>"},{"location":"api/sql/Function/#st_srid","title":"ST_SRID","text":"<p>Introduction: Return the spatial reference system identifier (SRID) of the geometry.</p> <p>Format: <code>ST_SRID (A: Geometry)</code></p> <p>Since: <code>v1.1.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SRID(ST_SetSRID(ST_GeomFromWKT('POLYGON((1 1, 8 1, 8 8, 1 8, 1 1))'), 3021))\n</code></pre> <p>Output:</p> <pre><code>3021\n</code></pre>"},{"location":"api/sql/Function/#st_startpoint","title":"ST_StartPoint","text":"<p>Introduction: Returns first point of given linestring.</p> <p>Format: <code>ST_StartPoint(geom: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_StartPoint(ST_GeomFromText('LINESTRING(100 150,50 60, 70 80, 160 170)'))\n</code></pre> <p>Output:</p> <pre><code>POINT(100 150)\n</code></pre>"},{"location":"api/sql/Function/#st_subdivide","title":"ST_SubDivide","text":"<p>Introduction: Returns list of geometries divided based of given maximum number of vertices.</p> <p>Format: <code>ST_SubDivide(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"POLYGON((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    POLYGON((37.857142857142854 20, 35 10, 10 20, 37.857142857142854 20)),\n    POLYGON((15 20, 10 20, 15 40, 15 20)),\n    POLYGON((20 20, 15 20, 15 30, 20 30, 20 20)),\n    POLYGON((26.428571428571427 20, 20 20, 20 30, 26.4285714 23.5714285, 26.4285714 20)),\n    POLYGON((15 30, 15 40, 20 40, 20 30, 15 30)),\n    POLYGON((20 40, 26.4285714 40, 26.4285714 32.1428571, 20 30, 20 40)),\n    POLYGON((37.8571428 20, 30 20, 34.0476190 32.1428571, 37.8571428 32.1428571, 37.8571428 20)),\n    POLYGON((34.0476190 34.6825396, 26.4285714 32.1428571, 26.4285714 40, 34.0476190 40, 34.0476190 34.6825396)),\n    POLYGON((34.0476190 32.1428571, 35 35, 37.8571428 35, 37.8571428 32.1428571, 34.0476190 32.1428571)),\n    POLYGON((35 35, 34.0476190 34.6825396, 34.0476190 35, 35 35)),\n    POLYGON((34.0476190 35, 34.0476190 40, 37.8571428 40, 37.8571428 35, 34.0476190 35)),\n    POLYGON((30 20, 26.4285714 20, 26.4285714 23.5714285, 30 20)),\n    POLYGON((15 40, 37.8571428 43.8095238, 37.8571428 40, 15 40)),\n    POLYGON((45 45, 37.8571428 20, 37.8571428 43.8095238, 45 45))\n]\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_SubDivide(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Output:</p> <pre><code>[\n    LINESTRING(0 0, 5 5)\n    LINESTRING(5 5, 10 10)\n    LINESTRING(10 10, 21 21)\n    LINESTRING(21 21, 60 60)\n    LINESTRING(60 60, 85 85)\n    LINESTRING(85 85, 100 100)\n    LINESTRING(100 100, 120 120)\n]\n</code></pre>"},{"location":"api/sql/Function/#st_subdivideexplode","title":"ST_SubDivideExplode","text":"<p>Introduction: It works the same as ST_SubDivide but returns new rows with geometries instead of list.</p> <p>Format: <code>ST_SubDivideExplode(geom: Geometry, maxVertices: Integer)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <p>Query:</p> <pre><code>SELECT ST_SubDivideExplode(ST_GeomFromText(\"LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)\"), 5)\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre> <p>Using Lateral View</p> <p>Table:</p> <pre><code>+-------------------------------------------------------------+\n|geometry                                                     |\n+-------------------------------------------------------------+\n|LINESTRING(0 0, 85 85, 100 100, 120 120, 21 21, 10 10, 5 5)  |\n+-------------------------------------------------------------+\n</code></pre> <p>Query</p> <pre><code>select geom from geometries LATERAL VIEW ST_SubdivideExplode(geometry, 5) AS geom\n</code></pre> <p>Result:</p> <pre><code>+-----------------------------+\n|geom                         |\n+-----------------------------+\n|LINESTRING(0 0, 5 5)         |\n|LINESTRING(5 5, 10 10)       |\n|LINESTRING(10 10, 21 21)     |\n|LINESTRING(21 21, 60 60)     |\n|LINESTRING(60 60, 85 85)     |\n|LINESTRING(85 85, 100 100)   |\n|LINESTRING(100 100, 120 120) |\n+-----------------------------+\n</code></pre>"},{"location":"api/sql/Function/#st_symdifference","title":"ST_SymDifference","text":"<p>Introduction: Return the symmetrical difference between geometry A and B (return parts of geometries which are in either of the sets, but not in their intersection)</p> <p>Format: <code>ST_SymDifference (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_SymDifference(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((-2 -3, 4 -3, 4 3, -2 3, -2 -3))'))\n</code></pre> <p>Output:</p> <pre><code>MULTIPOLYGON (((-2 -3, -3 -3, -3 3, -2 3, -2 -3)), ((3 -3, 3 3, 4 3, 4 -3, 3 -3)))\n</code></pre>"},{"location":"api/sql/Function/#st_transform","title":"ST_Transform","text":"<p>Introduction:</p> <p>Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS. For SourceCRS and TargetCRS, WKT format is also available since <code>v1.3.1</code>. Since <code>v1.5.1</code>, if the <code>SourceCRS</code> is not specified, CRS will be fetched from the geometry using ST_SRID.</p> <p>Lon/Lat Order in the input geometry</p> <p>If the input geometry is in lat/lon order, it might throw an error such as <code>too close to pole</code>, <code>latitude or longitude exceeded limits</code>, or give unexpected results. You need to make sure that the input geometry is in lon/lat order. If the input geometry is in lat/lon order, you can use ST_FlipCoordinates to swap X and Y.</p> <p>Lon/Lat Order in the source and target CRS</p> <p>Sedona will make sure the source and target CRS to be in lon/lat order. If the source CRS or target CRS is in lat/lon order, these CRS will be swapped to lon/lat order.</p> <p>CRS code</p> <p>The CRS code is the code of the CRS in the official EPSG database (https://epsg.org/) in the format of <code>EPSG:XXXX</code>. A community tool EPSG.io can help you quick identify a CRS code. For example, the code of WGS84 is <code>EPSG:4326</code>.</p> <p>WKT format</p> <p>You can also use OGC WKT v1 format to specify the source CRS and target CRS. An example OGC WKT v1 CRS of <code>EPGS:3857</code> is as follows:</p> <pre><code>PROJCS[\"WGS 84 / Pseudo-Mercator\",\n    GEOGCS[\"WGS 84\",\n        DATUM[\"WGS_1984\",\n            SPHEROID[\"WGS 84\",6378137,298.257223563,\n                AUTHORITY[\"EPSG\",\"7030\"]],\n            AUTHORITY[\"EPSG\",\"6326\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4326\"]],\n    PROJECTION[\"Mercator_1SP\"],\n    PARAMETER[\"central_meridian\",0],\n    PARAMETER[\"scale_factor\",1],\n    PARAMETER[\"false_easting\",0],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n    AUTHORITY[\"EPSG\",\"3857\"]]\n</code></pre> <p>Note</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p> <p>Note</p> <p>By default, ST_Transform follows the <code>lenient</code> mode which tries to fix issues by itself. You can append a boolean value at the end to enable the <code>strict</code> mode. In <code>strict</code> mode, ST_Transform will throw an error if it finds any issue.</p> <p>Format:</p> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String, lenientMode: Boolean)\n</code></pre> <pre><code>ST_Transform (A: Geometry, SourceCRS: String, TargetCRS: String)\n</code></pre> <pre><code>ST_Transform (A: Geometry, TargetCRS: String)\n</code></pre> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649'))\n</code></pre> <pre><code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POLYGON((170 50,170 72,-130 72,-130 50,170 50))'),'EPSG:4326', 'EPSG:32649', false))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((8766047.980342899 17809098.336766362, 5122546.516721856 18580261.912528664, 3240775.0740796793 -13688660.50985159, 4556241.924514083 -12463044.21488129, 8766047.980342899 17809098.336766362))\n</code></pre>"},{"location":"api/sql/Function/#st_translate","title":"ST_Translate","text":"<p>Introduction: Returns the input geometry with its X, Y and Z coordinates (if present in the geometry) translated by deltaX, deltaY and deltaZ (if specified)</p> <p>If the geometry is 2D, and a deltaZ parameter is specified, no change is done to the Z coordinate of the geometry and the resultant geometry is also 2D.</p> <p>If the geometry is empty, no change is done to it. If the given geometry contains sub-geometries (GEOMETRY COLLECTION, MULTI POLYGON/LINE/POINT), all underlying geometries are individually translated.</p> <p>Format:</p> <p><code>ST_Translate(geometry: Geometry, deltaX: Double, deltaY: Double, deltaZ: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('GEOMETRYCOLLECTION(MULTIPOLYGON(((3 2,3 3,4 3,4 2,3 2)),((3 4,5 6,5 7,3 4))), POINT(1 1 1), LINESTRING EMPTY)'), 2, 2, 3)\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (MULTIPOLYGON (((5 4, 5 5, 6 5, 6 4, 5 4)), ((5 6, 7 8, 7 9, 5 6))), POINT (3 3), LINESTRING EMPTY)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Translate(ST_GeomFromText('POINT(-71.01 42.37)'),1,2)\n</code></pre> <p>Output:</p> <pre><code>POINT (-70.01 44.37)\n</code></pre>"},{"location":"api/sql/Function/#st_triangulatepolygon","title":"ST_TriangulatePolygon","text":"<p>Introduction: Generates the constrained Delaunay triangulation for the input Polygon. The constrained Delaunay triangulation is a set of triangles created from the Polygon's vertices that covers the Polygon area precisely, while maximizing the combined interior angles across all triangles compared to other possible triangulations. This produces the highest quality triangulation representation of the Polygon geometry. The function returns a GeometryCollection of Polygon geometries comprising this optimized constrained Delaunay triangulation. Polygons with holes and MultiPolygon types are supported. For any other geometry type provided, such as Point, LineString, etc., an empty GeometryCollection will be returned.</p> <p>Format: <code>ST_TriangulatePolygon(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TriangulatePolygon(\n        ST_GeomFromWKT('POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (5 5, 5 8, 8 8, 8 5, 5 5))')\n    )\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION (POLYGON ((0 0, 0 10, 5 5, 0 0)), POLYGON ((5 8, 5 5, 0 10, 5 8)), POLYGON ((10 0, 0 0, 5 5, 10 0)), POLYGON ((10 10, 5 8, 0 10, 10 10)), POLYGON ((10 0, 5 5, 8 5, 10 0)), POLYGON ((5 8, 10 10, 8 8, 5 8)), POLYGON ((10 10, 10 0, 8 5, 10 10)), POLYGON ((8 5, 8 8, 10 10, 8 5)))\n</code></pre>"},{"location":"api/sql/Function/#st_unaryunion","title":"ST_UnaryUnion","text":"<p>Introduction: This variant of ST_Union operates on a single geometry input. The input geometry can be a simple Geometry type, a MultiGeometry, or a GeometryCollection. The function calculates the geometric union across all components and elements within the provided geometry object.</p> <p>Format: <code>ST_UnaryUnion(geometry: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_UnaryUnion(ST_GeomFromWKT('MULTIPOLYGON(((0 10,0 30,20 30,20 10,0 10)),((10 0,10 20,30 20,30 0,10 0)))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((10 0, 10 10, 0 10, 0 30, 20 30, 20 20, 30 20, 30 0, 10 0))\n</code></pre>"},{"location":"api/sql/Function/#st_union","title":"ST_Union","text":"<p>Introduction:</p> <p>Variant 1: Return the union of geometry A and B.</p> <p>Variant 2 : As of version <code>1.6.0</code>, this function accepts an array of Geometry objects and returns the geometric union of all geometries in the input array. If the polygons within the input array do not share common boundaries, the ST_Union result will be a MultiPolygon geometry.</p> <p>Format:</p> <p><code>ST_Union (A: Geometry, B: Geometry)</code></p> <p><code>ST_Union (geoms: Array(Geometry))</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Union(ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'), ST_GeomFromWKT('POLYGON ((1 -2, 5 0, 1 2, 1 -2))'))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((3 -1, 3 -3, -3 -3, -3 3, 3 3, 3 1, 5 0, 3 -1))\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Union(\n    Array(\n        ST_GeomFromWKT('POLYGON ((-3 -3, 3 -3, 3 3, -3 3, -3 -3))'),\n        ST_GeomFromWKT('POLYGON ((-2 1, 2 1, 2 4, -2 4, -2 1))')\n    )\n)\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((2 3, 3 3, 3 -3, -3 -3, -3 3, -2 3, -2 4, 2 4, 2 3))\n</code></pre>"},{"location":"api/sql/Function/#st_voronoipolygons","title":"ST_VoronoiPolygons","text":"<p>Introduction: Returns a two-dimensional Voronoi diagram from the vertices of the supplied geometry. The result is a GeometryCollection of Polygons that covers an envelope larger than the extent of the input vertices. Returns null if input geometry is null. Returns an empty geometry collection if the input geometry contains only one vertex. Returns an empty geometry collection if the extend_to envelope has zero area.</p> <p>Format: <code>ST_VoronoiPolygons(g1: Geometry, tolerance: Double, extend_to: Geometry)</code></p> <p>Optional parameters:</p> <p><code>tolerance</code> : The distance within which vertices will be considered equivalent. Robustness of the algorithm can be improved by supplying a nonzero tolerance distance. (default = 0.0)</p> <p><code>extend_to</code> : If a geometry is supplied as the \"extend_to\" parameter, the diagram will be extended to cover the envelope of the \"extend_to\" geometry, unless that envelope is smaller than the default envelope (default = NULL. By default, we extend the bounding box of the diagram by the max between bounding box's height and bounding box's width).</p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT st_astext(ST_VoronoiPolygons(ST_GeomFromText('MULTIPOINT ((0 0), (1 1))')));\n</code></pre> <p>Output:</p> <pre><code>GEOMETRYCOLLECTION(POLYGON((-1 2,2 -1,-1 -1,-1 2)),POLYGON((-1 2,2 2,2 -1,-1 2)))\n</code></pre>"},{"location":"api/sql/Function/#st_x","title":"ST_X","text":"<p>Introduction: Returns X Coordinate of given Point null otherwise.</p> <p>Format: <code>ST_X(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_X(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre>"},{"location":"api/sql/Function/#st_xmax","title":"ST_XMax","text":"<p>Introduction: Returns the maximum X coordinate of a geometry</p> <p>Format: <code>ST_XMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_XMax(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_xmin","title":"ST_XMin","text":"<p>Introduction: Returns the minimum X coordinate of a geometry</p> <p>Format: <code>ST_XMin (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_XMin(ST_GeomFromText('POLYGON ((-1 -11, 0 10, 1 11, 2 12, -1 -11))'))\n</code></pre> <p>Output:</p> <pre><code>-1\n</code></pre>"},{"location":"api/sql/Function/#st_y","title":"ST_Y","text":"<p>Introduction: Returns Y Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Y(pointA: Point)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Y(ST_POINT(0.0 25.0))\n</code></pre> <p>Output:</p> <pre><code>25.0\n</code></pre>"},{"location":"api/sql/Function/#st_ymax","title":"ST_YMax","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_YMax (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_YMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre>"},{"location":"api/sql/Function/#st_ymin","title":"ST_YMin","text":"<p>Introduction: Return the minimum Y coordinate of A</p> <p>Format: <code>ST_Y_Min (A: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_YMin(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>0\n</code></pre>"},{"location":"api/sql/Function/#st_z","title":"ST_Z","text":"<p>Introduction: Returns Z Coordinate of given Point, null otherwise.</p> <p>Format: <code>ST_Z(pointA: Point)</code></p> <p>Since: <code>v1.2.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Z(ST_POINT(0.0 25.0 11.0))\n</code></pre> <p>Output:</p> <pre><code>11.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmax","title":"ST_ZMax","text":"<p>Introduction: Returns Z maxima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMax(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ZMax(ST_GeomFromText('POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))'))\n</code></pre> <p>Output:</p> <pre><code>1.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmin","title":"ST_ZMin","text":"<p>Introduction: Returns Z minima of the given geometry or null if there is no Z coordinate.</p> <p>Format: <code>ST_ZMin(geom: Geometry)</code></p> <p>Since: <code>v1.3.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_ZMin(ST_GeomFromText('LINESTRING(1 3 4, 5 6 7)'))\n</code></pre> <p>Output:</p> <pre><code>4.0\n</code></pre>"},{"location":"api/sql/Function/#st_zmflag","title":"ST_Zmflag","text":"<p>Introduction: Returns a code indicating the Z and M coordinate dimensions present in the input geometry.</p> <p>Values are: 0 = 2D, 1 = 3D-M, 2 = 3D-Z, 3 = 4D.</p> <p>Format: <code>ST_Zmflag(geom: Geometry)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('LINESTRING Z(1 2 3, 4 5 6)')\n)\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Zmflag(\n        ST_GeomFromWKT('POINT ZM(1 2 3 4)')\n)\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/","title":"Nearest-Neighbour searching","text":"<p>Sedona supports nearest-neighbour searching on geospatial data by providing a geospatial k-Nearest Neighbors (kNN) join method. This method involves identifying the k-nearest neighbors for a given spatial point or region based on geographic proximity, typically using spatial coordinates and a suitable distance metric like Euclidean or great-circle distance.</p>"},{"location":"api/sql/NearestNeighbourSearching/#st_knn","title":"ST_KNN","text":"<p>Introduction: join operation to find the k-nearest neighbors of a point or region in a spatial dataset.</p> <p>Format: <code>ST_KNN(R: Table, S: Table, k: Integer, use_spheroid: Boolean)</code></p> <p>Where R is the queries side table and S is the object side table, K is the number of neighbors. use_spheroid is a boolean value that determines whether to use the spheroid distance or not.</p> <p>Queries side table contains geometries that are used to find the k-nearest neighbors in the object side table.</p> <p>When either queries or objects data contain non-point data (geometries), we take the centroid of each geometry.</p> <p>In case there are ties in the distance, the result will include all the tied geometries only when the following sedona config is set to true:</p> <pre><code>spark.sedona.join.knn.includeTieBreakers=true\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/#filter-pushdown-considerations","title":"Filter Pushdown Considerations:","text":"<p>When using ST_KNN with filters applied to the resulting DataFrame, some of these filters may be pushed down to the object side of the kNN join. This means the filters will be applied to the object side reader before the kNN join is executed. If you want the filters to be applied after the kNN join, ensure that you first materialize the kNN join results and then apply the filters.</p> <p>For example, you can use the following approach:</p> <p>Scala Example:</p> <pre><code>val knnResult = knnJoinDF.cache()\nval filteredResult = knnResult.filter(condition)\n</code></pre> <p>SQL Example:</p> <pre><code>CREATE OR REPLACE TEMP VIEW knnResult AS\nSELECT * FROM (\n  -- Your KNN join SQL here\n) AS knnView;\nCACHE TABLE knnResult;\nSELECT * FROM knnResult WHERE condition;\n</code></pre>"},{"location":"api/sql/NearestNeighbourSearching/#handling-sql-defined-tables-in-st_knn-joins","title":"Handling SQL-Defined Tables in ST_KNN Joins:","text":"<p>When creating DataFrames from hard-coded SQL select statements in Sedona, and later using them in <code>ST_KNN</code> joins, Sedona may attempt to optimize the query in a way that bypasses the intended kNN join logic. Specifically, if you create DataFrames with hard-coded SQL, such as:</p> <pre><code>val df1 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom1\")\nval df2 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom2\")\n\nval df = df1.join(df2, expr(\"ST_KNN(geom1, geom2, 1)\"))\n</code></pre> <p>Sedona may optimize the join to a form like this:</p> <pre><code>SELECT ST_KNN(ST_Point(0.0, 0.0), ST_Point(0.0, 0.0), 1)\n</code></pre> <p>As a result, the ST_KNN function is handled as a User-Defined Function (UDF) instead of a proper join operation, preventing Sedona from initiating the kNN join execution path. Unlike typical UDFs, the ST_KNN function operates on multiple rows across DataFrames, not just individual rows. When this occurs, the query fails with an UnsupportedOperationException, indicating that the KNN predicate is not supported.</p> <p>Workaround:</p> <p>To prevent Spark's optimization from bypassing the kNN join logic, the DataFrames created with hard-coded SQL select statements must be materialized before performing the join. By caching the DataFrames, you can instruct Spark to avoid this undesired optimization:</p> <pre><code>val df1 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom1\").cache()\nval df2 = sedona.sql(\"SELECT ST_Point(0.0, 0.0) as geom2\").cache()\n\nval df = df1.join(df2, expr(\"ST_KNN(geom1, geom2, 1)\"))\n</code></pre> <p>Materializing the DataFrames with .cache() ensures that the correct kNN join path is followed in the Spark logical plan and prevents the optimization that would treat ST_KNN as a simple UDF.</p>"},{"location":"api/sql/NearestNeighbourSearching/#sql-example","title":"SQL Example","text":"<p>Suppose we have two tables <code>QUERIES</code> and <code>OBJECTS</code> with the following data:</p> <p>QUERIES table:</p> <pre><code>ID  GEOMETRY            NAME\n1   POINT(1 1)          station1\n2   POINT(10 10)        station2\n3   POINT(-0.5 -0.5)    station3\n</code></pre> <p>OBJECTS table:</p> <pre><code>ID  GEOMETRY            NAME\n1   POINT(11 5)         bank1\n2   POINT(12 1)         bank2\n3   POINT(-1 -1)        bank3\n4   POINT(-3 5)         bank4\n5   POINT(9 8)          bank5\n6   POINT(4 3)          bank6\n7   POINT(-4 -5)        bank7\n8   POINT(4 -2)         bank8\n9   POINT(-3 1)         bank9\n10  POINT(-7 3)         bank10\n11  POINT(11 5)         bank11\n12  POINT(12 1)         bank12\n13  POINT(-1 -1)        bank13\n14  POINT(-3 5)         bank14\n15  POINT(9 8)          bank15\n16  POINT(4 3)          bank16\n17  POINT(-4 -5)        bank17\n18  POINT(4 -2)         bank18\n19  POINT(-3 1)         bank19\n20  POINT(-7 3)         bank20\n</code></pre> <pre><code>SELECT\n    QUERIES.ID AS QUERY_ID,\n    QUERIES.GEOMETRY AS QUERIES_GEOM,\n    OBJECTS.GEOMETRY AS OBJECTS_GEOM\nFROM QUERIES JOIN OBJECTS ON ST_KNN(QUERIES.GEOMETRY, OBJECTS.GEOMETRY, 4, FALSE)\n</code></pre> <p>Output:</p> <pre><code>+--------+-----------------+-------------+\n|QUERY_ID|QUERIES_GEOM     |OBJECTS_GEOM |\n+--------+-----------------+-------------+\n|3       |POINT (-0.5 -0.5)|POINT (-1 -1)|\n|3       |POINT (-0.5 -0.5)|POINT (-1 -1)|\n|3       |POINT (-0.5 -0.5)|POINT (-3 1) |\n|3       |POINT (-0.5 -0.5)|POINT (-3 1) |\n|1       |POINT (1 1)      |POINT (-1 -1)|\n|1       |POINT (1 1)      |POINT (-1 -1)|\n|1       |POINT (1 1)      |POINT (4 3)  |\n|1       |POINT (1 1)      |POINT (4 3)  |\n|2       |POINT (10 10)    |POINT (9 8)  |\n|2       |POINT (10 10)    |POINT (9 8)  |\n|2       |POINT (10 10)    |POINT (11 5) |\n|2       |POINT (10 10)    |POINT (11 5) |\n+--------+-----------------+-------------+\n</code></pre>"},{"location":"api/sql/Optimizer/","title":"Query optimization","text":"<p>Sedona Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:</p> <ul> <li>Automatically optimizes range join query and distance join query.</li> <li>Automatically performs predicate pushdown.</li> </ul> <p>Tip</p> <p>Sedona join performance is heavily affected by the number of partitions. If the join performance is not ideal, please increase the number of partitions by doing <code>df.repartition(XXX)</code> right after you create the original DataFrame.</p>"},{"location":"api/sql/Optimizer/#range-join","title":"Range join","text":"<p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by SedonaSQL can trigger a range join.</p> <p>SQL Example</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_DWithin(pointdf.pointshape, polygondf.polygonshape, 10.0)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre> <p>Note</p> <p>All join queries in SedonaSQL are inner joins</p>"},{"location":"api/sql/Optimizer/#distance-join","title":"Distance join","text":"<p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>ST_Distance</code>, <code>ST_HausdorffDistance</code>, <code>ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for planar Euclidean distance:</p> <p>Only consider fully within a certain distance</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape, 0.3) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt; 2\n</code></pre> <p>Consider intersects within a certain distance</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt;= 2\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv\n</code></pre> <p>Warning</p> <p>If you use planar euclidean distance functions like <code>ST_Distance</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <p>Spark SQL Example for meter-based geodesic distance <code>ST_DistanceSpheroid</code> (works for <code>ST_DistanceSphere</code> too):</p> <p>Less than a certain distance==</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <p>Less than or equal to a certain distance==</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt;= 2\n</code></pre> <p>Warning</p> <p>If you use <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code> as the predicate, the unit of the distance is meter. Currently, distance join with geodesic distance calculators work best for point data. For non-point data, it only considers their centroids.</p>"},{"location":"api/sql/Optimizer/#broadcast-index-join","title":"Broadcast index join","text":"<p>Introduction: Perform a range join or distance join but broadcast one of the sides of the join. This maintains the partitioning of the non-broadcast side and doesn't require a shuffle.</p> <p>Sedona will create a spatial index on the broadcasted table.</p> <p>Sedona uses broadcast join only if the correct side has a broadcast hint. The supported join type - broadcast side combinations are:</p> <ul> <li>Inner - either side, preferring to broadcast left if both sides have the hint</li> <li>Left semi - broadcast right</li> <li>Left anti - broadcast right</li> <li>Left outer - broadcast right</li> <li>Right outer - broadcast left</li> </ul> <pre><code>pointDf.alias(\"pointDf\").join(broadcast(polygonDf).alias(\"polygonDf\"), expr(\"ST_Contains(polygonDf.polygonshape, pointDf.pointshape)\"))\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildRight, false ST_Contains(polygonshape#30, pointshape#52)\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex polygonshape#30: geometry, QUADTREE, [id=#62]\n   +- Project [st_polygonfromenvelope(cast(_c0#22 as decimal(24,20)), cast(_c1#23 as decimal(24,20)), cast(_c2#24 as decimal(24,20)), cast(_c3#25 as decimal(24,20))) AS polygonshape#30]\n      +- FileScan csv\n</code></pre> <p>This also works for distance joins with <code>ST_Distance</code>, <code>ST_DistanceSpheroid</code>, <code>ST_DistanceSphere</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code>:</p> <pre><code>pointDf1.alias(\"pointDf1\").join(broadcast(pointDf2).alias(\"pointDf2\"), expr(\"ST_Distance(pointDf1.pointshape, pointDf2.pointshape) &lt;= 2\"))\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nBroadcastIndexJoin pointshape#52: geometry, BuildRight, BuildLeft, true, 2.0 ST_Distance(pointshape#52, pointshape#415) &lt;= 2.0\n:- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#52]\n:  +- FileScan csv\n+- SpatialIndex pointshape#415: geometry, QUADTREE, [id=#1068]\n   +- Project [st_point(cast(_c0#48 as decimal(24,20)), cast(_c1#49 as decimal(24,20))) AS pointshape#415]\n      +- FileScan csv\n</code></pre> <p>Note: If the distance is an expression, it is only evaluated on the first argument to ST_Distance (<code>pointDf1</code> above).</p>"},{"location":"api/sql/Optimizer/#automatic-broadcast-index-join","title":"Automatic broadcast index join","text":"<p>When one table involved a spatial join query is smaller than a threshold, Sedona will automatically choose broadcast index join instead of Sedona optimized join. The current threshold is controlled by sedona.join.autoBroadcastJoinThreshold and set to the same as <code>spark.sql.autoBroadcastJoinThreshold</code>.</p>"},{"location":"api/sql/Optimizer/#raster-join","title":"Raster join","text":"<p>The optimization for spatial join also works for raster predicates, such as <code>RS_Intersects</code>, <code>RS_Contains</code> and <code>RS_Within</code>.</p> <p>SQL Example:</p> <pre><code>-- Raster-geometry join\nSELECT df1.id, df2.id, RS_Value(df1.rast, df2.geom) FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.geom)\n\n-- Raster-raster join\nSELECT df1.id, df2.id FROM df1 JOIN df2 ON RS_Intersects(df1.rast, df2.rast)\n</code></pre> <p>These queries could be planned as RangeJoin or BroadcastIndexJoin. Here is an example of the physical plan using RangeJoin:</p> <pre><code>== Physical Plan ==\n*(1) Project [id#14, id#25]\n+- RangeJoin rast#13: raster, geom#24: geometry, INTERSECTS,  **org.apache.spark.sql.sedona_sql.expressions.RS_Intersects**\n   :- LocalTableScan [rast#13, id#14]\n   +- LocalTableScan [geom#24, id#25]\n</code></pre>"},{"location":"api/sql/Optimizer/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>If the performance of Sedona optimized join is not ideal, which is possibly caused by  complicated and overlapping geometries, you can resort to Sedona built-in Google S2-based approximate equi-join. This equi-join leverages Spark's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"api/sql/Optimizer/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, explode(ST_S2CellIDs(geom, 15)) as cellId\nFROM rights\n</code></pre>"},{"location":"api/sql/Optimizer/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"api/sql/Optimizer/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"api/sql/Optimizer/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, first(lcs_geom), first(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>first</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"api/sql/Optimizer/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives. Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"api/sql/Optimizer/#regular-spatial-predicate-pushdown","title":"Regular spatial predicate pushdown","text":"<p>Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query.</p> <p>SQL Example</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nAND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)\n</code></pre> <p>Spark SQL Physical plan:</p> <pre><code>== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n</code></pre>"},{"location":"api/sql/Optimizer/#push-spatial-predicates-to-geoparquet","title":"Push spatial predicates to GeoParquet","text":"<p>Sedona supports spatial predicate push-down for GeoParquet files. When spatial filters were applied to dataframes backed by GeoParquet files, Sedona will use the <code>bbox</code> properties in the metadata to determine if all data in the file will be discarded by the spatial predicate. This optimization could reduce the number of files scanned when the queried GeoParquet dataset was partitioned by spatial proximity.</p> <p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre> <p>The following figure is the visualization of a GeoParquet dataset. <code>bbox</code>es of all GeoParquet files were plotted as blue rectangles and the query window was plotted as a red rectangle. Sedona will only scan 1 of the 6 files to answer queries such as <code>SELECT * FROM geoparquet_dataset WHERE ST_Intersects(geom, &lt;query window&gt;)</code>, thus only part of the data covered by the light green rectangle needs to be scanned.</p> <p></p> <p>We can compare the metrics of querying the GeoParquet dataset with or without the spatial predicate and observe that querying with spatial predicate results in fewer number of rows scanned.</p> Without spatial predicate With spatial predicate <p>Spatial predicate push-down to GeoParquet is enabled by default. Users can manually disable it by setting the Spark configuration <code>spark.sedona.geoparquet.spatialFilterPushDown</code> to <code>false</code>.</p>"},{"location":"api/sql/Overview/","title":"Quick start","text":""},{"location":"api/sql/Overview/#introduction","title":"Introduction","text":""},{"location":"api/sql/Overview/#function-list","title":"Function list","text":"<p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\n</code></pre> <p>Alternatively, <code>expr</code> and <code>selectExpr</code> can be used:</p> <pre><code>myDataFrame.withColumn(\"geometry\", expr(\"ST_*\")).selectExpr(\"ST_*\")\n</code></pre> <ul> <li>Constructor: Construct a Geometry given an input string or coordinates<ul> <li>Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.</li> <li>Documentation: Here</li> </ul> </li> <li>Function: Execute a function on the given column or columns<ul> <li>Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.</li> <li>Documentation: Here</li> </ul> </li> <li>Aggregate function: Return the aggregated value on the given column<ul> <li>Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.</li> <li>Documentation: Here</li> </ul> </li> <li>Predicate: Execute a logic judgement on the given columns and return true or false<ul> <li>Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".</li> <li>Documentation: Here</li> </ul> </li> </ul> <p>Sedona also provides an Adapter to convert SpatialRDD &lt;-&gt; DataFrame. Please read Adapter Scaladoc</p> <p>SedonaSQL supports SparkSQL query optimizer, documentation is Here</p>"},{"location":"api/sql/Overview/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here Write a SQL/DataFrame application.</p> <ol> <li>Add Sedona-core and Sedona-SQL into your project pom.xml or build.sbt</li> <li>Create your Sedona config if you want to customize your SparkSession.</li> </ol> <pre><code>import org.apache.sedona.spark.SedonaContext\nval config = SedonaContext.builder().\n    master(\"local[*]\").appName(\"SedonaSQL\")\n    .getOrCreate()\n</code></pre> <ol> <li>Add the following line after your Sedona context declaration:</li> </ol> <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(config)\n</code></pre>"},{"location":"api/sql/Parameter/","title":"Parameter","text":""},{"location":"api/sql/Parameter/#usage","title":"Usage","text":"<p>SedonaSQL supports many parameters. To change their values,</p> <ol> <li>Set it through SparkConf:</li> </ol> <pre><code>sparkSession = SparkSession.builder().\n      config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\n      config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\").\n      config(\"sedona.global.index\",\"true\")\n      master(\"local[*]\").appName(\"mySedonaSQLdemo\").getOrCreate()\n</code></pre> <ol> <li>Check your current SedonaSQL configuration:</li> </ol> <pre><code>val sedonaConf = new SedonaConf(sparkSession.conf)\nprintln(sedonaConf)\n</code></pre> <ol> <li>Sedona parameters can be changed at runtime:</li> </ol> <pre><code>sparkSession.conf.set(\"sedona.global.index\",\"false\")\n</code></pre> <p>In addition, you can also add <code>spark</code> prefix to the parameter name, for example:</p> <pre><code>sparkSession.conf.set(\"spark.sedona.global.index\",\"false\")\n</code></pre> <p>However, any parameter set through <code>spark</code> prefix will be honored by Spark, which means you can set these parameters before hand via <code>spark-defaults.conf</code> or Spark on Kubernetes configuration.</p> <p>If you set the same parameter through both <code>sedona</code> and <code>spark.sedona</code> prefixes, the parameter set through <code>sedona</code> prefix will override the parameter set through <code>spark.sedona</code> prefix.</p>"},{"location":"api/sql/Parameter/#explanation","title":"Explanation","text":"<ul> <li>sedona.global.index<ul> <li>Use spatial index (currently, only supports in SQL range join and SQL distance join)</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.global.indextype<ul> <li>Spatial index type, only valid when \"sedona.global.index\" is true</li> <li>Default: rtree</li> <li>Possible values: rtree, quadtree</li> </ul> </li> <li>sedona.join.autoBroadcastJoinThreshold<ul> <li>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.   By setting this value to -1 automatic broadcasting can be disabled.</li> <li>Default: The default value is the same as spark.sql.autoBroadcastJoinThreshold</li> <li>Possible values: any integer with a byte suffix i.e. 10MB or 512KB</li> </ul> </li> <li>sedona.join.gridtype<ul> <li>Spatial partitioning grid type for join query</li> <li>Default: kdbtree</li> <li>Possible values: quadtree, kdbtree</li> </ul> </li> <li>spark.sedona.join.knn.includeTieBreakers<ul> <li>KNN join will include all ties in the result, possibly returning more than k results</li> <li>Default: false</li> <li>Possible values: true, false</li> </ul> </li> <li>sedona.join.indexbuildside (Advanced users only!)<ul> <li>The side which Sedona builds spatial indices on</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.numpartition (Advanced users only!)<ul> <li>Number of partitions for both sides in a join query</li> <li>Default: -1, which means use the existing partitions</li> <li>Possible values: any integers</li> </ul> </li> <li>sedona.join.spatitionside (Advanced users only!)<ul> <li>The dominant side in spatial partitioning stage</li> <li>Default: left</li> <li>Possible values: left, right</li> </ul> </li> <li>sedona.join.optimizationmode (Advanced users only!)<ul> <li>When should Sedona optimize spatial join SQL queries</li> <li>Default: nonequi</li> <li>Possible values:<ul> <li>all: Always optimize spatial join queries, even for equi-joins.</li> <li>none: Disable optimization for spatial joins.</li> <li>nonequi: Optimize spatial join queries that are not equi-joins.</li> </ul> </li> </ul> </li> <li>spark.sedona.enableParserExtension<ul> <li>Enable the parser extension to parse GEOMETRY data type in SQL DDL statements</li> <li>Default: true</li> <li>Possible values: true, false</li> </ul> </li> </ul>"},{"location":"api/sql/Predicate/","title":"Predicate","text":""},{"location":"api/sql/Predicate/#st_contains","title":"ST_Contains","text":"<p>Introduction: Return true if A fully contains B</p> <p>Format: <code>ST_Contains (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Contains(ST_GeomFromWKT('POLYGON((175 150,20 40,50 60,125 100,175 150))'), ST_GeomFromWKT('POINT(174 149)'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_crosses","title":"ST_Crosses","text":"<p>Introduction: Return true if A crosses B</p> <p>Format: <code>ST_Crosses (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Crosses(ST_GeomFromWKT('POLYGON((1 1, 4 1, 4 4, 1 4, 1 1))'),ST_GeomFromWKT('POLYGON((2 2, 5 2, 5 5, 2 5, 2 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_disjoint","title":"ST_Disjoint","text":"<p>Introduction: Return true if A and B are disjoint</p> <p>Format: <code>ST_Disjoint (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Disjoint(ST_GeomFromWKT('POLYGON((1 4, 4.5 4, 4.5 2, 1 2, 1 4))'),ST_GeomFromWKT('POLYGON((5 4, 6 4, 6 2, 5 2, 5 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_dwithin","title":"ST_DWithin","text":"<p>Introduction: Returns true if 'leftGeometry' and 'rightGeometry' are within a specified 'distance'.</p> <p>If <code>useSpheroid</code> is passed true, ST_DWithin uses Sedona's ST_DistanceSpheroid to check the spheroid distance between the centroids of two geometries. The unit of the distance in this case is meter.</p> <p>If <code>useSpheroid</code> is passed false, ST_DWithin uses Euclidean distance and the unit of the distance is the same as the CRS of the geometries. To obtain the correct result, please consider using ST_Transform to put data in an appropriate CRS.</p> <p>If useSpheroid is not given, it defaults to false</p> <p>Format: <code>ST_DWithin (leftGeometry: Geometry, rightGeometry: Geometry, distance: Double, useSpheroid: Optional(Boolean) = false)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT('POINT (0 0)'), ST_GeomFromWKT('POINT (1 0)'), 2.5)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <pre><code>Check for distance between New York and Seattle (&lt; 4000 km)\n</code></pre> <pre><code>SELECT ST_DWithin(ST_GeomFromWKT(-122.335167 47.608013), ST_GeomFromWKT(-73.935242 40.730610), 4000000, true)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_equals","title":"ST_Equals","text":"<p>Introduction: Return true if A equals to B</p> <p>Format: <code>ST_Equals (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Equals(ST_GeomFromWKT('LINESTRING(0 0,10 10)'), ST_GeomFromWKT('LINESTRING(0 0,5 5,10 10)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_intersects","title":"ST_Intersects","text":"<p>Introduction: Return true if A intersects B</p> <p>Format: <code>ST_Intersects (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Intersects(ST_GeomFromWKT('LINESTRING(-43.23456 72.4567,-43.23456 72.4568)'), ST_GeomFromWKT('POINT(-43.23456 72.4567772)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_orderingequals","title":"ST_OrderingEquals","text":"<p>Introduction: Returns true if the geometries are equal and the coordinates are in the same order</p> <p>Format: <code>ST_OrderingEquals(A: geometry, B: geometry)</code></p> <p>Since: <code>v1.2.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_OrderingEquals(ST_GeomFromWKT('POLYGON((2 0, 0 2, -2 0, 2 0))'), ST_GeomFromWKT('POLYGON((0 2, -2 0, 2 0, 0 2))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_overlaps","title":"ST_Overlaps","text":"<p>Introduction: Return true if A overlaps B</p> <p>Format: <code>ST_Overlaps (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Overlaps(ST_GeomFromWKT('POLYGON((2.5 2.5, 2.5 4.5, 4.5 4.5, 4.5 2.5, 2.5 2.5))'), ST_GeomFromWKT('POLYGON((4 4, 4 6, 6 6, 6 4, 4 4))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_relate","title":"ST_Relate","text":"<p>Introduction: The first variant of the function computes and returns the Dimensionally Extended 9-Intersection Model (DE-9IM) matrix string representing the spatial relationship between the two input geometry objects.</p> <p>The second variant of the function evaluates whether the two input geometries satisfy a specific spatial relationship defined by the provided <code>intersectionMatrix</code> pattern.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format:</p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry)</code></p> <p><code>ST_Relate(geom1: Geometry, geom2: Geometry, intersectionMatrix: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))')\n)\n</code></pre> <p>Output:</p> <pre><code>1010F0212\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_Relate(\n        ST_GeomFromWKT('LINESTRING (1 1, 5 5)'),\n        ST_GeomFromWKT('POLYGON ((3 3, 3 7, 7 7, 7 3, 3 3))'),\n       \"1010F0212\"\n)\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_relatematch","title":"ST_RelateMatch","text":"<p>Introduction: This function tests the relationship between two Dimensionally Extended 9-Intersection Model (DE-9IM) matrices representing geometry intersections. It evaluates whether the DE-9IM matrix specified in <code>matrix1</code> satisfies the intersection pattern defined by <code>matrix2</code>. The <code>matrix2</code> parameter can be an exact DE-9IM value or a pattern containing wildcard characters.</p> <p>Note</p> <p>It is important to note that this function is not optimized for use in spatial join operations. Certain DE-9IM relationships can hold true for geometries that do not intersect or are disjoint. As a result, it is recommended to utilize other dedicated spatial functions specifically optimized for spatial join processing.</p> <p>Format: <code>ST_RelateMatch(matrix1: String, matrix2: String)</code></p> <p>Since: <code>v1.6.1</code></p> <p>SQL Example:</p> <pre><code>SELECT ST_RelateMatch('101202FFF', 'TTTTTTFFF')\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_touches","title":"ST_Touches","text":"<p>Introduction: Return true if A touches B</p> <p>Format: <code>ST_Touches (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Touches(ST_GeomFromWKT('LINESTRING(0 0,1 1,0 2)'), ST_GeomFromWKT('POINT(0 2)'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_within","title":"ST_Within","text":"<p>Introduction: Return true if A is fully contained by B</p> <p>Format: <code>ST_Within (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Within(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'), ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Predicate/#st_covers","title":"ST_Covers","text":"<p>Introduction: Return true if A covers B</p> <p>Format: <code>ST_Covers (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Covers(ST_GeomFromWKT('POLYGON((-2 0,0 2,2 0,-2 0))'), ST_GeomFromWKT('POLYGON((-1 0,0 1,1 0,-1 0))'))\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Predicate/#st_coveredby","title":"ST_CoveredBy","text":"<p>Introduction: Return true if A is covered by B</p> <p>Format: <code>ST_CoveredBy (A: Geometry, B: Geometry)</code></p> <p>Since: <code>v1.3.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_CoveredBy(ST_GeomFromWKT('POLYGON((0 0,3 0,3 3,0 3,0 0))'),  ST_GeomFromWKT('POLYGON((1 1,2 1,2 2,1 2,1 1))'))\n</code></pre> <p>Output:</p> <pre><code>false\n</code></pre>"},{"location":"api/sql/Raster-affine-transformation/","title":"Raster affine transformation","text":"<p>This page explains the basic concepts of Affine Transformation in Sedona Raster.</p>"},{"location":"api/sql/Raster-affine-transformation/#affine-transformations","title":"Affine Transformations","text":"<p>Affine transformations are a fundamental concept in computer graphics, geometry, and image processing that involve manipulating an object in a way that preserves lines and parallelism (but not necessarily distances and angles). These transformations are linear transformations followed by a translation, which means they can translate, scale, rotate, and shear objects without altering the relative arrangement of points, lines, or planes.</p>"},{"location":"api/sql/Raster-affine-transformation/#components-of-affine-transformations","title":"Components of Affine Transformations","text":"<p>Affine transformations can be represented as a matrix operation. In two-dimensional space, a typical affine transformation matrix is a 3x3 matrix as follows:</p> <pre><code>| ScaleX  SkewX   TranslationX |\n| SkewY   ScaleY  TranslationY |\n| 0       0       1            |\n</code></pre> <p>Here, <code>ScaleX, ScaleY, SkewX, SkewY, TranslationX,</code> and <code>TranslationY</code> are parameters that define the transformation:</p> <ul> <li><code>ScaleX</code> and <code>ScaleY</code> are scaling factors for the x and y axes, respectively.</li> <li><code>SkewX</code> and <code>SkewY</code> introduce shearing and are responsible for \"skewing\" the shape.</li> <li><code>TranslationX</code> and <code>TranslationY</code> are translation parameters that move the shape in the x and y directions, respectively.</li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#types-of-affine-transformations","title":"Types of Affine Transformations","text":"<ol> <li> <p>Translation: Moves every point of a figure or space by the same distance in a given direction. This primarily affects the <code>TranslationX</code> and <code>TranslationY</code> components.</p> </li> <li> <p>Scaling: Multiplies the coordinates of each point by a constant (ScaleX for x-axis and ScaleY for y-axis), enlarging or reducing its size. Scaling can be uniform (the same factor for both axes) or non-uniform (different factors for each axis).</p> </li> <li> <p>Rotation: Rotates the object about a point (usually the origin or a specified point). This can be expressed through combinations of <code>ScaleX, ScaleY, SkewX,</code> and <code>SkewY</code> where these parameters are derived from the cosine and sine of the rotation angle.</p> </li> <li> <p>Shearing: Transforms parallel lines to still be parallel but moves them so that they are no longer perpendicular to their original orientations. This affects the <code>SkewX</code> and <code>SkewY</code> components.</p> </li> <li> <p>Reflection: Flips the object over a specified axis, which can be achieved by combining scaling and rotation.</p> </li> </ol>"},{"location":"api/sql/Raster-affine-transformation/#mathematical-properties","title":"Mathematical Properties","text":"<ul> <li>Collinearity and Concurrency: Affine transformations preserve points on a line (collinearity) and the intersection of lines (concurrency).</li> <li>Ratios of Segments: They also preserve the ratios of distances between points lying on a straight line.</li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#components-of-affine-transformations_1","title":"Components of Affine Transformations","text":"<p>In affine transformations, which are integral to manipulating graphics, images, and geometric data, the terms ScaleX, ScaleY, SkewX, and SkewY refer to specific types of transformations that alter the shape and position of objects:</p>"},{"location":"api/sql/Raster-affine-transformation/#scalex-and-scaley","title":"ScaleX and ScaleY","text":"<ul> <li> <p>ScaleX: This parameter represents the scaling factor along the x-axis. It modifies the width of an image or object. Values greater than 1 increase the width, values less than 1 decrease it, and negative values reflect the object along the x-axis while scaling.</p> </li> <li> <p>ScaleY: This parameter represents the scaling factor along the y-axis. It affects the height of the object. Similarly to ScaleX, values greater than 1 enlarge the object vertically, values less than 1 reduce it, and negative values invert it along the y-axis.</p> </li> </ul>"},{"location":"api/sql/Raster-affine-transformation/#skewx-and-skewy","title":"SkewX and SkewY","text":"<ul> <li> <p>SkewX: This parameter is used to skew or shear the object along the x-axis. It shifts each point's x-coordinate in proportion to its y-coordinate, creating a slanting effect. This transformation is useful for creating the illusion of depth or perspective in 2D representations.</p> </li> <li> <p>SkewY: Corresponding to SkewX, SkewY skews the object along the y-axis. It alters each point's y-coordinate relative to its x-coordinate, which also creates a slanting effect, but in the vertical direction.</p> </li> </ul> <p>These transformations are typically used together in a transformation matrix, which allows them to be applied to objects in a combined and coherent way. Here's a typical representation of such a matrix:</p> <pre><code>| ScaleX  SkewX   TranslationX |\n| SkewY   ScaleY  TranslationY |\n| 0       0       1            |\n</code></pre> <p>These parameters can be combined in various ways to perform complex transformations such as rotations, translations, scaling, and shearing of images or shapes in both 2D and 3D graphics applications.</p>"},{"location":"api/sql/Raster-aggregate-function/","title":"Raster aggregates","text":""},{"location":"api/sql/Raster-aggregate-function/#rs_union_aggr","title":"RS_Union_Aggr","text":"<p>Introduction: This function combines multiple rasters into a single multiband raster by stacking the bands of each input raster sequentially. The function arranges the bands in the output raster according to the order specified by the index column in the input. It is typically used in scenarios where rasters are grouped by certain criteria (e.g., time and/or location) and an aggregated raster output is desired.</p> <p>Note</p> <p>RS_Union_Aggr expects the following input, if not satisfied then will throw an IllegalArgumentException:</p> <ul> <li>Indexes to be in an arithmetic sequence without any gaps.</li> <li>Indexes to be unique and not repeated.</li> <li>Rasters should be of the same shape.</li> </ul> <p>Format: <code>RS_Union_Aggr(A: rasterColumn, B: indexColumn)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example:</p> <p>First, we enrich the dataset with time-based grouping columns and index the rasters based on time intervals:</p> <pre><code>// Add yearly and quarterly time interval columns for grouping\ndf = df\n .withColumn(\"year\", year($\"timestamp\"))\n .withColumn(\"quarter\", quarter($\"timestamp\"))\n\n// Define window specs for quarterly indexing within each geometry-year group\nwindowSpecQuarter = Window.partitionBy(\"geometry\", \"year\", \"quarter\").orderBy(\"timestamp\")\n\nindexedDf = df.withColumn(\"index\", row_number().over(windowSpecQuarter))\n\nindexedDf.show()\n</code></pre> <p>The indexed rasters will appear as follows, showing that each raster is tagged with a sequential index (ordered by timestamp) within its group (grouped by geometry, year and quarter).</p> <pre><code>+-------------------+-----------------------------+--------------+----+-------+-----+\n|timestamp          |raster                       |geometry      |year|quarter|index|\n+-------------------+-----------------------------+--------------+----+-------+-----+\n|2021-01-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |1    |\n|2021-01-25 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |2    |\n|2021-02-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |3    |\n|2021-03-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |4    |\n|2021-03-25 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (72 120)|2021|1      |5    |\n|2021-04-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |1    |\n|2021-04-22 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |2    |\n|2021-05-15 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |3    |\n|2021-05-20 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |4    |\n|2021-05-29 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |5    |\n|2021-06-10 00:00:00|GridCoverage2D[\"geotiff_co...|POINT (84 132)|2021|2      |6    |\n+-------------------+-----------------------------+------------- +----+-------+-----+\n</code></pre> <p>To create a stacked raster by grouping on geometry.</p> <pre><code>indexedDf.createOrReplaceTempView(\"indexedDf\")\n\nsedona.sql('''\n    SELECT geometry, year, quarter, RS_Union_Aggr(raster, index) AS aggregated_raster\n    FROM indexedDf\n    WHERE index &lt;= 4\n    GROUP BY geometry, year, quarter\n''').show()\n</code></pre> <p>Output:</p> <p>The query yields rasters grouped by geometry, year and quarter, each containing the first four time steps combined into a single multiband raster, where each band represents one time step.</p> <pre><code>+--------------+----+-------+--------------------+---------+\n|      geometry|year|quarter|              raster|Num_Bands|\n+--------------+----+-------+--------------------+---------+\n|POINT (72 120)|2021|1      |GridCoverage2D[\"g...|        4|\n|POINT (84 132)|2021|2      |GridCoverage2D[\"g...|        4|\n+--------------+----+-------+--------------------+---------+\n</code></pre>"},{"location":"api/sql/Raster-loader/","title":"Raster loader","text":"<p>Note</p> <p>Sedona loader are available in Scala, Java and Python and have the same APIs.</p> <p>The raster loader of Sedona leverages Spark built-in binary data source and works with several RS constructors to produce Raster type. Each raster is a row in the resulting DataFrame and stored in a <code>Raster</code> format.</p> <p>By default, these functions uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order.</p>"},{"location":"api/sql/Raster-loader/#step-1-load-raster-to-a-binary-dataframe","title":"Step 1: Load raster to a binary DataFrame","text":"<p>You can load any type of raster data using the code below. Then use the RS constructors below to create a Raster DataFrame.</p> <pre><code>sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\n</code></pre>"},{"location":"api/sql/Raster-loader/#step-2-create-a-raster-type-column","title":"Step 2: Create a raster type column","text":""},{"location":"api/sql/Raster-loader/#rs_fromarcinfoasciigrid","title":"RS_FromArcInfoAsciiGrid","text":"<p>Introduction: Returns a raster geometry from an Arc Info Ascii Grid file.</p> <p>Format: <code>RS_FromArcInfoAsciiGrid(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.asc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromArcInfoAsciiGrid(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromgeotiff","title":"RS_FromGeoTiff","text":"<p>Introduction: Returns a raster geometry from a GeoTiff file.</p> <p>Format: <code>RS_FromGeoTiff(asc: ARRAY[Byte])</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>var df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromGeoTiff(content)\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeemptyraster","title":"RS_MakeEmptyRaster","text":"<p>Introduction: Returns an empty raster geometry. Every band in the raster is initialized to <code>0.0</code>.</p> <p>Since: <code>v1.5.0</code></p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, cellSize: Double)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits unsigned Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>Cell Size (pixel size): The size of the cells in the raster, in terms of the CRS units.</li> </ul> <p>It uses the default Cartesian coordinate system.</p> <p>Format:</p> <pre><code>RS_MakeEmptyRaster(numBands: Integer, bandDataType: String = 'D', width: Integer, height: Integer, upperleftX: Double, upperleftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double, srid: Integer)\n</code></pre> <ul> <li>NumBands: The number of bands in the raster. If not specified, the raster will have a single band.</li> <li>BandDataType: Optional parameter specifying the data types of all the bands in the created raster. Accepts one of:<ol> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits Byte</li> </ol> </li> <li>Width: The width of the raster in pixels.</li> <li>Height: The height of the raster in pixels.</li> <li>UpperleftX: The X coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>UpperleftY: The Y coordinate of the upper left corner of the raster, in terms of the CRS units.</li> <li>ScaleX: The scaling factor of the cells on the X axis</li> <li>ScaleY: The scaling factor of the cells on the Y axis</li> <li>SkewX: The skew of the raster on the X axis, effectively tilting them in the horizontal direction</li> <li>SkewY: The skew of the raster on the Y axis, effectively tilting them in the vertical direction</li> <li>SRID: The SRID of the raster. Use 0 if you want to use the default Cartesian coordinate system. Use 4326 if you want to use WGS84.</li> </ul> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Note</p> <p>If any other value than the accepted values for the bandDataType is provided, RS_MakeEmptyRaster defaults to double as the data type for the raster.</p> <p>Spark SQL example 1 (with 2 bands):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 2 (with 2 bands and dataType):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'I', 10, 10, 0.0, 0.0, 1.0) - Create a raster with integer datatype\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0)|\n+--------------------------------------------+\n|                        GridCoverage2D[\"g...|\n+--------------------------------------------+\n</code></pre> <p>Spark SQL example 3 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre> <p>Spark SQL example 4 (with 2 bands, scale, skew, and SRID):</p> <pre><code>SELECT RS_MakeEmptyRaster(2, 'F', 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326) - Create a raster with float datatype\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------------+\n|rs_makeemptyraster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326)|\n+------------------------------------------------------------------+\n|                                              GridCoverage2D[\"g...|\n+------------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_makeraster","title":"RS_MakeRaster","text":"<p>Introduction: Creates a raster from the given array of pixel values. The width, height, geo-reference information, and the CRS will be taken from the given reference raster. The data type of the resulting raster will be DOUBLE and the number of bands of the resulting raster will be <code>data.length / (refRaster.width * refRaster.height)</code>.</p> <p>Since: <code>v1.6.0</code></p> <p>Format: <code>RS_MakeRaster(refRaster: Raster, bandDataType: String, data: ARRAY[Double])</code></p> <ul> <li>refRaster: The reference raster from which the width, height, geo-reference information, and the CRS will be taken.</li> <li>bandDataType: The data type of the bands in the resulting raster. Please refer to the <code>RS_MakeEmptyRaster</code> function for the accepted values.</li> <li>data: The array of pixel values. The size of the array cannot be 0, and should be multiple of width * height of the reference raster.</li> </ul> <p>SQL example:</p> <pre><code>WITH r AS (SELECT RS_MakeEmptyRaster(2, 3, 2, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 4326) AS rast)\nSELECT RS_AsMatrix(RS_MakeRaster(rast, 'D', ARRAY(1, 2, 3, 4, 5, 6))) FROM r\n</code></pre> <p>Output:</p> <pre><code>+------------------------------------------------------------+\n|rs_asmatrix(rs_makeraster(rast, D, array(1, 2, 3, 4, 5, 6)))|\n+------------------------------------------------------------+\n||1.0  2.0  3.0|\\n|4.0  5.0  6.0|\\n                          |\n+------------------------------------------------------------+\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_fromnetcdf","title":"RS_FromNetCDF","text":"<p>Introduction: Returns a raster geometry representing the given record variable short name from a NetCDF file. This API reads the array data of the record variable in memory along with all its dimensions Since the netCDF format has many variants, the reader might not work for your test case, if that is so, please report this using the public forums.</p> <p>This API has been tested for netCDF classic (NetCDF 1, 2, 5) and netCDF4/HDF5 files.</p> <p>This API requires the name of the record variable. It is assumed that a variable of the given name exists, and its last 2 dimensions are 'lat' and 'lon' dimensions respectively.</p> <p>If this assumption does not hold true for your case, you can choose to pass the lonDimensionName and latDimensionName explicitly.</p> <p>You can use RS_NetCDFInfo to get the details of the passed netCDF file (variables and its dimensions).</p> <p>Format 1: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String)</code></p> <p>Format 2: <code>RS_FromNetCDF(netCDF: ARRAY[Byte], recordVariableName: String, lonDimensionName: String, latDimensionName: String)</code></p> <p>Since: <code>v1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3')\"))\n</code></pre> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\ndf = df.withColumn(\"raster\", f.expr(\"RS_FromNetCDF(content, 'O3', 'lon', 'lat')\"))\n</code></pre>"},{"location":"api/sql/Raster-loader/#rs_netcdfinfo","title":"RS_NetCDFInfo","text":"<p>Introduction: Returns a string containing names of the variables in a given netCDF file along with its dimensions.</p> <p>Format: <code>RS_NetCDFInfo(netCDF: ARRAY[Byte])</code></p> <p>Since: <code>1.5.1</code></p> <p>Spark Example:</p> <pre><code>val df = sedona.read.format(\"binaryFile\").load(\"/some/path/test.nc\")\nrecordInfo = df.selectExpr(\"RS_NetCDFInfo(content) as record_info\").first().getString(0)\nprint(recordInfo)\n</code></pre> <p>Output:</p> <pre><code>O3(time=2, z=2, lat=48, lon=80)\n\nNO2(time=2, z=2, lat=48, lon=80)\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/","title":"Raster map algebra","text":""},{"location":"api/sql/Raster-map-algebra/#map-algebra","title":"Map Algebra","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations. The expression can be applied to a single raster band or multiple raster bands. The result of the expression is a new raster.</p> <p>Apache Sedona provides two ways to perform map algebra operations:</p> <ol> <li>Using the <code>RS_MapAlgebra</code> function.</li> <li>Using <code>RS_BandAsArray</code> and array based map algebra functions, such as <code>RS_Add</code>, <code>RS_Multiply</code>, etc.</li> </ol> <p>Generally, the <code>RS_MapAlgebra</code> function is more flexible and can be used to perform more complex operations. The function takes three to four arguments:</p> <pre><code>RS_MapAlgebra(rast: Raster, pixelType: String, script: String, [noDataValue: Double])\n</code></pre> <ul> <li><code>rast</code>: The raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Optional) The nodata value of the output raster.</li> </ul> <p>As of version <code>v1.5.1</code>, the <code>RS_MapAlgebra</code> function allows two raster column inputs, with multi-band rasters supported. The function accepts 5 parameters:</p> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <ul> <li><code>rast0</code>: The first raster to apply the map algebra expression to.</li> <li><code>rast1</code>: The second raster to apply the map algebra expression to.</li> <li><code>pixelType</code>: The data type of the output raster. This can be one of <code>D</code> (double), <code>F</code> (float), <code>I</code> (integer), <code>S</code> (short), <code>US</code> (unsigned short) or <code>B</code> (byte). If specified <code>NULL</code>, the output raster will have the same data type as the input raster.</li> <li><code>script</code>: The map algebra script. Refer here for more details on the format.</li> <li><code>noDataValue</code>: (Not optional) The nodata value of the output raster, <code>null</code> is allowed.</li> </ul> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p><code>RS_MapAlgebra</code> also has good performance, since it is backed by Jiffle and can be compiled to Java bytecode for execution. We'll demonstrate both approaches to implementing commonly used map algebra operations.</p> <p>Note</p> <p>The <code>RS_MapAlgebra</code> function can cast the output raster to a different data type specified by <code>pixelType</code>:</p> <ul> <li> <p>If <code>pixelType</code> is smaller than the input raster data type, narrowing casts will be performed, which may result in loss of data.</p> </li> <li> <p>If <code>pixelType</code> is larger, widening casts will retain data accuracy.</p> </li> <li> <p>If <code>pixelType</code> matches the input raster data type, no casting occurs.</p> </li> </ul> <p>This allows controlling the output pixel data type. Users should consider potential precision impacts when coercing to a smaller type.</p>"},{"location":"api/sql/Raster-map-algebra/#ndvi","title":"NDVI","text":"<p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements, typically, but not necessarily, from a space platform, and assess whether the target being observed contains live green vegetation or not. NDVI has become a de facto standard index used to determine whether a given area contains live green vegetation or not. The NDVI is calculated from these individual measurements as follows:</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <p>Assume that we have a bunch of rasters with 4 bands: red, green, blue, and near-infrared. We want to calculate the NDVI for each raster. We can use the <code>RS_MapAlgebra</code> function to do this:</p> <pre><code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>The Jiffle script is <code>out = (rast[3] - rast[0]) / (rast[3] + rast[0]);</code>. The <code>rast</code> variable is always bound to the input raster, and the <code>out</code> variable is bound to the output raster. Jiffle iterates over all the pixels in the input raster and executes the script for each pixel. the <code>rast[3]</code> and <code>rast[0]</code> refers to the current pixel values of the near-infrared and red bands, respectively. The <code>out</code> variable is the current output pixel value.</p> <p>The result of the <code>RS_MapAlgebra</code> function is a raster with a single band. The band is of type double, since we specified <code>D</code> as the <code>pixelType</code> argument.</p> <p>We can implement the same NDVI calculation using the array based map algebra functions:</p> <pre><code>SELECT RS_Divide(\n        RS_Subtract(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)),\n        RS_Add(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4))) as ndvi FROM raster_table\n</code></pre> <p>The <code>RS_BandAsArray</code> function extracts the specified band of the input raster to an array of double, and the <code>RS_Add</code>, <code>RS_Subtract</code>, and <code>RS_Divide</code> functions perform the arithmetic operations on the arrays. The code using the array based map algebra functions is more verbose. However, there is a <code>RS_NormalizedDifference</code> function that can be used to calculate the NDVI more concisely:</p> <pre><code>SELECT RS_NormalizedDifference(RS_BandAsArray(rast, 1), RS_BandAsArray(rast, 4)) as ndvi FROM raster_table\n</code></pre> <p>The result of array based map algebra functions is an array of double. User can use <code>RS_AddBandFromArray</code> to add the array to a raster as a new band.</p>"},{"location":"api/sql/Raster-map-algebra/#awei","title":"AWEI","text":"<p>The Automated Water Extraction Index (AWEI) is a spectral index that can be used to extract water bodies from remote sensing imagery. The AWEI is calculated from these individual measurements as follows:</p> <pre><code>AWEI = 4 * (Green - SWIR2) - (0.25 * NIR + 2.75 * SWIR1)\n</code></pre> <p>AWEI can be implemented easily using <code>RS_MapAlgebra</code>:</p> <pre><code>-- Assume that the raster includes all 13 Sentinel-2 bands\nSELECT RS_MapAlgebra(rast, 'D', 'out = 4 * (rast[2] - rast[11]) - (0.25 * rast[7] + 2.75 * rast[12]);') as awei FROM raster_table\n</code></pre> <p>We can also implement the same AWEI calculation using array based map algebra functions. The code looks more verbose:</p> <pre><code>SELECT RS_Subtract(\n    RS_Add(RS_MultiplyFactor(band_nir, 0.25), RS_MultiplyFactor(band_swir1, 2.75)),\n    RS_MultiplyFactor(RS_Subtract(band_swir2, band_green), 4)) as awei\nFROM (\nSELECT RS_BandAsArray(rast, 3) AS band_green,\n       RS_BandAsArray(rast, 12) AS band_swir2,\n       RS_BandAsArray(rast, 13) AS band_swir1,\n       RS_BandAsArray(rast, 8) AS band_nir\nFROM raster_table) t\n</code></pre>"},{"location":"api/sql/Raster-map-algebra/#further-reading","title":"Further Reading","text":"<ul> <li>Jiffle language summary</li> <li>Raster operators</li> </ul>"},{"location":"api/sql/Raster-operators/","title":"Raster operators","text":""},{"location":"api/sql/Raster-operators/#pixel-functions","title":"Pixel Functions","text":""},{"location":"api/sql/Raster-operators/#rs_pixelascentroid","title":"RS_PixelAsCentroid","text":"<p>Introduction: Returns the centroid (point geometry) of the specified pixel's area. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsCentroid(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsCentroid(RS_MakeEmptyRaster(1, 12, 13, 134, -53, 9), 3, 3))\n</code></pre> <p>Output:</p> <pre><code>POINT (156.5 -75.5)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelascentroids","title":"RS_PixelAsCentroids","text":"<p>Introduction: Returns a list of the centroid point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band. Each centroid represents the geometric center of the corresponding pixel's area.</p> <p>Format: <code>RS_PixelAsCentroids(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsCentroids(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POINT (-13065222 4021263.75),148.0,0,0], [POINT (-13065151 4021263.75),123.0,0,1], [POINT (-13065077 4021263.75),99.0,1,0], [POINT (-13065007 4021261.75),140.0,1,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsCentroids(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------------+-----+---+---+\n|geom                                          |value|x  |y  |\n+----------------------------------------------+-----+---+---+\n|POINT (-13095781.835693639 4021226.5856936392)|0.0  |1  |1  |\n|POINT (-13095709.507080918 4021226.5856936392)|0.0  |2  |1  |\n|POINT (-13095637.178468198 4021226.5856936392)|0.0  |3  |1  |\n+----------------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoint","title":"RS_PixelAsPoint","text":"<p>Introduction: Returns a point geometry of the specified pixel's upper-left corner. The pixel coordinates specified are 1-indexed.</p> <p>Note</p> <p>If the pixel coordinates specified do not exist in the raster (out of bounds), RS_PixelAsPoint throws an IndexOutOfBoundsException.</p> <p>Format: <code>RS_PixelAsPoint(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 2, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>POINT (123.19, -12)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoint(raster, 6, 2)) from rasters\n</code></pre> <p>Output:</p> <pre><code>IndexOutOfBoundsException: Specified pixel coordinates (6, 2) do not lie in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspoints","title":"RS_PixelAsPoints","text":"<p>Introduction: Returns a list of the pixel's upper-left corner point geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPoints(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPoints(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POINT (-13065223 4021262.75),148.0,0,0], [POINT (-13065150 4021262.75),123.0,0,1], [POINT (-13065078 4021262.75),99.0,1,0], [POINT (-13065006 4021262.75),140.0,1,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsPoints(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------+-----+---+---+\n|geom                                  |value|x  |y  |\n+--------------------------------------+-----+---+---+\n|POINT (-13095818 4021262.75)          |0.0  |1  |1  |\n|POINT (-13095745.67138728 4021262.75) |0.0  |2  |1  |\n|POINT (-13095673.342774557 4021262.75)|0.0  |3  |1  |\n+--------------------------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygon","title":"RS_PixelAsPolygon","text":"<p>Introduction: Returns a polygon geometry that bounds the specified pixel. The pixel coordinates specified are 1-indexed. If <code>colX</code> and <code>rowY</code> are out of bounds for the raster, they are interpolated assuming the same skew and translate values.</p> <p>Format: <code>RS_PixelAsPolygon(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPolygon(RS_MakeEmptyRaster(1, 5, 10, 123, -230, 8), 2, 3))\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((131 -246, 139 -246, 139 -254, 131 -254, 131 -246))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_pixelaspolygons","title":"RS_PixelAsPolygons","text":"<p>Introduction: Returns a list of the polygon geometry, the pixel value and its raster X and Y coordinates for each pixel in the raster at the specified band.</p> <p>Format: <code>RS_PixelAsPolygons(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT ST_AsText(RS_PixelAsPolygons(raster, 1)) from rasters\n</code></pre> <p>Output:</p> <pre><code>[[POLYGON ((123.19000244140625 -12, 127.19000244140625 -12, 127.19000244140625 -16, 123.19000244140625 -16, 123.19000244140625 -12)),0.0,1,1],\n[POLYGON ((127.19000244140625 -12, 131.19000244140625 -12, 131.19000244140625 -16, 127.19000244140625 -16, 127.19000244140625 -12)),0.0,2,1],\n[POLYGON ((131.19000244140625 -12, 135.19000244140625 -12, 135.19000244140625 -16, 131.19000244140625 -16, 131.19000244140625 -12)),0.0,3,1]]\n</code></pre> <p>Spark SQL example for extracting Point, value, raster x and y coordinates:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\nvar df = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\ndf = df.selectExpr(\"RS_FromGeoTiff(content) as raster\")\n\ndf.selectExpr(\n  \"explode(RS_PixelAsPolygons(raster, 1)) as exploded\"\n).selectExpr(\n  \"exploded.geom as geom\",\n  \"exploded.value as value\",\n  \"exploded.x as x\",\n  \"exploded.y as y\"\n).show(3)\n</code></pre> <p>Output:</p> <pre><code>+--------------------+-----+---+---+\n|                geom|value|  x|  y|\n+--------------------+-----+---+---+\n|POLYGON ((-130958...|  0.0|  1|  1|\n|POLYGON ((-130957...|  0.0|  2|  1|\n|POLYGON ((-130956...|  0.0|  3|  1|\n+--------------------+-----+---+---+\n</code></pre>"},{"location":"api/sql/Raster-operators/#geometry-functions","title":"Geometry Functions","text":""},{"location":"api/sql/Raster-operators/#rs_envelope","title":"RS_Envelope","text":"<p>Introduction: Returns the envelope of the raster as a Geometry.</p> <p>Format: <code>RS_Envelope (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Envelope(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((0 0,20 0,20 60,0 60,0 0))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_convexhull","title":"RS_ConvexHull","text":"<p>Introduction: Return the convex hull geometry of the raster including the NoDataBandValue band pixels. For regular shaped and non-skewed rasters, this gives more or less the same result as RS_Envelope and hence is only useful for irregularly shaped or skewed rasters.</p> <p>Format: <code>RS_ConvexHull(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ConvexHull(RS_MakeEmptyRaster(1, 5, 10, 156, -132, 5, 10, 3, 5, 0));\n</code></pre> <p>Output:</p> <pre><code>POLYGON ((156 -132, 181 -107, 211 -7, 186 -32, 156 -132))\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_minconvexhull","title":"RS_MinConvexHull","text":"<p>Introduction: Returns the min convex hull geometry of the raster excluding the NoDataBandValue band pixels, in the given band. If no band is specified, all the bands are considered when creating the min convex hull of the raster. The created geometry representing the min convex hull has world coordinates of the raster in its CRS as the corner coordinates.</p> <p>Note</p> <p>If the specified band does not exist in the raster, RS_MinConvexHull throws an IllegalArgumentException</p> <p>Format:</p> <p><code>RS_MinConvexHull(raster: Raster)</code></p> <p><code>RS_MinConvexHull(raster: Raster, band: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n        Seq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n        \"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0))) as minConvexHullAll\").show()\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------+\n|minConvexHullAll                        |\n+----------------------------------------+\n|POLYGON ((0 -1, 4 -1, 4 -5, 0 -5, 0 -1))|\n+----------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>val inputDf = Seq((Seq(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n        Seq(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0))).toDF(\"values2\", \"values1\")\ninputDf.selectExpr(\"ST_AsText(RS_MinConvexHull(RS_AddBandFromArray(\" +\n  \"RS_AddBandFromArray(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), values1, 1, 0), values2, 2, 0), 1)) as minConvexHull1\").show()\n</code></pre> <p>Output:</p> <pre><code>+----------------------------------------+\n|minConvexHull1                          |\n+----------------------------------------+\n|POLYGON ((1 -1, 4 -1, 4 -5, 1 -5, 1 -1))|\n+----------------------------------------+\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_MinConvexHull(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>Provided band index 3 does not lie in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-accessors","title":"Raster Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_georeference","title":"RS_GeoReference","text":"<p>Introduction: Returns the georeference metadata of raster as a string in GDAL or ESRI format. Default is GDAL if not specified.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_GeoReference(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format: <code>RS_GeoReference(raster: Raster, format: String = \"GDAL\")</code></p> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX\nUpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX\nSkewY\nSkewX\nScaleY\nUpperLeftX + ScaleX * 0.5\nUpperLeftY + ScaleY * 0.5\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 100, 100, -53, 51, 2, -2, 4, 5, 4326))\n</code></pre> <p>Output:</p> <pre><code>2.000000\n5.000000\n4.000000\n-2.000000\n-53.000000\n51.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"GDAL\")\n</code></pre> <p>Output:</p> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n100.000000\n200.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(ST_MakeEmptyRaster(1, 3, 4, 100.0, 200.0,2.0, -3.0, 0.1, 0.2, 0), \"ERSI\")\n</code></pre> <pre><code>2.000000\n0.200000\n0.100000\n-3.000000\n101.000000\n198.500000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_geotransform","title":"RS_GeoTransform","text":"<p>Introduction: Returns a struct of parameters that represent the GeoTransformation of the raster. The struct has the following schema:</p> <ul> <li>magnitudeI: size of a pixel along the transformed i axis</li> <li>magnitudeJ: size of a pixel along the transformed j axis</li> <li>thetaI: angle by which the raster is rotated (Radians positive clockwise)</li> <li>thetaIJ: angle from transformed i axis to transformed j axis (Radians positive counter-clockwise)</li> <li>offsetX: X ordinate of the upper-left corner of the upper-left pixel</li> <li>offsetY: Y ordinate of the upper-left corner of the upper-left pixel</li> </ul> <p>Note</p> <p>Refer to this image for a clear understanding between i &amp; j axis and x &amp; y-axis.</p> <p>Format: <code>RS_GeoTransform(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_GeoTransform(\n        RS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n       )\n</code></pre> <p>Output:</p> <pre><code>{2.23606797749979, 2.23606797749979, -1.1071487177940904, -2.214297435588181, 1.0, 2.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_height","title":"RS_Height","text":"<p>Introduction: Returns the height of the raster.</p> <p>Format: <code>RS_Height(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Height(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>512\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordx","title":"RS_RasterToWorldCoordX","text":"<p>Introduction: Returns the upper left X coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the X coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordX(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoordX(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>-123\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoordy","title":"RS_RasterToWorldCoordY","text":"<p>Introduction: Returns the upper left Y coordinate of the given row and column of the given raster geometric units of the geo-referenced raster. If any out of bounds values are given, the Y coordinate of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoordY(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoordY(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>54\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rastertoworldcoord","title":"RS_RasterToWorldCoord","text":"<p>Introduction: Returns the upper left X and Y coordinates of the given row and column of the given raster geometric units of the geo-referenced raster as a Point geometry. If any out of bounds values are given, the X and Y coordinates of the assumed point considering existing raster pixel size and skew values will be returned.</p> <p>Format: <code>RS_RasterToWorldCoord(raster: Raster, colX: Integer, rowY: Integer)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_RasterToWorldCoord(ST_MakeEmptyRaster(1, 5, 10, -123, 54, 5, -10, 0, 0, 4326), 1, 1) from rasters\n</code></pre> <p>Output:</p> <pre><code>POINT (-123 54)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_rotation","title":"RS_Rotation","text":"<p>Introduction: Returns the uniform rotation of the raster in radian.</p> <p>Format: <code>RS_Rotation(raster: Raster)</code></p> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Rotation(\n        RS_MakeEmptyRaster(2, 10, 15, 1, 2, 1, -2, 1, 2, 0)\n        )\n</code></pre> <p>Output:</p> <pre><code>-1.1071487177940904\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scalex","title":"RS_ScaleX","text":"<p>Introduction: Returns the pixel width of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleX attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleX will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format: <code>RS_ScaleX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ScaleX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_scaley","title":"RS_ScaleY","text":"<p>Introduction: Returns the pixel height of the raster in CRS units.</p> <p>Note</p> <p>RS_ScaleY attempts to get an Affine transform on the grid in order to return scaleX (See World File for more details). If the transform on the geometry is not an Affine transform, RS_ScaleY will throw an UnsupportedException: <pre><code>UnsupportedOperationException(\"Only AffineTransform2D is supported\")\n</code></pre></p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format: <code>RS_ScaleY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_ScaleY(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>-2\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_skewx","title":"RS_SkewX","text":"<p>Introduction: Returns the X skew or rotation parameter.</p> <p>Format: <code>RS_SkewX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SkewX(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.1\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_skewy","title":"RS_SkewY","text":"<p>Introduction: Returns the Y skew or rotation parameter.</p> <p>Format: <code>RS_SkewY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SkewY(RS_MakeEmptyRaster(2, 10, 10, 0.0, 0.0, 1.0, -1.0, 0.1, 0.2, 4326))\n</code></pre> <p>Output:</p> <pre><code>0.2\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperleftx","title":"RS_UpperLeftX","text":"<p>Introduction: Returns the X coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftX(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_UpperLeftX(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>5\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_upperlefty","title":"RS_UpperLeftY","text":"<p>Introduction: Returns the Y coordinate of the upper-left corner of the raster.</p> <p>Format: <code>RS_UpperLeftY(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_UpperLeftY(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_width","title":"RS_Width","text":"<p>Introduction: Returns the width of the raster.</p> <p>Format: <code>RS_Width(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Width(raster) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>517\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoord","title":"RS_WorldToRasterCoord","text":"<p>Introduction: Returns the grid coordinate of the given world coordinates as a Point.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Point)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (1 1)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoord(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0, 4326), ST_GeomFromText('POINT (-52 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>POINT (2 1)\n</code></pre> <p>Note</p> <p>If the given geometry point is not in the same CRS as the given raster, the given geometry will be transformed to the given raster's CRS. You can use ST_Transform to transform the geometry beforehand.</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordx","title":"RS_WorldToRasterCoordX","text":"<p>Introduction: Returns the X coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoord(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoord(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -53, 51) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordX(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-53 51)')) from rasters;\n</code></pre> <p>Output:</p> <pre><code>1\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for latitude and the intended value of world longitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#rs_worldtorastercoordy","title":"RS_WorldToRasterCoordY","text":"<p>Introduction: Returns the Y coordinate of the grid coordinate of the given world coordinates as an integer.</p> <p>Format:</p> <p><code>RS_WorldToRasterCoordY(raster: Raster, point: Geometry)</code></p> <p><code>RS_WorldToRasterCoordY(raster: Raster, x: Double, y: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), ST_GeomFromText('POINT (-50 50)'));\n</code></pre> <p>Output:</p> <pre><code>2\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_WorldToRasterCoordY(ST_MakeEmptyRaster(1, 5, 5, -53, 51, 1, -1, 0, 0), -50, 49);\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre> <p>Tip</p> <p>For non-skewed rasters, you can provide any value for longitude and the intended value of world latitude, to get the desired answer</p>"},{"location":"api/sql/Raster-operators/#raster-band-accessors","title":"Raster Band Accessors","text":""},{"location":"api/sql/Raster-operators/#rs_band","title":"RS_Band","text":"<p>Introduction: Returns a new raster consisting 1 or more bands of an existing raster. It can build new rasters from existing ones, export only selected bands from a multiband raster, or rearrange the order of bands in a raster dataset.</p> <p>Format:</p> <p><code>RS_Band(raster: Raster, bands: ARRAY[Integer])</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NumBands(\n        RS_Band(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(2, 5, 5, 3, -215, 2, -2, 2, 2, 0),\n                Array(16, 0, 24, 33, 43, 49, 64, 0, 76, 77, 79, 89, 0, 116, 118, 125, 135, 0, 157, 190, 215, 229, 241, 248, 249),\n                1, 0d\n            ), Array(1,1,1)\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandnodatavalue","title":"RS_BandNoDataValue","text":"<p>Introduction: Returns the no data value of the given band of the given raster. If no band is given, band 1 is assumed. The band parameter is 1-indexed. If there is no data value associated with the given band, RS_BandNoDataValue returns null.</p> <p>Note</p> <p>If the given band does not lie in the raster, RS_BandNoDataValue throws an IllegalArgumentException</p> <p>Format: <code>RS_BandNoDataValue (raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster, 1) from rasters;\n</code></pre> <p>Output:</p> <pre><code>0.0\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster) from rasters_without_nodata;\n</code></pre> <p>Output:</p> <pre><code>null\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(raster, 3) from rasters;\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster.\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandisnodata","title":"RS_BandIsNoData","text":"<p>Returns true if the band is filled with only nodata values. Band 1 is assumed if not specified.</p> <p>Format: <code>RS_BandIsNoData(raster: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>WITH rast_table AS (SELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 2, 2, 0, 0, 1), ARRAY(10d, 10d, 10d, 10d), 1, 10d) as rast)\nSELECT RS_BandIsNoData(rast) from rast_table\n</code></pre> <p>Output:</p> <pre><code>true\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bandpixeltype","title":"RS_BandPixelType","text":"<p>Introduction: Returns the datatype of each pixel in the given band of the given raster in string format. The band parameter is 1-indexed. If no band is specified, band 1 is assumed.</p> <p>Note</p> <p>If the given band index does not exist in the given raster, RS_BandPixelType throws an IllegalArgumentException.</p> <p>Following are the possible values returned by RS_BandPixelType:</p> <ol> <li><code>REAL_64BITS</code> - For Double values</li> <li><code>REAL_32BITS</code> - For Float values</li> <li><code>SIGNED_32BITS</code> - For Integer values</li> <li><code>SIGNED_16BITS</code> - For Short values</li> <li><code>UNSIGNED_16BITS</code> - For unsigned Short values</li> <li><code>UNSIGNED_8BITS</code> - For Byte values</li> </ol> <p>Format: <code>RS_BandPixelType(rast: Raster, band: Integer = 1)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"D\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 2);\n</code></pre> <p>Output:</p> <pre><code>REAL_64BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0));\n</code></pre> <p>Output:</p> <pre><code>SIGNED_32BITS\n</code></pre> <pre><code>SELECT RS_BandPixelType(RS_MakeEmptyRaster(2, \"I\", 5, 5, 53, 51, 1, 1, 0, 0, 0), 3);\n</code></pre> <p>Output:</p> <pre><code>IllegalArgumentException: Provided band index 3 is not present in the raster\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_count","title":"RS_Count","text":"<p>Introduction: Returns the number of pixels in a given band. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Format:</p> <p><code>RS_Count(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_Count(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_Count(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>25\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_Count(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>6\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_summarystats","title":"RS_SummaryStats","text":"<p>Introduction: Returns summary statistic for a particular band based on the <code>statType</code> parameter. The function defaults to band index of <code>1</code> when <code>band</code> is not specified and excludes noDataValue if <code>excludeNoDataValue</code> is not specified.</p> <p><code>statType</code> parameter takes the following strings:</p> <ul> <li><code>count</code>: Total count of all pixels in the specified band</li> <li><code>sum</code>: Sum of all pixel values in the specified band</li> <li><code>mean</code>: Mean value of all pixel values in the specified band</li> <li><code>stddev</code>: Standard deviation of all pixels in the specified band</li> <li><code>min</code>: Minimum pixel value in the specified band</li> <li><code>max</code>: Maximum pixel value in the specified band</li> </ul> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Formats:</p> <p><code>RS_SummaryStats(raster: Raster, statType: String, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_SummaryStats(raster: Raster, statType: String, band: Integer = 1)</code></p> <p><code>RS_SummaryStats(raster: Raster, statType: String)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStats(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), \"stddev\", 1, false)\n</code></pre> <p>Output:</p> <pre><code>9.4678403028357\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_summarystatsall","title":"RS_SummaryStatsAll","text":"<p>Introduction: Returns summary stats struct consisting of count, sum, mean, stddev, min, max for a given band in raster. If band is not specified then it defaults to <code>1</code>.</p> <p>Note</p> <p>If excludeNoDataValue is set <code>true</code> then it will only count pixels with value not equal to the nodata value of the raster. Set excludeNoDataValue to <code>false</code> to get count of all pixels in raster.</p> <p>Note</p> <p>If the mentioned band index doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>Formats:</p> <p><code>RS_SummaryStatsAll(raster: Raster, band: Integer = 1, excludeNoDataValue: Boolean = true)</code></p> <p><code>RS_SummaryStatsAll(raster: Raster, band: Integer = 1)</code></p> <p><code>RS_SummaryStatsAll(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStatsAll(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1, false)\n</code></pre> <p>Output:</p> <pre><code>{25.0, 204.0, 8.16, 9.4678403028357, 0.0, 25.0}\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_SummaryStatsAll(RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0), 1)\n</code></pre> <p>Output:</p> <pre><code>{14.0, 204.0, 14.571428571428571, 11.509091348732502, 1.0, 25.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstats","title":"RS_ZonalStats","text":"<p>Introduction: This returns a statistic value specified by <code>statType</code> over the region of interest defined by <code>zone</code>. It computes the statistic from the pixel values within the ROI geometry and returns the result. If the <code>excludeNoData</code> parameter is not specified, it will default to <code>true</code>. This excludes NoData values from the statistic calculation. Additionally, if the <code>band</code> parameter is not provided, band 1 will be used by default for the statistic computation. The valid options for <code>statType</code> are:</p> <ul> <li><code>count</code>: Number of pixels in the region.</li> <li><code>sum</code>: Sum of pixel values.</li> <li><code>mean|average|avg</code>: Arithmetic mean.</li> <li><code>median</code>: Middle value in the region.</li> <li><code>mode</code>: Most occurring value, if there are multiple values with same occurrence then will return the largest number.</li> <li><code>stddev|sd</code>: Standard deviation.</li> <li><code>variance</code>: Variance.</li> <li><code>min</code>: Minimum value in the region.</li> <li><code>max</code>: Maximum value in the region.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect when <code>lenient</code> parameter is set to <code>false</code>.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p><code>lenient</code> parameter is set to <code>true</code> by default. The function will return <code>null</code> if the <code>raster</code> and <code>zone</code> geometry do not intersect.</p> <p>Format:</p> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, excludeNoData: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String, excludeNoData: Boolean)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, band: Integer, statType: String)\n</code></pre> <pre><code>RS_ZonalStats(raster: Raster, zone: Geometry, statType: String)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>RS_ZonalStats(rast1, geom1, 1, 'sum', false)\n</code></pre> <p>Output:</p> <pre><code>10690406\n</code></pre> <p>SQL Example</p> <pre><code>RS_ZonalStats(rast2, geom2, 1, 'mean', true)\n</code></pre> <p>Output:</p> <pre><code>226.55992667794473\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_zonalstatsall","title":"RS_ZonalStatsAll","text":"<p>Introduction: Returns a struct of statistic values, where each statistic is computed over a region defined by the <code>zone</code> geometry. The struct has the following schema:</p> <ul> <li>count: Count of the pixels.</li> <li>sum: Sum of the pixel values.</li> <li>mean: Arithmetic mean.</li> <li>median: Median.</li> <li>mode: Mode.</li> <li>stddev: Standard deviation.</li> <li>variance: Variance.</li> <li>min: Minimum value of the zone.</li> <li>max: Maximum value of the zone.</li> </ul> <p>Note</p> <p>If the coordinate reference system (CRS) of the input <code>zone</code> geometry differs from that of the <code>raster</code>, then <code>zone</code> will be transformed to match the CRS of the <code>raster</code> before computation.</p> <p>The following conditions will throw an <code>IllegalArgumentException</code> if they are not met:</p> <ul> <li>The provided <code>raster</code> and <code>zone</code> geometry should intersect when <code>lenient</code> parameter is set to <code>false</code>.</li> <li>The option provided to <code>statType</code> should be valid.</li> </ul> <p><code>lenient</code> parameter is set to <code>true</code> by default. The function will return <code>null</code> if the <code>raster</code> and <code>zone</code> geometry do not intersect.</p> <p>Format:</p> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, excludeNodata: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer, excludeNodata: Boolean)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry, band: Integer)\n</code></pre> <pre><code>RS_ZonalStatsAll(raster: Raster, zone: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>SQL Example</p> <pre><code>RS_ZonalStatsAll(rast1, geom1, 1, false)\n</code></pre> <p>Output:</p> <pre><code>{184792.0, 1.0690406E7, 57.851021689230684, 0.0, 0.0, 92.13277429243035, 8488.448098819916, 0.0, 255.0}\n</code></pre> <p>SQL Example</p> <pre><code>RS_ZonalStatsAll(rast2, geom2, 1, true)\n</code></pre> <p>Output:</p> <pre><code>{14184.0, 3213526.0, 226.55992667794473, 255.0, 255.0, 74.87605357255357, 5606.423398599913, 1.0, 255.0}\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-predicates","title":"Raster Predicates","text":""},{"location":"api/sql/Raster-operators/#rs_contains","title":"RS_Contains","text":"<p>Introduction: Returns true if the geometry or raster on the left side contains the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format:</p> <p><code>RS_Contains(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Contains(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Contains(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((5 5, 5 10, 10 10, 10 5, 5 5))')) rast_geom,\n    RS_Contains(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 2, 22, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_intersects","title":"RS_Intersects","text":"<p>Introduction: Returns true if raster or geometry on the left side intersects with the raster or geometry on the right side. The convex hull of the raster is considered in the test.</p> <p>Rules for testing spatial relationship:</p> <ul> <li>If the raster or geometry does not have a defined SRID, it is assumed to be in WGS84.</li> <li>If both sides are in the same CRS, then perform the relationship test directly.</li> <li>Otherwise, both sides will be transformed to WGS84 before the relationship test.</li> </ul> <p>Format:</p> <p><code>RS_Intersects(raster: Raster, geom: Geometry)</code></p> <p><code>RS_Intersects(geom: Geometry, raster: Raster)</code></p> <p><code>RS_Intersects(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_SetSRID(ST_PolygonFromEnvelope(0, 0, 10, 10), 4326)) rast_geom,\n    RS_Intersects(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 10, 10, 1, 11, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_within","title":"RS_Within","text":"<p>Introduction: Returns true if the geometry or raster on the left side is within the geometry or raster on the right side. The convex hull of the raster is considered in the test.</p> <p>The rules for testing spatial relationship is the same as <code>RS_Intersects</code>.</p> <p>Format: <code>RS_Within(raster: Raster, geom: Geometry)</code></p> <p>Format: <code>RS_Within(geom: Geometry, raster: Raster)</code></p> <p>Format: <code>RS_Within(raster0: Raster, raster1: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), ST_GeomFromWKT('POLYGON ((0 0, 0 50, 100 50, 100 0, 0 0))')) rast_geom,\n    RS_Within(RS_MakeEmptyRaster(1, 20, 20, 2, 22, 1), RS_MakeEmptyRaster(1, 30, 30, 2, 22, 1)) rast_rast\n</code></pre> <p>Output:</p> <pre><code>+---------+---------+\n|rast_geom|rast_rast|\n+---------+---------+\n|     true|     true|\n+---------+---------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-based-operators","title":"Raster Based Operators","text":""},{"location":"api/sql/Raster-operators/#rs_addband","title":"RS_AddBand","text":"<p>Introduction: Adds a new band to a raster <code>toRaster</code> at a specified index <code>toRasterIndex</code>. The new band's values are copied from <code>fromRaster</code> at a specified band index <code>fromBand</code>. If no <code>toRasterIndex</code> is provided, the new band is appended to the end of <code>toRaster</code>. If no <code>fromBand</code> is specified, band <code>1</code> from <code>fromRaster</code> is copied by default.</p> <p>Note</p> <p>IllegalArgumentException will be thrown in these cases:</p> <ul> <li>The provided Rasters, <code>toRaster</code> &amp; <code>fromRaster</code> don't have same shape.</li> <li>The provided <code>fromBand</code> is not in <code>fromRaster</code>.</li> <li>The provided <code>toRasterIndex</code> is not in or at end of <code>toRaster</code>.</li> </ul> <p>Format:</p> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1, toRasterIndex: Integer = at_end)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster, fromBand: Integer = 1)\n</code></pre> <pre><code>RS_AddBand(toRaster: Raster, fromRaster: Raster)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 2, 1) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_clip","title":"RS_Clip","text":"<p>Introduction: Returns a raster that is clipped by the given geometry.</p> <p>If <code>crop</code> is not specified then it will default to <code>true</code>, meaning it will make the resulting raster shrink to the geometry's extent and if <code>noDataValue</code> is not specified then the resulting raster will have the minimum possible value for the band pixel data type.</p> <p>Note</p> <ul> <li>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</li> <li>Since <code>v1.7.0</code>, <code>RS_Clip</code> function will return <code>null</code> if the <code>raster</code> and <code>geometry</code> geometry do not intersect. If you want to throw an exception in this case, you can set the <code>lenient</code> parameter to <code>false</code>.</li> </ul> <p>Format:</p> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, noDataValue: Double, crop: Boolean, lenient: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, noDataValue: Double, crop: Boolean)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry, noDataValue: Double)\n</code></pre> <pre><code>RS_Clip(raster: Raster, band: Integer, geom: Geometry)\n</code></pre> <p>Since: <code>v1.5.1</code></p> <p>Original Raster:</p> <p></p> <p>SQL Example</p> <pre><code>SELECT RS_Clip(\n        RS_FromGeoTiff(content), 1,\n        ST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n        200, true\n    )\n</code></pre> <p>Output:</p> <p></p> <p>SQL Example</p> <pre><code>SELECT RS_Clip(\n        RS_FromGeoTiff(content), 1,\n        ST_GeomFromWKT('POLYGON ((236722 4204770, 243900 4204770, 243900 4197590, 221170 4197590, 236722 4204770))'),\n        200, false\n    )\n</code></pre> <p>Output:</p> <p></p>"},{"location":"api/sql/Raster-operators/#rs_interpolate","title":"RS_Interpolate","text":"<p>Introduction: This function performs interpolation on a raster using the Inverse Distance Weighted (IDW) method. This method estimates cell values by averaging the values of sample data points in the vicinity of each processing cell. The influence of a sample point on the interpolated value is inversely proportional to the distance from the cell being estimated, with nearer points having more influence or weight in the averaging process.</p> <p>This technique is effective in scenarios where continuity of spatial data is important, and it is essential to estimate values for locations that do not have direct measurements, often represented by NaN or noDataValue in raster data.</p> <p>Note</p> <p>This method assumes that the spatial influence of a variable diminishes with distance. In geospatial analysis, this means features or phenomena closer to a point of interest are given more weight than those further away. For example, in environmental data analysis, measurements from nearby locations have a greater impact on interpolated values than distant ones, reflecting the natural gradation and spatial continuity.</p> <p>Formats:</p> <pre><code>RS_Interpolate(raster: Raster)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double, maxRadiusOrMinPoints: Double)\n</code></pre> <pre><code>RS_Interpolate(raster: Raster, power: Double, mode: String, numPointsOrRadius: Double, maxRadiusOrMinPoints: Double, band: Integer)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>Parameters:</p> <ul> <li><code>raster</code>: The raster to be interpolated.</li> <li><code>band</code>: The band of the raster to be used for interpolation. If <code>band</code> is not provided, interpolation is performed across all bands.</li> <li><code>power</code>: A positive real number defining the exponent of distance in the IDW calculation. This parameter controls the influence of distant points on the interpolated values, default being set to 2.</li> <li><code>mode</code>: Specifies the interpolation mode - either <code>\"Variable\"</code> or <code>\"Fixed\"</code>.</li> <li>In <code>\"Variable\"</code> mode:<ul> <li><code>numPointsOrRadius</code>: Specifies the number of nearest input points to be used for interpolation. Defaults to 12 if not provided.</li> <li><code>maxRadiusOrMinPoints</code>: Sets the maximum search radius, with the default being the diagonal length of the raster.</li> </ul> </li> <li>In <code>\"Fixed\"</code> mode:<ul> <li><code>numPointsOrRadius</code>: Defines the radius within which input sample points are considered. Defaults to the diagonal length of the raster if not specified.</li> <li><code>maxRadiusOrMinPoints</code>: Represents the minimum number of points required within the radius. Defaults to 0 if not provided.</li> </ul> </li> </ul> <p>SQL Example:</p> <pre><code>SELECT RS_Interpolate(raster, 1, 2.0, 'Variable', 12, 1000)\n</code></pre> <p>Output (Shown as heatmap):</p> <p> </p>"},{"location":"api/sql/Raster-operators/#rs_metadata","title":"RS_MetaData","text":"<p>Introduction: Returns the metadata of the raster as a struct. The struct has the following schema:</p> <ul> <li>upperLeftX: upper left x coordinate of the raster, in terms of CRS units</li> <li>upperLeftX: upper left y coordinate of the raster, in terms of CRS units</li> <li>gridWidth: width of the raster, in terms of pixels</li> <li>gridHeight: height of the raster, in terms of pixels</li> <li>scaleX: ScaleX: the scaling factor in the x direction</li> <li>scaleY: ScaleY: the scaling factor in the y direction</li> <li>skewX: skew in x direction (rotation x)</li> <li>skewY: skew in y direction (rotation y)</li> <li>srid: srid of the raster</li> <li>numSampleDimensions: number of bands</li> <li>tileWidth: (Since <code>v1.6.1</code>) width of tiles in the raster</li> <li>tileHeight: (Since <code>v1.6.1</code>) height of tiles in the raster</li> </ul> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p><code>tileWidth</code> and <code>tileHeight</code> are available since <code>v1.6.1</code>, they are the dimensions of the tiles in the raster. For example, rasters written by <code>RS_FromGeoTiff</code> uses the tiling scheme of the loaded GeoTIFF file. For rasters that has only 1 tile, <code>tileWidth</code> and <code>tileHeight</code> will be equal to <code>gridWidth</code> and <code>gridHeight</code> respectively.</p> <p>Format: <code>RS_MetaData (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_MetaData(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>{-1.3095817809482181E7, 4021262.7487925636, 512, 517, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857, 1, 256, 256}\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_normalizeall","title":"RS_NormalizeAll","text":"<p>Introduction: Normalizes values in all bands of a raster between a given normalization range. The function maintains the data type of the raster values by ensuring that the normalized values are cast back to the original data type of each band in the raster. By default, the values are normalized to range [0, 255]. RS_NormalizeAll can take upto 7 of the following arguments.</p> <ul> <li><code>raster</code>: The raster to be normalized.</li> <li><code>minLim</code> and <code>maxLim</code> (Optional): The lower and upper limits of the normalization range. By default, normalization range is set to [0, 255].</li> <li><code>normalizeAcrossBands</code> (Optional): A boolean flag to determine the normalization method. If set to true (default), normalization is performed across all bands based on global min and max values. If false, each band is normalized individually based on its own min and max values.</li> <li><code>noDataValue</code> (Optional): Defines the value to be used for missing or invalid data in raster bands. By default, noDataValue is set to <code>maxLim</code> and Safety mode is triggered.</li> <li><code>minValue</code> and <code>maxValue</code> (Optional): Optionally, specific minimum and maximum values of the input raster can be provided. If not provided, these values are computed from the raster data.</li> </ul> <p>A Safety mode is triggered when <code>noDataValue</code> is not given. This sets <code>noDataValue</code> to <code>maxLim</code> and normalizes valid data values to the range [minLim, maxLim-1]. This is to avoid replacing valid data that might coincide with the new <code>noDataValue</code>.</p> <p>Warning</p> <p>Using a noDataValue that falls within the normalization range can lead to loss of valid data. If any data value within a raster band matches the specified noDataValue, it will be replaced and cannot be distinguished or recovered later. Exercise caution in selecting a noDataValue to avoid unintentional data alteration.</p> <p>Formats:</p> <pre><code>RS_NormalizeAll (raster: Raster)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean, noDataValue: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, noDataValue: Double, minValue: Double, maxValue: Double)\n</code></pre> <pre><code>RS_NormalizeAll (raster: Raster, minLim: Double, maxLim: Double, normalizeAcrossBands: Boolean, noDataValue: Double, minValue: Double, maxValue: Double )\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NormalizeAll(raster, 0, 1)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_numbands","title":"RS_NumBands","text":"<p>Introduction: Returns the number of the bands in the raster.</p> <p>Format: <code>RS_NumBands (raster: Raster)</code></p> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_NumBands(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>4\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_reprojectmatch","title":"RS_ReprojectMatch","text":"<p>Introduction: Reproject a raster to match the geo-reference, CRS, and envelope of a reference raster. The output raster always have the same extent and resolution as the reference raster. For pixels not covered by the input raster, nodata value is assigned, or 0 is assigned if the input raster does not have nodata value.</p> <p>The default resampling algorithm is <code>NearestNeighbor</code>. The following resampling algorithms are supported (case-insensitive):</p> <ol> <li>NearestNeighbor</li> <li>Bilinear</li> <li>Bicubic</li> </ol> <p>This function serves the same purpose as the <code>RasterArray.reproject_match</code> function in rioxarray.</p> <p>Format:</p> <p><code>RS_ReprojectMatch (raster: Raster, reference: Raster, algorithm: String)</code></p> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>WITH t AS (\n    SELECT RS_MapAlgebra(RS_MakeEmptyRaster(1, 500, 500, 308736,4091167, 1000, -1000, 0, 0, 32611), 'D', 'out = sin(x() * 0.2);') rast1,\n        RS_MapAlgebra(RS_MakeEmptyRaster(1, 500, 500, 16536,4185970, 1000, -1000, 0, 0, 32612), 'D', 'out = sin(y() * 0.2);') rast2\n) SELECT t.rast1, t.rast2, RS_ReprojectMatch(rast1, rast2) rast12, RS_ReprojectMatch(rast2, rast1) rast21 FROM t\n</code></pre> <p>Output:</p> <p></p>"},{"location":"api/sql/Raster-operators/#rs_resample","title":"RS_Resample","text":"<p>Introduction: Resamples a raster using a given resampling algorithm and new dimensions (width and height), a new grid corner to pivot the raster at (gridX and gridY) and a set of georeferencing attributes (scaleX and scaleY).</p> <p>RS_Resample also provides an option to pass a reference raster to draw the georeferencing attributes out of. However, the SRIDs of the input and reference raster must be same, otherwise RS_Resample throws an IllegalArgumentException.</p> <p>For the purpose of resampling, width-height pair and scaleX-scaleY pair are mutually exclusive, meaning any one of them can be used at a time.</p> <p>The <code>useScale</code> parameter controls whether to use width-height or scaleX-scaleY. If <code>useScale</code> is false, the provided <code>widthOrScale</code> and <code>heightOrScale</code> values will be floored to integers and considered as width and height respectively (floating point width and height are not allowed). Otherwise, they are considered as scaleX and scaleY respectively.</p> <p>Currently, RS_Resample does not support skewed rasters, and hence even if a skewed reference raster is provided, its skew values are ignored. If the input raster is skewed, the output raster geometry and interpolation may be incorrect.</p> <p>The default algorithm used for resampling is <code>NearestNeighbor</code>, and hence if a null, empty or invalid value of algorithm is provided, RS_Resample defaults to using <code>NearestNeighbor</code>. However, the algorithm parameter is non-optional.</p> <p>Following are valid values for the algorithm parameter (Case-insensitive):</p> <ol> <li>NearestNeighbor</li> <li>Bilinear</li> <li>Bicubic</li> </ol> <p>Tip</p> <p>If you just want to resize or rescale an input raster, you can use RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format:</p> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, gridX: Double, gridY: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, widthOrScale: Double, heightOrScale: Double, useScale: Boolean, algorithm: String)\n</code></pre> <pre><code>RS_Resample(raster: Raster, referenceRaster: Raster, useScale: Boolean, algorithm: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>WITH INPUT_RASTER AS (\n SELECT RS_AddBandFromArray(\n    RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\n    ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nRESAMPLED_RASTER AS (\n SELECT RS_Resample(rast, 6, 5, 1, -1, false, null) as resample_rast from INPUT_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0|\n\n(-0.33333333333333326,0.19999999999999996,6,5,1.388888888888889,-1.24,0,0,0,1)\n</code></pre> <p>SQL Example</p> <pre><code> WITH INPUT_RASTER AS (\n   SELECT RS_AddBandFromArray(\n    RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0),\n    ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n   ),\n   RESAMPLED_RASTER AS (\n    SELECT RS_Resample(rast, 1.2, -1.4, true, null) as resample_rast from INPUT_RASTER\n   )\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>|       NaN         NaN         NaN         NaN         NaN         NaN         NaN|\n|       NaN    3.050000    3.650000    4.250000    5.160000    6.690000    7.200000|\n|       NaN    5.150000    5.750000    6.350000    7.250000    8.750000    9.250000|\n|       NaN    7.250000    7.850000    8.450000    9.070000    9.730000    9.950000|\n|       NaN    7.400000    8.000000    8.600000    9.200000    9.800000   10.000000|\n\n(0.0, 0.0, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre> <p>SQL Example</p> <pre><code>WITH INPUT_RASTER AS (\n    SELECT RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 2, -2, 0, 0, 0), ARRAY(1, 2, 3, 5, 4, 5, 6, 9, 7, 8, 9, 10), 1, null) as rast\n),\nREF_RASTER AS (\n    SELECT RS_MakeEmptyRaster(2, 'd', 6, 5, 1, -1, 1.2, -1.4, 0, 0, 0) as ref_rast\n),\nRESAMPLED_RASTER AS (\n    SELECT RS_Resample(rast, ref_rast, true, null) as resample_rast from INPUT_RASTER, REF_RASTER\n)\nSELECT RS_AsMatrix(resample_rast) as rast_matrix, RS_Metadata(resample_rast) as rast_metadata from RESAMPLED_RASTER\n</code></pre> <p>Output:</p> <pre><code>| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 1.0   1.0   2.0   3.0   3.0   5.0   5.0|\n| 4.0   4.0   5.0   6.0   6.0   9.0   9.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n| 7.0   7.0   8.0   9.0   9.0  10.0  10.0|\n\n(-0.20000000298023224, 0.4000000059604645, 7.0, 5.0, 1.2, -1.4, 0.0, 0.0, 0.0, 1.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setbandnodatavalue","title":"RS_SetBandNoDataValue","text":"<p>Introduction: This sets the no data value for a specified band in the raster. If the band index is not provided, band 1 is assumed by default. Passing a <code>null</code> value for <code>noDataValue</code> will remove the no data value and that will ensure all pixels are included in functions rather than excluded as no data.</p> <p>Since <code>v1.5.1</code>, this function supports the ability to replace the current no-data value with the new <code>noDataValue</code>.</p> <p>Note</p> <p>When <code>replace</code> is true, any pixels matching the provided <code>noDataValue</code> will be considered as no-data in the output raster.</p> <p>An <code>IllegalArgumentException</code> will be thrown if the input raster does not already have a no-data value defined. Replacing existing values with <code>noDataValue</code> requires a defined no-data baseline to evaluate against.</p> <p>To use this for no-data replacement, the input raster must first set its no-data value, which can then be selectively replaced via this function.</p> <p>Format:</p> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer, noDataValue: Double, replace: Boolean)\n</code></pre> <pre><code>RS_SetBandNoDataValue(raster: Raster, bandIndex: Integer = 1, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandNoDataValue(\n        RS_SetBandNoDataValue(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            -999\n            )\n        )\n</code></pre> <p>Output:</p> <pre><code>-999\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setgeoreference","title":"RS_SetGeoReference","text":"<p>Introduction: Sets the Georeference information of an object in a single call. Accepts inputs in <code>GDAL</code> and <code>ESRI</code> format. Default format is <code>GDAL</code>. If all 6 parameters are not provided then will return null.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>Format:</p> <pre><code>RS_SetGeoReference(raster: Raster, geoRefCoord: String, format: String = \"GDAL\")\n</code></pre> <pre><code>RS_SetGeoReference(raster: Raster, upperLeftX: Double, upperLeftY: Double, scaleX: Double, scaleY: Double, skewX: Double, skewY: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Difference between format representation is as follows:</p> <p><code>GDAL</code></p> <pre><code>ScaleX SkewY SkewX ScaleY UpperLeftX UpperLeftY\n</code></pre> <p><code>ESRI</code></p> <pre><code>ScaleX SkewY SkewX ScaleY (UpperLeftX + ScaleX * 0.5) (UpperLeftY + ScaleY * 0.5)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            '3 1.5 1.5 2 22 3'\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n22.000000\n3.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(1, 20, 20, 2, 22, 2, 3, 1, 1, 0),\n            '3 1.5 1.5 2 22 3', 'ESRI'\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>3.000000\n1.500000\n1.500000\n2.000000\n20.500000\n2.000000\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_GeoReference(\n        RS_SetGeoReference(\n            RS_MakeEmptyRaster(2, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n            8, -3, 4, 5, 0.2, 0.2\n        )\n    )\n</code></pre> <p>Output:</p> <pre><code>4.000000\n0.200000\n0.200000\n5.000000\n8.000000\n-3.000000\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setpixeltype","title":"RS_SetPixelType","text":"<p>Introduction: Returns a modified raster with the desired pixel data type.</p> <p>The <code>dataType</code> parameter accepts one of the following strings.</p> <ul> <li>\"D\" - 64 bits Double</li> <li>\"F\" - 32 bits Float</li> <li>\"I\" - 32 bits signed Integer</li> <li>\"S\" - 16 bits signed Short</li> <li>\"US\" - 16 bits unsigned Short</li> <li>\"B\" - 8 bits unsigned Byte</li> </ul> <p>Note</p> <p>If the specified <code>dataType</code> is narrower than the original data type, the function will truncate the pixel values to fit the new data type range.</p> <p>Format:</p> <pre><code>RS_SetPixelType(raster: Raster, dataType: String)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example:</p> <pre><code>RS_SetPixelType(raster, \"I\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalue","title":"RS_SetValue","text":"<p>Introduction: Returns a raster by replacing the value of pixel specified by <code>colX</code> and <code>rowY</code>.</p> <p>Format:</p> <pre><code>RS_SetValue(raster: Raster, bandIndex: Integer = 1, colX: Integer, rowY: Integer, newValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n               RS_SetValue(\n                       RS_AddBandFromArray(\n                               RS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n                           [1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0], 1, 0d\n                           ),\n                       1, 2, 2, 255\n                   )\n           )\n</code></pre> <p>Output:</p> <pre><code>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 255.0, 2.0, 3.0, 3.0, 5.0, 6.0, 7.0, 0.0, 0.0, 3.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setvalues","title":"RS_SetValues","text":"<p>Introduction: Returns a raster by replacing the values of pixels in a specified rectangular region. The top left corner of the region is defined by the <code>colX</code> and <code>rowY</code> coordinates. The <code>width</code> and <code>height</code> parameters specify the dimensions of the rectangular region. The new values to be assigned to the pixels in this region can be specified as an array passed to this function.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>geom</code> geometry differs from that of the <code>raster</code>, then <code>geom</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>geom</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, colX: Integer, rowY: Integer, width: Integer, height: Integer, newValues: ARRAY[Double], keepNoData: Boolean = false)\n</code></pre> <pre><code>RS_SetValues(raster: Raster, bandIndex: Integer, geom: Geometry, newValue: Double, keepNoData: Boolean = false)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>The <code>colX</code>, <code>rowY</code>, and <code>bandIndex</code> are 1-indexed. If <code>keepNoData</code> is <code>true</code>, the pixels with NoData value will not be set to the corresponding value in <code>newValues</code>. The <code>newValues</code> should be provided in rows.</p> <p>The geometry variant of this function accepts all types of Geometries, and it sets the <code>newValue</code> in the specified region under the <code>geom</code>.</p> <p>Note</p> <p>If the shape of <code>newValues</code> doesn't match with provided <code>width</code> and <code>height</code>, <code>IllegalArgumentException</code> is thrown.</p> <p>Note</p> <p>If the mentioned <code>bandIndex</code> doesn't exist, this will throw an <code>IllegalArgumentException</code>.</p> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n        RS_SetValues(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(1, 5, 5, 0, 0, 1, -1, 0, 0, 0),\n                Array(1,1,1,0,0,0,1,2,3,3,5,6,7,0,0,3,0,0,3,0,0,0,0,0,0), 1, 0d\n                ),\n            1, 2, 2, 3, 3, [11,12,13,14,15,16,17,18,19]\n            )\n        )\n</code></pre> <p>Output:</p> <pre><code>Array(1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 11.0, 12.0, 13.0, 3.0, 5.0, 14.0, 15.0, 16.0, 0.0, 3.0, 17.0, 18.0, 19.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(\n        RS_SetValues(\n            RS_AddBandFromArray(\n                RS_MakeEmptyRaster(1, 5, 5, 1, -1, 1, -1, 0, 0, 0),\n                Array(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), 1\n                ),\n            1, ST_GeomFromWKT('POLYGON((1 -1, 3 -3, 6 -6, 4 -1, 1 -1))'), 255, false\n            )\n           )\n</code></pre> <p>Output:</p> <pre><code>Array(255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 255.0, 0.0, 0.0, 0.0, 255.0, 255.0, 0.0, 0.0, 0.0, 0.0, 255.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_setsrid","title":"RS_SetSRID","text":"<p>Introduction: Sets the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SetSRID (raster: Raster, srid: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SetSRID(raster, 4326)\nFROM raster_table\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_srid","title":"RS_SRID","text":"<p>Introduction: Returns the spatial reference system identifier (SRID) of the raster geometry.</p> <p>Format: <code>RS_SRID (raster: Raster)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_SRID(raster) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>3857\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_union","title":"RS_Union","text":"<p>Introduction: Returns a combined multi-band raster from 2 or more input Rasters. The order of bands in the resultant raster will be in the order of the input rasters. For example if <code>RS_Union</code> is called on two 2-banded raster, raster1 and raster2, the first 2 bands of the resultant 4-banded raster will be from raster1 and the last 2 from raster 2.</p> <p>Note</p> <p>If the provided input Rasters don't have same shape an IllegalArgumentException will be thrown.</p> <p>Format:</p> <pre><code>RS_Union (raster1: Raster, raster2: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster, raster6: Raster)\n</code></pre> <pre><code>RS_Union (raster1: Raster, raster2: Raster, raster3: Raster, raster4: Raster, raster5: Raster, raster6: Raster, raster7: Raster)\n</code></pre> <p>Since: <code>v1.6.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Union(raster1, raster2, raster3, raster4) FROM rasters\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_value","title":"RS_Value","text":"<p>Introduction: Returns the value at the given point in the raster. If no band number is specified it defaults to 1.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>point</code> geometry differs from that of the <code>raster</code>, then <code>point</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>point</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Value (raster: Raster, point: Geometry)</code></p> <p><code>RS_Value (raster: Raster, point: Geometry, band: Integer)</code></p> <p><code>RS_Value (raster: Raster, colX: Integer, colY: Integer, band: Integer)</code></p> <p>Since: <code>v1.4.0</code></p> <p>Spark SQL Examples:</p> <ul> <li>For Point Geometry:</li> </ul> <pre><code>SELECT RS_Value(raster, ST_Point(-13077301.685, 4002565.802)) FROM raster_table\n</code></pre> <ul> <li>For Grid Coordinates:</li> </ul> <pre><code>SELECT RS_Value(raster, 3, 4, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>5.0\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_values","title":"RS_Values","text":"<p>Introduction: Returns the values at the given points or grid coordinates in the raster. If no band number is specified it defaults to 1.</p> <p>RS_Values is similar to RS_Value but operates on an array of points or grid coordinates. RS_Values can be significantly faster since a raster only has to be loaded once for several points.</p> <p>Note</p> <p>Since <code>v1.5.1</code>, if the coordinate reference system (CRS) of the input <code>points</code> geometries differs from that of the <code>raster</code>, then <code>points</code> will be transformed to match the CRS of the <code>raster</code>. If the <code>raster</code> or <code>points</code> doesn't have a CRS then it will default to <code>4326/WGS84</code>.</p> <p>Format:</p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry])</code></p> <p><code>RS_Values (raster: Raster, points: ARRAY[Geometry], band: Integer)</code></p> <pre><code>RS_Values (raster: Raster, xCoordinates: ARRAY[Integer], yCoordinates: ARRAY[Integer], band: Integer)\n</code></pre> <p>Since: <code>v1.4.0</code></p> <p>SQL Example</p> <ul> <li>For Array of Point geometries:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(ST_Point(-1307.5, 400.8), ST_Point(-1403.3, 399.1)))\nFROM raster_table\n</code></pre> <ul> <li>For Arrays of grid coordinates:</li> </ul> <pre><code>SELECT RS_Values(raster, Array(4, 5), Array(3, 2), 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>Array(5.0, 3.0)\n</code></pre> <p>Spark SQL example for joining a point dataset with a raster dataset:</p> <pre><code>val pointDf = sedona.read...\nval rasterDf = sedona.read.format(\"binaryFile\").load(\"/some/path/*.tiff\")\n  .withColumn(\"raster\", expr(\"RS_FromGeoTiff(content)\"))\n  .withColumn(\"envelope\", expr(\"RS_Envelope(raster)\"))\n\n// Join the points with the raster extent and aggregate points to arrays.\n// We only use the path and envelope of the raster to keep the shuffle as small as possible.\nval df = pointDf.join(rasterDf.select(\"path\", \"envelope\"), expr(\"ST_Within(point_geom, envelope)\"))\n  .groupBy(\"path\")\n  .agg(collect_list(\"point_geom\").alias(\"point\"), collect_list(\"point_id\").alias(\"id\"))\n\ndf.join(rasterDf, \"path\")\n  .selectExpr(\"explode(arrays_zip(id, point, RS_Values(raster, point))) as result\")\n  .selectExpr(\"result.*\")\n  .show()\n</code></pre> <p>Output:</p> <pre><code>+----+------------+-------+\n| id | point      | value |\n+----+------------+-------+\n|  4 | POINT(1 1) |   3.0 |\n|  5 | POINT(2 2) |   7.0 |\n+----+------------+-------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-tiles","title":"Raster Tiles","text":""},{"location":"api/sql/Raster-operators/#rs_tile","title":"RS_Tile","text":"<p>Introduction: Returns an array of rasters resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_Tile(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_Tile(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.1</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_Tile(rast, 2, 2) AS tiles FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|               tiles|\n+--------------------+\n|[GridCoverage2D[\"...|\n+--------------------+\n</code></pre> <p>User can use <code>EXPLODE</code> function to expand the array of tiles into a table of tiles.</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT EXPLODE(RS_Tile(rast, 2, 2)) AS tile FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                tile|\n+--------------------+\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_tileexplode","title":"RS_TileExplode","text":"<p>Introduction: Generates records containing raster tiles resulting from the split of the input raster based upon the desired dimensions of the output rasters.</p> <p>Format: <code>RS_TileExplode(raster: Raster, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndex: Int, width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Format: <code>RS_TileExplode(raster: Raster, bandIndices: Array[Int], width: Int, height: Int, padWithNoData: Boolean = false, noDataVal: Double = null)</code></p> <p>Since: <code>v1.5.0</code></p> <p><code>width</code> and <code>height</code> specifies the size of generated tiles. If <code>bandIndices</code> is NULL or not specified, all bands will be included in the output tiles, otherwise bands specified by <code>bandIndices</code> will be included. <code>bandIndex</code> can be specified if there is only one selected band, which is equivalent to specifying <code>bandIndices</code> as <code>ARRAY(bandIndex)</code>.Band indices are 1-based.</p> <p>If <code>padWithNoData</code> = false, edge tiles on the right and bottom sides of the raster may have different dimensions than the rest of the tiles. If <code>padWithNoData</code> = true, all tiles will have the same dimensions with the possibility that edge tiles being padded with NODATA values. If raster band(s) do not have NODATA value(s) specified, one can be specified by setting <code>noDataVal</code>.</p> <p>The returned records have the following schema:</p> <ul> <li><code>x</code>: The index of the tile along X axis (0-based).</li> <li><code>y</code>: The index of the tile along Y axis (0-based).</li> <li><code>tile</code>: The tile.</li> </ul> <p>SQL example:</p> <pre><code>WITH raster_table AS (SELECT RS_MakeEmptyRaster(1, 6, 6, 300, 400, 10) rast)\nSELECT RS_TileExplode(rast, 2, 2) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+---+---+--------------------+\n|  x|  y|                tile|\n+---+---+--------------------+\n|  0|  0|GridCoverage2D[\"g...|\n|  1|  0|GridCoverage2D[\"g...|\n|  2|  0|GridCoverage2D[\"g...|\n|  0|  1|GridCoverage2D[\"g...|\n|  1|  1|GridCoverage2D[\"g...|\n|  2|  1|GridCoverage2D[\"g...|\n|  0|  2|GridCoverage2D[\"g...|\n|  1|  2|GridCoverage2D[\"g...|\n|  2|  2|GridCoverage2D[\"g...|\n+---+---+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#raster-to-map-algebra-operators","title":"Raster to Map Algebra Operators","text":"<p>To bridge the gap between the raster and map algebra worlds, the following operators are provided. These operators convert a raster to a map algebra object. The map algebra object can then be used with the map algebra operators described in the next section.</p>"},{"location":"api/sql/Raster-operators/#rs_bandasarray","title":"RS_BandAsArray","text":"<p>Introduction: Extract a band from a raster as an array of doubles.</p> <p>Format: <code>RS_BandAsArray (raster: Raster, bandIndex: Integer)</code>.</p> <p>Since: <code>v1.4.1</code></p> <p>BandIndex is 1-based and must be between 1 and RS_NumBands(raster). It returns null if the bandIndex is out of range or the raster is null.</p> <p>SQL Example</p> <pre><code>SELECT RS_BandAsArray(raster, 1) FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|                band|\n+--------------------+\n|[0.0, 0.0, 0.0, 0...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_addbandfromarray","title":"RS_AddBandFromArray","text":"<p>Introduction: Add a band to a raster from an array of doubles.</p> <p>Format:</p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double])</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer)</code></p> <p><code>RS_AddBandFromArray (raster: Raster, band: ARRAY[Double], bandIndex: Integer, noDataValue: Double)</code></p> <p>Since: <code>v1.5.0</code></p> <p>The bandIndex is 1-based and must be between 1 and RS_NumBands(raster) + 1. It throws an exception if the bandIndex is out of range or the raster is null. If not specified, the noDataValue of the band is assumed to be null.</p> <p>When the bandIndex is RS_NumBands(raster) + 1, it appends the band to the end of the raster. Otherwise, it replaces the existing band at the bandIndex.</p> <p>If the bandIndex and noDataValue is not given, a convenience implementation adds a new band with a null noDataValue.</p> <p>Adding a new band with a custom noDataValue requires bandIndex = RS_NumBands(raster) + 1 and non-null noDataValue to be explicitly specified.</p> <p>Modifying or Adding a customNoDataValue is also possible by giving an existing band in RS_AddBandFromArray</p> <p>In order to remove an existing noDataValue from an existing band, pass null as the noDataValue in the RS_AddBandFromArray.</p> <p>Note that: <code>bandIndex == RS_NumBands(raster) + 1</code> is an experimental feature and might lead to the loss of raster metadata and properties such as color models.</p> <p>Note</p> <p>RS_AddBandFromArray typecasts the double band values to the given datatype of the raster. This can lead to overflow values if values beyond the range of the raster's datatype are provided.</p> <p>SQL Example</p> <pre><code>SELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2)) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1) AS raster FROM raster_table\nSELECT RS_AddBandFromArray(raster, RS_MultiplyFactor(RS_BandAsArray(RS_FromGeoTiff(content), 1), 2), 1, -999) AS raster FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mapalgebra","title":"RS_MapAlgebra","text":"<p>Introduction: Apply a map algebra script on a raster.</p> <p>Format:</p> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String)\n</code></pre> <pre><code>RS_MapAlgebra (raster: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <pre><code>RS_MapAlgebra(rast0: Raster, rast1: Raster, pixelType: String, script: String, noDataValue: Double)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p><code>RS_MapAlgebra</code> runs a script on a raster. The script is written in a map algebra language called Jiffle. The script takes a raster as input and returns a raster of the same size as output. The script can be used to apply a map algebra expression on a raster. The input raster is named <code>rast</code> in the Jiffle script, and the output raster is named <code>out</code>.</p> <p>SQL Example</p> <p>Calculate the NDVI of a raster with 4 bands (R, G, B, NIR):</p> <pre><code>-- Assume that the input raster has 4 bands: R, G, B, NIR\n-- rast[0] refers to the R band, rast[3] refers to the NIR band.\nSELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') AS ndvi FROM raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n</code></pre> <p>Spark SQL Example for two raster input <code>RS_MapAlgebra</code>:</p> <pre><code>RS_MapAlgebra(rast0, rast1, 'D', 'out = rast0[0] * 0.5 + rast1[0] * 0.5;', null)\n</code></pre> <p>For more details and examples about <code>RS_MapAlgebra</code>, please refer to the Map Algebra documentation. To learn how to write map algebra script, please refer to Jiffle language summary.</p>"},{"location":"api/sql/Raster-operators/#map-algebra-operators","title":"Map Algebra Operators","text":"<p>Map algebra operators work on a single band of a raster. Each band is represented as an array of doubles. The operators return an array of doubles.</p>"},{"location":"api/sql/Raster-operators/#rs_add","title":"RS_Add","text":"<p>Introduction: Add two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Add (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val sumDF = spark.sql(\"select RS_Add(band1, band2) as sumOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_array","title":"RS_Array","text":"<p>Introduction: Create an array that is filled by the given value</p> <p>Format: <code>RS_Array(length: Integer, value: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Array(height * width, 0.0)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseand","title":"RS_BitwiseAND","text":"<p>Introduction: Find Bitwise AND between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseAND (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val biwiseandDF = spark.sql(\"select RS_BitwiseAND(band1, band2) as andvalue from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_bitwiseor","title":"RS_BitwiseOR","text":"<p>Introduction: Find Bitwise OR between two bands of Geotiff image</p> <p>Format: <code>RS_BitwiseOR (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val biwiseorDF = spark.sql(\"select RS_BitwiseOR(band1, band2) as or from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_countvalue","title":"RS_CountValue","text":"<p>Introduction: Returns count of a particular value from a spectral band in a raster image</p> <p>Format: <code>RS_CountValue (Band1: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val countDF = spark.sql(\"select RS_CountValue(band1, target) as count from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_divide","title":"RS_Divide","text":"<p>Introduction: Divide band1 with band2 from a geotiff image</p> <p>Format: <code>RS_Divide (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Divide(band1, band2) as divideBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_fetchregion","title":"RS_FetchRegion","text":"<p>Introduction: Fetch a subset of region from given Geotiff image based on minimumX, minimumY, maximumX and maximumY index as well original height and width of image</p> <p>Format:</p> <pre><code>RS_FetchRegion (Band: ARRAY[Double], coordinates: ARRAY[Integer], dimensions: ARRAY[Integer])\n</code></pre> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val region = spark.sql(\"select RS_FetchRegion(Band,Array(0, 0, 1, 2),Array(3, 3)) as Region from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthan","title":"RS_GreaterThan","text":"<p>Introduction: Mask all the values with 1 which are greater than a particular target value</p> <p>Format: <code>RS_GreaterThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val greaterDF = spark.sql(\"select RS_GreaterThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_greaterthanequal","title":"RS_GreaterThanEqual","text":"<p>Introduction: Mask all the values with 1 which are greater than equal to a particular target value</p> <p>Format: <code>RS_GreaterThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val greaterEqualDF = spark.sql(\"select RS_GreaterThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthan","title":"RS_LessThan","text":"<p>Introduction: Mask all the values with 1 which are less than a particular target value</p> <p>Format: <code>RS_LessThan (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val lessDF = spark.sql(\"select RS_LessThan(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_lessthanequal","title":"RS_LessThanEqual","text":"<p>Introduction: Mask all the values with 1 which are less than equal to a particular target value</p> <p>Format: <code>RS_LessThanEqual (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val lessEqualDF = spark.sql(\"select RS_LessThanEqual(band, target) as maskedvalues from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicaldifference","title":"RS_LogicalDifference","text":"<p>Introduction: Return value from band 1 if a value in band1 and band2 are different, else return 0</p> <p>Format: <code>RS_LogicalDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val logicalDifference = spark.sql(\"select RS_LogicalDifference(band1, band2) as logdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_logicalover","title":"RS_LogicalOver","text":"<p>Introduction: Return value from band1 if it's not equal to 0, else return band2 value</p> <p>Format: <code>RS_LogicalOver (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val logicalOver = spark.sql(\"select RS_LogicalOver(band1, band2) as logover from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mean","title":"RS_Mean","text":"<p>Introduction: Returns Mean value for a spectral band in a Geotiff image</p> <p>Format: <code>RS_Mean (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val meanDF = spark.sql(\"select RS_Mean(band) as mean from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_mode","title":"RS_Mode","text":"<p>Introduction: Returns Mode from a spectral band in a Geotiff image in form of an array</p> <p>Format: <code>RS_Mode (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val modeDF = spark.sql(\"select RS_Mode(band) as mode from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_modulo","title":"RS_Modulo","text":"<p>Introduction: Find modulo of pixels with respect to a particular value</p> <p>Format: <code>RS_Modulo (Band: ARRAY[Double], Target: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val moduloDF = spark.sql(\"select RS_Modulo(band, target) as modulo from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiply","title":"RS_Multiply","text":"<p>Introduction: Multiply two spectral bands in a Geotiff image</p> <p>Format: <code>RS_Multiply (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyDF = spark.sql(\"select RS_Multiply(band1, band2) as multiplyBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_multiplyfactor","title":"RS_MultiplyFactor","text":"<p>Introduction: Multiply a factor to a spectral band in a geotiff image</p> <p>Format: <code>RS_MultiplyFactor (Band1: ARRAY[Double], Factor: Double)</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val multiplyFactorDF = spark.sql(\"select RS_MultiplyFactor(band1, 2) as multiplyfactor from dataframe\")\n</code></pre> <p>This function only accepts integer as factor before <code>v1.5.0</code>.</p>"},{"location":"api/sql/Raster-operators/#rs_normalize","title":"RS_Normalize","text":"<p>Introduction: Normalize the value in the array to [0, 255]. Uniform arrays are set to 0 after normalization.</p> <p>Format: <code>RS_Normalize (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_Normalize(band)\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_normalizeddifference","title":"RS_NormalizedDifference","text":"<p>Introduction: Returns Normalized Difference between two bands(band2 and band1) in a Geotiff image(example: NDVI, NDBI)</p> <p>Format: <code>RS_NormalizedDifference (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val normalizedDF = spark.sql(\"select RS_NormalizedDifference(band1, band2) as normdifference from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_squareroot","title":"RS_SquareRoot","text":"<p>Introduction: Find Square root of band values in a geotiff image</p> <p>Format: <code>RS_SquareRoot (Band: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val rootDF = spark.sql(\"select RS_SquareRoot(band) as squareroot from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-operators/#rs_subtract","title":"RS_Subtract","text":"<p>Introduction: Subtract two spectral bands in a Geotiff image(band2 - band1)</p> <p>Format: <code>RS_Subtract (Band1: ARRAY[Double], Band2: ARRAY[Double])</code></p> <p>Since: <code>v1.1.0</code></p> <p>SQL Example</p> <pre><code>val subtractDF = spark.sql(\"select RS_Subtract(band1, band2) as differenceOfOfBands from dataframe\")\n</code></pre>"},{"location":"api/sql/Raster-visualizer/","title":"Raster visualization","text":"<p>Sedona offers some APIs to aid in easy visualization of a raster object.</p>"},{"location":"api/sql/Raster-visualizer/#image-based-visualization","title":"Image-based visualization","text":"<p>Sedona offers APIs to visualize a raster in an image form. This API only works for rasters with byte data, and bands &lt;= 4 (Grayscale - RGBA). You can check the data type of an existing raster by using RS_BandPixelType or create your own raster by passing 'B' while using RS_MakeEmptyRaster.</p>"},{"location":"api/sql/Raster-visualizer/#rs_asbase64","title":"RS_AsBase64","text":"<p>Introduction: Returns a base64 encoded string of the given raster. If the datatype is integral then this function internally takes the first 4 bands as RGBA, and converts them to the PNG format, finally produces a base64 string. When the datatype is not integral, the function converts the raster to TIFF format, and then generates a base64 string. To visualize other bands, please use it together with <code>RS_Band</code>. You can take the resulting base64 string in an online viewer to check how the image looks like.</p> <p>Warning</p> <p>This is not recommended for large files.</p> <p>Format:</p> <p><code>RS_AsBase64(raster: Raster, maxWidth: Integer)</code></p> <p><code>RS_AsBase64(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsBase64(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>iVBORw0KGgoAAAA...\n</code></pre>"},{"location":"api/sql/Raster-visualizer/#rs_asimage","title":"RS_AsImage","text":"<p>Introduction: Returns a HTML that when rendered using an HTML viewer or via a Jupyter Notebook, displays the raster as a square image of side length <code>imageWidth</code>. Optionally, an imageWidth parameter can be passed to RS_AsImage in order to increase the size of the rendered image (default: 200).</p> <p>Format: <code>RS_AsImage(raster: Raster, imageWidth: Integer = 200)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsImage(raster, 500) from rasters\nSELECT RS_AsImage(raster) from rasters\n</code></pre> <p>Output:</p> <pre><code>\"&lt;img src=\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAECAAAAABjWKqcAAAAIElEQVR42mPgPfGfkYUhhfcBNw+DT1KihS6DqLKztjcATWMFp9rkkJgAAAAASUVORK5CYII=\\\" width=\\\"200\\\" /&gt;\";\n</code></pre> <p>Tip</p> <p>RS_AsImage can be paired with SedonaUtils.display_image(df) wrapper inside a Jupyter notebook to directly print the raster as an image in the output, where the 'df' parameter is the dataframe containing the HTML data provided by RS_AsImage</p> <p>Example:</p> <pre><code>from sedona.raster_utils.SedonaUtils import SedonaUtils\n# Or from sedona.spark import *\n\ndf = sedona.read.format('binaryFile').load(DATA_DIR + 'raster.tiff').selectExpr(\"RS_FromGeoTiff(content) as raster\")\nhtmlDF = df.selectExpr(\"RS_AsImage(raster, 500) as raster_image\")\nSedonaUtils.display_image(htmlDF)\n</code></pre> <p></p>"},{"location":"api/sql/Raster-visualizer/#text-based-visualization","title":"Text-based visualization","text":""},{"location":"api/sql/Raster-visualizer/#rs_asmatrix","title":"RS_AsMatrix","text":"<p>Introduction: Returns a string, that when printed, outputs the raster band as a pretty printed 2D matrix. All the values of the raster are cast to double for the string. RS_AsMatrix allows specifying the number of digits to be considered after the decimal point. RS_AsMatrix expects a raster, and optionally a band (default: 1) and postDecimalPrecision (default: 6). The band parameter is 1-indexed.</p> <p>Note</p> <p>If the provided band is not present in the raster, RS_AsMatrix throws an IllegalArgumentException</p> <p>Note</p> <p>If the provided raster has integral values, postDecimalPrecision (if any) is simply ignored and integers are printed in the resultant string</p> <p>Note</p> <p>If you are using <code>show()</code> to display the output, it will show special characters as escape sequences. To get the expected behavior use the following code:</p> ScalaJavaPython <pre><code>println(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect().mkString(\"\\n\"))\n</code></pre> <pre><code>System.out.println(String.join(\"\\n\", df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <pre><code>print(\"\\n\".join(df.selectExpr(\"RS_AsMatrix(rast)\").sample(0.5).collect()))\n</code></pre> <p>The <code>sample()</code> function is only there to reduce the data sent to <code>collect()</code>, you may also use <code>filter()</code> if that's appropriate.</p> <p>Format:</p> <pre><code>RS_AsMatrix(raster: Raster, band: Integer = 1, postDecimalPrecision: Integer = 6)\n</code></pre> <p>Since: <code>1.5.0</code></p> <p>SQL Example</p> <pre><code>val inputDf = Seq(Seq(1, 3.333333, 4, 0.0001, 2.2222, 9, 10, 11.11111111, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'd', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output:</p> <pre><code>| 1.00000   3.33333   4.00000   0.00010|\n| 2.22220   9.00000  10.00000  11.11111|\n| 3.00000   4.00000   5.00000   6.00000|\n</code></pre> <p>SQL Example</p> <pre><code>val inputDf = Seq(Seq(1, 3, 4, 0, 2, 9, 10, 11, 3, 4, 5, 6)).toDF(\"band\")\nprint(inputDf.selectExpr(\"RS_AsMatrix(RS_AddBandFromArray(RS_MakeEmptyRaster(1, 'i', 4, 3, 0, 0, 1, -1, 0, 0, 0), band, 1, 0))\").sample(0.5).collect()(0))\n</code></pre> <p>Output:</p> <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre>"},{"location":"api/sql/Raster-writer/","title":"Raster writer","text":"<p>Note</p> <p>Sedona writers are available in Scala. Java and Python have the same APIs.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-raster-files","title":"Write Raster DataFrame to raster files","text":"<p>To write a Sedona Raster DataFrame to raster files, you need to (1) first convert the Raster DataFrame to a binary DataFrame using <code>RS_AsXXX</code> functions and (2) then write the binary DataFrame to raster files using Sedona's built-in <code>raster</code> data source.</p>"},{"location":"api/sql/Raster-writer/#write-raster-dataframe-to-a-binary-dataframe","title":"Write raster DataFrame to a binary DataFrame","text":"<p>You can use the following RS output functions (<code>RS_AsXXX</code>) to convert a Raster DataFrame to a binary DataFrame. Generally the output format of a raster can be different from the original input format. For example, you can use <code>RS_FromGeoTiff</code> to create rasters and save them using <code>RS_AsArcInfoAsciiGrid</code>.</p>"},{"location":"api/sql/Raster-writer/#rs_asarcgrid","title":"RS_AsArcGrid","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is an ArcGrid image in binary format. ArcGrid only takes 1 source band. If your raster has multiple bands, you need to specify which band you want to use as the source.</p> <p>Possible values for <code>sourceBand</code>: any non-negative value (&gt;=0). If not given, it will use Band 0.</p> <p>Format:</p> <p><code>RS_AsArcGrid(raster: Raster)</code></p> <p><code>RS_AsArcGrid(raster: Raster, sourceBand: Integer)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsArcGrid(raster) FROM my_raster_table\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsArcGrid(raster, 1) FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             arcgrid|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n |-- arcgrid: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_asgeotiff","title":"RS_AsGeoTiff","text":"<p>Introduction: Returns a binary DataFrame from a Raster DataFrame. Each raster object in the resulting DataFrame is a GeoTiff image in binary format.</p> <p>Possible values for <code>compressionType</code>: <code>None</code>, <code>PackBits</code>, <code>Deflate</code>, <code>Huffman</code>, <code>LZW</code> and <code>JPEG</code></p> <p>Possible values for <code>imageQuality</code>: any decimal number between 0 and 1. 0 means the lowest quality and 1 means the highest quality.</p> <p>Format:</p> <p><code>RS_AsGeoTiff(raster: Raster)</code></p> <p><code>RS_AsGeoTiff(raster: Raster, compressionType: String, imageQuality: Double)</code></p> <p>Since: <code>v1.4.1</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsGeoTiff(raster) FROM my_raster_table\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsGeoTiff(raster, 'LZW', '0.75') FROM my_raster_table\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|             geotiff|\n+--------------------+\n|[4D 4D 00 2A 00 0...|\n+--------------------+\n</code></pre> <p>Output schema:</p> <pre><code>root\n |-- geotiff: binary (nullable = true)\n</code></pre>"},{"location":"api/sql/Raster-writer/#rs_aspng","title":"RS_AsPNG","text":"<p>Introduction: Returns a PNG byte array, that can be written to raster files as PNGs using the sedona function. This function can only accept pixel data type of unsigned integer. PNG can accept 1 or 3 bands of data from the raster, refer to RS_Band for more details.</p> <p>Note</p> <p>Raster having <code>UNSIGNED_8BITS</code> pixel data type will have range of <code>0 - 255</code>, whereas rasters having <code>UNSIGNED_16BITS</code> pixel data type will have range of <code>0 - 65535</code>. If provided pixel value is greater than either <code>255</code> for <code>UNSIGNED_8BITS</code> or <code>65535</code> for <code>UNSIGNED_16BITS</code>, then the extra bit will be truncated.</p> <p>Note</p> <p>Raster that have float or double values will result in an empty byte array. PNG only accepts Integer values, if you want to write your raster to an image file, please refer to RS_AsGeoTiff.</p> <p>Format:</p> <p><code>RS_AsPNG(raster: Raster, maxWidth: Integer)</code></p> <p><code>RS_AsPNG(raster: Raster)</code></p> <p>Since: <code>v1.5.0</code></p> <p>SQL Example</p> <pre><code>SELECT RS_AsPNG(raster) FROM Rasters\n</code></pre> <p>Output:</p> <pre><code>[-119, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13, 73...]\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsPNG(RS_Band(raster, Array(3, 1, 2)))\n</code></pre> <p>Output:</p> <pre><code>[-103, 78, 94, -26, 61, -16, -91, -103, -65, -116...]\n</code></pre>"},{"location":"api/sql/Raster-writer/#write-a-binary-dataframe-to-raster-files","title":"Write a binary DataFrame to raster files","text":"<p>Introduction: You can write a Sedona binary DataFrame to external storage using Sedona's built-in <code>raster</code> data source. Note that: <code>raster</code> data source does not support reading rasters. Please use Spark built-in <code>binaryFile</code> and Sedona RS constructors together to read rasters.</p> <p>Since: <code>v1.4.1</code></p> <p>Available options:</p> <ul> <li>rasterField:<ul> <li>Default value: the <code>binary</code> type column in the DataFrame. If the input DataFrame has several binary columns, please specify which column you want to use. You can use one of the <code>RS_As*</code> functions mentioned above to convert the raster objects to binary raster file content to write.</li> <li>Allowed values: the name of the to-be-saved binary type column</li> </ul> </li> <li>fileExtension<ul> <li>Default value: <code>.tiff</code></li> <li>Allowed values: any string values such as <code>.png</code>, <code>.jpeg</code>, <code>.asc</code></li> </ul> </li> <li>pathField<ul> <li>No default value. If you use this option, then the column specified in this option must exist in the DataFrame schema. If this option is not used, each produced raster image will have a random UUID file name.</li> <li>Allowed values: any column name that indicates the paths of each raster file</li> </ul> </li> <li>useDirectCommitter (Since: <code>v1.6.1</code>)<ul> <li>Default value: <code>true</code>. If set to <code>true</code>, the output files will be written directly to the target location. If set to <code>false</code>, the output files will be written to a temporary location and finally be committed to their target location. It is usually slower to write large amount of raster files with <code>useDirectCommitter</code> set to <code>false</code>, especially when writing to object stores such as S3.</li> <li>Allowed values: <code>true</code> or <code>false</code></li> </ul> </li> </ul> <p>The schema of the Raster dataframe to be written can be one of the following two schemas:</p> <pre><code>root\n |-- raster_binary: binary (nullable = true)\n</code></pre> <p>or</p> <pre><code>root\n |-- raster_binary: binary (nullable = true)\n |-- path: string (nullable = true)\n</code></pre> <p>Spark SQL example 1:</p> <pre><code>// Assume that df contains a raster column named \"rast\"\ndf.withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(rast)\"))\\\n  .write.format(\"raster\").mode(\"overwrite\").save(\"my_raster_file\")\n</code></pre> <p>Spark SQL example 2:</p> <pre><code>// Assume that df contains a raster column named \"rast\" and a string column named \"path\"\ndf.withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(rast)\"))\\\n  .write.format(\"raster\")\\\n  .option(\"rasterField\", \"raster_binary\")\\\n  .option(\"pathField\", \"path\")\\\n  .option(\"fileExtension\", \".tiff\")\\\n  .mode(\"overwrite\")\\\n  .save(\"my_raster_file\")\n</code></pre> <p>The produced file structure will look like this:</p> <pre><code>my_raster_file\n- part-00000-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test1.tiff\n    - .test1.tiff.crc\n- part-00001-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test2.tiff\n    - .test2.tiff.crc\n- part-00002-6c7af016-c371-4564-886d-1690f3b27ca8-c000\n    - test3.tiff\n    - .test3.tiff.crc\n- _SUCCESS\n</code></pre> <p>To read it back to Sedona Raster DataFrame, you can use the following command (note the <code>*</code> in the path):</p> <pre><code>sparkSession.read.format(\"binaryFile\").load(\"my_raster_file/*\")\n</code></pre> <p>Then you can create Raster type in Sedona like this <code>RS_FromGeoTiff(content)</code> (if the written data was in GeoTiff format).</p> <p>The newly created DataFrame can be written to disk again but must be under a different name such as <code>my_raster_file_modified</code></p>"},{"location":"api/sql/Raster-writer/#write-geometry-to-raster-dataframe","title":"Write Geometry to Raster dataframe","text":""},{"location":"api/sql/Raster-writer/#rs_asraster","title":"RS_AsRaster","text":"<p>Introduction: Converts a Geometry to a Raster dataset. Defaults to using <code>1.0</code> for cell <code>value</code> and <code>null</code> for <code>noDataValue</code> if not provided. Supports all geometry types. The <code>pixelType</code> argument defines data type of the output raster. This can be one of the following, D (double), F (float), I (integer), S (short), US (unsigned short) or B (byte). The <code>useGeometryExtent</code> argument defines the extent of the resultant raster. When set to <code>true</code>, it corresponds to the extent of <code>geom</code>, and when set to false, it corresponds to the extent of <code>raster</code>. Default value is <code>true</code> if not set. Format:</p> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double, noDataValue: Double, useGeometryExtent: Boolean)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double, noDataValue: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String, value: Double)\n</code></pre> <pre><code>RS_AsRaster(geom: Geometry, raster: Raster, pixelType: String)\n</code></pre> <p>Since: <code>v1.5.0</code></p> <p>Note</p> <p>The function doesn't support rasters that have any one of the following properties: <pre><code>ScaleX &lt; 0\nScaleY &gt; 0\nSkewX != 0\nSkewY != 0\n</code></pre> If a raster is provided with anyone of these properties then IllegalArgumentException is thrown.</p> <p>For more information about ScaleX, ScaleY, SkewX, SkewY, please refer to the Affine Transformations section.</p> <p>SQL Example</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n        'D', 255.0, 0d\n    )\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <p>SQL Example</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, -215, 2, -2, 0, 0, 4326),\n        'D'\n    )\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((15 15, 18 20, 15 24, 24 25, 15 15))'),\n        RS_MakeEmptyRaster(2, 255, 255, 3, 215, 2, -2, 0, 0, 0),\n       'D',255, 0d, false\n)\n</code></pre> <p>Output:</p> <pre><code>GridCoverage2D[\"g...\n</code></pre>"},{"location":"api/sql/Reading-legacy-parquet/","title":"Reading Legacy Parquet Files","text":"<p>Due to a breaking change in Apache Sedona 1.4.0 to the SQL type of <code>GeometryUDT</code> (SEDONA-205) as well as the serialization format of geometry values (SEDONA-207), Parquet files containing geometry columns written by Apache Sedona 1.3.1 or earlier cannot be read by Apache Sedona 1.4.0 or later.</p> <p>For parquet files written by <code>\"parquet\"</code> format when using Apache Sedona 1.3.1-incubating or earlier:</p> <pre><code>df.write.format(\"parquet\").save(\"path/to/parquet/files\")\n</code></pre> <p>Reading such files with Apache Sedona 1.4.0 or later using <code>spark.read.format(\"parquet\").load(\"path/to/parquet/files\")</code> will result in an exception:</p> <pre><code>24/01/08 12:52:56 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 11)\norg.apache.spark.sql.AnalysisException: Invalid Spark read type: expected required group geom (LIST) {\n  repeated group list {\n    required int32 element (INTEGER(8,true));\n  }\n} to be list type but found Some(BinaryType)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:745)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertGroupField$3(ParquetSchemaConverter.scala:343)\n    at scala.Option.fold(Option.scala:251)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertGroupField(ParquetSchemaConverter.scala:324)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:188)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147)\n    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3$adapted(ParquetSchemaConverter.scala:117)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n    at scala.collection.immutable.Range.foreach(Range.scala:158)\n    at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n    ...\n</code></pre> <p>Since v1.5.1, GeoParquet supports reading legacy Parquet files. you can use <code>\"geoparquet\"</code> format with the <code>.option(\"legacyMode\", \"true\")</code> option. Here is an example:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").option(\"legacyMode\", \"true\").load(\"path/to/legacy-parquet-files\")\n</code></pre>"},{"location":"api/sql/Spider/","title":"Spider:Spatial Data Generator","text":"<p>Sedona offers a spatial data generator called Spider. It is a data source that generates random spatial data based on the user-specified parameters.</p>"},{"location":"api/sql/Spider/#quick-start","title":"Quick Start","text":"<p>Once you have your <code>SedonaContext</code> object created, you can create a DataFrame with the <code>spider</code> data source.</p> <pre><code>df_random_points = sedona.read.format(\"spider\").load(n=1000, distribution='uniform')\ndf_random_boxes = sedona.read.format(\"spider\").load(n=1000, distribution='gaussian', geometryType='box', maxWidth=0.05, maxHeight=0.05)\ndf_random_polygons = sedona.read.format(\"spider\").load(n=1000, distribution='bit', geometryType='polygon', minSegment=3, maxSegment=5, maxSize=0.1)\n</code></pre> <p>Now we have three DataFrames with random spatial data. We can show the first three rows of the <code>df_random_points</code> DataFrame to verify the data is generated correctly.</p> <pre><code>df_random_points.show(3, False)\n</code></pre> <p>Output:</p> <pre><code>+---+---------------------------------------------+\n|id |geometry                                     |\n+---+---------------------------------------------+\n|1  |POINT (0.8781393502074886 0.5925787985028703)|\n|2  |POINT (0.3159498147172185 0.1907316577342276)|\n|3  |POINT (0.2618294441170143 0.3623164670133922)|\n+---+---------------------------------------------+\nonly showing top 3 rows\n</code></pre> <p>The generated DataFrame has two columns: <code>id</code> and <code>geometry</code>. The <code>id</code> column is the unique identifier of each record, and the <code>geometry</code> column is the randomly generated spatial data.</p> <p>We can plot all 3 DataFrames using the following code.</p> <pre><code>import matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Convert DataFrames to GeoDataFrames\ngdf_random_points = gpd.GeoDataFrame(df_random_points.toPandas(), geometry='geometry')\ngdf_random_boxes = gpd.GeoDataFrame(df_random_boxes.toPandas(), geometry='geometry')\ngdf_random_polygons = gpd.GeoDataFrame(df_random_polygons.toPandas(), geometry='geometry')\n\n# Create a figure and a set of subplots\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot each GeoDataFrame on a different subplot\ngdf_random_points.plot(ax=axes[0], color='blue', markersize=5)\naxes[0].set_title('Random Points')\n\ngdf_random_boxes.boundary.plot(ax=axes[1], color='red')\naxes[1].set_title('Random Boxes')\n\ngdf_random_polygons.boundary.plot(ax=axes[2], color='green')\naxes[2].set_title('Random Polygons')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>You can browse the SpiderWeb website to play with the parameters and see how they affect the generated data. Once you are satisfied with the parameters, you can use them in your Spider DataFrame creation code. The following sections will explain the parameters in detail.</p>"},{"location":"api/sql/Spider/#common-parameters","title":"Common Parameters","text":"<p>The following parameters are common to all distributions.</p> Parameter Description Default Value n Number of records to generate 100 distribution Distribution type. See Distributions for details. <code>uniform</code> numPartitions Number of partitions to generate The default parallelism of your Spark Context seed Random seed Current timestamp in milliseconds <p>Warning</p> <p>The same <code>seed</code> parameter may produce different results with different Java versions or Sedona versions.</p>"},{"location":"api/sql/Spider/#distributions","title":"Distributions","text":"<p>Spider supports generating random points, boxes and polygons under various distributions. You can explore the capabilities of Spider by visiting the SpiderWeb website. You can specify the distribution type using the <code>distribution</code> parameter. The parameters for each distribution are listed below.</p>"},{"location":"api/sql/Spider/#uniform-distribution","title":"Uniform Distribution","text":"<p>The uniform distribution generates random geometries in the unit square <code>[0, 1] x [0, 1]</code>. This distribution can be selected by setting the <code>distribution</code> parameter to <code>uniform</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=300, distribution='uniform', geometryType='box', maxWidth=0.05, maxHeight=0.05)\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').boundary.plot()\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>The Gaussian distribution generates random geometries in a Gaussian distribution with mean <code>[0.5, 0.5]</code> and standard deviation <code>[0.1, 0.1]</code>. This distribution can be selected by setting the <code>distribution</code> parameter to <code>gaussian</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=300, distribution='gaussian', geometryType='polygon', maxSize=0.05)\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').boundary.plot()\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#bit-distribution","title":"Bit Distribution","text":"<p>The bit distribution generates random geometries in a bit distribution. This distribution can be selected by setting the <code>distribution</code> parameter to <code>bit</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> probability Probability of setting a bit 0.2 digits Number of digits in the generated data 10 maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=300, distribution='bit', geometryType='point', probability=0.2, digits=10)\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#diagonal-distribution","title":"Diagonal Distribution","text":"<p>The diagonal distribution generates random geometries on the diagonal line <code>y = x</code> with some dispersion for geometries that are not exactly on the diagonal. This distribution can be selected by setting the <code>distribution</code> parameter to <code>diagonal</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> percentage The percentage of records that are perfectly on the diagonal 0.5 buffer For points not exactly on the diagonal, the buffer in which they are dispersed 0.5 maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=300, distribution='diagonal', geometryType='point', percentage=0.5, buffer=0.5)\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#sierpinski-distribution","title":"Sierpinski Distribution","text":"<p>The Sierpinski distribution generates random geometries distributed on a Sierpinski triangle. This distribution can be selected by setting the <code>distribution</code> parameter to <code>sierpinski</code>.</p> Parameter Description Default Value geometryType Geometry type, either <code>point</code>, <code>box</code> or <code>polygon</code> <code>point</code> maxWidth Maximum width of the generated boxes 0.01 maxHeight Maximum height of the generated boxes 0.01 minSegment Minimum number of segments of the generated polygons 3 maxSegment Maximum number of segments of the generated polygons 3 maxSize Maximum size of the generated polygons 0.01 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=2000, distribution='sierpinski', geometryType='point')\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').plot(markersize=1)\n</code></pre> <p></p>"},{"location":"api/sql/Spider/#parcel-distribution","title":"Parcel Distribution","text":"<p>This generator produces boxes that resemble parcel areas. It works by recursively splitting the input domain (unit square) along the longest dimension and then randomly dithering each generated box to add some randomness. This generator can only generate boxes. This distribution can be selected by setting the <code>distribution</code> parameter to <code>parcel</code>.</p> Parameter Description Default Value dither The amount of dithering as a ratio of the side length. Allowed range [0, 1] 0.5 splitRange The allowed range for splitting boxes. Allowed range [0.0, 0.5] 0.0 means all values are allowed. 0.5 means always split in half. 0.5 <p>Example:</p> <pre><code>import geopandas as gpd\ndf = sedona.read.format(\"spider\").load(n=300, distribution='parcel', dither=0.5, splitRange=0.5)\ngpd.GeoDataFrame(df.toPandas(), geometry='geometry').boundary.plot()\n</code></pre> <p></p> <p>Note</p> <p>The number of partitions generated by the <code>parcel</code> distribution is always power of 4. This is for guaranteeing the quality of the generated data. If the specified <code>numPartitions</code> is not a power of 4, it will be automatically adjusted to the nearest power of 4 smaller or equal to the specified value.</p>"},{"location":"api/sql/Spider/#affine-transformation","title":"Affine Transformation","text":"<p>The random spatial data generated by Spider are mostly in the unit square <code>[0, 1] x [0, 1]</code>. If you need to generate random spatial data in a different region, you can specify affine transformation parameters to scale and translate the data to the target region.</p> <p>The following code demonstrates how to generate random spatial data in a different region using affine transformation.</p> <p>The affine transformation parameters are:</p> Parameter Description Default Value translateX Translate the data horizontally 0 translateY Translate the data vertically 0 scaleX Scale the data horizontally 1 scaleY Scale the data vertically 1 skewX Skew the data horizontally 0 skewY Skew the data vertically 0 <p>The affine transformation is applied to the generated data as follows:</p> <pre><code>x' = translateX + scaleX * x + skewX * y\ny' = translateY + skewY * x + scaleY * y\n</code></pre> <p>Example:</p> <pre><code>import geopandas as gpd\ndf_random_points = sedona.read.format(\"spider\").load(n=1000, distribution='uniform', translateX=0.5, translateY=0.5, scaleX=2, scaleY=2)\ngpd.GeoDataFrame(df_random_points.toPandas(), geometry='geometry').plot(markersize=1)\n</code></pre> <p>The data is now in the region <code>[0.5, 2.5] x [0.5, 2.5]</code>.</p> <p></p>"},{"location":"api/sql/Spider/#references","title":"References","text":"<ul> <li>Puloma Katiyar, Tin Vu, Sara Migliorini, Alberto Belussi, Ahmed Eldawy. \"SpiderWeb: A Spatial Data Generator on the Web\", ACM SIGSPATIAL 2020, Seattle, WA</li> <li>Beast Spatial Data Generator: https://bitbucket.org/bdlabucr/beast/src/master/doc/spatial-data-generator.md</li> <li>SpiderWeb: A Spatial Data Generator on the Web: https://spider.cs.ucr.edu/</li> <li>SpiderWeb YouTube Video: https://www.youtube.com/watch?v=h0xCG6Swdqw</li> </ul>"},{"location":"api/sql/Visualization_SedonaKepler/","title":"SedonaKepler","text":"<p>SedonaKepler offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaKepler, simply import Sedona using:</p> <pre><code>from sedona.spark import *\n</code></pre> <p>Alternatively it can also be imported using:</p> <pre><code>from sedona.maps.SedonaKepler import SedonaKepler\n</code></pre> <p>Following are details on all the APIs exposed via SedonaKepler:</p>"},{"location":"api/sql/Visualization_SedonaKepler/#creating-a-map-object-using-sedonakeplercreate_map","title":"Creating a map object using SedonaKepler.create_map","text":"<p>SedonaKepler exposes a create_map API with the following signature:</p> <pre><code>create_map(df: SedonaDataFrame=None, name: str='unnamed', config: dict=None) -&gt; map\n</code></pre> <p>The parameter 'name' is used to associate the passed SedonaDataFrame in the map object and any config applied to the map is linked to this name. It is recommended you pass a unique identifier to the dataframe here.</p> <p>If no SedonaDataFrame object is passed, an empty map (with config applied if passed) is returned. A SedonaDataFrame can be added later using the method <code>add_df</code></p> <p>A map config can be passed optionally to apply pre-apply customizations to the map.</p> <p>Note</p> <p>The map config references every customization with the name assigned to the SedonaDataFrame being displayed, if there is a mismatch in the name, the config will not be applied to the map object.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization_SedonaKepler/#adding-sedonadataframe-to-a-map-object-using-sedonakepleradd_df","title":"Adding SedonaDataFrame to a map object using SedonaKepler.add_df","text":"<p>SedonaKepler exposes an add_df API with the following signature:</p> <pre><code>add_df(map, df: SedonaDataFrame, name: str='unnamed')\n</code></pre> <p>This API can be used to add a SedonaDataFrame to an already created map object. The map object passed is directly mutated and nothing is returned.</p> <p>The parameters name has the same conditions as 'create_map'</p> <p>Tip</p> <p>This method can be used to add multiple dataframes to a map object to be able to visualize them together.</p> <p>Example usage (Referenced from Sedona Jupyter examples)</p> Python <pre><code>map = SedonaKepler.create_map()\nSedonaKepler.add_df(map, groupedresult, name=\"AirportCount\")\nmap\n</code></pre>"},{"location":"api/sql/Visualization_SedonaKepler/#setting-a-config-via-the-map","title":"Setting a config via the map","text":"<p>A map rendered by accessing the map object created by SedonaKepler includes a config panel which can be used to customize the map</p>"},{"location":"api/sql/Visualization_SedonaKepler/#saving-and-setting-config","title":"Saving and setting config","text":"<p>A map object's current config can be accessed by accessing its 'config' attribute like <code>map.config</code>. This config can be saved for future use or use across notebooks if the exact same map is to be rendered every time.</p> <p>Note</p> <p>The map config references each applied customization with the name given to the dataframe and hence will work only on maps with the same name of dataframe supplied. For more details refer to keplerGl documentation here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/","title":"SedonaPyDeck","text":"<p>SedonaPyDeck offers a number of APIs which aid in quick and interactive visualization of a geospatial data in a Jupyter notebook/lab environment.</p> <p>Inorder to start using SedonaPyDeck, simply import Sedona using:</p> <pre><code>from sedona.spark import *\n</code></pre> <p>Alternatively it can also be imported using:</p> <pre><code>from sedona.maps.SedonaPyDeck import SedonaPyDeck\n</code></pre> <p>Note</p> <p>For more information on the optional parameters please visit PyDeck docs.</p> <p>SedonaPyDeck assumes the map provider to be Mapbox when user selects 'salellite' option for <code>map_style</code>.</p> <p>Following are details on all the APIs exposed via SedonaPyDeck:</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#geometry-map","title":"Geometry Map","text":"<pre><code>def create_geometry_map(df, fill_color=\"[85, 183, 177, 255]\", line_color=\"[85, 183, 177, 255]\",\n                   elevation_col=0, initial_view_state=None,\n                   map_style=None, map_provider=None, api_keys=None, stroked=True):\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color polygons or point geometries in the map</p> <p>The parameter <code>line_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column, and is used to color the line geometries in the map.</p> <p>The parameter <code>elevation_col</code> can be given a static elevation or elevation based on column values like <code>fill_color</code>, this only works for the polygon geometries in the map.</p> <p>The parameter <code>stroked</code> determines whether to draw an outline around polygons and points, accepts a boolean value. For more information, please refer to this documentation of deck.gl.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#choropleth-map","title":"Choropleth Map","text":"<pre><code>def create_choropleth_map(df, fill_color=None, plot_col=None, initial_view_state=None, map_style=None,\n                          map_provider=None, api_keys=None, elevation_col=0, stroked=True)\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>The parameter <code>stroked</code> determines whether to draw an outline around polygons and points, accepts a boolean value. For more information please refer to this documentation of deck.gl.</p> <p>For example, all these are valid values of fill_color:</p> <pre><code>fill_color=[255, 12, 250]\nfill_color=[0, 12, 250, 255]\nfill_color='[0, 12, 240, AirportCount * 10]' ## AirportCount is a column in the passed df\n</code></pre> <p>Instead of giving a <code>fill_color</code> parameter, a 'plot_col' can be passed which specifies the column to decide the choropleth. SedonaPyDeck then creates a default color scheme based on the values of the column passed.</p> <p>The parameter <code>elevation_col</code> can be given a numeric or a string value (containing the column with/without operations on it) to set a 3D elevation to the plotted polygons if any.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website.</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#scatterplot","title":"Scatterplot","text":"<pre><code>def create_scatterplot_map(df, fill_color=\"[255, 140, 0]\", radius_col=1, radius_min_pixels = 1, radius_max_pixels = 10, radius_scale=1, initial_view_state=None,\n                           map_style=None, map_provider=None, api_keys=None)\n</code></pre> <p>The parameter <code>fill_color</code> can be given a list of RGB/RGBA values, or a string that contains RGB/RGBA values based on a column.</p> <p>The parameter <code>radius_col</code> can be given a numeric value or a string value consisting of any operations on the column, in order to specify the radius of the plotted point.</p> <p>The parameter <code>radius_min_pixels</code> can be given a numeric value that would set the minimum radius in pixels. This can be used to prevent the plotted circle from getting too small when zoomed out.</p> <p>The parameter <code>radius_max_pixels</code> can be given a numeric value that would set the maximum radius in pixels. This can be used to prevent the circle from getting too big when zoomed in.</p> <p>The parameter <code>radius_scale</code> can be given a numeric value that sets a global radius multiplier for all points.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/sql/Visualization_SedonaPyDeck/#heatmap","title":"Heatmap","text":"<pre><code>def create_heatmap(df, color_range=None, weight=1, aggregation=\"SUM\", initial_view_state=None, map_style=None,\n                map_provider=None, api_keys=None)\n</code></pre> <p>The parameter <code>color_range</code> can be optionally given a list of RGB values, SedonaPyDeck by default uses <code>6-class YlOrRd</code> as color_range. More examples can be found on colorbrewer</p> <p>The parameter <code>weight</code> can be given a numeric value or a string with column and operations on it to determine weight of each point while plotting a heatmap. By default, SedonaPyDeck assigns a weight of 1 to each point</p> <p>The parameter <code>aggregation</code> can be used to define aggregation strategy to use when aggregating heatmap to a lower resolution (zooming out). One of \"MEAN\" or \"SUM\" can be provided. By default, SedonaPyDeck uses \"MEAN\" as the aggregation strategy.</p> <p>Optionally, parameters <code>initial_view_state</code>, <code>map_style</code>, <code>map_provider</code>, <code>api_keys</code> can be passed to configure the map as per user's liking. More details on the parameters and their default values can be found on the PyDeck website as well by deck.gl here</p>"},{"location":"api/stats/sql/","title":"DataFrame","text":""},{"location":"api/stats/sql/#overview","title":"Overview","text":"<p>Sedona's stats module provides Scala and Python functions for conducting geospatial statistical analysis on dataframes with spatial columns. The stats module is built on top of the core module and provides a set of functions that can be used to perform spatial analysis on these dataframes. The stats module is designed to be used with the core module and the viz module to provide a complete set of geospatial analysis tools.</p>"},{"location":"api/stats/sql/#using-dbscan","title":"Using DBSCAN","text":"<p>The DBSCAN function is provided at <code>org.apache.sedona.stats.clustering.DBSCAN.dbscan</code> in scala/java and <code>sedona.stats.clustering.dbscan.dbscan</code> in python.</p> <p>The function annotates a dataframe with a cluster label for each data record using the DBSCAN algorithm. The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and none are named 'geometry', the column name must be provided. The new column will be named 'cluster'.</p>"},{"location":"api/stats/sql/#parameters","title":"Parameters","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - dataframe to cluster. Must contain at least one GeometryType column</li> <li>epsilon - minimum distance parameter of DBSCAN algorithm</li> <li>minPts (min_pts) - minimum number of points parameter of DBSCAN algorithm</li> <li>geometry - name of the geometry column</li> <li>includeOutliers (include_outliers) - whether to include outliers in the output. Default is false</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>The output is the input DataFrame with the cluster label added to each row. Outlier will have a cluster value of -1 if included.</p>"},{"location":"api/stats/sql/#using-local-outlier-factor-lof","title":"Using Local Outlier Factor (LOF)","text":"<p>The LOF function is provided at <code>org.apache.sedona.stats.outlierDetection.LocalOutlierFactor.localOutlierFactor</code> in scala/java and <code>sedona.stats.outlier_detection.local_outlier_factor.local_outlier_factor</code> in python.</p> <p>The function annotates a dataframe with a column containing the local outlier factor for each data record. The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and neither is named 'geometry', the column name must be provided.</p>"},{"location":"api/stats/sql/#parameters_1","title":"Parameters","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - dataframe containing the point geometries</li> <li>k - number of nearest neighbors that will be considered for the LOF calculation</li> <li>geometry - name of the geometry column</li> <li>handleTies (handle_ties) - whether to handle ties in the k-distance calculation. Default is false</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>The output is the input DataFrame with the lof added to each row.</p>"},{"location":"api/stats/sql/#using-getis-ord-gi","title":"Using Getis-Ord Gi(*)","text":"<p>The G Local function is provided at <code>org.apache.sedona.stats.hotspotDetection.GetisOrd.gLocal</code> in scala/java and <code>sedona.stats.hotspot_detection.getis_ord.g_local</code> in python.</p> <p>Performs the Gi or Gi* statistic on the x column of the dataframe.</p> <p>Weights should be the neighbors of this row. The members of the weights should be comprised of structs containing a value column and a neighbor column. The neighbor column should be the contents of the neighbors with the same types as the parent row (minus neighbors). Reference the Using the Distance Weighting Function header for instructions on generating this column. To calculate the Gi* statistic, ensure the focal observation is in the neighbors array (i.e. the row is in the weights column) and <code>star=true</code>. Significance is calculated with a z score.</p>"},{"location":"api/stats/sql/#parameters_2","title":"Parameters","text":"<ul> <li>dataframe - the dataframe to perform the G statistic on</li> <li>x - The column name we want to perform hotspot analysis on</li> <li>weights - The column name containing the neighbors array. The neighbor column should be the contents of the neighbors with the same types as the parent row (minus neighbors). You can use <code>Weighting</code> class functions to achieve this.</li> <li>star - Whether the focal observation is in the neighbors array. If true this calculates Gi*, otherwise Gi</li> </ul> <p>The output is the input DataFrame with the following columns added: G, E[G], V[G], Z, P.</p>"},{"location":"api/stats/sql/#using-the-distance-weighting-function","title":"Using the Distance Weighting Function","text":"<p>The Weighting functions are provided at <code>org.apache.sedona.stats.Weighting</code> in scala/java and <code>sedona.stats.weighting</code> in python.</p> <p>The function generates a column containing an array of structs containing a value column and a neighbor column.</p> <p>The generic <code>addDistanceBandColumn</code> (<code>add_distance_band_column</code> in python) function annotates a dataframe with a weights column containing the other records within the threshold and their weight.</p> <p>The dataframe should contain at least one <code>GeometryType</code> column. Rows must be unique. If one geometry column is present it will be used automatically. If two are present, the one named 'geometry' will be used. If more than one are present and neither is named 'geometry', the column name must be provided. The new column will be named 'cluster'.</p>"},{"location":"api/stats/sql/#parameters_3","title":"Parameters","text":""},{"location":"api/stats/sql/#adddistancebandcolumn","title":"addDistanceBandColumn","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - DataFrame with geometry column</li> <li>threshold - Distance threshold for considering neighbors</li> <li>binary - whether to use binary weights or inverse distance weights for neighbors (dist^alpha)</li> <li>alpha - alpha to use for inverse distance weights ignored when binary is true</li> <li>includeZeroDistanceNeighbors (include_zero_distance_neighbors) - whether to include neighbors that are 0 distance. If 0 distance neighbors are included and binary is false, values are infinity as per the floating point spec (divide by 0)</li> <li>includeSelf (include_self) - whether to include self in the list of neighbors</li> <li>selfWeight (self_weight) - the value to use for the self weight</li> <li>geometry - name of the geometry column</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul>"},{"location":"api/stats/sql/#addbinarydistancebandcolumn","title":"addBinaryDistanceBandColumn","text":"<p>names in parentheses are python variable names</p> <ul> <li>dataframe - DataFrame with geometry column</li> <li>threshold - Distance threshold for considering neighbors</li> <li>includeZeroDistanceNeighbors (include_zero_distance_neighbors) - whether to include neighbors that are 0 distance. If 0 distance neighbors are included and binary is false, values are infinity as per the floating point spec (divide by 0)</li> <li>includeSelf (include_self) - whether to include self in the list of neighbors</li> <li>selfWeight (self_weight) - the value to use for the self weight</li> <li>geometry - name of the geometry column</li> <li>useSpheroid (use_spheroid) - whether to use a cartesian or spheroidal distance calculation. Default is false</li> </ul> <p>In both cases the output is the input DataFrame with the weights column added to each row.</p>"},{"location":"api/viz/java-api/","title":"RDD","text":"<p>Please read Javadoc</p> <p>Note: Scala can call Java APIs seamlessly. That means Scala users use the same APIs with Java users</p>"},{"location":"api/viz/sql/","title":"DataFrame/SQL","text":""},{"location":"api/viz/sql/#quick-start","title":"Quick start","text":"<p>The detailed explanation is here: Visualize Spatial DataFrame/RDD.</p> <ol> <li>Add Sedona-core, Sedona-SQL, Sedona-Viz into your project pom.xml or build.sbt</li> <li>Declare your Spark Session</li> </ol> <pre><code>sparkSession = SparkSession.builder().\nconfig(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\").\nconfig(\"spark.kryo.registrator\", \"org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\").\nmaster(\"local[*]\").appName(\"mySedonaVizDemo\").getOrCreate()\n</code></pre> <ol> <li>Add the following lines after your SparkSession declaration:</li> </ol> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre>"},{"location":"api/viz/sql/#regular-functions","title":"Regular functions","text":""},{"location":"api/viz/sql/#st_colorize","title":"ST_Colorize","text":"<p>Introduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.</p> <p>Note</p> <p>The color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.</p> <p>Format:</p> <pre><code>ST_Colorize (weight: Double, maxWeight: Double, mandatory color: String (Optional))\n</code></pre> <p>Since: <code>v1.0.0</code></p>"},{"location":"api/viz/sql/#produce-various-colors-heat-map","title":"Produce various colors - heat map","text":"<p>This function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.</p> <p>SQL Example</p> <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999) AS color\nFROM pixels\n</code></pre>"},{"location":"api/viz/sql/#produce-uniform-colors-scatter-plot","title":"Produce uniform colors - scatter plot","text":"<p>If a mandatory color name is put as the third input argument, this function will directly output this color, without considering the weights. In this case, every pixel will possess the same color.</p> <p>SQL Example</p> <pre><code>SELECT pixels.px, ST_Colorize(pixels.weight, 999, 'red') AS color\nFROM pixels\n</code></pre> <p>Here are some example color names can be entered:</p> <pre><code>\"firebrick\"\n\"#aa38e0\"\n\"0x40A8CC\"\n\"rgba(112,36,228,0.9)\"\n</code></pre> <p>Please refer to AWT Colors for a list of pre-defined colors.</p>"},{"location":"api/viz/sql/#st_encodeimage","title":"ST_EncodeImage","text":"<p>Introduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.</p> <p>Format: <code>ST_EncodeImage (A: Image)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_EncodeImage(images.img)\nFROM images\n</code></pre>"},{"location":"api/viz/sql/#st_pixelize","title":"ST_Pixelize","text":"<p>Introduction: Convert a geometry to an array of pixels given a resolution</p> <p>You should use it together with <code>Lateral View</code> and <code>Explode</code></p> <p>Format:</p> <pre><code>ST_Pixelize (A: Geometry, ResolutionX: Integer, ResolutionY: Integer, Boundary: Geometry)\n</code></pre> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_Pixelize(shape, 256, 256, (ST_Envelope_Aggr(shape) FROM pointtable))\nFROM polygondf\n</code></pre>"},{"location":"api/viz/sql/#st_tilename","title":"ST_TileName","text":"<p>Introduction: Return the map tile name for a given zoom level. Please refer to OpenStreetMap ZoomLevel and OpenStreetMap tile name.</p> <p>Note</p> <p>Tile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.</p> <p>Format: <code>ST_TileName (A: Pixel, ZoomLevel: Integer)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT ST_TileName(pixels.px, 3)\nFROM pixels\n</code></pre>"},{"location":"api/viz/sql/#aggregate-functions","title":"Aggregate functions","text":""},{"location":"api/viz/sql/#st_render","title":"ST_Render","text":"<p>Introduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage. The 3<sup>rd</sup> parameter is optional and it is the zoom level. You should use zoom level when you want to render tiles, instead of a single image.</p> <p>Format: <code>ST_Render (A: Pixel, B: Color, C: Integer - optional zoom level)</code></p> <p>Since: <code>v1.0.0</code></p> <p>SQL Example</p> <pre><code>SELECT tilename, ST_Render(pixels.px, pixels.color) AS tileimg\nFROM pixels\nGROUP BY tilename\n</code></pre>"},{"location":"asf/asf/","title":"Foundation","text":""},{"location":"asf/asf/#copyright","title":"Copyright","text":"<p>Apache Sedona, Sedona, Apache, the Apache feather logo, and the Apache Sedona project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries. All other marks mentioned may be trademarks or registered trademarks of their respective owners. Please visit Apache Software Foundation for more details.</p>"},{"location":"asf/telemetry/","title":"Telemetry","text":"<p>Apache Sedona uses Scarf to collect anonymous usage data to help us understand how the software is being used and how we can improve it. You can opt out of telemetry collection by setting the environment variable <code>SCARF_NO_ANALYTICS</code> or <code>DO_NOT_TRACK</code> to <code>true</code> on your local machine, or the driver machine of your cluster.</p> <p>Scarf fully supports the GDPR and is allowed by the Apache Software Foundation privacy policy. The privacy policy of Scarf is available at https://about.scarf.sh/privacy-policy.</p>"},{"location":"community/contact/","title":"Community","text":""},{"location":"community/contact/#community","title":"Community","text":"<p>Every volunteer project obtains its strength from the people involved in it. We invite you to participate as much or as little as you choose.</p> <p>You can participate in the community as follows:</p> <ul> <li>Use our project and provide a feedback.</li> <li>Provide us with the use-cases.</li> <li>Report bugs and submit patches.</li> <li>Contribute code and documentation.</li> </ul>"},{"location":"community/contact/#community-events","title":"Community events","text":"<p>Everyone is welcome to join our community events. We have a community office hour every 4 weeks. Please import the event to your Google Calendar.</p>"},{"location":"community/contact/#twitter","title":"Twitter","text":"<p>Apache Sedona@Twitter</p>"},{"location":"community/contact/#discord-server","title":"Discord Server","text":""},{"location":"community/contact/#mailing-list","title":"Mailing list","text":"<p>Get help using Sedona or contribute to the project on our mailing lists</p> <p>Sedona Mailing Lists: dev@sedona.apache.org: project development and general questions</p> <ul> <li>Please first subscribe and then post emails. To subscribe, please send an email (leave the subject and content blank) to dev-subscribe@sedona.apache.org</li> </ul>"},{"location":"community/contact/#issue-tracker","title":"Issue tracker","text":""},{"location":"community/contact/#bug-reports","title":"Bug Reports","text":"<p>Found bug? Enter an issue in the Sedona JIRA</p> <p>Before submitting an issue, please:</p> <ul> <li>Verify that the bug does in fact exist.</li> <li>Search the issue tracker to verify there is no existing issue reporting the bug you\u2019ve found.</li> <li>Consider tracking down the bug yourself in the Sedona\u2019s source and submitting a patch along with your bug report. This is a great time saver for the Sedona developers and helps ensure the bug will be fixed quickly.</li> </ul>"},{"location":"community/contact/#feature-requests","title":"Feature Requests","text":"<p>Enhancement requests for new features are also welcome. The more concrete and rationale the request is, the greater the chance it will be incorporated into future releases.</p> <p>Enter an issue in the Sedona JIRA or email to dev@sedona.apache.org</p>"},{"location":"community/contributor/","title":"Project Management Committee","text":"<p>Sedona has received numerous help from the community. This page lists the committers and project management committee of Apache Sedona. People on this page are ordered by their last name.</p>"},{"location":"community/contributor/#committers","title":"Committers","text":"<p>A contributor who contributes enough code to Sedona will be promoted to a committer. A committer has the write access to Sedona main repository</p>"},{"location":"community/contributor/#project-management-committee-pmc","title":"Project Management Committee (PMC)","text":"<p>A committer will be promoted to a PMC member when the community thinks he/she is able to be in charge at least a major component of this project.</p> <p>Current Sedona PMC members are as follows:</p> Name GitHub ID Apache ID Adam Binford Kimahriman kimahriman@apache.org Kanchan Chowdhury kanchanchy kanchanchy@apache.org Kristin Cowalcijk Kontinuation kontinuation@apache.org Furqaan Khan furqaankhan furqaan@apache.org Pawe\u0142 Koci\u0144ski Imbruced imbruced@apache.org Yitao Li yitao-li yitaoli@apache.org Netanel Malka netanel246 malka@apache.org Mohamed Sarwat Sarwat mosarwat@apache.org Kengo Seki sekikn sekikn@apache.org Sachio Wakai SW186000 swakai@apache.org Jinxuan Wu jinxuan jinxuanw@apache.org Jia Yu jiayuasu jiayu@apache.org Zongsi Zhang zongsizhang zongsizhang@apache.org Felix Cheung felixcheung@apache.org Von Gosling vongosling@apache.org Sunil Govindan sunilg@apache.org Jean-Baptiste Onofr\u00e9 jbonofre@apache.org George Percivall percivall@apache.org"},{"location":"community/contributor/#become-a-committer","title":"Become a committer","text":"<p>To get started contributing to Sedona, learn how to contribute \u2013 anyone can submit patches, documentation and examples to the project.</p> <p>The PMC regularly adds new committers from the active contributors, based on their contributions to Sedona. The qualifications for new committers include:</p> <ul> <li>Sustained contributions to Sedona: Committers should have a history of major contributions to Sedona.</li> <li>Quality of contributions: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches.</li> <li>Community involvement: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev mailing list &amp; Discord, and help mentor newer contributors and users.</li> </ul> <p>The PMC also adds new PMC members. PMC members are expected to carry out PMC responsibilities as described in Apache Guidance, including helping vote on releases, enforce Apache project trademarks, take responsibility for legal and license issues, and ensure the project follows Apache project mechanics. The PMC periodically adds committers to the PMC who have shown they understand and can help with these activities.</p> <p>Current Sedona Committers are as follows:</p> Name GitHub ID Apache ID John Bampton jbampton johnbam@apache.org Nilesh Gajwani iGN5117 nilesh@apache.org"},{"location":"community/contributor/#nominate-a-committer-or-pmc-member","title":"Nominate a committer or PMC member","text":"<p>Steps are as follows:</p> <ol> <li>Call a vote (templates/committerVote.txt)</li> <li>Close the vote. If the result is positive, invite the new committer.</li> </ol>"},{"location":"community/contributor/#call-for-a-vote","title":"Call for a vote","text":"<p>We do the vote and discussion on the private@sedona.apache.org to enable a frank discussion.</p> <p>Let the Vote thread run for one week. A positive result is achieved by Consensus Approval: at least 3 +1 votes and no vetoes.</p>"},{"location":"community/contributor/#pmc-vote-template","title":"PMC vote template","text":"<p>This is the email to commence a vote for a new PMC candidate. New PMC members need to be voted for by the existing PMC members and subsequently approved by the Board.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE] New PMC candidate: [New PMC NAME]\n\n[ add the reasons behind your nomination here ]\n\nVoting ends one week from today, or until at least 3 +1 votes are cast.\n</code></pre>"},{"location":"community/contributor/#close-a-vote","title":"Close a vote","text":"<p>This email ends the vote and reports the result to the project.</p> <pre><code>To: private@sedona.apache.org\nSubject: [VOTE][RESULT] New PMC candidate: [New PMC NAME]\n\nThe vote has now closed: [paste the vote thread on https://lists.apache.org/list.html?private@sedona.apache.org]. The results are:\n\nBinding Votes:\n\n+1 [TOTAL BINDING +1 VOTES]\n 0 [TOTAL BINDING +0/-0 VOTES]\n-1 [TOTAL BINDING -1 VOTES]\n\nThe vote is ***successful/not successful***\n</code></pre>"},{"location":"community/contributor/#send-a-notice-to-asf-board","title":"Send a notice to ASF Board","text":"<p>The nominating PMC member should send a message to the ASF Board (board@apache.org) with a reference to the vote result in the following form:</p> <pre><code>To: board at apache.org\nCC: private at sedona.apache.org\nSubject: [NOTICE] New PMC NAME for Apache Sedona PMC\nBody:\n\nNew PMC NAME has been voted as a new member of the Apache Sedona PMC. The vote thread is at: *link to the vote result thread*\n</code></pre>"},{"location":"community/contributor/#send-the-invitation","title":"Send the invitation","text":"<pre><code>To: New PMC Email address\nCC: private@sedona.apache.org\n\nHello [New PMC NAME],\n\nThe Sedona Project Management Committee (PMC)\nhereby offers you committer privileges to the project\n[as well as membership in the PMC]. These privileges are\noffered on the understanding that you'll use them\nreasonably and with common sense. We like to work on trust\nrather than unnecessary constraints.\n\nBeing a committer enables you to more easily make\nchanges without needing to go through the patch\nsubmission process. Being a PMC member enables you\nto guide the direction of the project.\n\nBeing a committer does not require you to\nparticipate any more than you already do. It does\ntend to make one even more committed. You will\nprobably find that you spend more time here.\n\nOf course, you can decline and instead remain as a\ncontributor, participating as you do now.\n\nA. This personal invitation is a chance for you to\naccept or decline in private. Either way, please\nlet us know in reply to the private@sedona.apache.org\naddress only.\n\nB. If you accept, the next step is to register an iCLA:\n    1. Details of the iCLA and the forms are found\n    through this link: https://www.apache.org/licenses/#clas\n\n    2. Instructions for its completion and return to\n    the Secretary of the ASF are found at\n    https://www.apache.org/licenses/#submitting\n\n    3. When you transmit the completed iCLA, request\n    to notify the Apache Sedona project and choose a\n    unique Apache ID. Look to see if your preferred\n    ID is already taken at\n    https://people.apache.org/committer-index.html\n    This will allow the Secretary to notify the PMC\n    when your iCLA has been recorded.\n\nWhen recording of your iCLA is noted, you will\nreceive a follow-up message with the next steps for\nestablishing you as a committer.\n</code></pre>"},{"location":"community/contributor/#pmc-accept-and-icla-instructions","title":"PMC Accept and ICLA instructions","text":"<pre><code>To: New PMC Email address\nCc: private@sedona.apache.org\nSubject: Re: invitation to become Apache Sedona PMC\n\nWelcome. Here are the next steps in becoming a project committer. After that we will make an announcement to the dev@sedona.apache.org\n\n1. You need to send a Contributor License Agreement to the ASF.\nNormally you would send an Individual CLA. If you also make\ncontributions done in work time or using work resources,\nsee the Corporate CLA. Ask us if you have any issues.\nhttps://www.apache.org/licenses/#clas.\n\nYou need to choose a preferred ASF user name and alternatives.\nIn order to ensure it is available you can view a list of taken IDs at\nhttps://people.apache.org/committer-index.html\n\nPlease notify us when you have submitted the CLA and by what means\nyou did so. This will enable us to monitor its progress.\n\nWe will arrange for your Apache user account when the CLA has\nbeen recorded.\n\n2. After that is done, please use your ASF email to subscribe to the dev@sedona.apache.org\nand private@sedona.apache.org by sending an email to dev-subscribe@sedona.apache.org and\nprivate-subscribe@sedona.apache.org. We generally discuss everything on the dev list and\nkeep the private@sedona.apache.org list for occasional matters which must be private.\n\nThe developer section of the website describes roles within the ASF and provides other\nresources:\n  https://www.apache.org/foundation/how-it-works.html\n  https://www.apache.org/dev/\n\nJust as before you became a committer, participation in any ASF community\nrequires adherence to the ASF Code of Conduct:\n  https://www.apache.org/foundation/policies/conduct.html\n\nYours,\nThe Apache Sedona PMC\n</code></pre>"},{"location":"community/contributor/#create-asf-account","title":"Create ASF account","text":"<p>Once the ICLA has been filed, use the ASF New Account Request form to generate the request. Sedona mentors will request the account.</p> <p>Once Sedona graduates, the PMC chair will make the request.</p>"},{"location":"community/contributor/#add-to-the-system","title":"Add to the system","text":"<p>Once the new PMC subscribes to the Sedona mailing lists using his/her ASF account, one of the PMC needs to add the new PMC to the Whimsy system (https://whimsy.apache.org/roster/pmc/sedona).</p>"},{"location":"community/contributor/#pmc-announcement","title":"PMC announcement","text":"<p>This is the email to announce the new committer to sedona-dev once the account has been created.</p> <pre><code>To: dev@sedona.apache.org\nSubject: new committer: ###New PMC NAME\n\nThe Project Management Committee (PMC) for Apache Sedona\nhas invited New PMC NAME to become a committer and we are pleased\nto announce that they have accepted.\n\n### add specific details here ###\n\nBeing a committer enables easier contribution to the\nproject since there is no need to go via the patch\nsubmission process. This should enable better productivity.\nA PMC member helps manage and guide the direction of the project.\n</code></pre>"},{"location":"community/contributor/#committer-done-template","title":"Committer Done Template","text":"<p>After the committer account is established.</p> <pre><code>To: New Committer Email\nCC: private@sedona.apache.org\nSubject: account request: New Committer NAME\n\nNew Committer NAME, as you know, the ASF Infrastructure has set up your\ncommitter account with the username '####'.\n\nYou have commit access to specific sections of the\nASF repository, as follows:\nhttps://github.com/apache/sedona\n\nYou need to link your ASF Account with your GitHub account.\n\nHere are the steps\n\n1. Verify you have a GitHub ID enabled with 2FA\n    * https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/\n2. Enter your GitHub ID into your Apache ID profile https://id.apache.org/\n3. Merge your Apache and GitHub accounts using\n    * GitBox (Apache Account Linking utility) https://gitbox.apache.org/setup/\n    * You should see 3 green checks in GitBox.\n    * Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation\n4. After accepting the GitHub Invitation verify that you are a\nmember of the team https://github.com/orgs/apache/teams/sedona-committers\n\nOptionally, if you want, please follow the instructions to set up your GitHub, SSH, svn password, svn configuration, email forwarding, etc.\nhttps://www.apache.org/dev/#committers\n\nAdditionally, if you have been elected to the Sedona\n Project Mgmt. Committee (PMC): Verify you are part of the LDAP sedona\n  pmc https://whimsy.apache.org/roster/pmc/sedona\n</code></pre>"},{"location":"community/develop/","title":"Develop","text":""},{"location":"community/develop/#develop-sedona","title":"Develop Sedona","text":""},{"location":"community/develop/#scalajava-developers","title":"Scala/Java developers","text":""},{"location":"community/develop/#ide","title":"IDE","text":"<p>We recommend Intellij IDEA with Scala plugin installed. Please make sure that the Project has the SDK set to a JDK 1.8.</p>"},{"location":"community/develop/#import-the-project","title":"Import the project","text":""},{"location":"community/develop/#choose-open","title":"Choose <code>Open</code>","text":""},{"location":"community/develop/#go-to-the-sedona-root-folder-not-a-submodule-folder-and-choose-open","title":"Go to the Sedona root folder (not a submodule folder) and choose <code>open</code>","text":""},{"location":"community/develop/#the-ide-might-show-errors","title":"The IDE might show errors","text":"<p>The IDE usually has trouble understanding the complex project structure in Sedona.</p> <p></p>"},{"location":"community/develop/#fix-errors-by-changing-pomxml","title":"Fix errors by changing <code>pom.xml</code>","text":"<p>You need to comment out the following lines in <code>pom.xml</code> at the root folder, as follows. Remember that you should NOT submit this change to Sedona.</p> <pre><code>&lt;!--    &lt;parent&gt;--&gt;\n&lt;!--        &lt;groupId&gt;org.apache&lt;/groupId&gt;--&gt;\n&lt;!--        &lt;artifactId&gt;apache&lt;/artifactId&gt;--&gt;\n&lt;!--        &lt;version&gt;23&lt;/version&gt;--&gt;\n&lt;!--        &lt;relativePath /&gt;--&gt;\n&lt;!--    &lt;/parent&gt;--&gt;\n</code></pre>"},{"location":"community/develop/#reload-pomxml","title":"Reload <code>pom.xml</code>","text":"<p>Make sure you reload the <code>pom.xml</code> or reload the maven project. The IDE will ask you to remove some modules. Please select <code>yes</code>.</p> <p></p>"},{"location":"community/develop/#the-final-project-structure-should-be-like-this","title":"The final project structure should be like this:","text":""},{"location":"community/develop/#run-unit-tests","title":"Run unit tests","text":""},{"location":"community/develop/#run-all-unit-tests","title":"Run all unit tests","text":"<p>In a terminal, go to the Sedona root folder. Run <code>mvn clean install</code>. All tests will take more than 15 minutes. To only build the project jars, run <code>mvn clean install -DskipTests</code>.</p> <p>Note</p> <p><code>mvn clean install</code> will compile Sedona with Spark 3.3 and Scala 2.12. If you have a different version of Spark in $SPARK_HOME, make sure to specify that using -Dspark command line arg. For example, to compile sedona with Spark 3.4 and Scala 2.12, use: <code>mvn clean install -Dspark=3.4 -Dscala=2.12</code></p> <p>More details can be found on Compile Sedona</p>"},{"location":"community/develop/#run-a-single-unit-test","title":"Run a single unit test","text":"<p>In the IDE, right-click a test case and run this test case.</p> <p></p> <p>When you run a test case written in Scala, the IDE might tell you that the \"Path does not exist\" as follows:</p> <p></p> <p>Go to <code>Edit Configuration</code></p> <p></p> <p>Change the value of <code>Working Directory</code> to <code>$MODULE_WORKING_DIR$</code>.</p> <p></p> <p>Re-run the test case. Do NOT right-click the test case to re-run. Instead, click the button as shown in the figure below.</p> <p></p> <p>If you don't want to change the <code>Working Directory</code> configuration every time, you can change the default value of <code>Working Directory</code> in the <code>Run/Debug Configurations</code> window. Click <code>Edit configuration templates...</code> and change the value of <code>Working Directory</code> of ScalaTest to <code>$MODULE_WORKING_DIR$</code>.</p> <p> </p> <p>Now newly created run configurations for ScalaTest will have the correct value set for <code>Working Directory</code>.</p>"},{"location":"community/develop/#ide-configurations-when-using-java-11","title":"IDE Configurations When Using Java 11","text":"<p>If you are using Java 11, you may encounter the following error when running tests:</p> <pre><code>/path/to/sedona/common/src/main/java/org/apache/sedona/common/geometrySerde/UnsafeGeometryBuffer.java\npackage sun.misc does not exist\nsun.misc.Unsafe\n</code></pre> <p>You can fix this issue by disabling <code>Use '--release' option for cross-compilation</code> in the IDE settings.</p> <p></p>"},{"location":"community/develop/#run-tests-with-different-sparkscala-versions","title":"Run Tests with Different Spark/Scala Versions","text":"<p>If you want to test changes with different Spark/Scala versions, you can select the Spark and Scala profile in the Maven panel. Once you have selected the desired versions, reload the sedona-parent project. See picture below</p> <p>Note</p> <p>The profile change won't update the module names in the IDE. Don't be misled if a module still has a <code>-3.3-2.12</code> suffix in the name.</p> <p>Note</p> <p>Not all combinations of spark and scala versions are supported and so they will fail to compile.</p> <p></p>"},{"location":"community/develop/#python-developers","title":"Python developers","text":""},{"location":"community/develop/#ide_1","title":"IDE","text":"<p>We recommend PyCharm.</p>"},{"location":"community/develop/#run-python-tests","title":"Run Python tests","text":""},{"location":"community/develop/#run-all-python-tests","title":"Run all Python tests","text":"<p>To run all Python test cases, follow steps mentioned here.</p>"},{"location":"community/develop/#run-all-python-tests-in-a-single-test-file","title":"Run all Python tests in a single test file","text":"<p>To run a particular Python test file, specify the path of the <code>.py</code> file to <code>pipenv</code>.</p> <p>For example, to run all tests in <code>test_function.py</code> located in <code>python/tests/sql/</code>, use: <code>pipenv run pytest tests/sql/test_function.py</code>.</p>"},{"location":"community/develop/#run-a-single-test","title":"Run a single test","text":"<p>To run a particular test in a particular <code>.py</code> test file, specify <code>file_name::class_name::test_name</code> to the <code>pytest</code> command.</p> <p>For example, to run the test on <code>ST_Contains</code> function located in <code>sql/test_predicate.py</code>, use: <code>pipenv run pytest tests/sql/test_predicate.py::TestPredicate::test_st_contains</code></p>"},{"location":"community/develop/#import-the-project_1","title":"Import the project","text":""},{"location":"community/develop/#r-developers","title":"R developers","text":"<p>More details to come.</p>"},{"location":"community/develop/#ide_2","title":"IDE","text":"<p>We recommend RStudio</p>"},{"location":"community/develop/#import-the-project_2","title":"Import the project","text":""},{"location":"community/publication/","title":"Publications","text":""},{"location":"community/publication/#publication","title":"Publication","text":"<p>Apache Sedona was formerly called GeoSpark, initiated by Arizona State University Data Systems Lab.</p>"},{"location":"community/publication/#key-publications","title":"Key publications","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.</p>"},{"location":"community/publication/#third-party-evaluation","title":"Third-party evaluation","text":"<p>GeoSpark were evaluated by papers published on database top venues. It is worth noting that we do not have any collaboration with the authors.</p> <ul> <li>SIGMOD 2020 paper \"Architecting a Query Compiler for Spatial Workloads\" Ruby Y. Tahboub, Tiark  Rompf (Purdue University).</li> </ul> <p>In Figure 16a, GeoSpark distance join query runs around 7x - 9x faster than Simba, a spatial extension on Spark, on 1 - 24 core machines.</p> <ul> <li>PVLDB 2018 paper \"How Good Are Modern Spatial Analytics Systems?\" Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows:</li> </ul> <p>GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.</p>"},{"location":"community/publication/#full-publications","title":"Full publications","text":""},{"location":"community/publication/#geospark-ecosystem","title":"GeoSpark Ecosystem","text":"<p>\"Spatial Data Management in Apache Spark: The GeoSpark Perspective and Beyond\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2019.</p> <p>\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\" (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016</p> <p>\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\" (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015</p>"},{"location":"community/publication/#geosparkviz-visualization-system","title":"GeoSparkViz Visualization System","text":"<p>\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\" (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p> <p>\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\" (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018</p>"},{"location":"community/publication/#geosparksim-traffic-simulator","title":"GeoSparkSim Traffic Simulator","text":"<p>\"Dissecting GeoSparkSim: a scalable microscopic road network traffic simulator in Apache Spark\" (journal paper) Jia Yu, Zishan Fu, Mohamed Sarwat. Distributed Parallel Databases 38(4): 963-994 (2020)</p> <p>\"Demonstrating GeoSparkSim: A Scalable Microscopic Road Network Traffic Simulator Based on Apache Spark\". Zishan Fu, Jia Yu, Mohamed Sarwat. International Symposium on Spatial and Temporal Databases, SSTD, 2019</p> <p>\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\" (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019</p>"},{"location":"community/publication/#a-tutorial-about-geospatial-data-management-in-spark","title":"A Tutorial about Geospatial Data Management in Spark","text":"<p>\"Geospatial Data Management in Apache Spark: A Tutorial\" (Tutorial) Jia Yu and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019</p>"},{"location":"community/publish/","title":"Make a release","text":""},{"location":"community/publish/#make-a-sedona-release","title":"Make a Sedona release","text":"<p>This page is for Sedona PMC to publish Sedona releases.</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/publish/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run</li> </ol> <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre> <ol> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this script file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/publish/#1-check-asf-copyright-in-all-file-headers","title":"1. Check ASF copyright in all file headers","text":"<ol> <li>Run the following script:</li> </ol> <pre><code>#!/bin/bash\nwget -q https://archive.apache.org/dist/creadur/apache-rat-0.15/apache-rat-0.15-bin.tar.gz\ntar -xvf  apache-rat-0.15-bin.tar.gz\ngit clone --shared --branch master https://github.com/apache/sedona.git sedona-src\njava -jar apache-rat-0.15/apache-rat-0.15.jar -d sedona-src &gt; report.txt\n</code></pre> <ol> <li>Read the generated report.txt file and make sure all source code files have ASF header.</li> <li>Delete the generated report and cloned files</li> </ol> <pre><code>#!/bin/bash\nrm -rf apache-rat-0.15\nrm -rf sedona-src\nrm report.txt\n</code></pre>"},{"location":"community/publish/#2-update-sedona-python-r-and-zeppelin-versions","title":"2. Update Sedona Python, R and Zeppelin versions","text":"<p>Make sure the Sedona version in the following files are 1.7.0.</p> <ol> <li>https://github.com/apache/sedona/blob/master/python/sedona/version.py</li> <li>https://github.com/apache/sedona/blob/master/R/DESCRIPTION</li> <li>https://github.com/apache/sedona/blob/99239524f17389fc4ae9548ea88756f8ea538bb9/R/R/dependencies.R#L42</li> <li>https://github.com/apache/sedona/blob/master/zeppelin/package.json</li> </ol>"},{"location":"community/publish/#3-update-mkdocsyml","title":"3. Update mkdocs.yml","text":"<ul> <li>Please change the following variables in <code>mkdocs.yml</code> to the version you want to publish.<ul> <li><code>sedona_create_release.current_version</code></li> <li><code>sedona_create_release.current_rc</code></li> <li><code>sedona_create_release.current_git_tag</code></li> <li><code>sedona_create_release.current_snapshot</code></li> </ul> </li> <li>Then compile the website by <code>mkdocs serve</code>. This will generate the scripts listed on this page in your local browser.</li> <li>You can also publish this website if needed. See the instruction at bottom.</li> </ul>"},{"location":"community/publish/#4-stage-and-upload-release-candidates","title":"4. Stage and upload release candidates","text":"<pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\necho \"*****Step 1. Stage the Release Candidate to GitHub.\"\n\nmvn -q -B clean release:prepare -Dtag=sedona-1.7.0-rc1 -DreleaseVersion=1.7.0 -DdevelopmentVersion=1.7.1-SNAPSHOT -Dresume=false -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\necho \"Now the releases are staged. A tag and two commits have been created on Sedona GitHub repo\"\n\necho \"*****Step 2: Upload the Release Candidate to https://repository.apache.org.\"\n\n# For Spark 3.3 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.3 -Dscala=2.12\" -Dspark=3.3 -Dscala=2.12\n\n# For Spark 3.3 and Scala 2.13\n## Note that we use maven-release-plugin 2.3.2 instead of more recent version (e.g., 3.0.1) to get rid of a bug of maven-release-plugin,\n## which prevent us from cloning git repo with user specified -Dtag=&lt;tag&gt;.\n## Please refer to https://issues.apache.org/jira/browse/MRELEASE-933 and https://issues.apache.org/jira/browse/SCM-729 for details.\n##\n## Please also note that system properties `-Dspark` and `-Dscala` has to be specified both for release:perform and the actual build parameters\n## in `-Darguments`, because the build profiles activated for release:perform task will also affect the actual build task. It is safer to specify\n## these system properties for both tasks.\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.3 -Dscala=2.13\" -Dspark=3.3 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\n# For Spark 3.5 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.5 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.5 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n\necho \"*****Step 3: Upload Release Candidate on ASF SVN: https://dist.apache.org/repos/dist/dev/sedona\"\n\necho \"Creating 1.7.0-rc1 folder on SVN...\"\n\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1\n\necho \"Creating release files locally...\"\n\necho \"Downloading source code...\"\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.7.0-rc1.tar.gz\ntar -xvf sedona-1.7.0-rc1.tar.gz\nmkdir apache-sedona-1.7.0-src\ncp -r sedona-sedona-1.7.0-rc1/* apache-sedona-1.7.0-src/\ntar czf apache-sedona-1.7.0-src.tar.gz apache-sedona-1.7.0-src\nrm sedona-1.7.0-rc1.tar.gz\nrm -rf sedona-sedona-1.7.0-rc1\n\necho \"Compiling the source code...\"\n\nmkdir apache-sedona-1.7.0-bin\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.3 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\ncp apache-sedona-1.7.0-src/flink-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\ncp apache-sedona-1.7.0-src/snowflake/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.3 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.4 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.5 -Dscala=2.12 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ncd apache-sedona-1.7.0-src &amp;&amp; mvn -q clean install -DskipTests -Dspark=3.5 -Dscala=2.13 &amp;&amp; cd ..\ncp apache-sedona-1.7.0-src/spark-shaded/target/sedona-*1.7.0.jar apache-sedona-1.7.0-bin/\n\ntar czf apache-sedona-1.7.0-bin.tar.gz apache-sedona-1.7.0-bin\nshasum -a 512 apache-sedona-1.7.0-src.tar.gz &gt; apache-sedona-1.7.0-src.tar.gz.sha512\nshasum -a 512 apache-sedona-1.7.0-bin.tar.gz &gt; apache-sedona-1.7.0-bin.tar.gz.sha512\ngpg -ab apache-sedona-1.7.0-src.tar.gz\ngpg -ab apache-sedona-1.7.0-bin.tar.gz\n\necho \"Uploading local release files...\"\n\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz.asc https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz.sha512\n\necho \"Removing local release files...\"\n\nrm apache-sedona-1.7.0-src.tar.gz\nrm apache-sedona-1.7.0-src.tar.gz.asc\nrm apache-sedona-1.7.0-src.tar.gz.sha512\nrm apache-sedona-1.7.0-bin.tar.gz\nrm apache-sedona-1.7.0-bin.tar.gz.asc\nrm apache-sedona-1.7.0-bin.tar.gz.sha512\nrm -rf apache-sedona-1.7.0-src\nrm -rf apache-sedona-1.7.0-bin\n</code></pre>"},{"location":"community/publish/#5-vote-in-dev-sedonaapacheorg","title":"5. Vote in dev sedona.apache.org","text":""},{"location":"community/publish/#vote-email","title":"Vote email","text":"<p>Please add changes at the end if needed:</p> <pre><code>Subject: [VOTE] Release Apache Sedona 1.7.0-rc1\n\nHi all,\n\nThis is a call for vote on Apache Sedona 1.7.0-rc1. Please refer to the changes listed at the bottom of this email.\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.7.0-rc1/docs/setup/release-notes.md\n\nBuild instructions:\nhttps://github.com/apache/sedona/blob/sedona-1.7.0-rc1/docs/setup/compile.md\n\nGitHub tag:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.7.0-rc1\n\nGPG public key to verify the Release:\nhttps://downloads.apache.org/sedona/KEYS\n\nSource code and binaries:\nhttps://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/\n\nThe vote will be open for at least 72 hours or until at least 3 \"+1\" PMC votes are cast\n\nInstruction for checking items on the checklist: https://sedona.apache.org/latest/community/vote/\n\nWe recommend you use this Jupyter notebook on MyBinder to perform this task: https://mybinder.org/v2/gh/jiayuasu/sedona-tools/HEAD?labpath=binder%2Fverify-release.ipynb\n\n**Please vote accordingly and you must provide your checklist for your vote**.\n\n\n[ ] +1 approve\n\n[ ] +0 no opinion\n\n[ ] -1 disapprove with the reason\n\nChecklist:\n\n[ ] Download links are valid.\n\n[ ] Checksums and PGP signatures are valid.\n\n[ ] Source code artifacts have correct names matching the current release.\n\nFor a detailed checklist  please refer to:\nhttps://cwiki.apache.org/confluence/display/INCUBATOR/Incubator+Release+Checklist\n\n------------\n\nChanges according to the comments on the previous release\nOriginal comment (Permalink from https://lists.apache.org/list.html):\n</code></pre>"},{"location":"community/publish/#pass-email","title":"Pass email","text":"<p>Please count the votes and add the Permalink of the vote thread at the end.</p> <pre><code>Subject: [RESULT][VOTE] Release Apache Sedona 1.7.0-rc1\n\nDear all,\n\nThe vote closes now as 72hr have passed. The vote PASSES with\n\n+? (binding): NAME1, NAME2, NAME3\n+? (non-binding): NAME4\nNo -1 votes\n\nThe vote thread (Permalink from https://lists.apache.org/list.html):\n\nI will make an announcement soon.\n</code></pre>"},{"location":"community/publish/#announce-email","title":"Announce email","text":"<ol> <li>This email should be sent to dev@sedona.apache.org</li> <li>Please add the permalink of the vote thread</li> <li>Please add the permalink of the vote result thread</li> </ol> <pre><code>Subject: [ANNOUNCE] Apache Sedona 1.7.0 released\n\nDear all,\n\nWe are happy to report that we have released Apache Sedona 1.7.0. Thank you again for your help.\n\nApache Sedona is a cluster computing system for processing large-scale spatial data.\n\n\nVote thread (Permalink from https://lists.apache.org/list.html):\n\n\nVote result thread (Permalink from https://lists.apache.org/list.html):\n\n\nWebsite:\nhttp://sedona.apache.org/\n\nRelease notes:\nhttps://github.com/apache/sedona/blob/sedona-1.7.0/docs/setup/release-notes.md\n\nDownload links:\nhttps://github.com/apache/sedona/releases/tag/sedona-1.7.0\n\nAdditional resources:\nMailing list: dev@sedona.apache.org\nTwitter: https://twitter.com/ApacheSedona\nGitter: https://gitter.im/apache/sedona\n\nRegards,\nApache Sedona Team\n</code></pre>"},{"location":"community/publish/#7-failed-vote","title":"7. Failed vote","text":"<p>If a vote failed, do the following:</p> <ol> <li>In the vote email, say that we will create another release candidate.</li> <li>Restart from Step 3 <code>Update mkdocs.yml</code>. Please increment the release candidate ID (e.g., <code>1.7.0-rc2</code>) and update <code>sedona_create_release.current_rc</code> and <code>sedona_create_release.current_git_tag</code> in <code>mkdocs.yml</code> to generate the script listed on this webpage.</li> </ol>"},{"location":"community/publish/#8-release-source-code-and-maven-package","title":"8. Release source code and Maven package","text":""},{"location":"community/publish/#upload-releases","title":"Upload releases","text":"<pre><code>#!/bin/bash\n\necho \"Move all files in https://dist.apache.org/repos/dist/dev/sedona to https://dist.apache.org/repos/dist/release/sedona, using svn\"\nsvn mkdir -m \"Adding folder\" https://dist.apache.org/repos/dist/release/sedona/1.7.0\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-src.tar.gz.sha512\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz.asc\nwget https://dist.apache.org/repos/dist/dev/sedona/1.7.0-rc1/apache-sedona-1.7.0-bin.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-src.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-src.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.7.0-src.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-src.tar.gz.sha512\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-bin.tar.gz\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz.asc https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-bin.tar.gz.asc\nsvn import -m \"Adding file\" apache-sedona-1.7.0-bin.tar.gz.sha512 https://dist.apache.org/repos/dist/release/sedona/1.7.0/apache-sedona-1.7.0-bin.tar.gz.sha512\nrm apache-sedona-1.7.0-src.tar.gz\nrm apache-sedona-1.7.0-src.tar.gz.asc\nrm apache-sedona-1.7.0-src.tar.gz.sha512\nrm apache-sedona-1.7.0-bin.tar.gz\nrm apache-sedona-1.7.0-bin.tar.gz.asc\nrm apache-sedona-1.7.0-bin.tar.gz.sha512\n</code></pre>"},{"location":"community/publish/#manually-close-and-release-the-package","title":"Manually close and release the package","text":"<ol> <li>Click <code>Close</code> on the Sedona staging repo on https://repository.apache.org under <code>staging repository</code></li> <li>Once the staging repo is closed, click <code>Release</code> on this repo.</li> </ol> <p>NOTICE: The staging repo will be automatically dropped after 3 days without closing. If you find the staging repo being dropped, you can re-stage the release using the following script.</p> <pre><code>#!/bin/bash\n\necho \"Re-staging releases to https://repository.apache.org\"\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# For Spark 3.3 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.3 -Dscala=2.12\" -Dspark=3.3 -Dscala=2.12\n\n# For Spark 3.3 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.3 -Dscala=2.13\" -Dspark=3.3 -Dscala=2.13\n\n# For Spark 3.4 and Scala 2.12\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.12\" -Dspark=3.4 -Dscala=2.12\n\n# For Spark 3.4 and Scala 2.13\nmvn -q org.apache.maven.plugins:maven-release-plugin:2.3.2:perform -DconnectionUrl=scm:git:https://github.com/apache/sedona.git -Dtag=sedona-1.7.0-rc1 -Dresume=false -Darguments=\"-DskipTests -Dspark=3.4 -Dscala=2.13\" -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/publish/#9-release-sedona-python-and-zeppelin","title":"9. Release Sedona Python and Zeppelin","text":"<p>You must have the maintainer privilege of <code>https://pypi.org/project/apache-sedona/</code> and <code>https://www.npmjs.com/package/apache-sedona</code></p> <p>To publish Sedona pythons, you have to use GitHub actions since we release wheels for different platforms. Please use this repo: https://github.com/jiayuasu/sedona-publish-python</p> <pre><code>#!/bin/bash\n\nwget https://github.com/apache/sedona/archive/refs/tags/sedona-1.7.0-rc1.tar.gz\ntar -xvf sedona-1.7.0-rc1.tar.gz\nmkdir apache-sedona-1.7.0-src\ncp -r sedona-sedona-1.7.0-rc1/* apache-sedona-1.7.0-src/\n\nrm -rf apache-sedona-1.7.0-rc1\n\ncd apache-sedona-1.7.0-src/zeppelin &amp;&amp; npm publish &amp;&amp; cd ..\nrm -rf apache-sedona-1.7.0-src\n</code></pre>"},{"location":"community/publish/#10-release-sedona-r-to-cran","title":"10. Release Sedona R to CRAN.","text":"<pre><code>#!/bin/bash\nR CMD build .\nR CMD check --as-cran apache.sedona_*.tar.gz\n</code></pre> <p>Then submit to CRAN using this web form.</p>"},{"location":"community/publish/#11-publish-the-doc-website","title":"11. Publish the doc website","text":"<ol> <li>Check out the 1.7.0 Git tag on your local repo to a branch namely <code>branch-1.7.0</code></li> <li>Add the download link to Download page.</li> <li>Add the news to <code>docs/index.md</code>.</li> <li>Push the changes to this branch on GitHub.</li> <li>GitHub CI will pick up the changes and deploy to <code>website</code> branch.</li> <li>Normally this repo will automatically publish the website on a daily basis.</li> </ol>"},{"location":"community/release-manager/","title":"Become a release manager","text":""},{"location":"community/release-manager/#become-a-release-manager","title":"Become a release manager","text":"<p>You only need to perform these steps if this is your first time being a release manager.</p>"},{"location":"community/release-manager/#0-software-requirement","title":"0. Software requirement","text":"<ul> <li>JDK 8: <code>brew install openjdk@8</code></li> <li>Maven 3.X. Your Maven must point to JDK 8 (1.8). Check it by <code>mvn --version</code></li> <li>Git and SVN</li> </ul> <p>If your Maven (<code>mvn --version</code>) points to other JDK versions, you must change it to JDK 8. Steps are as follows:</p> <ol> <li>Find all Java installed on your machine: <code>/usr/libexec/java_home -V</code>. You should see multiple JDK versions including JDK 8.</li> <li>Run <code>whereis mvn</code> to get the installation location of your Maven. The result is a symlink to the actual location.</li> <li>Open it in the terminal (with <code>sudo</code> if needed). It will be like this</li> </ol> <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre> <ol> <li>Change <code>JAVA_HOME:-$(/usr/libexec/java_home)}</code> to <code>JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}</code>. The resulting content will be like this:</li> </ol> <pre><code>#!/bin/bash\nJAVA_HOME=\"${JAVA_HOME:-$(/usr/libexec/java_home -v 1.8)}\" exec \"/usr/local/Cellar/maven/3.6.3/libexec/bin/mvn\" \"$@\"\n</code></pre> <ol> <li>Run <code>mvn --version</code> again. It should now point to JDK 8.</li> </ol>"},{"location":"community/release-manager/#1-obtain-write-access-to-sedona-github-repo","title":"1. Obtain Write Access to Sedona GitHub repo","text":"<ol> <li>Verify you have a GitHub ID enabled with 2FA https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/</li> <li>Enter your GitHub ID into your Apache ID profile https://id.apache.org/</li> <li>Merge your Apache and GitHub accounts using GitBox (Apache Account Linking utility): https://gitbox.apache.org/setup/<ul> <li>You should see 5 green checks in GitBox</li> <li>Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation</li> </ul> </li> <li>After accepting the GitHub Invitation, verify that you are a member of the team https://github.com/orgs/apache/teams/sedona-committers</li> <li>Additionally, if you have been elected to the Sedona PMC, verify you are part of the LDAP Sedona PMC https://whimsy.apache.org/roster/pmc/sedona</li> </ol>"},{"location":"community/release-manager/#2-prepare-secret-gpg-key","title":"2. Prepare Secret GPG key","text":"<ol> <li>Install GNUPG if it was not installed before. On Mac: <code>brew install gnupg gnupg2</code></li> <li>Generate a secret key. It must be RSA4096 (4096 bits long).</li> <li>Run <code>gpg --full-generate-key</code>. If not work, run <code>gpg --default-new-key-algo rsa4096 --gen-key</code></li> <li>At the prompt, specify the kind of key you want: Select <code>RSA</code>, then press <code>enter</code></li> <li>At the prompt, specify the key size you want: Enter <code>4096</code></li> <li>At the prompt, enter the length of time the key should be valid: Press <code>enter</code> to make the key never expire.</li> <li>Verify that your selections are correct.</li> <li>Enter your user ID information: use your real name and Apache email address.</li> <li>Type a secure passphrase. Make sure you remember this because we will use it later.</li> <li>Use the <code>gpg --list-secret-keys --keyid-format=long</code> command to list the long form of the GPG keys.</li> <li>From the list of GPG keys, copy the long form of the GPG key ID you'd like to use (e.g., <code>3AA5C34371567BD2</code>)</li> <li>Run <code>gpg --export --armor 3AA5C34371567BD2</code>, substituting in the GPG key ID you'd like to use.</li> <li>Copy your GPG key, beginning with <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending with <code>-----END PGP PUBLIC KEY BLOCK-----</code>.</li> <li>There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Publish your armored key in major key servers: https://keyserver.pgp.com/</li> </ol>"},{"location":"community/release-manager/#3-use-svn-to-update-keys","title":"3. Use SVN to update KEYS","text":"<p>Use SVN to append your armored PGP public key to the <code>KEYS</code> files</p> <ul> <li>https://dist.apache.org/repos/dist/dev/sedona/KEYS</li> <li> <p>https://dist.apache.org/repos/dist/release/sedona/KEYS</p> </li> <li> <p>Check out both KEYS files</p> </li> </ul> <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/sedona/ sedona-dev --depth files\nsvn checkout https://dist.apache.org/repos/dist/release/sedona/ sedona-release --depth files\n</code></pre> <ol> <li>Use your favorite text editor to open <code>sedona-dev/KEYS</code> and <code>sedona-release/KEYS</code>.</li> <li>Paste your armored key to the end of both files. Note: There must be an empty line between <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and the actual key.</li> <li>Commit both KEYS. SVN might ask you to enter your ASF ID and password. Make sure you do it so SVN can always store your ID and password locally.</li> </ol> <pre><code>svn commit -m \"Update KEYS\" sedona-dev/KEYS\nsvn commit -m \"Update KEYS\" sedona-release/KEYS\n</code></pre> <ol> <li>Then remove both svn folders</li> </ol> <pre><code>rm -rf sedona-dev\nrm -rf sedona-release\n</code></pre>"},{"location":"community/release-manager/#4-add-gpg_tty-environment-variable","title":"4. Add GPG_TTY environment variable","text":"<p>In your <code>~/.bashrc</code> file, add the following content. Then restart your terminal.</p> <pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre>"},{"location":"community/release-manager/#5-get-github-personal-access-token-classic","title":"5. Get GitHub personal access token (classic)","text":"<p>You need to create a GitHub personal access token (classic). You can follow the instruction on GitHub.</p> <p>In short:</p> <ol> <li>On your GitHub interface -&gt; Settings</li> <li>In the left sidebar, click Developer settings.</li> <li>In the left sidebar, under  Personal access tokens, click Tokens (classic).</li> <li>Select Generate new token, then click Generate new token (classic).</li> <li>Give your token a descriptive name.</li> <li>To give your token an expiration, select the Expiration drop-down menu. Make sure you set the <code>Expiration</code> to <code>No expiration</code>.</li> <li>Select the scopes you'd like to grant this token. To use your token to access repositories from the command line, select <code>repo</code> and <code>admin:org</code>.</li> <li>Click <code>Generate token</code>.</li> <li>Please save your token somewhere because we will use it in the next step.</li> </ol>"},{"location":"community/release-manager/#6-set-up-credentials-for-maven","title":"6. Set up credentials for Maven","text":"<p>In your <code>~/.m2/settings.xml</code> file, add the following content. Please create this file or <code>.m2</code> folder if it does not exist.</p> <p>Please replace all capitalized text with your own ID and password.</p> <pre><code>&lt;settings&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;github&lt;/id&gt;\n      &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n      &lt;password&gt;YOUR_GITHUB_TOKEN&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.snapshots.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n    &lt;server&gt;\n      &lt;id&gt;apache.releases.https&lt;/id&gt;\n      &lt;username&gt;YOUR_ASF_ID&lt;/username&gt;\n      &lt;password&gt;YOUR_ASF_PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n  &lt;profiles&gt;\n    &lt;profile&gt;\n      &lt;id&gt;gpg&lt;/id&gt;\n      &lt;properties&gt;\n        &lt;gpg.passphrase&gt;YOUR_GPG_PASSPHRASE&lt;/gpg.passphrase&gt;\n      &lt;/properties&gt;\n    &lt;/profile&gt;\n  &lt;/profiles&gt;\n  &lt;activeProfiles&gt;\n    &lt;activeProfile&gt;gpg&lt;/activeProfile&gt;\n  &lt;/activeProfiles&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"community/rule/","title":"Rules","text":""},{"location":"community/rule/#contributing-to-apache-sedona","title":"Contributing to Apache Sedona","text":"<p>The project welcomes contributions. You can contribute to Sedona code or documentation by making Pull Requests on Sedona GitHub Repo.</p> <p>The following sections brief the workflow of how to complete a contribution.</p>"},{"location":"community/rule/#pick-announce-a-task-using-jira","title":"Pick / Announce a task using JIRA","text":"<p>It is important to confirm that your contribution is acceptable. You should create a JIRA ticket or pick an existing ticket. A new JIRA ticket will be automatically sent to <code>dev@sedona.apache.org</code></p>"},{"location":"community/rule/#develop-a-code-contribution","title":"Develop a code contribution","text":"<p>Code contributions should include the following:</p> <ul> <li>Detailed documentations on classes and methods.</li> <li>Unit Tests to demonstrate code correctness and allow this to be maintained going forward. In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any). Unit Tests can be JUnit test or Scala test. Some Sedona functions need to be tested in both Scala and Java.</li> <li>Updates on corresponding Sedona documentation if necessary.</li> </ul> <p>Code contributions must include an Apache 2.0 license header at the top of each file.</p> <p>Please run <code>mvn spotless:apply</code> to format the code before making a pull request. If you've modified code for a specific spark version (for example, source files in spark/spark-3.5/), please add additional Maven CLI arguments to format that code: <code>mvn spotless:apply -Dscala=2.12 -Dspark=3.5</code>.</p>"},{"location":"community/rule/#develop-a-document-contribution","title":"Develop a document contribution","text":"<p>Documentation contributions should satisfy the following requirements:</p> <ul> <li>Detailed explanation with examples.</li> <li>Place a newly added document in a proper folder</li> <li>Change the mkdocs.yml if necessary</li> </ul> <p>Note</p> <p>Please read Compile the source code to learn how to compile Sedona website.</p>"},{"location":"community/rule/#make-a-pull-request","title":"Make a Pull Request","text":"<p>After developing a contribution, the easiest and most visible way to submit a Pull Request (PR) to the GitHub repo.</p> <p>Please use the JIRA ticket ID in the PR name, such as \"[SEDONA-1] my subject\".</p> <p>When creating a PR, please answer the questions in the PR template.</p> <p>When a PR is submitted, GitHub Action will check the build correctness. Please check the PR status, and fix any reported problems.</p>"},{"location":"community/rule/#review-a-pull-request","title":"Review a Pull Request","text":"<ul> <li>Every PR requires (1) at least 1 approval from a committer and (2) no disapproval from a committer. Everyone is welcome to review a PR but only the committer can make the final decision.</li> <li>Other reviewers, including community members and committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.</li> <li>Lively, polite, rapid technical debate is encouraged from everyone in the community even if the outcome may be a rejection of the entire change.</li> <li>Keep in mind that changes to more critical parts of Sedona, like Sedona core and spatial join algorithms, will be subjected to more review, and may require more testing and proof of its correctness than other changes.</li> <li>Sometimes, other changes will be merged which conflict with your pull request\u2019s changes. The PR can\u2019t be merged until the conflict is resolved. This can be resolved by resolving the conflicts by hand, then pushing the result to your branch.</li> </ul>"},{"location":"community/rule/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read Apache Software Foundation Code of Conduct.</p> <p>We expect everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF in any role to honor this code of conduct.</p>"},{"location":"community/snapshot/","title":"Publish a snapshot version","text":""},{"location":"community/snapshot/#publish-a-snapshot-version","title":"Publish a SNAPSHOT version","text":"<p>This step is to publish Maven SNAPSHOTs to https://repository.apache.org</p> <p>This is a good practice for a release manager to try out his/her credential setup.</p> <p>The detailed requirement is on ASF Infra website</p> <p>Warning</p> <p>All scripts on this page should be run in your local Sedona Git repo under master branch via a single script file.</p>"},{"location":"community/snapshot/#0-prepare-an-empty-script-file","title":"0. Prepare an empty script file","text":"<ol> <li>In your local Sedona Git repo under master branch, run</li> </ol> <pre><code>echo '#!/bin/bash' &gt; create-release.sh\nchmod 777 create-release.sh\n</code></pre> <ol> <li>Use your favourite GUI text editor to open <code>create-release.sh</code>.</li> <li>Then keep copying the scripts on this web page to replace all content in this text file.</li> <li>Do NOT directly copy/paste the scripts to your terminal because a bug in <code>clipboard.js</code> will create link breaks in such case.</li> <li>Each time when you copy content to this script file, run <code>./create-release.sh</code> to execute it.</li> </ol>"},{"location":"community/snapshot/#1-upload-snapshot-versions","title":"1. Upload snapshot versions","text":"<p>In your Sedona GitHub repo, run this script:</p> <pre><code>#!/bin/bash\n\ngit checkout master\ngit pull\n\nrm -f release.*\nrm -f pom.xml.*\n\n# Validate the POMs and your credential setup\nmvn -q -B clean release:prepare -Dtag=sedona-1.7.0-rc1 -DreleaseVersion=1.7.0 -DdevelopmentVersion=1.7.1-SNAPSHOT -Dresume=false -DdryRun=true -Penable-all-submodules -Darguments=\"-DskipTests\"\nmvn -q -B release:clean -Penable-all-submodules\n\n# Spark 3.3 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.3 -Dscala=2.12\n\n# Spark 3.3 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.3 -Dscala=2.13\n\n# Spark 3.4 and Scala 2.12\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.12\n\n# Spark 3.4 and Scala 2.13\nmvn -q deploy -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre>"},{"location":"community/vote/","title":"Vote a release","text":""},{"location":"community/vote/#vote-a-sedona-release","title":"Vote a Sedona release","text":"<p>This page is for Sedona community to vote a Sedona release. The script below is tested on macOS.</p> <p>In order to vote a Sedona release, you must provide your checklist including the following minimum requirement:</p> <ul> <li>Download links are valid</li> <li>Checksums and PGP signatures are valid</li> <li>DISCLAIMER and NOTICE are included</li> <li>Source code artifacts have correct names matching the current release</li> <li>The project can compile from the source code</li> </ul> <p>To make your life easier, we have provided an online Jupyter notebook using MyBinder. Please click this button to open the notebook and verify the release: . Then you can vote <code>+1</code> in the vote email.</p> <p>If you prefer to run the steps on your local machine, please read the steps below. If you can successfully finish the steps, you will pass the items mentioned above. Then you can vote <code>+1</code> in the vote email and provide your checklist.</p>"},{"location":"community/vote/#install-necessary-software","title":"Install necessary software","text":"<ol> <li>GPG: On Mac <code>brew install gnupg gnupg2</code>. You can check in a terminal <code>gpg --version</code>.</li> <li>JDK 1.8 or 1.11. Your Mac might have many different Java versions installed. You can try to use it but not sure if it can pass. You can check in a terminal <code>java --version</code>.</li> <li>Apache Maven 3.3.1+. On Mac <code>brew install maven</code>. You can check it in a terminal <code>mvn -version</code>.</li> <li>Python3 installed on your machine. MacOS comes with Python3 by default. You can check in a terminal <code>python3 --version</code>.</li> </ol> <p>You can skip this step if you installed these software before.</p>"},{"location":"community/vote/#run-the-verify-script","title":"Run the verify script","text":"<p>Please replace SEDONA_CURRENT_RC and SEDONA_CURRENT_VERSION with the correct versions. Then paste the content in a script called <code>verify.sh</code> and re-direct the output to a file. To run a script, do the following:</p> <pre><code>#!/bin/bash\n\n## Change the permission of the script to executable\nchmod 777 verify.sh\n\n## Run and redirect the output to a file\n./verify.sh &amp;&gt; verify.out\n</code></pre> <p>The content of the <code>verify.sh</code> script is as follows. If you copy the following content, a line break is automatically added to a long line of code. Please remove it in your local script.</p> <pre><code>#!/bin/bash\n\nSEDONA_CURRENT_RC=1.7.0-rc1\nSEDONA_CURRENT_VERSION=1.7.0\n\n## Download a Sedona release\nwget -q https://downloads.apache.org/sedona/KEYS\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nwget -q https://dist.apache.org/repos/dist/dev/sedona/$SEDONA_CURRENT_RC/apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Verify the signature and checksum\ngpg --import KEYS\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.asc\ngpg --verify apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.asc\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz.sha512\nshasum -a 512 apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz\ncat apache-sedona-$SEDONA_CURRENT_VERSION-bin.tar.gz.sha512\n\n## Uncompress the source code folder\ntar -xvf apache-sedona-$SEDONA_CURRENT_VERSION-src.tar.gz\n\n## Compile the project from source\n(cd apache-sedona-$SEDONA_CURRENT_VERSION-src;mvn clean install -DskipTests)\n</code></pre> <ul> <li>If successful, in the output file, you should be able to see something similar to the following text. It should include <code>Good signature from</code> and the final 4 lines should be two pairs of checksum matching each other.</li> </ul> <pre><code>gpg: key 3A79A47AC26FF4CD: \"Jia Yu &lt;jiayu@apache.org&gt;\" not changed\ngpg: key 6C883CA80E7FD299: \"PawelKocinski &lt;imbruced@apache.org&gt;\" not changed\ngpg: Total number processed: 2\ngpg:              unchanged: 2\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-src.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:31 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\ngpg: assuming signed data in 'apache-sedona-1.2.0-incubating-bin.tar.gz'\ngpg: Signature made Mon Apr  4 11:48:42 2022 PDT\ngpg:                using RSA key 949DD6275C69AB954B1872FC6C883CA80E7FD299\ngpg:                issuer \"imbruced@apache.org\"\ngpg: Good signature from \"PawelKocinski &lt;imbruced@apache.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 949D D627 5C69 AB95 4B18  72FC 6C88 3CA8 0E7F D299\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\nd3bdfd4d870838ebe63f21cb93634d2421ec1ac1b8184636206a5dc0d89a78a88257798b1f17371ad3cfcc3b1eb79c69e1410afdefeb4d9b52fc8bb5ea18dd2e  apache-sedona-1.2.0-incubating-src.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n64cea38dd3ca171ee4e2a7365dbce999773862f2a11599bd0f27e9551d740659a519a9b976b3e7b0826088010967093e6acc9462f7073e9737c24b007a2df846  apache-sedona-1.2.0-incubating-bin.tar.gz\n</code></pre> <ul> <li>At the end of the output, you should also see the <code>BUILD SUCCESS</code> if you can compile the source code. If this step fails, you can contact Sedona PMC and see if this is just because of your environment.</li> </ul>"},{"location":"community/vote/#check-files-manually","title":"Check files manually","text":"<ol> <li> <p>Check if the downloaded files have the correct version.</p> </li> <li> <p>In the unzipped source code folder, and check if DISCLAIMER and NOTICE files and included and up to date.</p> </li> </ol>"},{"location":"setup/azure-synapse-analytics/","title":"Install on Azure Synpase Analytics","text":"<p>This tutorial will guide you through the process of installing Sedona on Azure Synapse Analytics when Data Exfiltration Protection (DEP) is enabled or when you have no internet connection from the Spark pools due to other networking constraints.</p>"},{"location":"setup/azure-synapse-analytics/#before-you-begin","title":"Before you begin","text":"<p>This tutorial focuses on getting you up and running with Sedona 1.6.1 on Spark 3.4 Python 3.10</p> <p>If you want to run newer version, you will need to dive into the detailed build and diagnose process detailed in the lower part of this document.</p>"},{"location":"setup/azure-synapse-analytics/#strong-recommendations","title":"Strong recommendations","text":"<ol> <li>Start with a clean Spark pool with no other packages installed to avoid package conflicts.</li> <li>Apache Spark pool -&gt; Apache Spark configuration: Use default configuration</li> </ol>"},{"location":"setup/azure-synapse-analytics/#sedona-161-on-spark-34-python-310","title":"Sedona 1.6.1 on Spark 3.4 Python 3.10","text":""},{"location":"setup/azure-synapse-analytics/#step1-download-packages-9","title":"Step1: Download packages (9)","text":"<p>Caution: Precise versions are critical, latest is not always greatest here.</p> <p>From Maven</p> <ul> <li> <p>sedona-spark-shaded-3.4_2.12-1.6.1.jar</p> </li> <li> <p>geotools-wrapper-1.6.1-28.2.jar</p> </li> </ul> <p>From PyPi</p> <ul> <li> <p>rasterio-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>apache_sedona-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> <li> <p>click_plugins-1.1.1-py2.py3-none-any.whl</p> </li> <li> <p>cligj-0.7.2-py3-none-any.whl</p> </li> <li> <p>affine-2.4.0-py3-none-any.whl</p> </li> <li> <p>numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</p> </li> </ul>"},{"location":"setup/azure-synapse-analytics/#step-2-upload-packages-to-synapse-workspace","title":"Step 2: Upload packages to Synapse Workspace","text":"<p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-workspace-packages</p>"},{"location":"setup/azure-synapse-analytics/#step-3-add-packages-to-spark-pool","title":"Step 3: Add packages to Spark Pool","text":"<p>This tutorial used the second method on this page: If you are updating from the Synapse Studio</p> <p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-pool-packages#manage-packages-from-synapse-studio-or-azure-portal</p>"},{"location":"setup/azure-synapse-analytics/#step-4-notebook","title":"Step 4: Notebook","text":"<p>Start your notebook with:</p> <pre><code>from sedona.spark import SedonaContext\n\nconfig = SedonaContext.builder() \\\n    .config('spark.jars.packages',\n            'org.apache.sedona:sedona-spark-shaded-3.4_2.12-1.6.1,'\n            'org.datasyslab:geotools-wrapper-1.6.1-28.2') \\\n    .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\") \\\n    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\") \\\n    .getOrCreate()\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Run a test</p> <pre><code>sedona.sql(\"SELECT ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)')\").show()\n</code></pre> <p>If you see the output of the point, then the installation is successful. Are you are all done with the setup.</p>"},{"location":"setup/azure-synapse-analytics/#packages-for-sedona-160-on-spark-34-python-10","title":"Packages for Sedona 1.6.0 on Spark 3.4 Python 10","text":"<pre><code>spark-xml_2.12-0.17.0.jar\nsedona-spark-shaded-3.4_2.12-1.6.0.jar\n\nclick_plugins-1.1.1-py2.py3-none-any.whl\naffine-2.4.0-py3-none-any.whl\napache_sedona-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\ncligj-0.7.2-py3-none-any.whl\nrasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl\nshapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nsnuggs-1.4.7-py3-none-any.whl\ngeotools-wrapper-1.6.0-28.2.jar\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#background-how-to-identify-packages-for-otherfuture-versions-of-spark-andor-sedona","title":"Background: How to identify packages for other/future versions of Spark and/or Sedona","text":"<p>Warning: this process is going to require some tenacious technical skills and troubleshooting.</p> <p>Broad steps: build a linux VM from the same image as the deployed Spark Pool, configure for Synapse, install Sedona packages, identify required packages over and above baseline Synapse config.</p> <p>This is the process for Sedona 1.6.1 on Spark 3.4 Python 3.10. (The same process was used for Sedona 1.6.0)</p>"},{"location":"setup/azure-synapse-analytics/#step-1-identify-the-linux-image-of-the-spark-pool-by-version","title":"Step 1: Identify the Linux image of the Spark Pool by version","text":"<p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-34-runtime</p>"},{"location":"setup/azure-synapse-analytics/#step-2-download-the-iso","title":"Step 2 : Download the ISO","text":"<p>https://github.com/microsoft/azurelinux/tree/2.0</p>"},{"location":"setup/azure-synapse-analytics/#step-3-build-the-vm","title":"Step 3: build the VM","text":"<p>https://github.com/microsoft/azurelinux/blob/2.0/toolkit/docs/quick_start/quickstart.md#iso-image</p> <p>Important settings if using Hyper-V</p> <ul> <li>Enable Secure Boot: Microsoft UEFI Certificate authority</li> <li>Cores 2</li> <li>Disable Dynamic Memory (fix at 8Gb), forgetting this setting causes havoc.</li> </ul>"},{"location":"setup/azure-synapse-analytics/#step-4-patch-the-vm","title":"Step 4: patch the VM","text":"<p>Connect the VM. Note: it will take longer to first boot than you'd expect</p> <pre><code>sudo dnf upgrade\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-5-optional-but-strongly-recommended-install-ssh-server-for-best-copy-and-paste-experience","title":"Step 5: optional but strongly recommended - install ssh-server (for best copy and paste experience)","text":"<pre><code>sudo tdnf install -y openssh-server\n</code></pre> <p>Enable root and password auth</p> <pre><code>sudo vi /etc/ssh/sshd_config\n-   PasswordAuthentication yes\n-   PermitRootLogin yes\n</code></pre> <p>Start ssh-server</p> <pre><code>sudo systemctl enable --now sshd.service\n</code></pre> <p>Identify the ip of the VM (I'm using Hyper-V on windows 10 desktop)</p> <pre><code>Get-VMNetworkAdapter -VMName \"Synapse Spark 3.4 Python 3.10 Sedona 1.6.1\" | Select-Object -ExpandProperty IPAddresses\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-6-install-miniconda","title":"Step 6: install Miniconda","text":"<pre><code>cd /tmp\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-7-install-compilers","title":"Step 7: install compilers","text":"<pre><code>sudo tdnf -y install gcc g++\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-8-create-baseline-synapse-virtual-env","title":"Step 8: create baseline synapse virtual env","text":"<p>Download the virtual env spec</p> <pre><code>wget -O Synapse-Python310-CPU.yml https://raw.githubusercontent.com/microsoft/synapse-spark-runtime/refs/heads/main/Synapse/spark3.4/Synapse-Python310-CPU.yml source\n</code></pre> <pre><code>conda env create -f Synapse-Python310-CPU.yml -n synapse\n</code></pre> <p>if you get errors due to <code>fsspec_wrapper</code> then remove <code>fsspec_wrapper==0.1.13=py_3</code> from the yml and run again</p> <p>if you get further but different errors from <code>pip</code> after making the above change, ignore them you can still proceed</p>"},{"location":"setup/azure-synapse-analytics/#step-9-install-sedona-python-packages","title":"Step 9: install sedona python packages","text":"<pre><code>conda activate synapse\necho \"apache-sedona==1.6.1\" &gt; requirements.txt\npip install -r requirements.txt &gt; pip-output.txt\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-10-identify-python-packages-to-download","title":"Step 10: identify Python packages to download","text":"<pre><code>grep Downloading pip-output.txt\n</code></pre> <p>This will be the list of packages you need to locate and download from PyPi</p> <p>Example output</p> <pre><code>Downloading apache_sedona-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\nDownloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\nDownloading rasterio-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\nDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n</code></pre>"},{"location":"setup/azure-synapse-analytics/#step-11-identify-package-conflicts-in-your-deployed-azure-synapse-spark-pool-the-real-one-not-the-vm","title":"Step 11: identify package conflicts in your deployed Azure Synapse Spark Pool (the real one, not the VM)","text":"<ul> <li>upload packages to workspace</li> <li>add packages to your (clean!) Spark pool</li> </ul> <p>Pay careful attention to errors reported back from Synpase and troubleshoot to resolve conflicts.</p> <p>Note: We didn't have issues with Sedona 1.6.0 on Spark 3.4, but Sedona 1.6.1 and supporting packages had a conflict around <code>numpy</code> which requires us to download a specific version and add it to the packages list. <code>numpy</code> was not listed in the output of the grep.</p>"},{"location":"setup/cluster/","title":"Set up Spark cluster manually","text":""},{"location":"setup/cluster/#set-up-your-apache-spark-cluster","title":"Set up your Apache Spark cluster","text":"<p>Download a Spark distribution from Spark download page.</p>"},{"location":"setup/cluster/#preliminary","title":"Preliminary","text":"<ol> <li>Set up a password-less SSH on your cluster. Each master-worker pair should have bidirectional password-less SSH.</li> <li>Make sure you have installed JRE 1.8 or later.</li> <li>Add the list of your workers' IP address in ./conf/slaves</li> <li>Besides the necessary Spark settings, you may need to add the following lines in the Spark configuration files to avoid Sedona memory errors:</li> </ol> <p>In <code>./conf/spark-defaults.conf</code></p> <pre><code>spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n</code></pre> <ul> <li><code>spark.driver.memory</code> tells Spark to allocate enough memory for the driver program because Sedona needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.</li> <li><code>spark.network.timeout</code> is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.</li> <li><code>spark.driver.maxResultSize</code> is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.</li> </ul> <p>For more details of Spark parameters, please visit Spark Website.</p>"},{"location":"setup/cluster/#start-your-cluster","title":"Start your cluster","text":"<p>Go the root folder of the uncompressed Apache Spark folder. Start your Spark cluster via a terminal</p> <pre><code>./sbin/start-all.sh\n</code></pre>"},{"location":"setup/compile/","title":"Compile the code","text":""},{"location":"setup/compile/#compile-sedona-source-code","title":"Compile Sedona source code","text":""},{"location":"setup/compile/#compile-scala-java-source-code","title":"Compile Scala / Java source code","text":"<p>Sedona Scala/Java code is a project with multiple modules. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.</p> <ul> <li>Make sure your Linux/Mac machine has Java 1.8, Apache Maven 3.3.1+, and Python3.7+. The compilation of Sedona is not tested on Windows machines.</li> </ul> <p>To compile all modules, please make sure you are in the root folder of all modules. Then enter the following command in the terminal:</p> Without unit testsWith unit testsWith Geotools jars packaged <p><pre><code>mvn clean install -DskipTests\n</code></pre> This command will first delete the old binary files and compile all modules. This compilation will skip the unit tests. To compile a single module, please make sure you are in the folder of that module. Then enter the same command.</p> <p><pre><code>mvn clean install\n</code></pre> The maven unit tests of all modules may take up to 30 minutes.</p> <p><pre><code>mvn clean install -DskipTests -Dgeotools\n</code></pre> Geotools jars will be packaged into the produced fat jars.</p> <p>Note</p> <p>By default, this command will compile Sedona with Spark 3.3 and Scala 2.12</p>"},{"location":"setup/compile/#compile-with-different-targets","title":"Compile with different targets","text":"<p>User can specify <code>-Dspark</code> and <code>-Dscala</code> command line options to compile with different targets. Available targets are:</p> <ul> <li><code>-Dspark</code>: <code>{major}.{minor}</code>: For example, specify <code>-Dspark=3.4</code> to build for Spark 3.4.</li> <li><code>-Dscala</code>: <code>2.12</code> or <code>2.13</code></li> </ul> Spark 3.3+ Scala 2.12Spark 3.3+ Scala 2.13 <p><pre><code>mvn clean install -DskipTests -Dspark=3.3 -Dscala=2.12\n</code></pre> Please replace <code>3.3</code> with Spark major.minor version when building for higher Spark versions.</p> <p><pre><code>mvn clean install -DskipTests -Dspark=3.4 -Dscala=2.13\n</code></pre> Please replace <code>3.3</code> with Spark major.minor version when building for higher Spark versions.</p> <p>Tip</p> <p>To get the Sedona Spark Shaded jar with all GeoTools jars included, simply append <code>-Dgeotools</code> option. The command is like this:<code>mvn clean install -DskipTests -Dscala=2.12 -Dspark=3.0 -Dgeotools</code></p>"},{"location":"setup/compile/#download-staged-jars","title":"Download staged jars","text":"<p>Sedona uses GitHub Actions to automatically generate jars per commit. You can go here and download the jars by clicking the commits Artifacts tag.</p>"},{"location":"setup/compile/#run-python-test","title":"Run Python test","text":"<ol> <li>Set up the environment variable SPARK_HOME and PYTHONPATH</li> </ol> <p>For example,</p> <pre><code>export SPARK_HOME=$PWD/spark-3.0.1-bin-hadoop2.7\nexport PYTHONPATH=$SPARK_HOME/python\n</code></pre> <ol> <li>Put JAI jars to SPARK_HOME/jars/ folder.</li> </ol> <pre><code>export JAI_CORE_VERSION=\"1.1.3\"\nexport JAI_CODEC_VERSION=\"1.1.3\"\nexport JAI_IMAGEIO_VERSION=\"1.1\"\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_core/${JAI_CORE_VERSION}/jai_core-${JAI_CORE_VERSION}.jar\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_codec/${JAI_CODEC_VERSION}/jai_codec-${JAI_CODEC_VERSION}.jar\nwget -P $SPARK_HOME/jars/ https://repo.osgeo.org/repository/release/javax/media/jai_imageio/${JAI_IMAGEIO_VERSION}/jai_imageio-${JAI_IMAGEIO_VERSION}.jar\n</code></pre> <ol> <li>Compile the Sedona Scala and Java code with <code>-Dgeotools</code> and then copy the sedona-spark-shaded-1.7.0.jar to SPARK_HOME/jars/ folder.</li> </ol> <pre><code>cp spark-shaded/target/sedona-spark-shaded-xxx.jar $SPARK_HOME/jars/\n</code></pre> <ol> <li>Install the following libraries</li> </ol> <pre><code>sudo apt-get -y install python3-pip python-dev libgeos-dev\nsudo pip3 install -U setuptools\nsudo pip3 install -U wheel\nsudo pip3 install -U virtualenvwrapper\nsudo pip3 install -U pipenv\n</code></pre> <p>Homebrew can be used to install libgeos-dev in macOS: <code>brew install geos</code> 5. Set up pipenv to the desired Python version: 3.7, 3.8, or 3.9</p> <pre><code>cd python\npipenv --python 3.7\n</code></pre> <ol> <li>Install the PySpark version and the other dependency</li> </ol> <pre><code>cd python\npipenv install pyspark\npipenv install --dev\n</code></pre> <p><code>pipenv install pyspark</code> installs the latest version of pyspark. In order to remain consistent with the installed spark version, use <code>pipenv install pyspark==&lt;spark_version&gt;</code> 7. Run the Python tests</p> <pre><code>cd python\npipenv run python setup.py build_ext --inplace\npipenv run pytest tests\n</code></pre>"},{"location":"setup/compile/#compile-the-documentation","title":"Compile the documentation","text":"<p>The website is automatically built after each commit. The built website can be downloaded here:</p>"},{"location":"setup/compile/#mkdocs-website","title":"MkDocs website","text":"<p>The source code of the documentation website is written in Markdown and then compiled by MkDocs. The website is built upon the Material for MkDocs template.</p> <p>In the Sedona repository, the MkDocs configuration file mkdocs.yml is in the root folder and all documentation source code is in docs folder.</p> <p>To compile the source code and test the website on your local machine, please read the MkDocs Tutorial and Materials for MkDocs Tutorial.</p> <p>In short, you need to run:</p> <pre><code>pip install mkdocs\npip install mkdocs-jupyter\npip install mkdocs-material\npip install mkdocs-macros-plugin\npip install mkdocs-git-revision-date-localized-plugin\npip install mike\npip install pymdown-extensions\n</code></pre> <p>After installing MkDocs and MkDocs-Material, run these commands in the Sedona root folder:</p> <pre><code>mkdocs build\nmike deploy --update-aliases latest-snapshot -b website -p\nmike serve\n</code></pre>"},{"location":"setup/compile/#pre-commit","title":"pre-commit","text":"<p>We run pre-commit with GitHub Actions so installation on your local machine is currently optional.</p> <p>The pre-commit configuration file is in the repository root. Before you can run the hooks, you need to have pre-commit installed.</p> <p>The hooks run when running <code>git commit</code>. Some of the hooks will auto fix the code after the hook fails whilst most will print error messages from the linters.</p> <p>If you want to test all hooks against all files and when you are adding a new hook you should always run:</p> <p><code>pre-commit run --all-files</code></p> <p>Sometimes you might need to skip a hook to commit for example:</p> <p><code>SKIP=markdownlint git commit -m \"foo\"</code></p> <p>We have a Makefile in the repository root which has three pre-commit convenience commands.</p>"},{"location":"setup/databricks/","title":"Install on Databricks","text":"<p>Please pay attention to the Spark version postfix and Scala version postfix on our Maven Coordinate page. Databricks Spark and Apache Spark's compatibility can be found here.</p>"},{"location":"setup/databricks/#community-edition-free-tier","title":"Community edition (free-tier)","text":"<p>You just need to install the Sedona jars and Sedona Python on Databricks using Databricks default web UI. Then everything will work.</p>"},{"location":"setup/databricks/#install-libraries","title":"Install libraries","text":"<p>1) From the Libraries tab install from Maven Coordinates</p> <pre><code>org.apache.sedona:sedona-spark-shaded-3.4_2.12:1.7.0\norg.datasyslab:geotools-wrapper:1.7.0-28.5\n</code></pre> <p>2) For enabling python support, from the Libraries tab install from PyPI</p> <pre><code>apache-sedona==1.7.0\nkeplergl==0.3.2\npydeck==0.8.0\n</code></pre>"},{"location":"setup/databricks/#initialize","title":"Initialize","text":"<p>After you have installed the libraries and started the cluster, you can initialize the Sedona <code>ST_*</code> functions and types by running from your code:</p> <p>(scala)</p> <pre><code>import org.apache.sedona.sql.utils.SedonaSQLRegistrator\nSedonaSQLRegistrator.registerAll(spark)\n</code></pre> <p>(or python)</p> <pre><code>from sedona.register.geo_registrator import SedonaRegistrator\nSedonaRegistrator.registerAll(spark)\n</code></pre>"},{"location":"setup/databricks/#advanced-editions","title":"Advanced editions","text":"<p>In Databricks advanced editions, you need to install Sedona via cluster init-scripts as described below. We recommend Databricks 10.x+. Sedona is not guaranteed to be 100% compatible with <code>Databricks photon acceleration</code>. Sedona requires Spark internal APIs to inject many optimization strategies, which sometimes is not accessible in <code>Photon</code>.</p> <p>In Spark 3.2, <code>org.apache.spark.sql.catalyst.expressions.Generator</code> class added a field <code>nodePatterns</code>. Any SQL functions that rely on Generator class may have issues if compiled for a runtime with a differing spark version. For Sedona, those functions are:</p> <ul> <li>ST_MakeValid</li> <li>ST_SubDivideExplode</li> </ul> <p>Note</p> <p>The following steps use DBR including Apache Spark 3.4.x as an example. Please change the Spark version according to your DBR version.</p>"},{"location":"setup/databricks/#download-sedona-jars","title":"Download Sedona jars","text":"<p>Download the Sedona jars to a DBFS location. You can do that manually via UI or from a notebook by executing this code in a cell:</p> <pre><code>%sh\n# Create JAR directory for Sedona\nmkdir -p /Workspace/Shared/sedona/1.7.0\n\n# Download the dependencies from Maven into DBFS\ncurl -o /Workspace/Shared/sedona/1.7.0/geotools-wrapper-1.7.0-28.5.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.0-28.5/geotools-wrapper-1.7.0-28.5.jar\"\n\ncurl -o /Workspace/Shared/sedona/1.7.0/sedona-spark-shaded-3.4_2.12-1.7.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.7.0/sedona-spark-shaded-3.4_2.12-1.7.0.jar\"\n</code></pre> <p>Of course, you can also do the steps above manually.</p>"},{"location":"setup/databricks/#create-an-init-script","title":"Create an init script","text":"<p>Warning</p> <p>Starting from December 2023, Databricks has disabled all DBFS based init script (/dbfs/XXX/.sh). So you will have to store the init script from a workspace level (<code>/Workspace/Users/&lt;user-name&gt;/&lt;script-name&gt;.sh</code>) or Unity Catalog volume (<code>/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path-to-script&gt;/&lt;script-name&gt;.sh</code>). Please see Databricks init scripts for more information. <p>Note</p> <p>If you are creating a Shared cluster, you won't be able to use init scripts and jars stored under <code>Workspace</code>. Please instead store them in <code>Volumes</code>. The overall process should be the same.</p> <p>Create an init script in <code>Workspace</code> that loads the Sedona jars into the cluster's default jar directory. You can create that from any notebook by running:</p> <pre><code>%sh\n\n# Create init script directory for Sedona\nmkdir -p /Workspace/Shared/sedona/\n\n# Create init script\ncat &gt; /Workspace/Shared/sedona/sedona-init.sh &lt;&lt;'EOF'\n#!/bin/bash\n#\n# File: sedona-init.sh\n#\n# On cluster startup, this script will copy the Sedona jars to the cluster's default jar directory.\n\ncp /Workspace/Shared/sedona/1.7.0/*.jar /databricks/jars\n\nEOF\n</code></pre> <p>Of course, you can also do the steps above manually.</p>"},{"location":"setup/databricks/#set-up-cluster-config","title":"Set up cluster config","text":"<p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Spark</code>) activate the Sedona functions and the kryo serializer by adding to the Spark Config</p> <pre><code>spark.sql.extensions org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> <p>From your cluster configuration (<code>Cluster</code> -&gt; <code>Edit</code> -&gt; <code>Configuration</code> -&gt; <code>Advanced options</code> -&gt; <code>Init Scripts</code>) add the newly created <code>Workspace</code> init script</p> Type File path Workspace /Shared/sedona/sedona-init.sh <p>For enabling python support, from the Libraries tab install from PyPI</p> <pre><code>apache-sedona==1.7.0\ngeopandas==0.11.1\nkeplergl==0.3.2\npydeck==0.8.0\n</code></pre> <p>Tips</p> <p>You need to install the Sedona libraries via init script because the libraries installed via UI are installed after the cluster has already started, and therefore the classes specified by the config <code>spark.sql.extensions</code>, <code>spark.serializer</code>, and <code>spark.kryo.registrator</code> are not available at startup time.*</p>"},{"location":"setup/databricks/#verify-installation","title":"Verify installation","text":"<p>After you have started the cluster, you can verify that Sedona is correctly installed by running the following code in a notebook:</p> <pre><code>spark.sql(\"SELECT ST_Point(1, 1)\").show()\n</code></pre> <p>Note that: you don't need to run the <code>SedonaRegistrator.registerAll(spark)</code> or <code>SedonaContext.create(spark)</code> in the advanced edition because <code>org.apache.sedona.sql.SedonaSqlExtensions</code> in the Cluster Config will take care of that.</p>"},{"location":"setup/docker/","title":"Play Sedona in Docker","text":""},{"location":"setup/docker/#sedona-jupyterlab-docker-image","title":"Sedona JupyterLab Docker Image","text":"<p>Sedona Docker images are available on Sedona official DockerHub repo.</p> <p>We provide a Docker image for Apache Sedona with Python JupyterLab and 1 master node and 1 worker node.</p>"},{"location":"setup/docker/#how-to-use","title":"How to use","text":""},{"location":"setup/docker/#pull-the-image-from-dockerhub","title":"Pull the image from DockerHub","text":"<p>Format:</p> <pre><code>docker pull apache/sedona:&lt;sedona_version&gt;\n</code></pre> <p>Example 1: Pull the latest image of Sedona master branch</p> <pre><code>docker pull apache/sedona:latest\n</code></pre> <p>Example 2: Pull the image of a specific Sedona release</p> <pre><code>docker pull apache/sedona:1.7.0\n</code></pre>"},{"location":"setup/docker/#start-the-container","title":"Start the container","text":"<p>Format:</p> <pre><code>docker run -e DRIVER_MEM=&lt;driver_mem&gt; -e EXECUTOR_MEM=&lt;executor_mem&gt; -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:&lt;sedona_version&gt;\n</code></pre> <p>Driver memory and executor memory are optional. If their values are not given, the container will take 4GB RAM for the driver and 4GB RAM for the executor.</p> <p>Example 1:</p> <pre><code>docker run -e DRIVER_MEM=6g -e EXECUTOR_MEM=8g -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:latest\n</code></pre> <p>This command will start a container with 6GB RAM for the driver and 8GB RAM for the executor and use the latest Sedona image.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040 to the host's ports 8888, 8080, 8081, 4040 respectively.</p> <p>Example 2:</p> <pre><code>docker run -p 8888:8888 -p 8080:8080 -p 8081:8081 -p 4040:4040 apache/sedona:1.7.0\n</code></pre> <p>This command will start a container with 4GB RAM for the driver and 4GB RAM for the executor and use Sedona 1.7.0 image.</p> <p>This command will bind the container's ports 8888, 8080, 8081, 4040 to the host's ports 8888, 8080, 8081, 4040 respectively.</p>"},{"location":"setup/docker/#start-coding","title":"Start coding","text":"<p>Open your browser and go to http://localhost:8888/ to start coding with Sedona.</p>"},{"location":"setup/docker/#notes","title":"Notes","text":"<ul> <li>This container assumes you have at least 8GB RAM and takes all your CPU cores and 8GM RAM. The 1 worker will take 4GB and the Jupyter program will take the remaining 4GB.</li> <li>Sedona in this container runs in the cluster mode. Only 1 notebook can be run at a time. If you want to run another notebook, please shut down the kernel of the current notebook first (How?).</li> </ul>"},{"location":"setup/docker/#how-to-build","title":"How to build","text":"<p>Clone the Sedona GitHub repository</p>"},{"location":"setup/docker/#build-the-image-against-a-sedona-release","title":"Build the image against a Sedona release","text":"<p>Requirements: docker (How?)</p> <p>Format:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh 3.4.1 1.7.0\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#build-the-image-against-the-latest-sedona-master","title":"Build the image against the latest Sedona master","text":"<p>Requirements: docker (How?), JDK &lt;= 19, maven3</p> <p>Format:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; latest &lt;build_mode&gt;\n</code></pre> <p>Example:</p> <pre><code>./docker/sedona-spark-jupyterlab/build.sh 3.4.1 latest\n</code></pre> <p><code>build_mode</code> is optional. If its value is not given or is <code>local</code>, the script will build the image locally. Otherwise, it will start a cross-platform compilation and push images directly to DockerHub.</p>"},{"location":"setup/docker/#notes_1","title":"Notes","text":"<p>This docker image can only be built against Sedona 1.7.0+ and Spark 3.3+</p>"},{"location":"setup/docker/#cluster-configuration","title":"Cluster Configuration","text":""},{"location":"setup/docker/#software","title":"Software","text":"<ul> <li>OS: Ubuntu 22.02</li> <li>JDK: openjdk-19</li> <li>Python: 3.10</li> <li>Spark 3.4.1</li> </ul>"},{"location":"setup/docker/#web-ui","title":"Web UI","text":"<ul> <li>JupyterLab: http://localhost:8888/</li> <li>Spark master URL: spark://localhost:7077</li> <li>Spark job UI: http://localhost:4040</li> <li>Spark master web UI: http://localhost:8080/</li> <li>Spark work web UI: http://localhost:8081/</li> </ul>"},{"location":"setup/docker/#how-to-push-to-dockerhub","title":"How to push to DockerHub","text":"<p>Format:</p> <pre><code>docker login\n./docker/sedona-spark-jupyterlab/build.sh &lt;spark_version&gt; &lt;sedona_version&gt; release\n</code></pre> <p>Example:</p> <pre><code>docker login\n./docker/sedona-spark-jupyterlab/build.sh 3.4.1 1.7.0 release\n</code></pre>"},{"location":"setup/emr/","title":"Install on AWS EMR","text":"<p>We recommend Sedona-1.3.1-incubating and above for EMR. In the tutorial, we use AWS Elastic MapReduce (EMR) 6.9.0. It has the following applications installed: Hadoop 3.3.3, JupyterEnterpriseGateway 2.6.0, Livy 0.7.1, Spark 3.3.0.</p> <p>This tutorial is tested on EMR on EC2 with EMR Studio (notebooks). EMR on EC2 uses YARN to manage resources.</p> <p>Note</p> <p>If you are using Spark 3.4+ and Scala 2.12, please use <code>sedona-spark-shaded-3.4_2.12</code>. Please pay attention to the Spark version postfix and Scala version postfix.</p>"},{"location":"setup/emr/#prepare-initialization-script","title":"Prepare initialization script","text":"<p>In your S3 bucket, add a script that has the following content:</p> <pre><code>#!/bin/bash\n\n# EMR clusters only have ephemeral local storage. It does not really matter where we store the jars.\nsudo mkdir /jars\n\n# Download Sedona jar\nsudo curl -o /jars/sedona-spark-shaded-3.3_2.12-1.7.0.jar \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.7.0/sedona-spark-shaded-3.3_2.12-1.7.0.jar\"\n\n# Download GeoTools jar\nsudo curl -o /jars/geotools-wrapper-1.7.0-28.5.jar \"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.0-28.5/geotools-wrapper-1.7.0-28.5.jar\"\n\n# Install necessary python libraries\nsudo python3 -m pip install pandas\nsudo python3 -m pip install shapely\nsudo python3 -m pip install geopandas\nsudo python3 -m pip install keplergl==0.3.2\nsudo python3 -m pip install pydeck==0.8.0\nsudo python3 -m pip install attrs matplotlib descartes apache-sedona==1.7.0\n</code></pre> <p>When you create an EMR cluster, in the <code>bootstrap action</code>, specify the location of this script.</p>"},{"location":"setup/emr/#add-software-configuration","title":"Add software configuration","text":"<p>When you create an EMR cluster, in the software configuration, add the following content:</p> <pre><code>[\n  {\n    \"Classification\":\"spark-defaults\",\n    \"Properties\":{\n      \"spark.yarn.dist.jars\": \"/jars/sedona-spark-shaded-3.3_2.12-1.7.0.jar,/jars/geotools-wrapper-1.7.0-28.5.jar\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n      \"spark.sql.extensions\": \"org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\"\n      }\n  }\n]\n</code></pre>"},{"location":"setup/emr/#verify-installation","title":"Verify installation","text":"<p>After the cluster is created, you can verify the installation by running the following code in a Jupyter notebook:</p> <pre><code>spark.sql(\"SELECT ST_Point(0, 0)\").show()\n</code></pre> <p>Note that: you don't need to run the <code>SedonaRegistrator.registerAll(spark)</code> or <code>SedonaContext.create(spark)</code> because <code>org.apache.sedona.sql.SedonaSqlExtensions</code> in the config will take care of that.</p>"},{"location":"setup/fabric/","title":"Install on Microsoft Fabric","text":"<p>This tutorial will guide you through the process of installing Sedona on Microsoft Fabric Synapse Data Engineering's Spark environment.</p>"},{"location":"setup/fabric/#step-1-open-microsoft-fabric-synapse-data-engineering","title":"Step 1: Open Microsoft Fabric Synapse Data Engineering","text":"<p>Go to the Microsoft Fabric portal and choose the <code>Data Engineering</code> option.</p> <p></p>"},{"location":"setup/fabric/#step-2-create-a-microsoft-fabric-data-engineering-environment","title":"Step 2: Create a Microsoft Fabric Data Engineering environment","text":"<p>On the left side, click <code>My Workspace</code> and then click <code>+ New</code> to create a new <code>Environment</code>. Let's name it <code>ApacheSedona</code>.</p> <p></p>"},{"location":"setup/fabric/#step-3-select-the-apache-spark-version","title":"Step 3: Select the Apache Spark version","text":"<p>In the <code>Environment</code> page, click the <code>Home</code> tab and select the appropriate version of Apache Spark. You will need this version to install the correct version of Apache Sedona.</p> <p></p>"},{"location":"setup/fabric/#step-4-install-the-sedona-python-package","title":"Step 4: Install the Sedona Python package","text":"<p>In the <code>Environment</code> page, click the <code>Public libraries</code> tab and then type in <code>apache-sedona</code>. Please select the appropriate version of Apache Sedona. The source is <code>PyPI</code>.</p> <p></p>"},{"location":"setup/fabric/#step-5-set-spark-properties","title":"Step 5: Set Spark properties","text":"<p>In the <code>Environment</code> page, click the <code>Spark properties</code> tab, then create the following 3 properties:</p> <ul> <li><code>spark.sql.extensions</code>: <code>org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code></li> <li><code>spark.serializer</code>: <code>org.apache.spark.serializer.KryoSerializer</code></li> <li><code>spark.kryo.registrator</code>: <code>org.apache.sedona.core.serde.SedonaKryoRegistrator</code></li> </ul> <p></p>"},{"location":"setup/fabric/#step-6-save-and-publish-the-environment","title":"Step 6: Save and publish the environment","text":"<p>Click the <code>Save</code> button and then click the <code>Publish</code> button to save and publish the environment. This will create the environment with the Apache Sedona Python package installed. The publishing process will take about 10 minutes.</p> <p></p>"},{"location":"setup/fabric/#step-7-find-the-download-links-of-sedona-jars","title":"Step 7: Find the download links of Sedona jars","text":"<ol> <li>Learn the Sedona jars you need from our Sedona maven coordinate</li> <li>Find the <code>sedona-spark-shaded</code> jar from Maven Central. Please pay attention to the Spark version and Scala version of the jars. If you select Spark 3.4 in the Fabric environment, you should download the Sedona jars with Spark 3.4 and Scala 2.12 and the jar name should be like <code>sedona-spark-shaded-3.4_2.12-1.5.1.jar</code>.</li> <li>Find the <code>geotools-wrapper</code> jar from Maven Central. Please pay attention to the Sedona versions of the jar. If you select Sedona 1.5.1, you should download the <code>geotools-wrapper</code> jar with version 1.5.1 and the jar name should be like <code>geotools-wrapper-1.5.1-28.2.jar</code>.</li> </ol> <p>The download links are like:</p> <pre><code>https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar\nhttps://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\n</code></pre>"},{"location":"setup/fabric/#step-8-start-the-notebook-with-the-sedona-environment-and-install-the-jars","title":"Step 8: Start the notebook with the Sedona environment and install the jars","text":"<p>In the notebook page, select the <code>ApacheSedona</code> environment you created before.</p> <p></p> <p>In the notebook, you can install the jars by running the following code. Please replace the <code>jars</code> with the download links of the 2 jars from the previous step.</p> <pre><code>%%configure -f\n{\n    \"jars\": [\"https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar\", \"https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.5.1/sedona-spark-shaded-3.4_2.12-1.5.1.jar\"]\n}\n</code></pre>"},{"location":"setup/fabric/#step-9-verify-the-installation","title":"Step 9: Verify the installation","text":"<p>You can verify the installation by running the following code in the notebook.</p> <pre><code>from sedona.spark import *\n\n\nsedona = SedonaContext.create(spark)\n\n\nsedona.sql(\"SELECT ST_GeomFromEWKT('SRID=4269;POINT(40.7128 -74.0060)')\").show()\n</code></pre> <p>If you see the output of the point, then the installation is successful.</p> <p></p>"},{"location":"setup/fabric/#optional-manually-upload-sedona-jars-to-the-fabric-environment-lakehouse-storage","title":"Optional: manually upload Sedona jars to the Fabric environment LakeHouse storage","text":"<p>If your cluster has no internet access or you want to skip the slow on-the-fly download, you can manually upload the Sedona jars to the Fabric environment LakeHouse storage.</p> <p>In the notebook page, choose the <code>Explorer</code> and click the <code>LakeHouses</code> option. If you don't have a LakeHouse, you can create one. Then choose <code>Files</code> and upload the 2 jars you downloaded in the previous step.</p> <p>After the upload, you should be able to see the 2 jars in the LakeHouse storage. Then please copy the <code>ABFS</code> paths of the 2 jars. In this example, the paths are</p> <pre><code>abfss://9e9d4196-870a-4901-8fa5-e24841492ab8@onelake.dfs.fabric.microsoft.com/e15f3695-af7e-47de-979e-473c3caa9f5b/Files/sedona-spark-shaded-3.4_2.12-1.5.1.jar\n\nabfss://9e9d4196-870a-4901-8fa5-e24841492ab8@onelake.dfs.fabric.microsoft.com/e15f3695-af7e-47de-979e-473c3caa9f5b/Files/geotools-wrapper-1.5.1-28.2.jar\n</code></pre> <p></p> <p></p> <p>If you use this option, the config files in your notebook should be</p> <pre><code>%%configure -f\n{\n    \"conf\": {\n        \"spark.jars\": \"abfss://XXX/Files/sedona-spark-shaded-3.4_2.12-1.5.1.jar,abfss://XXX/Files/geotools-wrapper-1.5.1-28.2.jar\",\n    }\n}\n</code></pre>"},{"location":"setup/glue/","title":"Install on AWS Glue","text":"<p>This tutorial will cover how to configure both a glue notebook and a glue ETL job. The tutorial is written assuming you have a working knowledge of AWS Glue jobs.</p> <p>In the tutorial, we use Sedona 1.7.0 and Glue 4.0 which runs on Spark 3.3.0, Java 8, Scala 2.12, and Python 3.10. We recommend Sedona-1.3.1-incubating and above for Glue.</p>"},{"location":"setup/glue/#gather-maven-links","title":"Gather Maven Links","text":"<p>You will need to point your glue job to the Sedona and Geotools jars. We recommend using the jars available from maven. The links below are those intended for Glue 4.0</p> <p>Sedona Jar: Maven Central</p> <p>Geotools Jar: Maven Central</p> <p>Note</p> <p>Ensure you pick a version for Scala 2.12 and Spark 3.3. The Spark 3.4 and Scala 2.13 jars are not compatible with Glue 4.0.</p>"},{"location":"setup/glue/#configure-glue-job","title":"Configure Glue Job","text":"<p>Once you have your jar links, you can configure your Glue job to use them, as well as the apache-sedona Python package. How you do this varies slightly between the notebook and the script job types.</p> <p>Note</p> <p>Always ensure that the Sedona version of the jars and the Python package match.</p>"},{"location":"setup/glue/#notebook-job","title":"Notebook Job","text":"<p>Add the following cell magics before starting your sparkContext or glueContext. The first points to the jars, and the second installs the Sedona Python package directly from pip.</p> <pre><code># Sedona Config\n%extra_jars https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.7.0/sedona-spark-shaded-3.3_2.12-1.7.0.jar, https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.0-28.5/geotools-wrapper-1.7.0-28.5.jar\n%additional_python_modules apache-sedona==1.7.0\n</code></pre> <p>If you are using the example notebook from glue, the first cell should now look like this:</p> <pre><code>%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n# Sedona Config\n%extra_jars https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.3_2.12/1.7.0/sedona-spark-shaded-3.3_2.12-1.7.0.jar, https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.7.0-28.5/geotools-wrapper-1.7.0-28.5.jar\n%additional_python_modules apache-sedona==1.7.0\n\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n</code></pre> <p>You can confirm your installation by running the following cell:</p> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(spark)\nsedona.sql(\"SELECT ST_POINT(1., 2.) as geom\").show()\n</code></pre>"},{"location":"setup/glue/#etl-job","title":"ETL Job","text":"<p>Glue also calls these Scripts. From your job's page, navigate to the \"Job details\" tab. At the bottom of the page expand the \"Advanced properties\" section. In the \"Dependent JARs path\" field, add the paths to the jars, separated by a comma.</p> <p>To add the Sedona Python package, navigate to the \"Job Parameters\" section and add a new parameter with the key <code>--additional-python-modules</code> and the value <code>apache-sedona==1.7.0</code>.</p> <p>To confirm the installation add the follow code to the script:</p> <pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder().getOrCreate()\nsedona = SedonaContext.create(config)\n\nsedona.sql(\"SELECT ST_POINT(1., 2.) as geom\").show()\n</code></pre> <p>Once added to the script, save and run the job. If the job runs successfully, the installation was successful.</p>"},{"location":"setup/install-python/","title":"Install Sedona Python","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Apache Sedona extends pyspark functions which depends on libraries:</p> <ul> <li>pyspark</li> <li>shapely</li> <li>attrs</li> </ul> <p>You need to install necessary packages if your system does not have them installed. See \"packages\" in our Pipfile.</p>"},{"location":"setup/install-python/#install-sedona","title":"Install sedona","text":"<ul> <li>Installing from PyPI repositories. You can find the latest Sedona Python on PyPI. There is a known issue in Sedona v1.0.1 and earlier versions.</li> </ul> <pre><code>pip install apache-sedona\n</code></pre> <ul> <li>Since Sedona v1.1.0, pyspark is an optional dependency of Sedona Python because spark comes pre-installed on many spark platforms. To install pyspark along with Sedona Python in one go, use the <code>spark</code> extra:</li> </ul> <pre><code>pip install apache-sedona[spark]\n</code></pre> <ul> <li>Installing from Sedona Python source</li> </ul> <p>Clone Sedona GitHub source code and run the following command</p> <pre><code>cd python\npython3 setup.py install\n</code></pre>"},{"location":"setup/install-python/#prepare-sedona-spark-jar","title":"Prepare sedona-spark jar","text":"<p>Sedona Python needs one additional jar file called <code>sedona-spark-shaded</code> or <code>sedona-spark</code> to work properly. Please make sure you use the correct version for Spark and Scala.</p> <p>Please use Spark major.minor version number in artifact names.</p> <p>You can get it using one of the following methods:</p> <ol> <li>If you run Sedona in Databricks, AWS EMR, or other cloud platform's notebook, use the <code>shaded jar</code>: Download sedona-spark-shaded jar and geotools-wrapper jar from Maven Central, and put them in SPARK_HOME/jars/ folder.</li> <li>If you run Sedona in an IDE or a local Jupyter notebook, use the <code>unshaded jar</code>. Call the Maven Central coordinate in your python program. For example, Sedona &gt;= 1.4.1</li> </ol> <pre><code>from sedona.spark import *\nconfig = SedonaContext.builder(). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all'). \\\n    getOrCreate()\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>SedonaRegistrator is deprecated in Sedona 1.4.1 and later versions. Please use the above method instead.</p> <pre><code>from pyspark.sql import SparkSession\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nspark = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    getOrCreate()\nSedonaRegistrator.registerAll(spark)\n</code></pre>"},{"location":"setup/install-python/#setup-environment-variables","title":"Setup environment variables","text":"<p>If you manually copy the sedona-spark-shaded jar to <code>SPARK_HOME/jars/</code> folder, you need to setup two environment variables</p> <ul> <li>SPARK_HOME. For example, run the command in your terminal</li> </ul> <pre><code>export SPARK_HOME=~/Downloads/spark-3.0.1-bin-hadoop2.7\n</code></pre> <ul> <li>PYTHONPATH. For example, run the command in your terminal</li> </ul> <pre><code>export PYTHONPATH=$SPARK_HOME/python\n</code></pre> <p>You can then play with Sedona Python Jupyter notebook.</p>"},{"location":"setup/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Spark cluster is ready.</p> <p>There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run Sedona.</p> <ul> <li>Spark interactive Scala or SQL shell: easy to start, good for new learners to try simple functions</li> <li>Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects</li> </ul>"},{"location":"setup/install-scala/#spark-scala-shell","title":"Spark Scala shell","text":""},{"location":"setup/install-scala/#download-sedona-jar-automatically","title":"Download Sedona jar automatically","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Run Spark shell with <code>--packages</code> option. This command will automatically download Sedona jars from Maven Central.</p> </li> </ol> <pre><code>./bin/spark-shell --packages MavenCoordinates\n</code></pre> <p>Please refer to Sedona Maven Central coordinates to select the corresponding Sedona packages for your Spark version.</p> <pre><code>* Local mode: test Sedona without setting up a cluster\n```\n./bin/spark-shell --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,org.datasyslab:geotools-wrapper:1.7.0-28.5\n```\n\n* Cluster mode: you need to specify Spark Master IP\n```\n./bin/spark-shell --master spark://localhost:7077 --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,org.datasyslab:geotools-wrapper:1.7.0-28.5\n```\n</code></pre>"},{"location":"setup/install-scala/#download-sedona-jar-manually","title":"Download Sedona jar manually","text":"<ol> <li> <p>Have your Spark cluster ready.</p> </li> <li> <p>Download Sedona jars:</p> <ul> <li>Download the pre-compiled jars from Sedona Releases</li> <li>Download / Git clone Sedona source code and compile the code by yourself (see Compile Sedona)</li> </ul> </li> <li>Run Spark shell with <code>--jars</code> option.</li> </ol> <pre><code>./bin/spark-shell --jars /Path/To/SedonaJars.jar\n</code></pre> <p>Please use jars with Spark major.minor versions in the filename, such as <code>sedona-spark-shaded-3.3_2.12-1.7.0</code>.</p> <pre><code>* Local mode: test Sedona without setting up a cluster\n```\n./bin/spark-shell --jars /path/to/sedona-spark-shaded-3.3_2.12-1.7.0.jar,/path/to/geotools-wrapper-1.7.0-28.5.jar\n```\n\n* Cluster mode: you need to specify Spark Master IP\n```\n./bin/spark-shell --master spark://localhost:7077 --jars /path/to/sedona-spark-shaded-3.3_2.12-1.7.0.jar,/path/to/geotools-wrapper-1.7.0-28.5.jar\n```\n</code></pre>"},{"location":"setup/install-scala/#spark-sql-shell","title":"Spark SQL shell","text":"<p>Please see Use Sedona in a pure SQL environment</p>"},{"location":"setup/install-scala/#self-contained-spark-projects","title":"Self-contained Spark projects","text":"<p>A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use Sedona in your self-contained Spark project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using SBT. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:</li> </ol> <pre><code>./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n</code></pre> <p>Note</p> <p>The detailed explanation of spark-submit is available on Spark website.</p>"},{"location":"setup/maven-coordinates/","title":"Maven Central coordinate","text":""},{"location":"setup/maven-coordinates/#maven-coordinates","title":"Maven Coordinates","text":""},{"location":"setup/maven-coordinates/#use-sedona-shaded-fat-jars","title":"Use Sedona shaded (fat) jars","text":"<p>Warning</p> <p>For Scala/Java/Python users, this is the most common way to use Sedona in your environment. Do not use separate Sedona jars unless you are sure that you do not need shaded jars.</p> <p>Warning</p> <p>For R users, this is the only way to use Sedona in your environment.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <p>Please use the artifact with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-shaded-3.4_2.12</code>.</p> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.3 and Scala 2.12Spark 3.4 and Scala 2.12Spark 3.5 and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.3_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.4_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.5_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.3 and Scala 2.13Spark 3.4 and Scala 2.13Spark 3.5 and Scala 2.13 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.3_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.4_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-shaded-3.5_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-flink-shaded_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Snowflake</p> Snowflake 7.0+ (Year 2023 and later) <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-snowflake&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#netcdf-java-542","title":"netCDF-Java 5.4.2","text":"<p>This is required only if you want to read HDF/NetCDF files using <code>RS_FromNetCDF</code>. Note that this JAR is not in Maven Central so you will need to add this repository to your pom.xml or build.sbt, or specify the URL in Spark Config <code>spark.jars.repositories</code> or spark-submit <code>--repositories</code> option.</p> <p>Warning</p> <p>This jar was a required dependency due to a bug in Sedona 1.5.1. You will need to specify the URL of the repository in <code>spark.jars.repositories</code> if you use 1.5.1. This has been fixed in Sedona 1.5.2 and later.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your pom.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n    &lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n    &lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n    &lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#use-sedona-unshaded-jars","title":"Use Sedona unshaded jars","text":"<p>Warning</p> <p>For Scala, Java, Python users, please use the following jars only if you satisfy these conditions: (1) you know how to exclude transient dependencies in a complex application. (2) your environment has internet access (3) you are using some sort of Maven package resolver, or pom.xml, or build.sbt. It usually directly takes an input like this <code>GroupID:ArtifactID:Version</code>. If you don't understand what we are talking about, the following jars are not for you.</p> <p>Apache Sedona provides different packages for each supported version of Spark.</p> <p>Please use the artifacts with Spark major.minor version in the artifact name. For example, for Spark 3.4, the artifacts to use should be <code>sedona-spark-3.4_2.12</code>.</p> <p>If you are using the Scala 2.13 builds of Spark, please use the corresponding packages for Scala 2.13, which are suffixed by <code>_2.13</code>.</p> <p>The optional GeoTools library is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars. The only purpose of this library is to bring GeoTools jars from OSGEO repository to Maven Central. This library is under GNU Lesser General Public License (LGPL) license, so we cannot package it in Sedona official release.</p> <p>Sedona with Apache Spark and Scala 2.12</p> Spark 3.3 and Scala 2.12Spark 3.4 and Scala 2.12Spark 3.5 and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.3_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.4_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.5_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Spark and Scala 2.13</p> Spark 3.3 and Scala 2.13Spark 3.4 and Scala 2.13Spark 3.5 and Scala 2.13 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.3_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.4_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-spark-3.5_2.13&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona with Apache Flink</p> Flink 1.12+ and Scala 2.12 <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.sedona&lt;/groupId&gt;\n  &lt;artifactId&gt;sedona-flink_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.7.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;geotools-wrapper&lt;/artifactId&gt;\n    &lt;version&gt;1.7.0-28.5&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Sedona Snowflake does not have an unshaded version.</p>"},{"location":"setup/maven-coordinates/#netcdf-java-542_1","title":"netCDF-Java 5.4.2","text":"<p>This is required only if you want to read HDF/NetCDF files using <code>RS_FromNetCDF</code>. Note that this JAR is not in Maven Central so you will need to add this repository to your pom.xml or build.sbt, or specify the URL in Spark Config <code>spark.jars.repositories</code> or spark-submit <code>--repositories</code> option.</p> <p>Under BSD 3-clause (compatible with Apache 2.0 license)</p> <p>Add HDF/NetCDF dependency</p> Sedona 1.3.1+Before Sedona 1.3.1 <p>Add unidata repo to your pom.xml</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;unidata-all&lt;/id&gt;\n        &lt;name&gt;Unidata All&lt;/name&gt;\n        &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>Then add cdm-core to your POM dependency.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;edu.ucar&lt;/groupId&gt;\n    &lt;artifactId&gt;cdm-core&lt;/artifactId&gt;\n    &lt;version&gt;5.4.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.datasyslab/sernetcdf --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.datasyslab&lt;/groupId&gt;\n    &lt;artifactId&gt;sernetcdf&lt;/artifactId&gt;\n    &lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"setup/maven-coordinates/#snapshot-versions","title":"SNAPSHOT versions","text":"<p>Sometimes Sedona has a SNAPSHOT version for the upcoming release. It follows the same naming conversion but has \"SNAPSHOT\" as suffix in the version. For example, <code>1.7.1-SNAPSHOT</code></p> <p>In order to download SNAPSHOTs, you need to add the following repositories in your pom.xml or build.sbt</p>"},{"location":"setup/maven-coordinates/#buildsbt","title":"build.sbt","text":"<p>resolvers +=   \"Apache Software Foundation Snapshots\" at \"https://repository.apache.org/content/groups/snapshots\"</p>"},{"location":"setup/maven-coordinates/#pomxml","title":"pom.xml","text":"<pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;snapshots-repo&lt;/id&gt;\n        &lt;url&gt;https://repository.apache.org/content/groups/snapshots&lt;/url&gt;\n        &lt;releases&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/releases&gt;\n        &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre>"},{"location":"setup/modules/","title":"Modules","text":""},{"location":"setup/modules/#sedona-modules-for-apache-spark","title":"Sedona modules for Apache Spark","text":"Name API Introduction spark RDD / SQL / DataFrame SpatialRDD and Spatial DataFrame spark-shaded Shaded version python Python interface for SpatialRDD and Spatial DataFrame Zeppelin Apache Zeppelin Plugin for Apache Zeppelin 0.8.1+"},{"location":"setup/modules/#api-availability","title":"API availability","text":"Core/RDD DataFrame/SQL Viz RDD/SQL Scala/Java \u2705 \u2705 \u2705 Python \u2705 \u2705 SQL only R \u2705 \u2705 \u2705"},{"location":"setup/overview/","title":"Overview","text":""},{"location":"setup/overview/#download-statistics","title":"Download statistics","text":"Download statistics Maven PyPI Conda-forge CRAN DockerHub Apache Sedona 225k/month Archived GeoSpark releases 10k/month"},{"location":"setup/overview/#what-can-sedona-do","title":"What can Sedona do?","text":""},{"location":"setup/overview/#distributed-spatial-datasets","title":"Distributed spatial datasets","text":"<ul> <li> Spatial RDD on Spark</li> <li> Spatial DataFrame/SQL on Spark</li> <li> Spatial DataStream on Flink</li> <li> Spatial Table/SQL on Flink</li> <li> Spatial SQL on Snowflake</li> </ul>"},{"location":"setup/overview/#complex-spatial-objects","title":"Complex spatial objects","text":"<ul> <li> Vector geometries / trajectories</li> <li> Raster images with Map Algebra</li> <li> Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, ArcGrid, NetCDF/HDF</li> </ul>"},{"location":"setup/overview/#distributed-spatial-queries","title":"Distributed spatial queries","text":"<ul> <li> Spatial query: range query, range join query, distance join query, K Nearest Neighbor query</li> <li> Spatial index: R-Tree, Quad-Tree</li> </ul>"},{"location":"setup/overview/#rich-spatial-analytics-tools","title":"Rich spatial analytics tools","text":"<ul> <li> Coordinate Reference System / Spatial Reference System Transformation</li> <li> Apache Zeppelin dashboard integration</li> <li> Integrate with a variety of Python tools including Jupyter notebook, GeoPandas, Shapely</li> <li> Integrate with a variety of visualization tools including KeplerGL, DeckGL</li> <li> High resolution and scalable map generation: Visualize Spatial DataFrame/RDD</li> <li> Support Scala, Java, Python, R</li> </ul>"},{"location":"setup/platform/","title":"Language wrappers","text":"<p>Sedona binary releases are compiled by Java 1.8 and Scala 2.11/2.12 and tested in the following environments:</p> <p>Warning</p> <p>Support of Spark 3.0, 3.1, 3.2 was removed in Sedona 1.7.0+ although some parts of the source code might still be compatible.</p> Sedona Scala/JavaSedona PythonSedona R Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Spark 3.5 Scala 2.11 not tested not tested not tested not tested not tested not tested Scala 2.12 not tested not tested not tested \u2705 \u2705 \u2705 Scala 2.13 not tested not tested not tested \u2705 \u2705 \u2705 Spark 3.0 (Scala 2.12) Spark 3.1 (Scala 2.12) Spark 3.2 (Scala 2.12) Spark 3.3 (Scala 2.12) Spark 3.4 (Scala 2.12) Spark 3.5 (Scala 2.12) Python 3.7 not tested not tested not tested \u2705 \u2705 \u2705 Python 3.8 not tested not tested not tested \u2705 \u2705 \u2705 Python 3.9 not tested not tested not tested \u2705 \u2705 \u2705 Python 3.10 not tested not tested not tested \u2705 \u2705 \u2705 Spark 3.0 Spark 3.1 Spark 3.2 Spark 3.3 Spark 3.4 Spark 3.5 Scala 2.11 not tested not tested not tested not tested not tested not tested Scala 2.12 not tested not tested not tested \u2705 \u2705 \u2705"},{"location":"setup/release-notes/","title":"Release notes","text":""},{"location":"setup/release-notes/#sedona-170","title":"Sedona 1.7.0","text":"<p>Sedona 1.7.0 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a major release that includes new features, improvements, bug fixes, API breaking changes, and behavior changes.</p>"},{"location":"setup/release-notes/#new-contributors","title":"New Contributors","text":"<ul> <li>@mvaaltola made their first contribution in https://github.com/apache/sedona/pull/1574</li> <li>@emmanuel-ferdman made their first contribution in https://github.com/apache/sedona/pull/1658</li> <li>@MohammadLotfiA made their first contribution in https://github.com/apache/sedona/pull/1659</li> <li>@golfalot made their first contribution in https://github.com/apache/sedona/pull/1673</li> <li>@AmirTallap made their first contribution in https://github.com/apache/sedona/pull/1675</li> <li>@freamdx made their first contribution in https://github.com/apache/sedona/pull/1704</li> </ul>"},{"location":"setup/release-notes/#highlights","title":"Highlights","text":"<ul> <li> Add a new join algorithm for distributed K Nearest Neighbor Join and a corresponding ST_KNN function</li> <li> Add new spatial statistics algorithms DBSCAN, Local Outlier Factor, and Getis Ord Hot Spot Analysis</li> <li> Add new DataFrame based readers for Shapefile, and GeoPackage</li> <li> Add 10 new ST functions</li> </ul>"},{"location":"setup/release-notes/#api-breaking-changes","title":"API breaking changes","text":"<ul> <li> The support of Spark 3.0, 3.1, 3.2 is dropped. Sedona is now only compatible with Spark 3.3, 3.4, and 3.5.</li> <li> Rasterio is no longer a mandatory dependency. You can still use Sedona Raster without rasterio. If you need to write rasterio UDF in Sedona, you can install it separately.</li> </ul>"},{"location":"setup/release-notes/#behavior-changes","title":"Behavior changes","text":"<ul> <li> JTS version is upgraded to 1.20.0. This may cause some behavior changes in ST functions that rely on JTS.</li> <li> ST_Length, ST_Length2D and ST_LengthSpheroid now only return the length for line objects. It now returns 0 for polygon objects.</li> <li> ST_Perimeter now only returns the perimeter for polygon objects. It now returns 0 for line objects.</li> </ul>"},{"location":"setup/release-notes/#bug","title":"Bug","text":"<ul> <li>[SEDONA-650] -         Fiona-Geopandas Compatibility Issue in Python 3.8 </li> <li>[SEDONA-665] -         Docker build failed at ubuntu 22 with rasterio 1.4.0+ </li> <li>[SEDONA-669] -         GeoParquet format should handle timestamp_ntz columns properly </li> <li>[SEDONA-670] -         GeoJSON reader does not work properly on DBR </li> <li>[SEDONA-672] -         Bug fix for ST_LengthSpheroid </li> <li>[SEDONA-673] -         Cannot load GeoParquet without bbox metadata when spatial filter is applied </li> <li>[SEDONA-677] -         Kryo deserialization for null envelopes results in unit envelopes </li> <li>[SEDONA-682] -         Sedona Spark 3.3 does not compile on Scala 2.13 </li> </ul>"},{"location":"setup/release-notes/#new-feature","title":"New Feature","text":"<ul> <li>[SEDONA-646] -         Shapefile data source for DataFrame API </li> <li>[SEDONA-647] -         Add ST_RemoveRepeatedPoints </li> <li>[SEDONA-648] -         Implement Distributed K Nearest Neighbor Join </li> <li>[SEDONA-652] -         Add ST_MakeEnvelope </li> <li>[SEDONA-654] -         Add ST_RotateY </li> <li>[SEDONA-655] -         DBSCAN </li> <li>[SEDONA-656] -         Add ST_Project </li> <li>[SEDONA-658] -         Add ST_Simplify </li> <li>[SEDONA-659] -         Upgrade jts version to 1.20.0 </li> <li>[SEDONA-661] -         Local Outlier Factor </li> <li>[SEDONA-664] -         Add native GeoPackage reader </li> <li>[SEDONA-666] -         Add ST_Scale and ST_ScaleGeom </li> <li>[SEDONA-667] -         Getis Ord G Local </li> <li>[SEDONA-671] -         Spider random spatial data generator </li> <li>[SEDONA-675] -         Add ST_InterpolatePoint </li> <li>[SEDONA-676] -         Add ST_Perimeter </li> </ul>"},{"location":"setup/release-notes/#improvement","title":"Improvement","text":"<ul> <li>[SEDONA-636] -         datatype geometry is not supported when 'create table xxx (geom geometry) </li> <li>[SEDONA-640] -         Refactor support for multiple spark versions in the build </li> <li>[SEDONA-642] -         R \u2013 Adapt R package for split version of jars </li> <li>[SEDONA-644] -         R \u2013 Update for SedonaContext </li> <li>[SEDONA-649] -         Fix spelling in Java files </li> <li>[SEDONA-653] -         Add lenient mode for RS_Clip </li> <li>[SEDONA-663] -         Support spark connect in dataframe api </li> <li>[SEDONA-678] -         Fix ST_Length and ST_Length2D behavior </li> <li>[SEDONA-679] -         Fix ST_LengthSpheroid behavior </li> </ul>"},{"location":"setup/release-notes/#task","title":"Task","text":"<ul> <li>[SEDONA-651] -         Add spark prefix to all sedona spark config </li> <li>[SEDONA-662] -         Clean Up Dead Code from DBSCAN </li> <li>[SEDONA-668] -         Drop the support of Spark 3.0, 3.1, 3.2 </li> <li>[SEDONA-674] -         Make the rasterio binding for sedona-python work with GDAL 3.10 </li> <li>[SEDONA-680] -         Remove rasterio from mandatory dependency </li> <li>[SEDONA-681] -         Bump GeoTools version from 28.2 to 28.5 </li> <li>[SEDONA-683] -         Exclude some repetitive dependencies </li> </ul>"},{"location":"setup/release-notes/#sedona-161","title":"Sedona 1.6.1","text":"<p>Sedona 1.6.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes bug fixes and minor improvements.</p>"},{"location":"setup/release-notes/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@zhangfengcdt made their first contribution in https://github.com/apache/sedona/pull/1431</li> <li>@james-willis made their first contribution in https://github.com/apache/sedona/pull/1453</li> </ul>"},{"location":"setup/release-notes/#highlights_1","title":"Highlights","text":"<ul> <li> Add native DataFrame based GeoJSON reader and writer</li> <li> 48 new ST functions added</li> <li> GeoParquet reader and writer supports GeoParquet 1.1.0 covering column</li> <li> Improve the error handling of ST functions so that the error message includes the geometry that caused the error</li> </ul>"},{"location":"setup/release-notes/#api-breaking-changes_1","title":"API breaking changes","text":"<ul> <li> The following raster functions now return struct type outputs instead of array types.</li> <li>RS_Metadata</li> <li>RS_SummaryStatsAll</li> <li>RS_ZonalStatsAll</li> <li>RS_GeoTransform</li> </ul>"},{"location":"setup/release-notes/#bug_1","title":"Bug","text":"<ul> <li>[SEDONA-560] -         Spatial join involving dataframe containing 0 partition throws exception </li> <li>[SEDONA-561] -         Failed to run examples in the core.showcase package </li> <li>[SEDONA-580] -         New instances of RasterUDT object is not equal to the RasterUDT case object </li> <li>[SEDONA-581] -         SedonaKepler fails to reload if a raster column exists </li> <li>[SEDONA-605] -         RS_AsRaster(useGeometryExtent=false) does not work with reference rasters with scaleX/Y &lt; 1 </li> <li>[SEDONA-608] -         Fix ST_IsPolygonCW, ST_IsPolygonCCW, ST_ForcePolygonCW and ST_ForcePolygonCCW </li> <li>[SEDONA-609] -         Fix python 3.12 build issue caused by binary compatibility issues with numpy 2.0.0 </li> <li>[SEDONA-611] -         Cannot write rasters to S3 on EMR </li> <li>[SEDONA-618] -         Maven build failed with javadoc classes and package list files missing </li> <li>[SEDONA-624] -         Distance join throws java.lang.reflect.InvocationTargetException when working with aggregation functions </li> <li>[SEDONA-626] -         SRID of geometries returned by many ST functions are incorrect </li> <li>[SEDONA-628] -         Python DataFrame Functions Cannot Be Imported As Documented </li> <li>[SEDONA-639] -         ST_Split may produce inaccurate results when splitting linestrings </li> </ul>"},{"location":"setup/release-notes/#new-feature_1","title":"New Feature","text":"<ul> <li>[SEDONA-462] -         ST_IsValidDetail </li> <li>[SEDONA-486] -         Implement ST_MMin </li> <li>[SEDONA-487] -         Implement ST_MMax </li> <li>[SEDONA-562] -         Add native DataFrame based GeoJSON reader and writer </li> <li>[SEDONA-563] -         Add ST_GeomFromEWKB </li> <li>[SEDONA-564] -         Add ST_NumInteriorRing </li> <li>[SEDONA-565] -         Add ST_ForceRHR </li> <li>[SEDONA-566] -         Add ST_TriangulatePolygon </li> <li>[SEDONA-567] -         Add ST_M </li> <li>[SEDONA-569] -         Add ST_PointZM </li> <li>[SEDONA-570] -         Add ST_PointM </li> <li>[SEDONA-571] -         Add ST_MMin </li> <li>[SEDONA-572] -         Add ST_PointFromWKB </li> <li>[SEDONA-573] -         Add ST_HasM </li> <li>[SEDONA-574] -         Add ST_MMax </li> <li>[SEDONA-575] -         Add ST_LineFromWKB </li> <li>[SEDONA-576] -         Add ST_HasZ </li> <li>[SEDONA-577] -         Add ST_GeometryFromText </li> <li>[SEDONA-578] -         Add ST_Points </li> <li>[SEDONA-579] -         Add ST_AsHEXEWKB </li> <li>[SEDONA-582] -         Add ST_PointFromGeoHash </li> <li>[SEDONA-583] -         Add ST_Length2D </li> <li>[SEDONA-584] -         Add ST_Zmflag </li> <li>[SEDONA-585] -         Add ST_ForceCollection </li> <li>[SEDONA-586] -         Add ST_Force3DZ </li> <li>[SEDONA-587] -         Add ST_Force3DM </li> <li>[SEDONA-588] -         Add ST_Force4D </li> <li>[SEDONA-589] -         Add ST_LongestLine </li> <li>[SEDONA-590] -         Add ST_GeomColFromText </li> <li>[SEDONA-591] -         Add ST_MaxDistance </li> <li>[SEDONA-592] -         Add ST_MPointFromText </li> <li>[SEDONA-593] -         Add ST_Relate </li> <li>[SEDONA-594] -         Add ST_RelatedMatch </li> <li>[SEDONA-595] -         Add ST_LineStringFromWKB </li> <li>[SEDONA-596] -         Add ST_SimplifyVW </li> <li>[SEDONA-597] -         Add ST_SimplifyPolygonHull </li> <li>[SEDONA-598] -         Add ST_UnaryUnion </li> <li>[SEDONA-599] -         Add ST_MinimumClearance </li> <li>[SEDONA-600] -         Add ST_MinimumClearanceLine </li> <li>[SEDONA-601] -         Add ST_DelaunyTriangles </li> <li>[SEDONA-602] -         Add ST_LocateAlong </li> <li>[SEDONA-603] -         Add ST_MakePointM </li> <li>[SEDONA-604] -         Add ST_AddMeasure </li> <li>[SEDONA-606] -         Add ST_IsValidDetail </li> <li>[SEDONA-607] -         Include Geometry in ST Function Exceptions </li> <li>[SEDONA-610] -         Add ST_IsValidTrajectory </li> <li>[SEDONA-615] -         Add ST_MaximumInscribedCircle </li> <li>[SEDONA-617] -         Add ST_Rotate </li> <li>[SEDONA-625] -         Add ST_GeneratePoints </li> <li>[SEDONA-627] -         Writing covering column metadata to GeoParquet files </li> <li>[SEDONA-631] -         Add ST_Expand </li> <li>[SEDONA-643] -         Fix Flink constructor functions signatures </li> <li>[SEDONA-645] -         Add ST_RotateX </li> </ul>"},{"location":"setup/release-notes/#improvement_1","title":"Improvement","text":"<ul> <li>[SEDONA-558] -         Fix and improve SedonaPyDeck behavior </li> <li>[SEDONA-559] -         Make the flink example work </li> <li>[SEDONA-568] -         Refactor TestBaseScala to use method instead of a class-level variable for sparkSession </li> <li>[SEDONA-616] -         Apply spotless to snowflake module </li> <li>[SEDONA-620] -         Simplify Java if statements </li> <li>[SEDONA-621] -         Remove redundant call to `toString()` </li> <li>[SEDONA-622] -         Improve SedonaPyDeck behavior </li> <li>[SEDONA-623] -         Simplify Java `if` statements </li> <li>[SEDONA-629] -         Return Structs for RS_ Functions </li> <li>[SEDONA-632] -         Don't use a conventional output committer when writing raster files using df.write.format(\"raster\") </li> <li>[SEDONA-633] -         Add tileWidth and tileHeight fields to the result of RS_Metadata </li> <li>[SEDONA-634] -         Support omitting tileWidth and tileHeight parameters when calling RS_Tile or RS_TileExplode on rasters with decent tiling scheme </li> <li>[SEDONA-635] -         Allow feature and feature collection format in ST_AsGeoJSON </li> <li>[SEDONA-637] -         Show spatial filters pushed to GeoParquet scans in the query plan </li> <li>[SEDONA-638] -         Send telemetry data asynchronously to avoid blocking the initialization of SedonaContext </li> </ul>"},{"location":"setup/release-notes/#task_1","title":"Task","text":"<ul> <li>[SEDONA-101] -         Add Scala Formatter to MVN </li> <li>[SEDONA-102] -         Java Code Formatting using formatter plugin </li> <li>[SEDONA-553] -         Update Sedona docker to use newer GeoPandas </li> </ul>"},{"location":"setup/release-notes/#sedona-160","title":"Sedona 1.6.0","text":"<p>Sedona 1.6.0 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.19, Snowflake 7+, Java 8.</p>"},{"location":"setup/release-notes/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@mpetazzoni made their first contribution in https://github.com/apache/sedona/pull/1216</li> <li>@sebdiem made their first contribution in https://github.com/apache/sedona/pull/1217</li> <li>@guilhem-dvr made their first contribution in https://github.com/apache/sedona/pull/1229</li> <li>@niklas-petersen made their first contribution in https://github.com/apache/sedona/pull/1252</li> <li>@mebrein made their first contribution in https://github.com/apache/sedona/pull/1334</li> <li>@docete made their first contribution in https://github.com/apache/sedona/pull/1409</li> </ul>"},{"location":"setup/release-notes/#highlights_2","title":"Highlights","text":"<ul> <li> Sedona is now compatible with Shapely 2.0 and GeoPandas 0.11.1+.</li> <li> Sedona added enhanced support for geography data. This includes<ul> <li>ST_Buffer with spheroid distance</li> <li>ST_BestSRID to find the best SRID for a geometry</li> <li>ST_ShiftLongitude to shift the longitude of a geometry to mitigate the issue of crossing the date line</li> <li>ST_CrossesDateLine to check if a geometry crosses the date line</li> <li>ST_DWithin now supports spheroid distance</li> </ul> </li> <li> Sedona Spark Sedona Raster allows RS_ReropjectMatch to wrap the extent of one raster to another raster, similar to RasterArray.reproject_match function in rioxarray</li> <li> Sedona Spark Sedona Raster now supports Rasterio and NumPy UDF by <code>raster.as_numpy</code>, <code>raster.as_numpy_masked</code>, <code>raster.as_rasterio</code>. You can perform any native function from rasterio and numpy and run them in parallel. See the example below.</li> </ul> <pre><code>from pyspark.sql.types import DoubleType\n\ndef mean_udf(raster):\n    return float(raster.as_numpy().mean())\n\nsedona.udf.register(\"mean_udf\", mean_udf, DoubleType())\ndf_raster.withColumn(\"mean\", expr(\"mean_udf(rast)\")).show()\n</code></pre>"},{"location":"setup/release-notes/#bug_2","title":"Bug","text":"<ul> <li>[SEDONA-532] - Sedona Spark SQL optimizer cannot optimize joins with complex conditions </li> <li>[SEDONA-543] - RS_Union_aggr gives referenceRaster is null error when run on cluster </li> </ul>"},{"location":"setup/release-notes/#new-feature_2","title":"New Feature","text":"<ul> <li>[SEDONA-467] - Add optimized join support for ST_DWithin </li> <li>[SEDONA-468] - Add provision to use spheroid distance in ST_DWithin </li> <li>[SEDONA-475] - Add RS_NormalizeAll to normalize all bands of a raster </li> <li>[SEDONA-480] - Implement ST_S2ToGeom </li> <li>[SEDONA-481] - Implements ST_Snap </li> <li>[SEDONA-484] - Implement ST_IsPolygonCW </li> <li>[SEDONA-488] - ST_Buffer with spheroid distance </li> <li>[SEDONA-498] - Add ST_BestSRID </li> <li>[SEDONA-499] - Add Spheroidal ST_Buffer </li> <li>[SEDONA-504] - Add ST_ShiftLongitude </li> <li>[SEDONA-508] - Add ST_CrossesDateLine </li> <li>[SEDONA-509] - Add Single Statistic RS_SummaryStats </li> <li>[SEDONA-514] - Add RS_SetPixelType </li> <li>[SEDONA-516] - Add RS_Interpolate </li> <li>[SEDONA-517] - Add RS_MakeRaster for constructing a new raster using given array data as band data </li> <li>[SEDONA-518] - Add RS_ReprojectMatch for wrapping the extent of one raster to another raster </li> <li>[SEDONA-522] - Add ST_Union with array of Geometry as input </li> <li>[SEDONA-533] - Implement ST_Polygonize </li> <li>[SEDONA-539] - Support Snowflake geography type </li> </ul>"},{"location":"setup/release-notes/#improvement_2","title":"Improvement","text":"<ul> <li>[SEDONA-483] - Implements ST_IsPolygonCCW </li> <li>[SEDONA-493] - Update default behavior of RS_NormalizeAll </li> <li>[SEDONA-503] - Support Shapely 2.0 in PySpark binding </li> <li>[SEDONA-521] - Change ST_H3ToGeom Behavior </li> <li>[SEDONA-549] - RS_Union_aggr should support combining all bands in multi-band rasters </li> </ul>"},{"location":"setup/release-notes/#task_2","title":"Task","text":"<ul> <li>[SEDONA-540] - Fix failed ST_Buffer and ST_Snap Snowflake tests </li> <li>[SEDONA-550] - Remove the version upper bound of Pandas, GeoPandas </li> <li>[SEDONA-557] - Bump Flink from 1.14.x to 1.19.0"},{"location":"setup/release-notes/#sedona-153","title":"Sedona 1.5.3","text":"<p>Sedona 1.5.3 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes one bug fix on top of Sedona 1.5.2. No new features or major changes are added in this release.</p>"},{"location":"setup/release-notes/#bug_3","title":"Bug","text":"<ul> <li>[SEDONA-556] - Hidden requirement for geopandas in apache-sedona 1.5.2 </li> <li>[SEDONA-555] - Snowflake Native App should not always create a new role </li> </ul>"},{"location":"setup/release-notes/#sedona-152","title":"Sedona 1.5.2","text":"<p>Sedona 1.5.2 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p> <p>This release is a maintenance release that includes bug fixes and minor improvements. No new features or major changes are added in this release.</p>"},{"location":"setup/release-notes/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@mpetazzoni made their first contribution in https://github.com/apache/sedona/pull/1216</li> <li>@sebdiem made their first contribution in https://github.com/apache/sedona/pull/1217</li> <li>@guilhem-dvr made their first contribution in https://github.com/apache/sedona/pull/1229</li> <li>@niklas-petersen made their first contribution in https://github.com/apache/sedona/pull/1252</li> <li>@mebrein made their first contribution in https://github.com/apache/sedona/pull/1334</li> </ul>"},{"location":"setup/release-notes/#bug_4","title":"Bug","text":"<ul> <li>[SEDONA-470] - Cannot distinguish between missing or null crs from the result of geoparquet.metadata </li> <li>[SEDONA-471] - SedonaKepler cannot work with Uber H3 hex since 1.5.1 </li> <li>[SEDONA-472] - Adapter API no longer works with unshaded jar </li> <li>[SEDONA-473] - cdm-core mistakenly becomes a compile dependency for sedona-spark-shaded </li> <li>[SEDONA-477] - Avoid producing rasters with images having non-zero origins </li> <li>[SEDONA-478] - Sedona 1.5.1 context initialization fails without GeoTools coverage </li> <li>[SEDONA-479] - Fix RS_Normalize: Incorrect behavior for double arrays </li> <li>[SEDONA-494] - Raster data source cannot write to HDFS </li> <li>[SEDONA-495] - Raster data source uses shared FileSystem connections which lead to race condition </li> <li>[SEDONA-497] - SpatialRDD read from multiple Shapefiles has incorrect fieldName property </li> <li>[SEDONA-500] - Cannot correctly read data from directories containing multiple shapefiles </li> <li>[SEDONA-501] - ST_Split maps to wrong Java-call </li> <li>[SEDONA-505] - Treat geometry with SRID=0 as if it was in EPSG:4326 in various raster functions </li> <li>[SEDONA-507] - RS_AsImage cannot visualize rasters with non-integral band data </li> <li>[SEDONA-510] - geometry columns with snake_case names in GeoParquet files cannot be recognized as geometry column </li> <li>[SEDONA-511] - geometry columns with snake_case names in GeoParquet files cannot be recognized as geometry column </li> <li>[SEDONA-519] - ST_SubDivide (Snowflake) fails even on documentation example </li> <li>[SEDONA-520] - Missing dependencies in Snowflake JAR </li> <li>[SEDONA-531] - RDD spatial join in Python throws Not available error </li> <li>[SEDONA-534] - Disable Python warning message of finding jars </li> <li>[SEDONA-545] - Sedona Python DataFrame API fail due to missing commas </li> <li>[SEDONA-548] - Fix Python Dataframe API Constructor registrations </li> </ul>"},{"location":"setup/release-notes/#improvement_3","title":"Improvement","text":"<ul> <li>[SEDONA-474] - Remove manipulation of warnings config </li> <li>[SEDONA-506] - Add lenient mode for RS_ZonalStats and RS_ZonalStatsAll </li> <li>[SEDONA-512] - Python serializer should report the object type in the error message </li> <li>[SEDONA-515] - Add handling for noDataValues in RS_Resample </li> <li>[SEDONA-529] - Add basic `EditorConfig` file </li> <li>[SEDONA-535] - Add the pull request labeler </li> <li>[SEDONA-536] - Add CODEOWNERS file </li> <li>[SEDONA-541] - Allow concurrent snowflake testers </li> </ul>"},{"location":"setup/release-notes/#test","title":"Test","text":"<ul> <li>[SEDONA-513] - Add pre-commit hook `mixed-line-ending` </li> <li>[SEDONA-523] - Add pre-commit hook `fix-byte-order-marker` </li> <li>[SEDONA-524] - Clean up the `pre-commit` config </li> <li>[SEDONA-525] - Add two more pre-commit hooks </li> <li>[SEDONA-528] - Add `pre-commit` hook `check-yaml` </li> <li>[SEDONA-530] - Add `pre-commit` hook `debug-statements` </li> <li>[SEDONA-537] - Add pre-commit hook `requirements-txt-fixer` </li> <li>[SEDONA-538] - Add four more pre-commit hooks </li> <li>[SEDONA-542] - Add `pre-commit` hook `check-executables-have-shebangs` </li> <li>[SEDONA-544] - Add `ruff-pre-commit` for `Python` linting </li> <li>[SEDONA-546] - Python linting enable rule `E712` </li> </ul>"},{"location":"setup/release-notes/#task_3","title":"Task","text":"<ul> <li>[SEDONA-469] - Update Sedona docker and binder to use 1.5.1 </li> <li>[SEDONA-496] - Dependabot: reduce the open pull requests limit to 2 </li> <li>[SEDONA-526] - Upgrade `actions/setup-java` to `v4` </li> </ul>"},{"location":"setup/release-notes/#sedona-151","title":"Sedona 1.5.1","text":"<p>Sedona 1.5.1 is compiled against Spark 3.3 / Spark 3.4 / Spark 3.5, Flink 1.12, Snowflake 7+, Java 8.</p>"},{"location":"setup/release-notes/#highlights_3","title":"Highlights","text":"<ul> <li> Sedona Snowflake Add support for Snowflake</li> <li> Sedona Spark Support Spark 3.5</li> <li> Sedona Spark Support Snowflake 7+</li> <li> Sedona Spark Added 20+ raster functions (or variants)</li> <li> Sedona Spark/Flink/Snowflake Added 7 vector functions (or variants)</li> <li> Sedona Spark GeoParquet reader and writer supports projjson in metadata</li> <li> Sedona Spark GeoParquet reader and writer conform to GeoParquet spec 1.0.0 instead of 1.0.0-beta1</li> <li> Sedona Spark Added a legacyMode in GeoParquet reader for 1.5.1+ users to read Parquet files written by Sedona 1.3.1 and earlier</li> <li> Sedona Spark Fixed a bug in GeoParquet writer so 1.3.1 and earlier users can read Parquet files written by 1.5.1+</li> </ul>"},{"location":"setup/release-notes/#behavior-change","title":"Behavior change","text":"<ul> <li>All raster functions that take a geometry will implicitly transform the CRS of the geometry if needed.</li> <li>The default CRS for these functions is 4326 for raster and geometry involved in raster functions, if not specified.</li> <li>KeplerGL and DeckGL become optional dependencies for Sedona Spark Python.</li> </ul>"},{"location":"setup/release-notes/#new-contributors_4","title":"New Contributors","text":"<ul> <li>@hongbo-miao made their first contribution in https://github.com/apache/sedona/pull/1063</li> <li>@prantogg made their first contribution in https://github.com/apache/sedona/pull/1122</li> <li>@MyEnthusiastic made their first contribution in https://github.com/apache/sedona/pull/1130</li> <li>@duhaode520 made their first contribution in https://github.com/apache/sedona/pull/1193</li> </ul>"},{"location":"setup/release-notes/#bug_5","title":"Bug","text":"<ul> <li>[SEDONA-414] - ST_MakeLine in sedona-spark does not work with array inputs </li> <li>[SEDONA-417] - Fix SedonaUtils.display_image </li> <li>[SEDONA-419] - SedonaKepler and SedonaPyDeck should not be in `sedona.spark` </li> <li>[SEDONA-420] - Make SedonaKepler and SedonaPydeck optional dependencies </li> <li>[SEDONA-424] - Specify jt-jiffle as a provided dependency </li> <li>[SEDONA-426] - Change cloning of rasters to be able to include metadata. </li> <li>[SEDONA-440] - GeoParquet reader should support filter pushdown on nested fields </li> <li>[SEDONA-443] - Upload-artifact leads to 503 error </li> <li>[SEDONA-453] - Performance degrade when indexing points using Quadtree </li> <li>[SEDONA-456] - SedonaKepler cannot work with geopandas &gt;= 0.13.0 correctly </li> </ul>"},{"location":"setup/release-notes/#new-feature_3","title":"New Feature","text":"<ul> <li>[SEDONA-369] - Add ST_DWITHIN </li> <li>[SEDONA-411] - Add RS_Rotation </li> <li>[SEDONA-413] - Add buffer parameters to ST_Buffer </li> <li>[SEDONA-415] - Add optional parameter to ST_Transform </li> <li>[SEDONA-421] - Add RS_Clip </li> <li>[SEDONA-422] - Add a feature in RS_SetBandNoDataValue and fix NoDataValue in RS_Clip </li> <li>[SEDONA-427] - Add RS_RasterToWorldCoord </li> <li>[SEDONA-428] - Add RS_ZonalStats &amp; RS_ZonalStatsAll </li> <li>[SEDONA-430] - geoparquet writer should have an option called `writeToCrs` </li> <li>[SEDONA-431] - Add RS_PixelAsPoints </li> <li>[SEDONA-432] - Add RS_PixelAsCentroids </li> <li>[SEDONA-433] - Improve RS_SummaryStats performance </li> <li>[SEDONA-435] - Add RS_PixelAsPolygons </li> <li>[SEDONA-438] - Add NetCDF reader to Sedona </li> <li>[SEDONA-439] - Add RS_Union_Aggr </li> <li>[SEDONA-441] - Implement ST_LineLocatePoint </li> <li>[SEDONA-449] - Add two raster column support to RS_MapAlgebra </li> <li>[SEDONA-455] - Add a new data source namely geoparquet.metadata </li> <li>[SEDONA-459] - Add Snowflake support </li> <li>[SEDONA-460] - RS_Tile and RS_TileExplode </li> <li>[SEDONA-461] - ST_IsValidReason </li> <li>[SEDONA-465] - Support reading legacy parquet files written by Apache Sedona &lt;= 1.3.1-incubating </li> </ul>"},{"location":"setup/release-notes/#improvement_4","title":"Improvement","text":"<ul> <li>[SEDONA-339] - Skip irrelevant GitHub actions </li> <li>[SEDONA-416] - importing SedonaContext, kepler.gl is not found. </li> <li>[SEDONA-429] - geoparquet reader/writer should print \"1.0.0\" in its version </li> <li>[SEDONA-434] - Improve reliability by resolve the nondeterministic of the order of the Map </li> <li>[SEDONA-436] - Fix RS_SetValues bug </li> <li>[SEDONA-437] - Add implicit CRS transformation </li> <li>[SEDONA-446] - Add floating point datatype support in RS_AsBase64 </li> <li>[SEDONA-448] - RS_SetBandNoDataValue should have `replace` option </li> <li>[SEDONA-454] - Change the default value of sedona.global.indextype from quadtree to rtree </li> <li>[SEDONA-457] - Don't write GeometryUDT into org.apache.spark.sql.parquet.row.metadata when writing GeoParquet files </li> <li>[SEDONA-464] - ST_Valid should have integer flags </li> <li>[SEDONA-466] - RS_AsRaster does not use the weight and height of the raster in its parameters. </li> </ul>"},{"location":"setup/release-notes/#test_1","title":"Test","text":"<ul> <li>[SEDONA-410] - pre-commit: check that scripts with shebangs are executable </li> <li>[SEDONA-412] - pre-commit: add hook `end-of-file-fixer` </li> <li>[SEDONA-423] - pre-commit: apply hook `end-of-file-fixer` to more files </li> <li>[SEDONA-442] - pre-commit: add hook markdown-lint </li> <li>[SEDONA-444] - pre-commit: add hook to trim trailing whitespace </li> <li>[SEDONA-445] - pre-commit: apply hook end-of-file-fixer to more files </li> <li>[SEDONA-447] - pre-commit: apply end-of-file-fixer to more files </li> <li>[SEDONA-463] - Add a Makefile for convenience </li> </ul>"},{"location":"setup/release-notes/#task_4","title":"Task","text":"<ul> <li>[SEDONA-450] - Support Spark 3.5 </li> <li>[SEDONA-458] - The docs should have examples for UDF </li> </ul>"},{"location":"setup/release-notes/#sedona-150","title":"Sedona 1.5.0","text":"<p>Sedona 1.5.0 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_4","title":"Highlights","text":"<p>API breaking changes:</p> <ul> <li>The following functions in Sedona requires the input data must be in longitude/latitude order otherwise they might throw errors. You can use <code>FlipCoordinates</code> to swap X and Y.<ul> <li>ST_Transform</li> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_GeoHash</li> <li>All ST_H3 functions</li> <li>All ST_S2 functions</li> <li>All RS constructors</li> <li>All RS predicates</li> <li>Spark RDD: CRStransform</li> </ul> </li> <li>Rename <code>RS_Count</code> to <code>RS_CountValue</code></li> <li>Drop <code>RS_HTML</code></li> <li>Unshaded Sedona Spark code are all merged to a single jar <code>sedona-spark</code></li> </ul> <p>New features</p> <ul> <li>Add 18 more ST functions for vector data processing in Sedona Spark and Sedona Flink</li> <li>Add 36 more RS functions in Sedona Spark to support comprehensive raster data ETL and analytics<ul> <li>You can now directly join vector and raster datasets together</li> <li>Flexible map algebra equations: <code>SELECT RS_MapAlgebra(rast, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table</code></li> </ul> </li> <li>Add native support of Uber H3 functions in Sedona Spark and Sedona Flink.</li> <li>Add SedonaKepler and SedonaPyDeck for interactive map visualization on Sedona Spark.</li> </ul>"},{"location":"setup/release-notes/#bug_6","title":"Bug","text":"<ul> <li>[SEDONA-318] - SerDe for RasterUDT performs poorly </li> <li>[SEDONA-319] - RS_AddBandFromArray does not always produce serializable rasters </li> <li>[SEDONA-322] - The \"Scala and Java build\" CI job occasionally fail </li> <li>[SEDONA-325] - RS_FromGeoTiff is leaking file descriptors </li> <li>[SEDONA-329] - Remove geometry_col parameter from SedonaKepler APIs </li> <li>[SEDONA-330] - Fix bugs in SedonaPyDeck </li> <li>[SEDONA-332] - RS_Value and RS_Values don't need to fetch all the pixel data </li> <li>[SEDONA-337] - Failure falling back to pure python implementation when geomserde_speedup is unavailable </li> <li>[SEDONA-338] - Refactor Raster construction in sedona to use AffineTransform instead of envelope </li> <li>[SEDONA-358] - Refactor Functions to remove geotools dependency for most vector functions </li> <li>[SEDONA-362] - RS_BandAsArray truncates the decimal part of float/double pixel values. </li> <li>[SEDONA-373] - Move RasterPredicates to correct raster package to prevent redundant imports </li> <li>[SEDONA-394] - fix RS_Band data type bug </li> <li>[SEDONA-401] - Handle null values in RS_AsMatrix </li> <li>[SEDONA-402] - Floor grid coordinates received from geotools </li> <li>[SEDONA-403] - Add Null tolerance to RS_AddBandFromArray </li> <li>[SEDONA-405] - Sedona driver Out of Memory on 1.4.1 </li> </ul>"},{"location":"setup/release-notes/#new-feature_4","title":"New Feature","text":"<ul> <li>[SEDONA-200] - Add ST_CoordDim to Sedona </li> <li>[SEDONA-213] - Add ST_BoundingDiagonal to Sedona </li> <li>[SEDONA-237] - Implement ST_Dimension </li> <li>[SEDONA-238] - Implement OGC GeometryType </li> <li>[SEDONA-293] - Implement ST_IsCollection </li> <li>[SEDONA-294] - Implement ST_Angle </li> <li>[SEDONA-295] - Implement ST_LineInterpolatePoint in Flink </li> <li>[SEDONA-296] - Implement ST_Multi in Sedona Flink </li> <li>[SEDONA-298] - Implement ST_ClosestPoint </li> <li>[SEDONA-299] - Implement ST_FrechetDistance </li> <li>[SEDONA-300] - Implement ST_HausdorffDistance </li> <li>[SEDONA-301] - Implement ST_Affine </li> <li>[SEDONA-303] - Port all Sedona Spark functions to Sedona Flink </li> <li>[SEDONA-310] - Add ST_Degrees to sedona </li> <li>[SEDONA-314] - Support Optimized join on ST_HausdorffDistance </li> <li>[SEDONA-315] - Support Optimized join on ST_FrechetDistance </li> <li>[SEDONA-321] - Implement RS_Intersects(raster, geom) </li> <li>[SEDONA-323] - Add wrapper for KeplerGl visualization in sedona </li> <li>[SEDONA-328] - Add wrapper for pydeck visualizations in sedona </li> <li>[SEDONA-331] - Add RS_Height and RS_Width </li> <li>[SEDONA-334] - Add ScaleX and ScaleY </li> <li>[SEDONA-335] - Add RS_PixelAsPoint </li> <li>[SEDONA-336] - Add RS_UpperLeftX and RS_UpperLeftY </li> <li>[SEDONA-340] - Add RS_ConvexHull </li> <li>[SEDONA-343] - Add raster predicates: Contains and Within </li> <li>[SEDONA-344] - Add RS_RasterToWorldCoordX, RS_RasterToWorldCoordY </li> <li>[SEDONA-346] - Add RS_WorldToRaster APIs </li> <li>[SEDONA-353] - Add RS_BandNoDataValue </li> <li>[SEDONA-354] - Add RS_SkewX and RS_SkewY </li> <li>[SEDONA-355] - Add RS_BandPixelType </li> <li>[SEDONA-357] - Implement ST_VoronoiPolygons </li> <li>[SEDONA-359] - Add RS_GeoReference </li> <li>[SEDONA-361] - Add RS_MapAlgebra for performing map algebra operations using simple expressions </li> <li>[SEDONA-363] - Add RS_PixelAsPolygon </li> <li>[SEDONA-364] - Add RS_MinConvexHull </li> <li>[SEDONA-366] - Add RS_Count </li> <li>[SEDONA-367] - Add RS_PixelAsCentroid </li> <li>[SEDONA-368] - Add RS_SummaryStats </li> <li>[SEDONA-371] - Add optimized join support for raster-vector and raster-raster(if any) joins </li> <li>[SEDONA-372] - Add RS_SetGeoReference </li> <li>[SEDONA-375] - Add RS_SetBandNoDataValue </li> <li>[SEDONA-376] - Add RS_SetValues </li> <li>[SEDONA-378] - Add RS_SetValue </li> <li>[SEDONA-379] - Add RS_AsBase64 </li> <li>[SEDONA-383] - Add RS_Band </li> <li>[SEDONA-387] - Add RS_BandIsNoData </li> <li>[SEDONA-388] - Add RS_AsRaster </li> <li>[SEDONA-391] - Add RS_AsMatrix </li> <li>[SEDONA-393] - Add RS_AsPNG </li> <li>[SEDONA-395] - Add RS_AsImage </li> <li>[SEDONA-396] - Add RS_SetValues Geometry variant </li> <li>[SEDONA-398] - Add RS_AddBand </li> <li>[SEDONA-404] - Add RS_Resample </li> </ul>"},{"location":"setup/release-notes/#improvement_5","title":"Improvement","text":"<ul> <li>[SEDONA-39] - Fix the Lon/lat order issue in Sedona </li> <li>[SEDONA-114] - Add ST_MakeLine to Apache Sedona </li> <li>[SEDONA-142] - Add ST_Collect to Flink Catalog </li> <li>[SEDONA-311] - Refactor InferredExpression to handle functions with arbitrary arity </li> <li>[SEDONA-313] - Refactor ST_Affine to support signature like PostGIS </li> <li>[SEDONA-324] - R \u2013 Fix failing tests </li> <li>[SEDONA-326] - Improve raster band algebra functions for easier preprocessing of raster data </li> <li>[SEDONA-327] - Refactor InferredExpression to handle GridCoverage2D </li> <li>[SEDONA-333] - Support EWKT parser in ST_GeomFromWKT </li> <li>[SEDONA-347] - Centralize usages of transform() </li> <li>[SEDONA-350] - Refactor RS_AddBandFromArray to allow adding a custom noDataValue </li> <li>[SEDONA-352] - Refactor MakeEmptyRaster to allow setting custom datatype for the raster </li> <li>[SEDONA-360] - Handle nodata values of raster bands in a more concise way </li> <li>[SEDONA-365] - Refactor RS_Count to RS_CountValue </li> <li>[SEDONA-374] - RS predicates should support (geom, rast) and (rast, rast) as arguments, and use the convex hull of rasters for spatial relationship testing </li> <li>[SEDONA-385] - Set the Maven Central to be the first repository to check </li> <li>[SEDONA-386] - Speed up GridCoverage2D serialization </li> <li>[SEDONA-392] - Add five more pre-commit hooks </li> <li>[SEDONA-399] - Support Uber H3 cells </li> <li>[SEDONA-400] - pre-commit add hook to ensure that links to vcs websites are permalinks </li> <li>[SEDONA-408] - Set a reasonable default size for RasterUDT </li> </ul>"},{"location":"setup/release-notes/#task_5","title":"Task","text":"<ul> <li>[SEDONA-316] - Refactor Sedona Jupyter notebook examples with unified SedonaContext entrypoint </li> <li>[SEDONA-317] - Change map visualization in Jupyter notebooks with KeplerGL </li> <li>[SEDONA-341] - Move RS_Envelope to GeometryFunctions </li> <li>[SEDONA-356] - Change CRS transformation from lat/lon to lon/lat order </li> <li>[SEDONA-370] - Completely drop the old GeoTiff reader and writer </li> <li>[SEDONA-377] - Change sphere/spheroid functions to work with coordinates in lon/lat order </li> <li>[SEDONA-380] - Merge all Sedona Spark module to a single module </li> <li>[SEDONA-381] - Merge python-adapter to sql module </li> <li>[SEDONA-382] - Merge SQL and Core module to a single Spark module </li> <li>[SEDONA-384] - Merge viz module to the spark module </li> <li>[SEDONA-397] - Move Map Algebra functions </li> </ul>"},{"location":"setup/release-notes/#sedona-141","title":"Sedona 1.4.1","text":"<p>Sedona 1.4.1 is compiled against Spark 3.3 / Spark 3.4 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_5","title":"Highlights","text":"<ul> <li> Sedona Spark More raster functions and bridge RasterUDT and Map Algebra operators. See Raster based operators and Raster to Map Algebra operators.</li> <li> Sedona Spark &amp; Flink Added geodesic / geography functions:<ul> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> <li>ST_AreaSpheroid</li> <li>ST_LengthSpheroid</li> </ul> </li> <li> Sedona Spark &amp; Flink Introduced <code>SedonaContext</code> to unify Sedona entry points.</li> <li> Sedona Spark Support Spark 3.4.</li> <li> Sedona Spark Added a number of new ST functions.</li> <li> Zeppelin Zeppelin helium plugin supports plotting geometries like linestring, polygon.</li> </ul>"},{"location":"setup/release-notes/#api-change","title":"API change","text":"<ul> <li> <p>Sedona Spark &amp; Flink Introduced a new entry point called SedonaContext to unify all Sedona entry points in different compute engines and deprecate old Sedona register entry points. Users no longer have to register Sedona kryo serializer and import many tedious Python classes.</p> <ul> <li> <p>Sedona Spark:</p> <ul> <li>Scala:</li> </ul> <pre><code>import org.apache.sedona.spark.SedonaContext\nval sedona = SedonaContext.create(SedonaContext.builder().master(\"local[*]\").getOrCreate())\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre> <ul> <li>Python:</li> </ul> <pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n   config('spark.jars.packages',\n       'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.4.1,'\n       'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n   getOrCreate()\nsedona = SedonaContext.create(config)\nsedona.sql(\"SELECT ST_GeomFromWKT(XXX) FROM\")\n</code></pre> </li> <li> <p>Sedona Flink:</p> </li> </ul> <pre><code>import org.apache.sedona.flink.SedonaContext;\nStreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\nsedona.sqlQuery(\"SELECT ST_GeomFromWKT(XXX) FROM\");\n</code></pre> </li> </ul>"},{"location":"setup/release-notes/#bug_7","title":"Bug","text":"<ul> <li>[SEDONA-266] - RS_Values throws UnsupportedOperationException for shuffled point arrays </li> <li>[SEDONA-267] - Cannot pip install apache-sedona 1.4.0 from source distribution </li> <li>[SEDONA-273] - Set a upper bound for Shapely, Pandas and GeoPandas </li> <li>[SEDONA-277] - Sedona spark artifacts for scala 2.13 do not have proper POMs </li> <li>[SEDONA-283] - Artifacts were deployed twice when running mvn clean deploy </li> <li>[SEDONA-284] - Property values in dependency deduced POMs for shaded modules were not substituted </li> </ul>"},{"location":"setup/release-notes/#new-feature_5","title":"New Feature","text":"<ul> <li>[SEDONA-196] - Add ST_Force3D to Sedona </li> <li>[SEDONA-239] - Implement ST_NumPoints </li> <li>[SEDONA-264] - zeppelin helium plugin supports plotting geometry like linestring, polygon </li> <li>[SEDONA-280] - Add ST_GeometricMedian </li> <li>[SEDONA-281] - Support geodesic / geography functions </li> <li>[SEDONA-286] - Support optimized distance join on ST_DistanceSpheroid and ST_DistanceSphere </li> <li>[SEDONA-287] - Use SedonaContext to unify Sedona entry points </li> <li>[SEDONA-292] - Bridge Sedona Raster and Map Algebra operators </li> <li>[SEDONA-297] - Implement ST_NRings </li> <li>[SEDONA-302] - Implement ST_Translate </li> </ul>"},{"location":"setup/release-notes/#improvement_6","title":"Improvement","text":"<ul> <li>[SEDONA-167] - Add __pycache__ to Python .gitignore </li> <li>[SEDONA-265] - Migrate all ST functions to Sedona Inferred Expressions </li> <li>[SEDONA-269] - Add data source for writing binary files </li> <li>[SEDONA-270] - Remove redundant serialization for rasters </li> <li>[SEDONA-271] - Add raster function RS_SRID </li> <li>[SEDONA-274] - Move all ST function logics to Sedona common </li> <li>[SEDONA-275] - Add raster function RS_SetSRID </li> <li>[SEDONA-276] - Add support for Spark 3.4 </li> <li>[SEDONA-279] - Sedona-Flink should not depend on Sedona-Spark modules </li> <li>[SEDONA-282] - R \u2013 Add raster write function </li> <li>[SEDONA-290] - RDD Spatial Joins should follow the iterator model </li> </ul>"},{"location":"setup/release-notes/#sedona-140","title":"Sedona 1.4.0","text":"<p>Sedona 1.4.0 is compiled against, Spark 3.3 / Flink 1.12, Java 8.</p>"},{"location":"setup/release-notes/#highlights_6","title":"Highlights","text":"<ul> <li> Sedona Spark &amp; Flink Serialize and deserialize geometries 3 - 7X faster</li> <li> Sedona Spark &amp; Flink Google S2 based spatial join for fast approximate point-in-polygon join. See Join query in Spark and Join query in Flink</li> <li> Sedona Spark Pushdown spatial predicate on GeoParquet to reduce memory consumption by 10X: see explanation</li> <li> Sedona Spark Automatically use broadcast index spatial join for small datasets</li> <li> Sedona Spark New RasterUDT added to Sedona GeoTiff reader.</li> <li> Sedona Spark A number of bug fixes and improvement to the Sedona R module.</li> </ul>"},{"location":"setup/release-notes/#api-change_1","title":"API change","text":"<ul> <li>Sedona Spark &amp; Flink Packaging strategy changed. See Maven Coordinate. Please change your Sedona dependencies if needed. We recommend <code>sedona-spark-shaded-3.0_2.12-1.4.0</code> and <code>sedona-flink-shaded_2.12-1.4.0</code></li> <li>Sedona Spark &amp; Flink GeoTools-wrapper version upgraded. Please use <code>geotools-wrapper-1.4.0-28.2</code>.</li> </ul>"},{"location":"setup/release-notes/#behavior-change_1","title":"Behavior change","text":"<ul> <li>Sedona Flink Sedona Flink no longer outputs any LinearRing type geometry. All LinearRing are changed to LineString.</li> <li>Sedona Spark Join optimization strategy changed. Sedona no longer optimizes spatial join when use a spatial predicate together with an equijoin predicate. By default, it prefers equijoin whenever possible. SedonaConf adds a config option called <code>sedona.join.optimizationmode</code>, it can be configured as one of the following values:<ul> <li><code>all</code>: optimize all joins having spatial predicate in join conditions. This was the behavior of Apache Sedona prior to 1.4.0.</li> <li><code>none</code>: disable spatial join optimization.</li> <li><code>nonequi</code>: only enable spatial join optimization on non-equi joins. This is the default mode.</li> </ul> </li> </ul> <p>When <code>sedona.join.optimizationmode</code> is configured as <code>nonequi</code>, it won't optimize join queries such as <code>SELECT * FROM A, B WHERE A.x = B.x AND ST_Contains(A.geom, B.geom)</code>, since it is an equi-join with equi-condition <code>A.x = B.x</code>. Sedona will optimize for <code>SELECT * FROM A, B WHERE ST_Contains(A.geom, B.geom)</code></p>"},{"location":"setup/release-notes/#bug_8","title":"Bug","text":"<ul> <li>[SEDONA-218] - Flaky test caused by improper handling of null struct values in Adapter.toDf </li> <li>[SEDONA-221] - Outer join throws NPE for null geometries </li> <li>[SEDONA-222] - GeoParquet reader does not work in non-local mode </li> <li>[SEDONA-224] - java.lang.NoSuchMethodError when loading GeoParquet files using Spark 3.0.x ~ 3.2.x </li> <li>[SEDONA-225] - Cannot count dataframes loaded from GeoParquet files </li> <li>[SEDONA-227] - Python SerDe Performance Degradation </li> <li>[SEDONA-230] - rdd.saveAsGeoJSON should generate feature properties with field names </li> <li>[SEDONA-233] - Incorrect results for several joins in a single stage </li> <li>[SEDONA-236] - Flakey python tests in tests.serialization.test_[de]serializers </li> <li>[SEDONA-242] - Update jars dependencies in Sedona R to Sedona 1.4.0 version </li> <li>[SEDONA-250] - R Deprecate use of Spark 2.4 </li> <li>[SEDONA-252] - Fix disabled RS_Base64 test </li> <li>[SEDONA-255] - R \u2013 Translation issue for ST_Point and ST_PolygonFromEnvelope </li> <li>[SEDONA-258] - Cannot directly assign raw spatial RDD to CircleRDD using Python binding </li> <li>[SEDONA-259] - Adapter.toSpatialRdd in Python binding does not have valid implementation for specifying custom field names for user data </li> <li>[SEDONA-261] - Cannot run distance join using broadcast index join when the distance expression references to attributes from the right-side relation </li> </ul>"},{"location":"setup/release-notes/#new-feature_6","title":"New Feature","text":"<ul> <li>[SEDONA-156] - predicate pushdown support for GeoParquet </li> <li>[SEDONA-215] - Add ST_ConcaveHull </li> <li>[SEDONA-216] - Upgrade jts version to 1.19.0 </li> <li>[SEDONA-235] - Create ST_S2CellIds in Sedona </li> <li>[SEDONA-246] - R GeoTiff read/write </li> <li>[SEDONA-254] - R \u2013 Add raster type </li> <li>[SEDONA-262] - Don't optimize equi-join by default, add an option to configure when to optimize spatial joins </li> </ul>"},{"location":"setup/release-notes/#improvement_7","title":"Improvement","text":"<ul> <li>[SEDONA-205] - Use BinaryType in GeometryUDT in Sedona Spark </li> <li>[SEDONA-207] - Faster serialization/deserialization of geometry objects </li> <li>[SEDONA-212] - Move shading to separate maven modules </li> <li>[SEDONA-217] - Automatically broadcast small datasets </li> <li>[SEDONA-220] - Upgrade Ubuntu build image from 18.04 to 20.04 </li> <li>[SEDONA-226] - Support reading and writing GeoParquet file metadata </li> <li>[SEDONA-228] - Standardize logging dependencies </li> <li>[SEDONA-231] - Redundant Serde Removal </li> <li>[SEDONA-234] - ST_Point inconsistencies </li> <li>[SEDONA-243] - Improve Sedona R file readers: GeoParquet and Shapefile </li> <li>[SEDONA-244] - Align R read/write functions with the Sparklyr framework </li> <li>[SEDONA-249] - Add jvm flags for running tests on Java 17 </li> <li>[SEDONA-251] - Add raster type to Sedona </li> <li>[SEDONA-253] - Upgrade geotools to version 28.2 </li> <li>[SEDONA-260] - More intuitive configuration of partition and index-build side of spatial joins in Sedona SQL </li> </ul>"},{"location":"setup/release-notes/#sedona-131","title":"Sedona 1.3.1","text":"<p>This version is a minor release on Sedona 1.3.0 line. It fixes a few critical bugs in 1.3.0. We suggest all 1.3.0 users to migrate to this version.</p>"},{"location":"setup/release-notes/#bug-fixes","title":"Bug fixes","text":"<ul> <li>SEDONA-204 - Init value in X/Y/Z max should be -Double.MAX</li> <li>SEDONA-206 - Performance regression of ST_Transform in 1.3.0-incubating</li> <li>SEDONA-210 - 1.3.0-incubating doesn't work with Scala 2.12 sbt projects</li> <li>SEDONA-211 - Enforce release managers to use JDK 8</li> <li>SEDONA-201 - Implement ST_MLineFromText and ST_MPolyFromText methods</li> </ul>"},{"location":"setup/release-notes/#new-feature_7","title":"New Feature","text":"<ul> <li>SEDONA-196 - Add ST_Force3D to Sedona</li> <li>SEDONA-197 - Add ST_ZMin, ST_ZMax to Sedona</li> <li>SEDONA-199 - Add ST_NDims to Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_8","title":"Improvement","text":"<ul> <li>SEDONA-194 - Merge org.datasyslab.sernetcdf into Sedona</li> <li>SEDONA-208 - Use Spark RuntimeConfig in SedonaConf</li> </ul> <p>Note</p> <p>Support of Spark 2.X and Scala 2.11 was removed in Sedona 1.3.0+ although some parts of the source code might still be compatible. Sedona 1.3.0+ releases binary for both Scala 2.12 and 2.13.</p>"},{"location":"setup/release-notes/#sedona-130","title":"Sedona 1.3.0","text":"<p>This version is a major release on Sedona 1.3.0 line and consists of 50 PRs. It includes many new functions, optimization and bug fixes.</p>"},{"location":"setup/release-notes/#highlights_7","title":"Highlights","text":"<ul> <li> Sedona on Spark in this release is compiled against Spark 3.3.</li> <li> Sedona on Flink in this release is compiled against Flink 1.14.</li> <li> Scala 2.11 support is removed.</li> <li> Spark 2.X support is removed.</li> <li> Python 3.10 support is added.</li> <li> Aggregators in Flink are added</li> <li> Correctness fixes for corner cases in range join and distance join.</li> <li> Native GeoParquet read and write (../../tutorial/sql/#load-geoparquet).<ul> <li><code>df = spark.read.format(\"geoparquet\").option(\"fieldGeometry\", \"myGeometryColumn\").load(\"PATH/TO/MYFILE.parquet\")</code></li> <li><code>df.write.format(\"geoparquet\").save(\"PATH/TO/MYFILE.parquet\")</code></li> </ul> </li> <li> DataFrame style API (../../tutorial/sql.md/#dataframe-style-api)<ul> <li><code>df.select(ST_Point(min_value, max_value).as(\"point\"))</code></li> </ul> </li> <li> Allow WKT format CRS in ST_Transform<ul> <li><code>ST_Transform(geom, \"srcWktString\", \"tgtWktString\")</code></li> </ul> </li> </ul> <pre><code>GEOGCS[\"WGS 84\",\n  DATUM[\"WGS_1984\",\n  SPHEROID[\"WGS 84\",6378137,298.257223563,\n  AUTHORITY[\"EPSG\",\"7030\"]],\n  AUTHORITY[\"EPSG\",\"6326\"]],\n  PRIMEM[\"Greenwich\",0,\n  AUTHORITY[\"EPSG\",\"8901\"]],\n  UNIT[\"degree\",0.0174532925199433,\n  AUTHORITY[\"EPSG\",\"9122\"]],\n  AUTHORITY[\"EPSG\",\"4326\"]]\n</code></pre>"},{"location":"setup/release-notes/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>SEDONA-119 - ST_Touches join query returns true for polygons whose interiors intersect</li> <li>SEDONA-136 - Enable testAsEWKT for Flink</li> <li>SEDONA-137 - Fix ST_Buffer for Flink to work</li> <li>SEDONA-138 - Fix ST_GeoHash for Flink to work</li> <li>SEDONA-153 - Python Serialization Fails with Nulls</li> <li>SEDONA-158 - Fix wrong description about ST_GeometryN in the API docs</li> <li>SEDONA-169 - Fix ST_RemovePoint in accordance with the API document</li> <li>SEDONA-178 - Correctness issue in distance join queries</li> <li>SEDONA-182 - ST_AsText should not return SRID</li> <li>SEDONA-186 - collecting result rows of a spatial join query with SELECT * fails with serde error</li> <li>SEDONA-188 - Python warns about missing <code>jars</code> even when some are found</li> <li>SEDONA-193 - ST_AsBinary produces EWKB by mistake</li> </ul>"},{"location":"setup/release-notes/#new-features","title":"New Features","text":"<ul> <li>SEDONA-94 - GeoParquet  Support For Sedona</li> <li>SEDONA-125 - Allows customized CRS in ST_Transform</li> <li>SEDONA-166 - Provide Type-safe DataFrame Style API</li> <li>SEDONA-168 - Add ST_Normalize to Apache Sedona</li> <li>SEDONA-171 - Add ST_SetPoint to Apache Sedona</li> </ul>"},{"location":"setup/release-notes/#improvement_9","title":"Improvement","text":"<ul> <li>SEDONA-121 - Add equivalent constructors left over from Spark to Flink</li> <li>SEDONA-132 - Create common module for SQL functions</li> <li>SEDONA-133 - Allow user-defined schemas in Adapter.toDf()</li> <li>SEDONA-139 - Fix wrong argument order in Flink unit tests</li> <li>SEDONA-140 - Update Sedona Dependencies in R Package</li> <li>SEDONA-143 - Add missing unit tests for the Flink predicates</li> <li>SEDONA-144 - Add ST_AsGeoJSON to the Flink API</li> <li>SEDONA-145 - Fix ST_AsEWKT to reserve the Z coordinate</li> <li>SEDONA-146 - Add missing output functions to the Flink API</li> <li>SEDONA-147 - Add SRID functions to the Flink API</li> <li>SEDONA-148 - Add boolean functions to the Flink API</li> <li>SEDONA-149 - Add Python 3.10 support</li> <li>SEDONA-151 - Add ST aggregators to Sedona Flink</li> <li>SEDONA-152 - Add reader/writer functions for GML and KML</li> <li>SEDONA-154 - Add measurement functions to the Flink API</li> <li>SEDONA-157 - Add coordinate accessors to the Flink API</li> <li>SEDONA-159 - Add Nth accessor functions to the Flink API</li> <li>SEDONA-160 - Fix geoparquetIOTests.scala to cleanup after test</li> <li>SEDONA-161 - Add ST_Boundary to the Flink API</li> <li>SEDONA-162 - Add ST_Envelope to the Flink API</li> <li>SEDONA-163 - Better handle of unsupported types in shapefile reader</li> <li>SEDONA-164 - Add geometry count functions to the Flink API</li> <li>SEDONA-165 - Upgrade Apache Rat to 0.14</li> <li>SEDONA-170 - Add ST_AddPoint and ST_RemovePoint to the Flink API</li> <li>SEDONA-172 - Add ST_LineFromMultiPoint to Apache Sedona</li> <li>SEDONA-176 - Make ST_Contains conform with OGC standard, and add ST_Covers and ST_CoveredBy functions.</li> <li>SEDONA-177 - Support spatial predicates other than INTERSECTS and COVERS/COVERED_BY in RangeQuery.SpatialRangeQuery and JoinQuery.SpatialJoinQuery</li> <li>SEDONA-181 - Build fails with java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$</li> <li>SEDONA-189 - Prepare geometries in broadcast join</li> <li>SEDONA-192 - Null handling in predicates</li> <li>SEDONA-195 - Add wkt validation and an optional srid to ST_GeomFromWKT/ST_GeomFromText</li> </ul>"},{"location":"setup/release-notes/#task_6","title":"Task","text":"<ul> <li>SEDONA-150 - Drop Spark 2.4 and Scala 2.11 support</li> </ul>"},{"location":"setup/release-notes/#sedona-121","title":"Sedona 1.2.1","text":"<p>This version is a maintenance release on Sedona 1.2.0 line. It includes bug fixes.</p> <p>Sedona on Spark is now compiled against Spark 3.3, instead of Spark 3.2.</p>"},{"location":"setup/release-notes/#sql-for-spark","title":"SQL (for Spark)","text":"<p>Bug fixes:</p> <ul> <li>SEDONA-104: Bug in reading band values of GeoTiff images</li> <li>SEDONA-118: Fix the wrong result in ST_Within</li> <li>SEDONA-123: Fix the check for invalid lat/lon in ST_GeoHash</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-96: Refactor ST_MakeValid to use GeometryFixer</li> <li>SEDONA-108: Write support for GeoTiff images</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> <li>SEDONA-127: Add null safety to ST_GeomFromWKT/WKB/Text</li> <li>SEDONA-129: Support Spark 3.3</li> <li>SEDONA-135: Consolidate and upgrade hadoop dependency</li> </ul> <p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-106: Add ST_LineFromText</li> <li>SEDONA-117: Add RS_AppendNormalizedDifference</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_YMin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-124: Add ST_CollectionExtract</li> <li>SEDONA-109: Add ST_OrderingEquals</li> </ul>"},{"location":"setup/release-notes/#flink","title":"Flink","text":"<p>New features:</p> <ul> <li>SEDONA-107: Add St_Reverse function</li> <li>SEDONA-105: Add ST_PointOnSurface function</li> <li>SEDONA-95: Add ST_Disjoint predicate</li> <li>SEDONA-112: Add ST_AsEWKT</li> <li>SEDONA-97: Add ST_Force_2D</li> <li>SEDONA-98: Add ST_IsEmpty</li> <li>SEDONA-116: Add ST_YMax and ST_YMin</li> <li>SEDONA-115: Add ST_XMax and ST_Min</li> <li>SEDONA-120: Add ST_BuildArea</li> <li>SEDONA-113: Add ST_PointN</li> <li>SEDONA-110: Add ST_GeomFromGeoHash</li> <li>SEDONA-121: More ST constructors to Flink</li> <li>SEDONA-122: Overload ST_GeomFromWKB for BYTES column</li> </ul>"},{"location":"setup/release-notes/#sedona-120","title":"Sedona 1.2.0","text":"<p>This version is a major release on Sedona 1.2.0 line. It includes bug fixes and new features: Sedona with Apache Flink.</p>"},{"location":"setup/release-notes/#rdd","title":"RDD","text":"<p>Bug fix:</p> <ul> <li>SEDONA-18: Fix an error reading Shapefile</li> <li>SEDONA-73: Exclude scala-library from scala-collection-compat</li> </ul> <p>Improvement:</p> <ul> <li>SEDONA-77: Refactor Format readers and spatial partitioning functions to be standalone libraries. So they can be used by Flink and others.</li> </ul>"},{"location":"setup/release-notes/#sql","title":"SQL","text":"<p>New features:</p> <ul> <li>SEDONA-4: Handle nulls in SQL functions</li> <li>SEDONA-65: Create ST_Difference function</li> <li>SEDONA-68 Add St_Collect function.</li> <li>SEDONA-82: Create ST_SymDifference function</li> <li>SEDONA-75: Add support for \"3D\" geometries: Preserve Z coordinates on geometries when serializing, ST_AsText, ST_Z, ST_3DDistance</li> <li>SEDONA-86: Support empty geometries in ST_AsBinary and ST_AsEWKB</li> <li>SEDONA-90: Add ST_Union</li> <li>SEDONA-100: Add st_multi function</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-89: GeometryUDT equals should test equivalence of the other object</li> </ul>"},{"location":"setup/release-notes/#flink_1","title":"Flink","text":"<p>Major update:</p> <ul> <li>SEDONA-80: Geospatial stream processing support in Flink Table API</li> <li>SEDONA-85: ST_Geohash function in Flink</li> <li>SEDONA-87: Support Flink Table and DataStream conversion</li> <li>SEDONA-93: Add ST_GeomFromGeoJSON</li> </ul>"},{"location":"setup/release-notes/#sedona-111","title":"Sedona 1.1.1","text":"<p>This version is a maintenance release on Sedona 1.1.X line. It includes bug fixes and a few new functions.</p>"},{"location":"setup/release-notes/#global","title":"Global","text":"<p>New feature:</p> <ul> <li>SEDONA-73: Scala source code supports Scala 2.13</li> </ul>"},{"location":"setup/release-notes/#sql_1","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-67: Support Spark 3.2</li> </ul> <p>New features:</p> <ul> <li>SEDONA-43: Add ST_GeoHash and ST_GeomFromGeoHash</li> <li>SEDONA-45: Add ST_MakePolygon</li> <li>SEDONA-71: Add ST_AsBinary, ST_AsEWKB, ST_SRID, ST_SetSRID</li> </ul>"},{"location":"setup/release-notes/#sedona-110","title":"Sedona 1.1.0","text":"<p>This version is a major release on Sedona 1.1.0 line. It includes bug fixes and new features: R language API, Raster data and Map algebra support</p>"},{"location":"setup/release-notes/#global_1","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-30: Use Geotools-wrapper 1.1.0-24.1 to include geotools GeoTiff libraries.</li> </ul> <p>Improvement on join queries in core and SQL:</p> <ul> <li>SEDONA-63: Skip empty partitions in NestedLoopJudgement</li> <li>SEDONA-64: Broadcast dedupParams to improve performance</li> </ul> <p>Behavior change:</p> <ul> <li>SEDONA-62: Ignore HDF test in order to avoid NASA copyright issue</li> </ul>"},{"location":"setup/release-notes/#core","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-41: Fix rangeFilter bug when the leftCoveredByRight para is false</li> <li>SEDONA-53: Fix SpatialKnnQuery NullPointerException</li> </ul>"},{"location":"setup/release-notes/#sql_2","title":"SQL","text":"<p>Major update:</p> <ul> <li>SEDONA-30: Add GeoTiff raster I/O and Map Algebra function</li> </ul> <p>New function:</p> <ul> <li>SEDONA-27: Add ST_Subdivide and ST_SubdivideExplode functions</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-56: Fix broadcast join with Adapter Query Engine enabled</li> <li>SEDONA-22, SEDONA-60: Fix join queries in SparkSQL when one side has no rows or only one row</li> </ul>"},{"location":"setup/release-notes/#viz","title":"Viz","text":"<p>N/A</p>"},{"location":"setup/release-notes/#python","title":"Python","text":"<p>Improvement:</p> <ul> <li>SEDONA-59: Make pyspark dependency of Sedona Python optional</li> </ul> <p>Bug fix:</p> <ul> <li>SEDONA-50: Remove problematic logging conf that leads to errors on Databricks</li> <li>Fix the issue: Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake.</li> </ul>"},{"location":"setup/release-notes/#r","title":"R","text":"<p>Major update:</p> <ul> <li>SEDONA-31: Add R interface for Sedona</li> </ul>"},{"location":"setup/release-notes/#sedona-101","title":"Sedona 1.0.1","text":"<p>This version is a maintenance release on Sedona 1.0.0 line. It includes bug fixes, some new features, one API change</p>"},{"location":"setup/release-notes/#known-issue","title":"Known issue","text":"<p>In Sedona v1.0.1 and earlier versions, the Spark dependency in setup.py was configured to be &lt; v3.1.0 by mistake. When you install Sedona Python (apache-sedona v1.0.1) from PyPI, pip might uninstall PySpark 3.1.1 and install PySpark 3.0.2 on your machine.</p> <p>Three ways to fix this:</p> <ol> <li> <p>After install apache-sedona v1.0.1, uninstall PySpark 3.0.2 and reinstall PySpark 3.1.1</p> </li> <li> <p>Ask pip not to install Sedona dependencies: <code>pip install --no-deps apache-sedona</code></p> </li> <li> <p>Install Sedona from the latest setup.py (on GitHub) manually.</p> </li> </ol>"},{"location":"setup/release-notes/#global_2","title":"Global","text":"<p>Dependency upgrade:</p> <ul> <li>SEDONA-16: Use a GeoTools Maven Central wrapper to fix failed Jupyter notebook examples</li> <li>SEDONA-29: upgrade to Spark 3.1.1</li> <li>SEDONA-33: jts2geojson version from 0.14.3 to 0.16.1</li> </ul>"},{"location":"setup/release-notes/#core_1","title":"Core","text":"<p>Bug fix:</p> <ul> <li>SEDONA-35: Address user-data mutability issue with Adapter.toDF()</li> </ul>"},{"location":"setup/release-notes/#sql_3","title":"SQL","text":"<p>Bug fix:</p> <ul> <li>SEDONA-14: Saving dataframe to CSV or Parquet fails due to unknown type</li> <li>SEDONA-15: Add ST_MinimumBoundingRadius and ST_MinimumBoundingCircle functions</li> <li>SEDONA-19: Global indexing does not work with SQL joins</li> <li>SEDONA-20: Case object GeometryUDT and GeometryUDT instance not equal in Spark 3.0.2</li> </ul> <p>New function:</p> <ul> <li>SEDONA-21: allows Sedona to be used in pure SQL environment</li> <li>SEDONA-24: Add ST_LineSubString and ST_LineInterpolatePoint</li> <li>SEDONA-26: Add broadcast join support</li> </ul>"},{"location":"setup/release-notes/#viz_1","title":"Viz","text":"<p>Improvement:</p> <ul> <li>SEDONA-32: Speed up ST_Render</li> </ul> <p>API change:</p> <ul> <li>SEDONA-29: Upgrade to Spark 3.1.1 and fix ST_Pixelize</li> </ul>"},{"location":"setup/release-notes/#python_1","title":"Python","text":"<p>Bug fix:</p> <ul> <li>SEDONA-19: Global indexing does not work with SQL joins</li> </ul>"},{"location":"setup/release-notes/#sedona-100","title":"Sedona 1.0.0","text":"<p>This version is the first Sedona release since it joins the Apache Incubator. It includes new functions, bug fixes, and API changes.</p>"},{"location":"setup/release-notes/#global_3","title":"Global","text":"<p>Key dependency upgrade:</p> <ul> <li>SEDONA-1: upgrade to JTS 1.18</li> <li>upgrade to GeoTools 24.0</li> <li>upgrade to jts2geojson 0.14.3</li> </ul> <p>Key dependency packaging strategy change:</p> <ul> <li>JTS, GeoTools, jts2geojson are no longer packaged in Sedona jars. End users need to add them manually. See here.</li> </ul> <p>Key compilation target change:</p> <ul> <li>SEDONA-3: Paths and class names have been changed to Apache Sedona</li> <li>SEDONA-7: build the source code for Spark 2.4, 3.0, Scala 2.11, 2.12, Python 3.7, 3.8, 3.9. See here.</li> </ul>"},{"location":"setup/release-notes/#sedona-core","title":"Sedona-core","text":"<p>Bug fix:</p> <ul> <li>PR 443: read multiple Shape Files by multiPartitions</li> <li>PR 451 (API change): modify CRSTransform to ignore datum shift</li> </ul> <p>New function:</p> <ul> <li>SEDONA-8: spatialRDD.flipCoordinates()</li> </ul> <p>API / behavior change:</p> <ul> <li>PR 488: JoinQuery.SpatialJoinQuery/DistanceJoinQuery now returns <code>&lt;Geometry, List&gt;</code> instead of <code>&lt;Geometry, HashSet&gt;</code> because we can no longer use HashSet in Sedona for duplicates removal. All original duplicates in both input RDDs will be preserved in the output.</li> </ul>"},{"location":"setup/release-notes/#sedona-sql","title":"Sedona-sql","text":"<p>Bug fix:</p> <ul> <li>SEDONA-8 (API change): ST_Transform slow due to lock contention.</li> <li>PR 427: ST_Point and ST_PolygonFromEnvelope now allows Double type</li> </ul> <p>New function:</p> <ul> <li>PR 499: ST_Azimuth, ST_X, ST_Y, ST_StartPoint, ST_Boundary, ST_EndPoint, ST_ExteriorRing, ST_GeometryN, ST_InteriorRingN, ST_Dump, ST_DumpPoints, ST_IsClosed, ST_NumInteriorRings, ST_AddPoint, ST_RemovePoint, ST_IsRing</li> <li>PR 459: ST_LineMerge</li> <li>PR 460: ST_NumGeometries</li> <li>PR 469: ST_AsGeoJSON</li> <li>SEDONA-8: ST_FlipCoordinates</li> </ul> <p>Behavior change:</p> <ul> <li>PR 480: Aggregate Functions rewrite for new Aggregator API. The functions can be used as typed functions in code and enable compilation-time type check.</li> </ul> <p>API change:</p> <ul> <li>SEDONA-11: Adapter.toDf() will directly generate a geometry type column. ST_GeomFromWKT is no longer needed.</li> </ul>"},{"location":"setup/release-notes/#sedona-viz","title":"Sedona-viz","text":"<p>API change: Drop the function which can generate SVG vector images because the required library has an incompatible license and the SVG image is not good at plotting big data</p>"},{"location":"setup/release-notes/#sedona-python","title":"Sedona Python","text":"<p>API/Behavior change:</p> <ul> <li>Python-to-Sedona adapter is moved to a separate module. To use Sedona Python, see here</li> </ul> <p>New function:</p> <ul> <li>PR 448: Add support for partition number in spatialPartitioning function <code>spatial_rdd.spatialPartitioning(grid_type, NUM_PARTITION)</code></li> </ul>"},{"location":"setup/wherobots/","title":"Install on Wherobots","text":""},{"location":"setup/wherobots/#wherobotsdb","title":"WherobotsDB","text":"<p>Wherobots Cloud offers fully-managed and fully provisioned cloud services for WherobotsDB, a comprehensive spatial analytics database system. You can play with it using in a cloud-hosted Jupyter notebook with Python, Java or Scala kernels; no installation is needed.</p> <p>WherobotsDB is 100% compatible with Apache Sedona in terms of public APIs but provides more functionality and better performance.</p> <p>It is easy to migrate your existing Sedona workflow to Wherobots Cloud. Please sign up here to create your account.</p>"},{"location":"setup/zeppelin/","title":"Install Sedona-Zeppelin","text":""},{"location":"setup/zeppelin/#install-sedona-zeppelin","title":"Install Sedona-Zeppelin","text":"<p>Warning</p> <p>Known issue: due to an issue in Leaflet JS, Sedona can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fledged visualization, please use SedonaViz to plot scatter plots and heat maps on Zeppelin map.</p>"},{"location":"setup/zeppelin/#compatibility","title":"Compatibility","text":"<p>Apache Spark 2.3+</p> <p>Apache Zeppelin 0.8.1+</p> <p>Sedona 1.0.0+: Sedona-core, Sedona-SQL, Sedona-Viz</p>"},{"location":"setup/zeppelin/#installation","title":"Installation","text":"<p>Note</p> <p>You only need to do Step 1 and 2 only if you cannot see Apache-sedona or GeoSpark Zeppelin in Zeppelin Helium package list.</p>"},{"location":"setup/zeppelin/#create-helium-folder-optional","title":"Create Helium folder (optional)","text":"<p>Create a folder called <code>helium</code> in Zeppelin root folder.</p>"},{"location":"setup/zeppelin/#add-sedona-zeppelin-description-optional","title":"Add Sedona-Zeppelin description (optional)","text":"<p>Create a file called <code>sedona-zeppelin.json</code> in this folder and put the following content in this file. You need to change the artifact path!</p> <pre><code>{\n  \"type\": \"VISUALIZATION\",\n  \"name\": \"sedona-zeppelin\",\n  \"description\": \"Zeppelin visualization support for Sedona\",\n  \"artifact\": \"/Absolute/Path/sedona/zeppelin\",\n  \"license\": \"BSD-2-Clause\",\n  \"icon\": \"&lt;i class='fa fa-globe'&gt;&lt;/i&gt;\"\n}\n</code></pre>"},{"location":"setup/zeppelin/#enable-sedona-zeppelin","title":"Enable Sedona-Zeppelin","text":"<p>Restart Zeppelin then open Zeppelin Helium interface and enable Sedona-Zeppelin.</p> <p></p>"},{"location":"setup/zeppelin/#add-sedona-dependencies-in-zeppelin-spark-interpreter","title":"Add Sedona dependencies in Zeppelin Spark Interpreter","text":""},{"location":"setup/zeppelin/#visualize-sedonasql-results","title":"Visualize SedonaSQL results","text":""},{"location":"setup/zeppelin/#display-sedonaviz-results","title":"Display SedonaViz results","text":"<p>Now, you are good to go! Please read Sedona-Zeppelin tutorial for a hands-on tutorial.</p>"},{"location":"setup/flink/install-scala/","title":"Install Sedona Scala/Java","text":"<p>Before starting the Sedona journey, you need to make sure your Apache Flink cluster is ready.</p> <p>Then you can create a self-contained Scala / Java project. A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place.</p> <p>To use Sedona in your self-contained Flink project, you just need to add Sedona as a dependency in your pom.xml or build.sbt.</p> <ol> <li>To add Sedona as dependencies, please read Sedona Maven Central coordinates</li> <li>Read Sedona Flink guide and use Sedona Template project to start: Sedona Template Project</li> <li>Compile your project using Maven. Make sure you obtain the fat jar which packages all dependencies.</li> <li>Submit your compiled fat jar to Flink cluster. Make sure you are in the root folder of Flink distribution. Then run the following command:</li> </ol> <pre><code>./bin/flink run /Path/To/YourJar.jar\n</code></pre>"},{"location":"setup/flink/modules/","title":"Modules","text":""},{"location":"setup/flink/modules/#sedona-modules-for-apache-flink","title":"Sedona modules for Apache Flink","text":"Name Introduction flink Spatial Table and DataStream implementation flink-shaded shaded version"},{"location":"setup/flink/modules/#api-availability","title":"API availability","text":"DataStream Table Scala/Java \u2705 \u2705 Python no no R no no"},{"location":"setup/flink/platform/","title":"Language wrappers","text":"<p>Sedona Flink binary releases are compiled by Java 1.8 and Scala 2.12, and tested with Flink 1.12 - 1.19.</p>"},{"location":"setup/snowflake/install/","title":"Install Sedona SQL","text":"<p>Note</p> <p>This tutorial is for you to manually install Sedona on Snowflake. If you want to use Sedona on Snowflake without manually installing it, you can use the free SedonaSnow native app shipped by Wherobots.</p>"},{"location":"setup/snowflake/install/#prerequisites","title":"Prerequisites","text":"<p>To install Sedona on Snowflake, you need to prepare a Snowflake account and a Snowflake user that can access at least one <code>DATABASE</code> and run at least one <code>WAREHOUSE</code>. Then you can follow the steps below to install Sedona on Snowflake.</p> <p>You can refer to Snowflake Documentation to how to create a DATABASE.</p> <p>In this tutorial, we will use a database created by the following SQL statement. But you can use any database you want.</p> <pre><code>CREATE DATABASE SEDONA_TEST;\n</code></pre>"},{"location":"setup/snowflake/install/#step-1-create-a-stage-in-the-database","title":"Step 1: Create a stage in the database","text":"<p>A stage is a Snowflake object that maps to a location in a cloud storage provider, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage. You can use a stage to load data into a table, or unload data from a table.</p> <p>In this case, we will create a stage named <code>ApacheSedona</code> in the <code>public</code> schema of the database created in the previous step. The stage will be used to load Sedona's JAR files into the database. We will choose a <code>Snowflake managed</code> stage.</p> <p></p> <p>After creating the stage, you should be able to see the stage in the database.</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a stage.</p>"},{"location":"setup/snowflake/install/#step-2-upload-sedonas-jar-files-to-the-stage","title":"Step 2: Upload Sedona's JAR files to the stage","text":"<p>You will need to download the following 2 JAR files:</p> <ul> <li>sedona-snowflake-1.7.0.jar: Sedona's Maven Central repository</li> <li>geotools-wrapper-1.7.0-28.5.jar: GeoTools-wrapper's Maven Central repository</li> </ul> <p>Then you can upload the 2 JAR files to the stage created in the previous step.</p> <p>After uploading the 2 JAR files, you should be able to see the 2 JAR files in the stage.</p> <p></p> <p>You can refer to Snowflake Documentation to how to upload files to a stage.</p>"},{"location":"setup/snowflake/install/#step-3-create-a-schema-in-the-database","title":"Step 3: Create a schema in the database","text":"<p>A schema is a Snowflake object that maps to a database. You can use a schema to organize your tables into groups based on business functions or other categories.</p> <p>In this case, we will create a schema named <code>SEDONA</code> in the database created in the previous step. The schema will be used to create Sedona's functions.</p> <p></p> <p>You can find your schema in the database as follows:</p> <p></p> <p>You can refer to Snowflake Documentation to how to create a schema.</p>"},{"location":"setup/snowflake/install/#step-4-get-the-sql-script-for-creating-sedonas-functions","title":"Step 4: Get the SQL script for creating Sedona's functions","text":"<p>You will need to get this SQL script by running the following command:</p> <pre><code>java -jar sedona-snowflake-1.7.0.jar --geotools-version 1.7.0-28.5 &gt; sedona-snowflake.sql\n</code></pre> <p>sedona-snowflake-1.7.0.jar is the JAR file downloaded in Step 2.</p>"},{"location":"setup/snowflake/install/#step-5-run-the-sql-script-to-create-sedonas-functions","title":"Step 5: Run the SQL script to create Sedona's functions","text":"<p>We will create a worksheet in the database created in the previous step, and run the SQL script to create Sedona's functions.</p> <p>In this case, we will choose the option <code>Create Worksheet from SQL File</code>.</p> <p></p> <p>In the worksheet, choose <code>SEDONA_TEST</code> as the database, and <code>PUBLIC</code> as the schema. The SQL script should be in the worksheet. Then right-click the worksheet and choose <code>Run All</code>. Snowflake will take 3 minutes to create Sedona's functions.</p> <p></p>"},{"location":"setup/snowflake/install/#step-6-verify-the-installation","title":"Step 6: Verify the installation","text":"<p>Open a new worksheet, choose <code>SEDONA_TEST</code> as the database, and any schema as the schema. Then run the following SQL statement:</p> <pre><code>SELECT SEDONA.ST_AsEWKT(SEDONA.ST_SETSRID(SEDONA.ST_POINT(1, 2), 4326));\n</code></pre> <p>You should be able to see the following result:</p> <pre><code>SRID=4326;POINT (1 2)\n</code></pre> <p>The worksheet should look like this:</p> <p></p>"},{"location":"setup/snowflake/modules/","title":"Modules","text":""},{"location":"setup/snowflake/modules/#sedona-modules-for-snowflake","title":"Sedona modules for Snowflake","text":"Name Introduction snowflake Spatial SQL functions for Snowflake snowflake-tester Automated tester of Sedona Snowflake functions"},{"location":"setup/snowflake/modules/#api-availability","title":"API availability","text":"Table SnowSQL \u2705 Snowpark \u2705"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/","title":"Tune RDD application","text":""},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#advanced-tutorial-tune-your-sedona-rdd-application","title":"Advanced tutorial: Tune your Sedona RDD application","text":"<p>Before getting into this advanced tutorial, please make sure that you have tried several Sedona functions on your local machine.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#pick-a-proper-sedona-version","title":"Pick a proper Sedona version","text":"<p>The versions of Sedona have three levels: X.X.X (i.e., 0.8.1)</p> <p>The first level means that this version contains big structure redesign which may bring big changes in APIs and performance.</p> <p>The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old Sedona user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read Sedona version release notes and make sure you are ready to accept the API changes.</p> <p>The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all Sedona users that stay at the same level move to the latest version in this level.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#choose-a-proper-spatial-rdd-constructor","title":"Choose a proper Spatial RDD constructor","text":"<p>Sedona provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.</p> <ol> <li>Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:</li> </ol> <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel)\n</code></pre> <ol> <li>Initialize a SpatialRDD from an existing RDD. A typical example is as follows:</li> </ol> <pre><code>public PointRDD(JavaRDD&lt;Point&gt; rawSpatialRDD, StorageLevel newLevel)\n</code></pre> <p>You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why Sedona does this is that Sedona wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.</p> <p>However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:</p> <pre><code>public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, FileDataSplitter splitter, boolean carryInputData, Integer partitions, Envelope datasetBoundary, Integer approximateTotalCount) {\n</code></pre> <p>Manually providing the dataset boundary and approximate total count helps Sedona avoiding several slow \"Action\"s during initialization.</p>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#cache-the-spatial-rdd-that-is-repeatedly-used","title":"Cache the Spatial RDD that is repeatedly used","text":"<p>Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:</p> <ol> <li>rawSpatialRDD: The RDD generated by SpatialRDD constructors.</li> <li>spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.</li> <li>indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.</li> <li>indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.</li> </ol> <p>These four RDDs don't co-exist so you don't need to worry about the memory issue. These four RDDs are invoked in different queries:</p> <ol> <li>Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.</li> <li>Spatial Range Query / KNN Query, use index: indexedRawRDD is used.</li> <li>Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.</li> <li>Spatial Join Query / Distance Join Query, use index: indexed RDD is used.</li> </ol> <p>Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:</p> <ol> <li>In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.</li> <li>In Spark RDD sharing applications such as Livy and Spark Job Server, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.</li> </ol>"},{"location":"tutorial/Advanced-Tutorial-Tune-your-Application/#be-aware-of-spatial-rdd-partitions","title":"Be aware of Spatial RDD partitions","text":"<p>Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.</p> <p>After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.</p>"},{"location":"tutorial/benchmark/","title":"Benchmark","text":""},{"location":"tutorial/benchmark/#benchmark","title":"Benchmark","text":"<p>We welcome people to use Sedona for benchmark purpose. To achieve the best performance or enjoy all features of Sedona,</p> <ul> <li>Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.</li> <li>Please open Sedona kryo serializer to reduce the memory footprint.</li> </ul>"},{"location":"tutorial/demo/","title":"Scala/Java","text":""},{"location":"tutorial/demo/#scala-and-java-examples","title":"Scala and Java Examples","text":"<p>Scala and Java Examples contains template projects for Sedona Spark (RDD, SQL and Viz) and Sedona Flink. The template projects have been configured properly.</p> <p>Note that, although the template projects are written in Scala, the same APIs can be  used in Java as well.</p>"},{"location":"tutorial/demo/#folder-structure","title":"Folder structure","text":"<p>The folder structure of this repository is as follows.</p> <ul> <li>spark-sql: a Scala template shows how to use Sedona RDD, DataFrame and SQL API</li> <li>flink-sql: a Java template show how to use Sedona SQL via Flink Table APIs</li> </ul>"},{"location":"tutorial/demo/#compile-and-package","title":"Compile and package","text":""},{"location":"tutorial/demo/#prerequisites","title":"Prerequisites","text":"<p>Please make sure you have the following software installed on your local machine:</p> <ul> <li>For Scala: Scala 2.12</li> <li>For Java: JDK 1.8, Apache Maven 3</li> </ul>"},{"location":"tutorial/demo/#compile","title":"Compile","text":"<p>Run a terminal command <code>mvn clean package</code> within the folder of each template</p>"},{"location":"tutorial/demo/#submit-your-fat-jar-to-spark","title":"Submit your fat jar to Spark","text":"<p>After running the command mentioned above, you are able to see a fat jar in <code>./target</code> folder. Please take it and use <code>./bin/spark-submit</code> to submit this jar.</p> <p>To run the jar in this way, you need to:</p> <ul> <li> <p>Either change Spark Master Address in template projects or simply delete it. Currently, they are hard coded to <code>local[*]</code> which means run locally with all cores.</p> </li> <li> <p>Change the dependency packaging scope of Apache Spark from \"compile\" to \"provided\". This is a common packaging strategy in Maven and SBT which means do not package Spark into your fat jar. Otherwise, this may lead to a huge jar and version conflicts!</p> </li> <li> <p>Make sure the dependency versions in build.sbt are consistent with your Spark version.</p> </li> </ul>"},{"location":"tutorial/demo/#run-template-projects-locally","title":"Run template projects locally","text":"<p>We highly suggest you use IDEs to run template projects on your local machine. For Scala, we recommend IntelliJ IDEA with Scala plug-in. For Java, we recommend IntelliJ IDEA and Eclipse. With the help of IDEs, you don't have to prepare anything (even don't need to download and set up Spark!). As long as you have Scala and Java, everything works properly!</p>"},{"location":"tutorial/demo/#scala","title":"Scala","text":"<p>Import the Scala template project as SBT project. Then run the Main file in this project.</p>"},{"location":"tutorial/geopandas-shapely/","title":"Work with GeoPandas and Shapely","text":""},{"location":"tutorial/geopandas-shapely/#work-with-geopandas-and-shapely","title":"Work with GeoPandas and Shapely","text":"<p>Note</p> <p>Sedona before 1.6.0 only works with Shapely 1.x. If you want to work with Shapely 2.x, please use Sedona no earlier than 1.6.0.</p> <p>If you use Sedona &lt; 1.6.0, please use GeoPandas &lt;= <code>0.11.1</code> since GeoPandas &gt; 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use &lt;= <code>1.8.5</code>.</p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-geopandas","title":"Interoperate with GeoPandas","text":"<p>Sedona Python has implemented serializers and deserializers which allows to convert Sedona Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.</p>"},{"location":"tutorial/geopandas-shapely/#from-geopandas-to-sedona-dataframe","title":"From GeoPandas to Sedona DataFrame","text":"<p>Loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\\\n      getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ngdf = gpd.read_file(\"gis_osm_pois_free_1.shp\")\n\nsedona.createDataFrame(\n  gdf\n).show()\n</code></pre> <p>This query will show the following outputs:</p> <pre><code>+---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#from-sedona-dataframe-to-geopandas","title":"From Sedona DataFrame to GeoPandas","text":"<p>Reading data with Spark and converting to GeoPandas</p> <pre><code>import geopandas as gpd\nfrom sedona.spark import *\n\nconfig = SedonaContext.builder().\n    getOrCreate()\n\nsedona = SedonaContext.create(config)\n\ncounties = sedona.\\\n    read.\\\n    option(\"delimiter\", \"|\").\\\n    option(\"header\", \"true\").\\\n    csv(\"counties.csv\")\n\ncounties.createOrReplaceTempView(\"county\")\n\ncounties_geom = sedona.sql(\n    \"SELECT *, st_geomFromWKT(geom) as geometry from county\"\n)\n\ndf = counties_geom.toPandas()\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\n\ngdf.plot(\n    figsize=(10, 8),\n    column=\"value\",\n    legend=True,\n    cmap='YlOrBr',\n    scheme='quantiles',\n    edgecolor='lightgray'\n)\n</code></pre> <p> </p> <p></p> <p> </p>"},{"location":"tutorial/geopandas-shapely/#interoperate-with-shapely-objects","title":"Interoperate with shapely objects","text":""},{"location":"tutorial/geopandas-shapely/#supported-shapely-objects","title":"Supported Shapely objects","text":"shapely object Available Point MultiPoint LineString MultiLinestring Polygon MultiPolygon GeometryCollection <p>To create Spark DataFrame based on mentioned Geometry types, please use  GeometryType  from   sedona.sql.types  module. Converting works for list or tuple with shapely objects.</p> <p>Schema for target table with integer id and geometry type can be defined as follows:</p> <pre><code>from pyspark.sql.types import IntegerType, StructField, StructType\n\nfrom sedona.spark import *\n\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), False),\n        StructField(\"geom\", GeometryType(), False)\n    ]\n)\n</code></pre> <p>Also, Spark DataFrame with geometry type can be converted to list of shapely objects with  collect  method.</p>"},{"location":"tutorial/geopandas-shapely/#point-example","title":"Point example","text":"<pre><code>from shapely.geometry import Point\n\ndata = [\n    [1, Point(21.0, 52.0)],\n    [1, Point(23.0, 42.0)],\n    [1, Point(26.0, 32.0)]\n]\n\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show()\n</code></pre> <pre><code>+---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+\n</code></pre> <pre><code>gdf.printSchema()\n</code></pre> <pre><code>root\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipoint-example","title":"MultiPoint example","text":"<pre><code>from shapely.geometry import MultiPoint\n\ndata = [\n    [1, MultiPoint([[19.511463, 51.765158], [19.446408, 51.779752]])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n).show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#linestring-example","title":"LineString example","text":"<pre><code>from shapely.geometry import LineString\n\nline = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, LineString(line)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multilinestring-example","title":"MultiLineString example","text":"<pre><code>from shapely.geometry import MultiLineString\n\nline1 = [(10, 10), (20, 20), (10, 40)]\nline2 = [(40, 40), (30, 30), (40, 20), (30, 10)]\n\ndata = [\n    [1, MultiLineString([line1, line2])]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#polygon-example","title":"Polygon example","text":"<pre><code>from shapely.geometry import Polygon\n\npolygon = Polygon(\n    [\n         [19.51121, 51.76426],\n         [19.51056, 51.76583],\n         [19.51216, 51.76599],\n         [19.51280, 51.76448],\n         [19.51121, 51.76426]\n    ]\n)\n\ndata = [\n    [1, polygon]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#multipolygon-example","title":"MultiPolygon example","text":"<pre><code>from shapely.geometry import MultiPolygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\n\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\npolygons = [\n    Polygon(exterior_p1, [interior_p1]),\n    Polygon(exterior_p2)\n]\n\ndata = [\n    [1, MultiPolygon(polygons)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/geopandas-shapely/#geometrycollection-example","title":"GeometryCollection example","text":"<pre><code>from shapely.geometry import GeometryCollection, Point, LineString, Polygon\n\nexterior_p1 = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\ninterior_p1 = [(1, 1), (1, 1.5), (1.5, 1.5), (1.5, 1), (1, 1)]\nexterior_p2 = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n\ngeoms = [\n    Polygon(exterior_p1, [interior_p1]),\n    Polygon(exterior_p2),\n    Point(1, 1),\n    LineString([(0, 0), (1, 1), (2, 2)])\n]\n\ndata = [\n    [1, GeometryCollection(geoms)]\n]\n\ngdf = sedona.createDataFrame(\n    data,\n    schema\n)\n\ngdf.show(1, False)\n</code></pre> <pre><code>+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                                                                                     |\n+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1  |GEOMETRYCOLLECTION (POLYGON ((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1 1.5, 1.5 1.5, 1.5 1, 1 1)), POLYGON ((0 0, 1 0, 1 1, 0 1, 0 0)), POINT (1 1), LINESTRING (0 0, 1 1, 2 2))|\n+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/jupyter-notebook/","title":"Python","text":""},{"location":"tutorial/jupyter-notebook/#python-jupyter-notebook-examples","title":"Python Jupyter Notebook Examples","text":"<p>Click  and play the interactive Sedona Python Jupyter Notebook immediately!</p> <p>Sedona Python provides a number of Jupyter Notebook examples.</p> <p>Please use the following steps to run Jupyter notebook with Pipenv on your machine</p> <ol> <li>Clone Sedona GitHub repo or download the source code</li> <li>Install Sedona Python from PyPI or GitHub source: Read Install Sedona Python to learn.</li> <li>Prepare spark-shaded jar: Read Install Sedona Python to learn.</li> <li>Setup pipenv python version. Please use your desired Python version.</li> </ol> <pre><code>cd docs/usecases\npipenv --python 3.8\n</code></pre> <ol> <li>Install dependencies</li> </ol> <pre><code>cd docs/usecases\npipenv install\n</code></pre> <ol> <li>Install jupyter notebook kernel for pipenv</li> </ol> <pre><code>pipenv install ipykernel\npipenv shell\n</code></pre> <ol> <li>In the pipenv shell, do</li> </ol> <pre><code>python -m ipykernel install --user --name=apache-sedona\n</code></pre> <ol> <li>Setup environment variables <code>SPARK_HOME</code> and <code>PYTHONPATH</code> if you didn't do it before. Read Install Sedona Python to learn.</li> <li>Launch jupyter notebook: <code>jupyter notebook</code></li> <li>Select Sedona notebook. In your notebook, Kernel -&gt; Change Kernel. Your kernel should now be an option.</li> </ol>"},{"location":"tutorial/python-vector-osm/","title":"Python vector osm","text":""},{"location":"tutorial/python-vector-osm/#example-of-spark-sedona-hdfs-with-slave-nodes-and-osm-vector-data-consults","title":"Example of spark + sedona + hdfs with slave nodes and OSM vector data consults","text":"<pre><code>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import StructType, StructField,StringType, LongType, IntegerType, DoubleType, ArrayType\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit, flatten\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date\nfrom pyspark.sql.functions import monotonically_increasing_id\nimport json\n</code></pre>"},{"location":"tutorial/python-vector-osm/#registering-spark-session-adding-node-executor-configurations-and-sedona-registrator","title":"Registering spark session, adding node executor configurations and sedona registrator","text":"<pre><code>spark = SparkSession.\\\n    builder.\\\n    appName(\"Overpass-API\").\\\n    enableHiveSupport().\\\n    master(\"local[*]\").\\\n    master(\"spark://spark-master:7077\").\\\n    config(\"spark.executor.memory\", \"15G\").\\\n    config(\"spark.driver.maxResultSize\", \"135G\").\\\n    config(\"spark.sql.shuffle.partitions\", \"500\").\\\n    config(' spark.sql.adaptive.coalescePartitions.enabled', True).\\\n    config('spark.sql.adaptive.enabled', True).\\\n    config('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 125).\\\n    config(\"spark.sql.execution.arrow.pyspark.enabled\", True).\\\n    config(\"spark.sql.execution.arrow.fallback.enabled\", True).\\\n    config('spark.kryoserializer.buffer.max', 2047).\\\n    config(\"spark.serializer\", KryoSerializer.getName).\\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName).\\\n    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,org.datasyslab:geotools-wrapper:1.4.0-28.2\") .\\\n    enableHiveSupport().\\\n    getOrCreate()\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-to-overpass-api-to-search-and-downloading-data-for-saving-into-hdfs","title":"Connecting to Overpass API to search and downloading data for saving into HDFS","text":"<pre><code>import requests\nimport json\n\noverpass_url = \"http://overpass-api.de/api/interpreter\"\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Foz do Igua\u00e7u\"];\nway(area)[\"highway\"~\"\"];\nout geom;\n&gt;;\nout skel qt;\n\"\"\"\n\nresponse = requests.get(overpass_url,\n                         params={'data': overpass_query})\ndata = response.json()\nhdfs = PyWebHdfsClient(host='179.106.229.159',port='50070', user_name='root')\nfile_name = \"foz_roads_osm.json\"\nhdfs.delete_file_dir(file_name)\nhdfs.create_file(file_name, json.dumps(data))\n</code></pre>"},{"location":"tutorial/python-vector-osm/#connecting-spark-sedona-with-saved-hdfs-file","title":"Connecting spark sedona with saved hdfs file","text":"<pre><code>path = \"hdfs://776faf4d6a1e:8020/\"+file_name\ndf = spark.read.json(path, multiLine = \"true\")\n</code></pre>"},{"location":"tutorial/python-vector-osm/#consulting-and-organizing-data-for-analysis","title":"Consulting and organizing data for analysis","text":"<pre><code>from pyspark.sql.functions import explode, arrays_zip\n\ndf.createOrReplaceTempView(\"df\")\ntb = spark.sql(\"select *, size(elements) total_nodes from df\")\ntb.show(5)\n\nisolate_total_nodes = tb.select(\"total_nodes\").toPandas()\ntotal_nodes = isolate_total_nodes[\"total_nodes\"].iloc[0]\nprint(total_nodes)\n\nisolate_ids = tb.select(\"elements.id\").toPandas()\nids = pd.DataFrame(isolate_ids[\"id\"].iloc[0]).drop_duplicates()\nprint(ids[0].iloc[1])\n\nformatted_df = tb\\\n.withColumn(\"id\", explode(\"elements.id\"))\n\nformatted_df.show(5)\n\nformatted_df = tb\\\n.withColumn(\"new\", arrays_zip(\"elements.id\", \"elements.geometry\", \"elements.nodes\", \"elements.tags\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\nformatted_df.show(5)\n\n# formatted_df.printSchema()\n\nformatted_df = formatted_df.select(\"new.0\",\"new.1\",\"new.2\",\"new.3.maxspeed\",\"new.3.incline\",\"new.3.surface\", \"new.3.name\", \"total_nodes\")\nformatted_df = formatted_df.withColumnRenamed(\"0\",\"id\").withColumnRenamed(\"1\",\"geom\").withColumnRenamed(\"2\",\"nodes\").withColumnRenamed(\"3\",\"tags\")\nformatted_df.createOrReplaceTempView(\"formatted_df\")\nformatted_df.show(5)\n# TODO atualizar daqui para baixo para considerar a linha inteira na l\u00f3gica\npoints_tb = spark.sql(\"select geom, id from formatted_df where geom IS NOT NULL\")\npoints_tb = points_tb\\\n.withColumn(\"new\", arrays_zip(\"geom.lat\", \"geom.lon\"))\\\n.withColumn(\"new\", explode(\"new\"))\n\npoints_tb = points_tb.select(\"new.0\",\"new.1\", \"id\")\n\npoints_tb = points_tb.withColumnRenamed(\"0\",\"lat\").withColumnRenamed(\"1\",\"lon\")\npoints_tb.printSchema()\n\npoints_tb.createOrReplaceTempView(\"points_tb\")\n\npoints_tb.show(5)\n\ncoordinates_tb = spark.sql(\"select (select collect_list(CONCAT(p1.lat,',',p1.lon)) from points_tb p1 where p1.id = p2.id group by p1.id) as coordinates, p2.id, p2.maxspeed, p2.incline, p2.surface, p2.name, p2.nodes, p2.total_nodes from formatted_df p2\")\ncoordinates_tb.createOrReplaceTempView(\"coordinates_tb\")\ncoordinates_tb.show(5)\n\nroads_tb = spark.sql(\"SELECT ST_LineStringFromText(REPLACE(REPLACE(CAST(coordinates as string),'[',''),']',''), ',') as geom, id, maxspeed, incline, surface, name, nodes, total_nodes FROM coordinates_tb WHERE coordinates IS NOT NULL\")\nroads_tb.createOrReplaceTempView(\"roads_tb\")\nroads_tb.show(5)\n</code></pre>"},{"location":"tutorial/raster/","title":"Raster SQL app","text":"<p>Note</p> <p>Sedona uses 1-based indexing for all raster functions except map algebra function, which uses 0-based indexing.</p> <p>Note</p> <p>Since v<code>1.5.0</code>, Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>Starting from <code>v1.1.0</code>, Sedona SQL supports raster data sources and raster operators in DataFrame and SQL. Raster support is available in all Sedona language bindings including Scala, Java, Python, and R.</p> <p>This page outlines the steps to manage raster data using SedonaSQL.</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"rasterDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example raster data in Sedona GitHub repo.</p>"},{"location":"tutorial/raster/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example. You can interact with Sedona Python Jupyter Notebook immediately on Binder. Click  to interact with Sedona Python Jupyter notebook immediately on Binder.</li> </ol>"},{"location":"tutorial/raster/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    getOrCreate()\n</code></pre> Please replace the <code>3.3</code> in the package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.7.0</code>.</p> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> ScalaJavaPython <p><pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaKryoRegistrator].getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>SparkSession sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaKryoRegistrator.class.getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", KryoSerializer.class.getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>sparkSession = SparkSession. \\\n    builder. \\\n    appName('appName'). \\\n    config(\"spark.serializer\", KryoSerializer.getName). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    getOrCreate()\n</code></pre> Please replace the <code>3.3</code> in the package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.7.0</code>.</p>"},{"location":"tutorial/raster/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating the Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> ScalaJavaPython <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>from sedona.register import SedonaRegistrator\n\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/raster/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a single raster data file called rasterData.tiff, at Path.</p> <p>Use the following code to load the data and create a raw Dataframe.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"binaryFile\").load(path_to_raster_data)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n</code></pre> <p>For multiple raster data files use the following code to load the data from path and create raw DataFrame.</p> <p>Note</p> <p>The above code works too for loading multiple raster data files. If the raster files are in separate directories and the option also makes sure that only <code>.tif</code> or <code>.tiff</code> files are being loaded.</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder);\nrawDf.createOrReplaceTempView(\"rawdf\");\nrawDf.show();\n</code></pre> <pre><code>rawDf = sedona.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.tif*\").load(path_to_raster_data_folder)\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will look like this:</p> <pre><code>|                path|    modificationTime|length|             content|\n+--------------------+--------------------+------+--------------------+\n|file:/Download/ra...|2023-09-06 16:24:...|209199|[4D 4D 00 2A 00 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|174803|[49 49 2A 00 08 0...|\n|file:/Download/ra...|2023-09-06 16:24:...|  6619|[49 49 2A 00 08 0...|\n</code></pre> <p>The content column in the raster table is still in the raw form, binary form.</p>"},{"location":"tutorial/raster/#create-a-raster-type-column","title":"Create a Raster type column","text":"<p>All raster operations in SedonaSQL require Raster type objects. Therefore, this should be the next step after loading the data.</p>"},{"location":"tutorial/raster/#from-geotiff","title":"From Geotiff","text":"<pre><code>SELECT RS_FromGeoTiff(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>rasterDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- rast: raster (nullable = true)\n |-- modificationTime: timestamp (nullable = true)\n |-- length: long (nullable = true)\n |-- path: string (nullable = true)\n</code></pre>"},{"location":"tutorial/raster/#from-arc-grid","title":"From Arc Grid","text":"<p>The raster data is loaded the same way as <code>tiff</code> file, but the raster data is stored with the extension <code>.asc</code>, ASCII format. The following code creates a Raster type objects from binary data:</p> <pre><code>SELECT RS_FromArcInfoAsciiGrid(content) AS rast, modificationTime, length, path FROM rawdf\n</code></pre>"},{"location":"tutorial/raster/#rasters-metadata","title":"Raster's metadata","text":"<p>Sedona has a function to get the metadata for the raster, and also a function to get the world file of the raster.</p>"},{"location":"tutorial/raster/#metadata","title":"Metadata","text":"<p>This function will return an array of metadata, it will have all the necessary information about the raster, Please refer to RS_MetaData.</p> <pre><code>SELECT RS_MetaData(rast) FROM rasterDf\n</code></pre> <p>Output for the following function will be:</p> <pre><code>[-1.3095817809482181E7, 4021262.7487925636, 512.0, 517.0, 72.32861272132695, -72.32861272132695, 0.0, 0.0, 3857.0, 1.0]\n</code></pre> <p>The first two elements of the array represent the real-world geographic coordinates (like longitude/latitude) of the raster image's top left pixel, while the next two elements represent the pixel dimensions of the raster.</p>"},{"location":"tutorial/raster/#world-file","title":"World File","text":"<p>There are two kinds of georeferences, GDAL and ESRI seen in world files. For more information please refer to RS_GeoReference.</p> <pre><code>SELECT RS_GeoReference(rast, \"ESRI\") FROM rasterDf\n</code></pre> <p>The Output will be as follows:</p> <pre><code>72.328613\n0.000000\n0.000000\n-72.328613\n-13095781.645176\n4021226.584486\n</code></pre> <p>World files are used to georeference and geolocate images by establishing an image-to-world coordinate transformation that assigns real-world geographic coordinates to the pixels of the image.</p>"},{"location":"tutorial/raster/#raster-manipulation","title":"Raster Manipulation","text":"<p>Since <code>v1.5.0</code> there have been many additions to manipulate raster data, we will show you a few example queries.</p> <p>Note</p> <p>Read SedonaSQL Raster operators to learn how you can use Sedona for raster manipulation.</p>"},{"location":"tutorial/raster/#coordinate-translation","title":"Coordinate translation","text":"<p>Sedona allows you to translate coordinates as per your needs. It can translate pixel locations to world coordinates and vice versa.</p>"},{"location":"tutorial/raster/#pixelaspoint","title":"PixelAsPoint","text":"<p>Use RS_PixelAsPoint to translate pixel coordinates to world location.</p> <pre><code>SELECT RS_PixelAsPoint(rast, 450, 400) FROM rasterDf\n</code></pre> <p>Output:</p> <pre><code>POINT (-13063342 3992403.75)\n</code></pre>"},{"location":"tutorial/raster/#world-to-raster-coordinate","title":"World to Raster Coordinate","text":"<p>Use RS_WorldToRasterCoord to translate world location to pixel coordinates. To just get X coordinate use RS_WorldToRasterCoordX and for just Y coordinate use RS_WorldToRasterCoordY.</p> <pre><code>SELECT RS_WorldToRasterCoord(rast, -1.3063342E7, 3992403.75)\n</code></pre> <p>Output:</p> <pre><code>POINT (450 400)\n</code></pre>"},{"location":"tutorial/raster/#pixel-manipulation","title":"Pixel Manipulation","text":"<p>Use RS_Values to fetch values for a specified array of Point Geometries. The coordinates in the point geometry are indicative of real-world location.</p> <pre><code>SELECT RS_Values(rast, Array(ST_Point(-13063342, 3992403.75), ST_Point(-13074192, 3996020)))\n</code></pre> <p>Output:</p> <pre><code>[132.0, 148.0]\n</code></pre> <p>To change values over a grid or area defined by geometry, we will use RS_SetValues.</p> <pre><code>SELECT RS_SetValues(\n        rast, 1, 250, 260, 3, 3,\n        Array(10, 12, 17, 26, 28, 37, 43, 64, 66)\n    )\n</code></pre> <p>Follow the links to get more information on how to use the functions appropriately.</p>"},{"location":"tutorial/raster/#band-manipulation","title":"Band Manipulation","text":"<p>Sedona provides APIs to select specific bands from a raster image and create a new raster. For example, to select 2 bands from a raster, you can use the RS_Band API to retrieve the desired multi-band raster.</p> <p>Let's use a multi-band raster for this example. The process of loading and converting it to raster type is the same.</p> <pre><code>SELECT RS_Band(colorRaster, Array(1, 2))\n</code></pre> <p>Let's say you have many single-banded rasters and want to add a band to the raster to perform map algebra operations. You can do so using RS_AddBand Sedona function.</p> <pre><code>SELECT RS_AddBand(raster1, raster2, 1, 2)\n</code></pre> <p>This will result in <code>raster1</code> having <code>raster2</code>'s specified band.</p>"},{"location":"tutorial/raster/#resample-raster-data","title":"Resample raster data","text":"<p>Sedona allows you to resample raster data using different interpolation methods like the nearest neighbor, bilinear, and bicubic to change the cell size or align raster grids, using RS_Resample.</p> <pre><code>SELECT RS_Resample(rast, 50, -50, -13063342, 3992403.75, true, \"bicubic\")\n</code></pre> <p>For more information please follow the link.</p>"},{"location":"tutorial/raster/#execute-map-algebra-operations","title":"Execute map algebra operations","text":"<p>Map algebra is a way to perform raster calculations using mathematical expressions. The expression can be a simple arithmetic operation or a complex combination of multiple operations.</p> <p>The Normalized Difference Vegetation Index (NDVI) is a simple graphical indicator that can be used to analyze remote sensing measurements from a space platform and assess whether the target being observed contains live green vegetation or not.</p> <pre><code>NDVI = (NIR - Red) / (NIR + Red)\n</code></pre> <p>where NIR is the near-infrared band and Red is the red band.</p> <pre><code>SELECT RS_MapAlgebra(raster, 'D', 'out = (rast[3] - rast[0]) / (rast[3] + rast[0]);') as ndvi FROM raster_table\n</code></pre> <p>For more information please refer to Map Algebra API.</p>"},{"location":"tutorial/raster/#interoperability-between-raster-and-vector-data","title":"Interoperability between raster and vector data","text":""},{"location":"tutorial/raster/#geometry-as-raster","title":"Geometry As Raster","text":"<p>Sedona allows you to rasterize a geometry by using RS_AsRaster.</p> <pre><code>SELECT RS_AsRaster(\n        ST_GeomFromWKT('POLYGON((150 150, 220 260, 190 300, 300 220, 150 150))'),\n        RS_MakeEmptyRaster(1, 'b', 4, 6, 1, -1, 1),\n        'b', 230\n    )\n</code></pre> <p>The image created is as below for the vector:</p> <p></p> <p>Note</p> <p>The vector coordinates are buffed up to showcase the output, the real use case, may or may not match the example.</p>"},{"location":"tutorial/raster/#spatial-range-query","title":"Spatial range query","text":"<p>Sedona provides raster predicates to do a range query using a geometry window, for example, let's use RS_Intersects.</p> <pre><code>SELECT rast FROM rasterDf WHERE RS_Intersect(rast, ST_GeomFromWKT('POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))'))\n</code></pre>"},{"location":"tutorial/raster/#spatial-join-query","title":"Spatial join query","text":"<p>Sedona's raster predicates also can do a spatial join using the raster column and geometry column, using the same function as above.</p> <pre><code>SELECT r.rast, g.geom FROM rasterDf r, geomDf g WHERE RS_Interest(r.rast, g.geom)\n</code></pre> <p>Note</p> <p>These range and join queries will filter rasters using the provided geometric boundary and the spatial boundary of the raster.</p> <p>Sedona offers more raster predicates to do spatial range queries and spatial join queries. Please refer to raster predicates docs.</p>"},{"location":"tutorial/raster/#visualize-raster-images","title":"Visualize raster images","text":"<p>Sedona provides APIs to visualize raster data in an image form.</p>"},{"location":"tutorial/raster/#base64-string","title":"Base64 String","text":"<p>The RS_AsBase64 encodes the raster data as a Base64 string and can be visualized using online decoder.</p> <pre><code>SELECT RS_AsBase64(rast) FROM rasterDf\n</code></pre>"},{"location":"tutorial/raster/#html-image","title":"HTML Image","text":"<p>The RS_AsImage returns an HTML image tag, that can be visualized using an HTML viewer or in Jupyter Notebook. For more information please click on the link.</p> <pre><code>SELECT RS_AsImage(rast, 500) FROM rasterDf\n</code></pre> <p>The output looks like this:</p> <p></p>"},{"location":"tutorial/raster/#2-d-matrix","title":"2-D Matrix","text":"<p>Sedona offers an API to visualize raster data that is not sufficient for the other APIs mentioned above.</p> <pre><code>SELECT RS_AsMatrix(rast) FROM rasterDf\n</code></pre> <p>Output will be as follows:</p> <pre><code>| 1   3   4   0|\n| 2   9  10  11|\n| 3   4   5   6|\n</code></pre> <p>Please refer to Raster visualizer docs to learn how to make the most of the visualizing APIs.</p>"},{"location":"tutorial/raster/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>Sedona has APIs that can save an entire raster column to files in a specified location. Before saving, the raster type column needs to be converted to a binary format. Sedona provides several functions to convert a raster column into a binary column suitable for file storage. Once in binary format, the raster data can then be written to files on disk using the Sedona file storage APIs.</p> <pre><code>rasterDf.write.format(\"raster\").option(\"rasterField\", \"raster\").option(\"fileExtension\", \".tiff\").mode(SaveMode.Overwrite).save(dirPath)\n</code></pre> <p>Sedona has a few writer functions that create the binary DataFrame necessary for saving the raster images.</p>"},{"location":"tutorial/raster/#as-arc-grid","title":"As Arc Grid","text":"<p>Use RS_AsArcGrid to get the binary Dataframe of the raster in Arc Grid format.</p> <pre><code>SELECT RS_AsArcGrid(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-geotiff","title":"As GeoTiff","text":"<p>Use RS_AsGeoTiff to get the binary Dataframe of the raster in GeoTiff format.</p> <pre><code>SELECT RS_AsGeoTiff(raster)\n</code></pre>"},{"location":"tutorial/raster/#as-png","title":"As PNG","text":"<p>Use RS_AsPNG to get the binary Dataframe of the raster in PNG format.</p> <pre><code>SELECT RS_AsPNG(raster)\n</code></pre> <p>Please refer to Raster writer docs for more details.</p>"},{"location":"tutorial/raster/#collecting-raster-dataframes-and-working-with-them-locally-in-python","title":"Collecting raster Dataframes and working with them locally in Python","text":"<p>Sedona allows collecting Dataframes with raster columns and working with them locally in Python since <code>v1.6.0</code>. The raster objects are represented as <code>SedonaRaster</code> objects in Python, which can be used to perform raster operations.</p> <pre><code>df_raster = sedona.read.format(\"binaryFile\").load(\"/path/to/raster.tif\").selectExpr(\"RS_FromGeoTiff(content) as rast\")\nrows = df_raster.collect()\nraster = rows[0].rast\nraster  # &lt;sedona.raster.sedona_raster.InDbSedonaRaster at 0x1618fb1f0&gt;\n</code></pre> <p>You can retrieve the metadata of the raster by accessing the properties of the <code>SedonaRaster</code> object.</p> <pre><code>raster.width        # width of the raster\nraster.height       # height of the raster\nraster.affine_trans # affine transformation matrix\nraster.crs_wkt      # coordinate reference system as WKT\n</code></pre> <p>You can get a numpy array containing the band data of the raster using the <code>as_numpy</code> or <code>as_numpy_masked</code> method. The band data is organized in CHW order.</p> <pre><code>raster.as_numpy()        # numpy array of the raster\nraster.as_numpy_masked() # numpy array with nodata values masked as nan\n</code></pre> <p>If you want to work with the raster data using <code>rasterio</code>, you can retrieve a <code>rasterio.DatasetReader</code> object using the <code>as_rasterio</code> method.</p> <p>Note</p> <p>You need to have the <code>rasterio</code> package installed (version &gt;= 1.2.10) to use this method. You can install it using <code>pip install rasterio</code>.</p> <pre><code>ds = raster.as_rasterio()  # rasterio.DatasetReader object\n# Work with the raster using rasterio\nband1 = ds.read(1)         # read the first band\n</code></pre>"},{"location":"tutorial/raster/#writing-python-udf-to-work-with-raster-data","title":"Writing Python UDF to work with raster data","text":"<p>You can write Python UDFs to work with raster data in Python. The UDFs can take <code>SedonaRaster</code> objects as input and return any Spark data type as output. This is an example of a Python UDF that calculates the mean of the raster data.</p> <pre><code>from pyspark.sql.types import DoubleType\n\ndef mean_udf(raster):\n    return float(raster.as_numpy().mean())\n\nsedona.udf.register(\"mean_udf\", mean_udf, DoubleType())\ndf_raster.withColumn(\"mean\", expr(\"mean_udf(rast)\")).show()\n</code></pre> <pre><code>+--------------------+------------------+\n|                rast|              mean|\n+--------------------+------------------+\n|GridCoverage2D[\"g...|1542.8092886117788|\n+--------------------+------------------+\n</code></pre> <p>It is much trickier to write an UDF that returns a raster object, since Sedona does not support serializing Python raster objects yet. However, you can write a UDF that returns the band data as an array and then construct the raster object using <code>RS_MakeRaster</code>. This is an example of a Python UDF that creates a mask raster based on the first band of the input raster.</p> <pre><code>from pyspark.sql.types import ArrayType, DoubleType\nimport numpy as np\n\ndef mask_udf(raster):\n    band1 = raster.as_numpy()[0,:,:]\n    mask = (band1 &lt; 1400).astype(np.float64)\n    return mask.flatten().tolist()\n\nsedona.udf.register(\"mask_udf\", band_udf, ArrayType(DoubleType()))\ndf_raster.withColumn(\"mask\", expr(\"mask_udf(rast)\")).withColumn(\"mask_rast\", expr(\"RS_MakeRaster(rast, 'I', mask)\")).show()\n</code></pre> <pre><code>+--------------------+--------------------+--------------------+\n|                rast|                mask|           mask_rast|\n+--------------------+--------------------+--------------------+\n|GridCoverage2D[\"g...|[0.0, 0.0, 0.0, 0...|GridCoverage2D[\"g...|\n+--------------------+--------------------+--------------------+\n</code></pre>"},{"location":"tutorial/raster/#performance-optimization","title":"Performance optimization","text":"<p>When working with large raster datasets, refer to the documentation on storing raster geometries in Parquet format for recommendations to optimize performance.</p>"},{"location":"tutorial/rdd/","title":"Spatial RDD app","text":"<p>The page outlines the steps to create Spatial RDDs and run spatial queries using Sedona-core.</p>"},{"location":"tutorial/rdd/#set-up-dependencies","title":"Set up dependencies","text":"<p>Please refer to Set up dependencies to set up dependencies.</p>"},{"location":"tutorial/rdd/#create-sedona-config","title":"Create Sedona config","text":"<p>Please refer to Create Sedona config to create a Sedona config.</p>"},{"location":"tutorial/rdd/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Please refer to Initiate SedonaContext to initiate a SedonaContext.</p>"},{"location":"tutorial/rdd/#create-a-spatialrdd","title":"Create a SpatialRDD","text":""},{"location":"tutorial/rdd/#create-a-typed-spatialrdd","title":"Create a typed SpatialRDD","text":"<p>Sedona-core provides three special SpatialRDDs: PointRDD, PolygonRDD, and LineStringRDD.</p> <p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#create-a-generic-spatialrdd","title":"Create a generic SpatialRDD","text":"<p>A generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instance, a WKT file contains three types geometries LineString, Polygon and MultiPolygon.</p>"},{"location":"tutorial/rdd/#from-wktwkb","title":"From WKT/WKB","text":"<p>Geometries in a WKT and WKB file always occupy a single column no matter how many coordinates they have. Sedona provides <code>WktReader</code> and <code>WkbReader</code> to create generic SpatialRDD.</p> <p>Suppose we have a <code>checkin.tsv</code> WKT TSV file at Path <code>/Download/checkin.tsv</code> as follows:</p> <pre><code>POINT (-88.331492 32.324142)    hotel\nPOINT (-88.175933 32.360763)    gas\nPOINT (-88.388954 32.357073)    bar\nPOINT (-88.221102 32.35078) restaurant\n</code></pre> <p>This file has two columns and corresponding offsets(Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.</p> <p>Use the following code to create a SpatialRDD</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/checkin.tsv\"\nval wktColumn = 0 // The WKT string starts from Column 0\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/checkin.tsv\"\nint wktColumn = 0 // The WKT string starts from Column 0\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import WktReader\nfrom sedona.core.formatMapper import WkbReader\n\nWktReader.readToGeometryRDD(sc, wkt_geometries_location, 0, True, False)\n\nWkbReader.readToGeometryRDD(sc, wkb_geometries_location, 0, True, False)\n</code></pre>"},{"location":"tutorial/rdd/#from-geojson","title":"From GeoJSON","text":"<p>Note</p> <p>Reading GeoJSON using SpatialRDD is not recommended. Please use Sedona SQL and DataFrame API to read GeoJSON files.</p> <p>Geometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.</p> <p>Suppose we have a <code>polygon.json</code> GeoJSON file at Path <code>/Download/polygon.json</code> as follows:</p> <pre><code>{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"077\", \"TRACTCE\": \"011501\", \"BLKGRPCE\": \"5\", \"AFFGEOID\": \"1500000US010770115015\", \"GEOID\": \"010770115015\", \"NAME\": \"5\", \"LSAD\": \"BG\", \"ALAND\": 6844991, \"AWATER\": 32636 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"045\", \"TRACTCE\": \"021102\", \"BLKGRPCE\": \"4\", \"AFFGEOID\": \"1500000US010450211024\", \"GEOID\": \"010450211024\", \"NAME\": \"4\", \"LSAD\": \"BG\", \"ALAND\": 11360854, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"055\", \"TRACTCE\": \"001300\", \"BLKGRPCE\": \"3\", \"AFFGEOID\": \"1500000US010550013003\", \"GEOID\": \"010550013003\", \"NAME\": \"3\", \"LSAD\": \"BG\", \"ALAND\": 1378742, \"AWATER\": 247387 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"089\", \"TRACTCE\": \"001700\", \"BLKGRPCE\": \"2\", \"AFFGEOID\": \"1500000US010890017002\", \"GEOID\": \"010890017002\", \"NAME\": \"2\", \"LSAD\": \"BG\", \"ALAND\": 1040641, \"AWATER\": 0 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },\n</code></pre> <p>Use the following code to create a generic SpatialRDD:</p> ScalaJavaPython <pre><code>val inputLocation = \"/Download/polygon.json\"\nval allowTopologyInvalidGeometries = true // Optional\nval skipSyntaxInvalidGeometries = false // Optional\nval spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>String inputLocation = \"/Download/polygon.json\"\nboolean allowTopologyInvalidGeometries = true // Optional\nboolean skipSyntaxInvalidGeometries = false // Optional\nSpatialRDD spatialRDD = GeoJsonReader.readToGeometryRDD(sedona.sparkContext, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\n</code></pre> <pre><code>from sedona.core.formatMapper import GeoJsonReader\n\nGeoJsonReader.readToGeometryRDD(sc, geo_json_file_location)\n</code></pre> <p>Warning</p> <p>The way that Sedona reads JSON file is different from SparkSQL</p>"},{"location":"tutorial/rdd/#from-shapefile","title":"From Shapefile","text":"ScalaJavaPython <pre><code>val shapefileInputLocation=\"/Download/myshapefile\"\nval spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>String shapefileInputLocation=\"/Download/myshapefile\"\nSpatialRDD spatialRDD = ShapefileReader.readToGeometryRDD(sedona.sparkContext, shapefileInputLocation)\n</code></pre> <pre><code>from sedona.core.formatMapper.shapefileParser import ShapefileReader\n\nShapefileReader.readToGeometryRDD(sc, shape_file_location)\n</code></pre> <p>Note</p> <p>The path to the shapefile is the path to the folder that contains the .shp file, not the path to the .shp file itself. The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called myShapefile, the path should be <code>XXX/myShapefile</code>. The file structure should be like this:</p> <pre><code>- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n</code></pre> <p>If the file you are reading contains non-ASCII characters you'll need to explicitly set the Spark config before initializing the SparkSession, then you can use <code>ShapefileReader.readToGeometryRDD</code>.</p> <p>Example:</p> <pre><code>spark.driver.extraJavaOptions  -Dsedona.global.charset=utf8\nspark.executor.extraJavaOptions  -Dsedona.global.charset=utf8\n</code></pre>"},{"location":"tutorial/rdd/#from-sedonasql-dataframe","title":"From SedonaSQL DataFrame","text":"<p>Note</p> <p>More details about SedonaSQL, please read the SedonaSQL tutorial.</p> <p>To create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you can use SedonaSQL.</p> <p>We use checkin.csv CSV file as the example. You can create a generic SpatialRDD using the following steps:</p> <ol> <li>Load data in SedonaSQL.</li> </ol> <pre><code>var df = sedona.read.format(\"csv\").option(\"header\", \"false\").load(csvPointInputLocation)\ndf.createOrReplaceTempView(\"inputtable\")\n</code></pre> <ol> <li>Create a Geometry type column in SedonaSQL</li> </ol> <pre><code>var spatialDf = sedona.sql(\n    \"\"\"\n        |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin\n        |FROM inputtable\n    \"\"\".stripMargin)\n</code></pre> <ol> <li>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD</li> </ol> <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"checkin\")\n</code></pre> <p>\"checkin\" is the name of the geometry column</p> <p>For WKT/WKB data, please use ==ST_GeomFromWKT / ST_GeomFromWKB == instead.</p>"},{"location":"tutorial/rdd/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in Sedona is same as the unit of all geometries in an SpatialRDD.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use spatialRDD.flipCoordinates to swap X and Y.</p> <p>To convert Coordinate Reference System of an SpatialRDD, use the following code:</p> ScalaJavaPython <pre><code>val sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nval targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>String sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\nString targetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, false)\n</code></pre> <pre><code>sourceCrsCode = \"epsg:4326\" // WGS84, the most common degree-based CRS\ntargetCrsCode = \"epsg:3857\" // The most common meter-based CRS\nobjectRDD.CRSTransform(sourceCrsCode, targetCrsCode, False)\n</code></pre> <p><code>false</code> in CRSTransform(sourceCrsCode, targetCrsCode, false) means that it will not tolerate Datum shift. If you want it to be lenient, use <code>true</code> instead.</p> <p>Warning</p> <p>CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instance, use something like this:</p> ScalaJavaPython <pre><code>val objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>SpatialRDD objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", false)\n</code></pre> <pre><code>objectRDD = WktReader.readToGeometryRDD(sedona.sparkContext, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)\nobjectRDD.CRSTransform(\"epsg:4326\", \"epsg:3857\", False)\n</code></pre> <p>The details CRS information can be found on EPSG.io</p>"},{"location":"tutorial/rdd/#read-other-attributes-in-an-spatialrdd","title":"Read other attributes in an SpatialRDD","text":"<p>Each SpatialRDD can carry non-spatial attributes such as price, age and name.</p> <p>The other attributes are combined together to a string and stored in UserData field of each geometry.</p> <p>To retrieve the UserData field, use the following code:</p> ScalaJavaPython <pre><code>val rddWithOtherAttributes = objectRDD.rawSpatialRDD.rdd.map[String](f=&gt;f.getUserData.asInstanceOf[String])\n</code></pre> <pre><code>SpatialRDD&lt;Geometry&gt; spatialRDD = Adapter.toSpatialRdd(spatialDf, \"arealandmark\");\nspatialRDD.rawSpatialRDD.map(obj -&gt; {return obj.getUserData();});\n</code></pre> <pre><code>rdd_with_other_attributes = object_rdd.rawSpatialRDD.map(lambda x: x.getUserData())\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-range-query","title":"Write a Spatial Range Query","text":"<p>A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that have specified relationship with the query window.</p> <p>Assume you now have a SpatialRDD (typed or generic). You can use the following code to issue a Spatial Range Query on it.</p> <p>spatialPredicate can be set to <code>SpatialPredicate.INTERSECTS</code> to return all geometries intersect with query window. Supported spatial predicates are:</p> <ul> <li><code>CONTAINS</code>: geometry is completely inside the query window</li> <li><code>INTERSECTS</code>: geometry have at least one point in common with the query window</li> <li><code>WITHIN</code>: geometry is completely within the query window (no touching edges)</li> <li><code>COVERS</code>: query window has no point outside of the geometry</li> <li><code>COVERED_BY</code>: geometry has no point outside of the query window</li> <li><code>OVERLAPS</code>: geometry and the query window spatially overlap</li> <li><code>CROSSES</code>: geometry and the query window spatially cross</li> <li><code>TOUCHES</code>: the only points shared between geometry and the query window are on the boundary of geometry and the query window</li> <li><code>EQUALS</code>: geometry and the query window are spatially equal</li> </ul> <p>Note</p> <p>Spatial range query is equivalent with a SELECT query with spatial predicate as search condition in Spatial SQL. An example query is as follows: <pre><code>SELECT *\nFROM checkin\nWHERE ST_Intersects(checkin.location, queryWindow)\n</code></pre></p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nval usingIndex = false\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\nboolean usingIndex = false\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQuery.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\n</code></pre> <p>Note</p> <p>Sedona Python users: Please use RangeQueryRaw from the same module if you want to avoid jvm python serde while converting to Spatial DataFrame. It takes the same parameters as RangeQuery but returns reference to jvm rdd which can be converted to dataframe without python - jvm serde using Adapter.</p> <p>Example: <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.spatialOperator import RangeQueryRaw\nfrom sedona.utils.adapter import Adapter\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False  ## Only return gemeotries fully covered by the window\nusing_index = False\nquery_result = RangeQueryRaw.SpatialRangeQuery(spatial_rdd, range_query_window, consider_boundary_intersection, using_index)\ngdf = Adapter.toDf(query_result, spark, [\"col1\", ..., \"coln\"])\n</code></pre></p>"},{"location":"tutorial/rdd/#range-query-window","title":"Range query window","text":"<p>Besides the rectangle (Envelope) type range query window, Sedona range query window can be Point/Polygon/LineString.</p> <p>The code to create a point, linestring (4 vertices) and polygon (4 vertices) is as follows:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nval polygonObject = geometryFactory.createPolygon(coordinates)\n\nval geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nval linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\n\nGeometryFactory geometryFactory = new GeometryFactory()\nCoordinate[] coordinates = new Array[Coordinate](5)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\ncoordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring\nPolygon polygonObject = geometryFactory.createPolygon(coordinates)\n\nGeometryFactory geometryFactory = new GeometryFactory()\nval coordinates = new Array[Coordinate](4)\ncoordinates(0) = new Coordinate(0,0)\ncoordinates(1) = new Coordinate(0,4)\ncoordinates(2) = new Coordinate(4,4)\ncoordinates(3) = new Coordinate(4,0)\nLineString linestringObject = geometryFactory.createLineString(coordinates)\n</code></pre> <p>A Shapely geometry can be used as a query window. To create shapely geometries, please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes","title":"Use spatial indexes","text":"<p>Sedona provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, Sedona will build a local tree index on each of the SpatialRDD partition.</p> <p>To utilize a spatial index in a spatial range query, use the following code:</p> ScalaJavaPython <pre><code>val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nvar queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>Envelope rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by the window\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nspatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, spatialPredicate, usingIndex)\n</code></pre> <pre><code>from sedona.core.geom.envelope import Envelope\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import RangeQuery\n\nrange_query_window = Envelope(-90.01, -80.01, 30.01, 40.01)\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by the window\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\n\nquery_result = RangeQuery.SpatialRangeQuery(\n    spatial_rdd,\n    range_query_window,\n    consider_boundary_intersection,\n    using_index\n)\n</code></pre> <p>Tip</p> <p>Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.</p>"},{"location":"tutorial/rdd/#output-format","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial range query is another SpatialRDD.</p> <p>The output format of the spatial range query is another RDD which consists of GeoData objects.</p> <p>SpatialRangeQuery result can be used as RDD with map or other spark RDD functions. Also it can be used as Python objects when using collect method. Example:</p> <pre><code>query_result.map(lambda x: x.geom.length).collect()\n</code></pre> <pre><code>[\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]\n</code></pre> <p>Or transformed to GeoPandas GeoDataFrame</p> <pre><code>import geopandas as gpd\ngpd.GeoDataFrame(\n    query_result.map(lambda x: [x.geom, x.userData]).collect(),\n    columns=[\"geom\", \"user_data\"],\n    geometry=\"geom\"\n)\n</code></pre>"},{"location":"tutorial/rdd/#write-a-spatial-knn-query","title":"Write a Spatial KNN Query","text":"<p>A spatial K Nearest Neighbor query takes as input a K, a query point and a SpatialRDD and finds the K geometries in the RDD which are the closest to the query point.</p> <p>Assume you now have a SpatialRDD (typed or generic). You can use the following code to issue a Spatial KNN Query on it.</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\nval usingIndex = false\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nint K = 1000 // K Nearest Neighbors\nboolean usingIndex = false\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 1000 ## K Nearest Neighbors\nusing_index = False\nresult = KNNQuery.SpatialKnnQuery(object_rdd, point, k, using_index)\n</code></pre> <p>Note</p> <p>Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL <pre><code>SELECT ck.name, ck.rating, ST_Distance(ck.location, myLocation) AS distance\nFROM checkins ck\nORDER BY distance DESC\nLIMIT 5\n</code></pre></p>"},{"location":"tutorial/rdd/#query-center-geometry","title":"Query center geometry","text":"<p>Besides the Point type, Sedona KNN query center can be Polygon and LineString.</p> Scala/JavaPython <p>To learn how to create Polygon and LineString object, see Range query window.</p> <p>To create Polygon or Linestring object please follow Shapely official docs</p>"},{"location":"tutorial/rdd/#use-spatial-indexes_1","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial KNN query, use the following code:</p> ScalaJavaPython <pre><code>val geometryFactory = new GeometryFactory()\nval pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nval buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nval usingIndex = true\nval result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>GeometryFactory geometryFactory = new GeometryFactory()\nPoint pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))\nval K = 1000 // K Nearest Neighbors\n\n\nboolean buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query\nobjectRDD.buildIndex(IndexType.RTREE, buildOnSpatialPartitionedRDD)\n\nboolean usingIndex = true\nJavaRDD result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)\n</code></pre> <pre><code>from sedona.core.spatialOperator import KNNQuery\nfrom sedona.core.enums import IndexType\nfrom shapely.geometry import Point\n\npoint = Point(-84.01, 34.01)\nk = 5 ## K Nearest Neighbors\n\nbuild_on_spatial_partitioned_rdd = False ## Set to TRUE only if run join query\nspatial_rdd.buildIndex(IndexType.RTREE, build_on_spatial_partitioned_rdd)\n\nusing_index = True\nresult = KNNQuery.SpatialKnnQuery(spatial_rdd, point, k, using_index)\n</code></pre> <p>Warning</p> <p>Only R-Tree index supports Spatial KNN query</p>"},{"location":"tutorial/rdd/#output-format_1","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.</p> <p>The output format of the spatial KNN query is a list of GeoData objects. The list has K GeoData objects.</p> <p>Example: <pre><code>&gt;&gt; result\n\n[GeoData, GeoData, GeoData, GeoData, GeoData]\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-spatial-join-query","title":"Write a Spatial Join Query","text":"<p>A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue a Spatial Join Query on them.</p> ScalaJavaPython <pre><code>val spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>SpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nobjectRDD.analyze()\n\nobjectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nconsider_boundary_intersection = False ## Only return geometries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nobject_rdd.analyze()\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nresult = JoinQuery.SpatialJoinQuery(object_rdd, query_window_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Note</p> <p>Spatial join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom);\n</code></pre> Find the superheroes in each city</p>"},{"location":"tutorial/rdd/#use-spatial-partitioning","title":"Use spatial partitioning","text":"<p>Sedona spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.</p> <p>If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.</p> Scala/JavaPython <pre><code>objectRDD.spatialPartitioning(GridType.KDBTREE)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n</code></pre> <pre><code>object_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n</code></pre> <p>Or</p> Scala/JavaPython <pre><code>queryWindowRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRDD.spatialPartitioning(queryWindowRDD.getPartitioner)\n</code></pre> <pre><code>query_window_rdd.spatialPartitioning(GridType.KDBTREE)\nobject_rdd.spatialPartitioning(query_window_rdd.getPartitioner())\n</code></pre>"},{"location":"tutorial/rdd/#use-spatial-indexes_2","title":"Use spatial indexes","text":"<p>To utilize a spatial index in a spatial join query, use the following code:</p> ScalaJavaPython <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nval buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nval usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nval result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRDD.spatialPartitioning(joinQueryPartitioningType)\nqueryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)\n\nboolean buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\nboolean usingIndex = true\nqueryWindowRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nJavaPairRDD result = JoinQuery.SpatialJoinQueryFlat(objectRDD, queryWindowRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.enums import GridType\nfrom sedona.core.enums import IndexType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.spatialPartitioning(GridType.KDBTREE)\nquery_window_rdd.spatialPartitioning(object_rdd.getPartitioner())\n\nbuild_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\nusing_index = True\nquery_window_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n\nresult = JoinQuery.SpatialJoinQueryFlat(object_rdd, query_window_rdd, using_index, True)\n</code></pre> <p>The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.</p>"},{"location":"tutorial/rdd/#output-format_2","title":"Output format","text":"Scala/JavaPython <p>The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.</p> <pre><code>Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n</code></pre> <p>Each object on the left is covered/intersected by the object on the right.</p> <p>Result for this query is RDD which holds two GeoData objects within list of lists. Example: <pre><code>result.collect()\n</code></pre></p> <pre><code>[[GeoData, GeoData], [GeoData, GeoData] ...]\n</code></pre> <p>It is possible to do some RDD operation on result data ex. Getting polygon centroid. <pre><code>result.map(lambda x: x[0].geom.centroid).collect()\n</code></pre></p> <p>Note</p> <p>Sedona Python users: Please use JoinQueryRaw from the same module for methods</p> <ul> <li> <p>spatialJoin</p> </li> <li> <p>DistanceJoinQueryFlat</p> </li> <li> <p>SpatialJoinQueryFlat</p> </li> </ul> <p>For better performance while converting to dataframe with adapter. That approach allows to avoid costly serialization between Python and jvm and in result operating on python object instead of native geometries.</p> <p>Example: <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQueryRaw\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQueryRaw.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n\ngdf = Adapter.toDf(result, [\"left_col1\", ..., \"lefcoln\"], [\"rightcol1\", ..., \"rightcol2\"], spark)\n</code></pre></p>"},{"location":"tutorial/rdd/#write-a-distance-join-query","title":"Write a Distance Join Query","text":"<p>Warning</p> <p>RDD distance joins are only reliable for points. For other geometry types, please use Spatial SQL.</p> <p>A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained here.</p> <p>If you don't want to transform your data and are ok with sacrificing the query accuracy, you can use an approximate degree value for distance. Please use this calculator.</p> <p>Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue a Distance Join Query on them.</p> ScalaJavaPython <pre><code>objectRddA.analyze()\n\nval circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nval spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nval usingIndex = false\n\nval result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>objectRddA.analyze()\n\nCircleRDD circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance\n\ncircleRDD.spatialPartitioning(GridType.KDBTREE)\nobjectRddB.spatialPartitioning(circleRDD.getPartitioner)\n\nSpatialPredicate spatialPredicate = SpatialPredicate.COVERED_BY // Only return gemeotries fully covered by each query window in queryWindowRDD\nboolean usingIndex = false\n\nJavaPairRDD result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, spatialPredicate)\n</code></pre> <pre><code>from sedona.core.SpatialRDD import CircleRDD\nfrom sedona.core.enums import GridType\nfrom sedona.core.spatialOperator import JoinQuery\n\nobject_rdd.analyze()\n\ncircle_rdd = CircleRDD(object_rdd, 0.1) ## Create a CircleRDD using the given distance\ncircle_rdd.analyze()\n\ncircle_rdd.spatialPartitioning(GridType.KDBTREE)\nspatial_rdd.spatialPartitioning(circle_rdd.getPartitioner())\n\nconsider_boundary_intersection = False ## Only return gemeotries fully covered by each query window in queryWindowRDD\nusing_index = False\n\nresult = JoinQuery.DistanceJoinQueryFlat(spatial_rdd, circle_rdd, using_index, consider_boundary_intersection)\n</code></pre> <p>Distance join can only accept <code>COVERED_BY</code> and <code>INTERSECTS</code> as spatial predicates. The rest part of the join query is same as the spatial join query.</p> <p>The details of spatial partitioning in join query is here.</p> <p>The details of using spatial indexes in join query is here.</p> <p>The output format of the distance join query is here.</p> <p>Note</p> <p>Distance join query is equal to the following query in Spatial SQL: <pre><code>SELECT superhero.name\nFROM city, superhero\nWHERE ST_Distance(city.geom, superhero.geom) &lt;= 10;\n</code></pre> Find the superheroes within 10 miles of each city</p>"},{"location":"tutorial/rdd/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.</p> <p>Note</p> <p>Non-spatial attributes such as price, age and name will also be stored to permanent storage.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-not-indexed","title":"Save an SpatialRDD (not indexed)","text":"<p>Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.</p>"},{"location":"tutorial/rdd/#save-to-distributed-wkt-text-file","title":"Save to distributed WKT text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKT text file:</p> <pre><code>objectRDD.rawSpatialRDD.saveAsTextFile(\"hdfs://PATH\")\nobjectRDD.saveAsWKT(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-wkb-text-file","title":"Save to distributed WKB text file","text":"<p>Use the following code to save an SpatialRDD as a distributed WKB text file:</p> <pre><code>objectRDD.saveAsWKB(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-geojson-text-file","title":"Save to distributed GeoJSON text file","text":"<p>Note</p> <p>Saving GeoJSON using SpatialRDD is not recommended. Please use Sedona SQL and DataFrame API to write GeoJSON files.</p> <p>Use the following code to save an SpatialRDD as a distributed GeoJSON text file:</p> <pre><code>objectRDD.saveAsGeoJSON(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-to-distributed-object-file","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> Scala/JavaPython <pre><code>objectRDD.rawSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <pre><code>object_rdd.rawJvmSpatialRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre> <p>Note</p> <p>Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.</p>"},{"location":"tutorial/rdd/#save-an-spatialrdd-indexed","title":"Save an SpatialRDD (indexed)","text":"<p>Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.</p>"},{"location":"tutorial/rdd/#save-to-distributed-object-file_1","title":"Save to distributed object file","text":"<p>Use the following code to save an SpatialRDD as a distributed object file:</p> <pre><code>objectRDD.indexedRawRDD.saveAsObjectFile(\"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed","title":"Save an SpatialRDD (spatialPartitioned W/O indexed)","text":"<p>A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!</p>"},{"location":"tutorial/rdd/#reload-a-saved-spatialrdd","title":"Reload a saved SpatialRDD","text":"<p>You can easily reload an SpatialRDD that has been saved to a distributed object file.</p>"},{"location":"tutorial/rdd/#load-to-a-typed-spatialrdd","title":"Load to a typed SpatialRDD","text":"<p>Warning</p> <p>Typed SpatialRDD has been deprecated for a long time. We do NOT recommend it anymore.</p>"},{"location":"tutorial/rdd/#load-to-a-generic-spatialrdd","title":"Load to a generic SpatialRDD","text":"<p>Use the following code to reload the SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.rawSpatialRDD = sc.objectFile[Geometry](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.rawSpatialRDD = sc.objectFile&lt;Geometry&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = load_spatial_rdd_from_disc(sc, \"hdfs://PATH\", GeoType.GEOMETRY)\n</code></pre> <p>Use the following code to reload the indexed SpatialRDD:</p> ScalaJavaPython <pre><code>var savedRDD = new SpatialRDD[Geometry]\nsavedRDD.indexedRawRDD = sc.objectFile[SpatialIndex](\"hdfs://PATH\")\n</code></pre> <pre><code>SpatialRDD savedRDD = new SpatialRDD&lt;Geometry&gt;\nsavedRDD.indexedRawRDD = sc.objectFile&lt;SpatialIndex&gt;(\"hdfs://PATH\")\n</code></pre> <pre><code>saved_rdd = SpatialRDD()\nsaved_rdd.indexedRawRDD = load_spatial_index_rdd_from_disc(sc, \"hdfs://PATH\")\n</code></pre>"},{"location":"tutorial/sql-pure-sql/","title":"Pure SQL environment","text":"<p>Starting from Sedona v1.0.1, you can use Sedona in a pure Spark SQL environment. The example code is written in SQL.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/sql-pure-sql/#initiate-session","title":"Initiate Session","text":"<p>Start <code>spark-sql</code> as following (replace <code>&lt;VERSION&gt;</code> with actual version like <code>1.7.0</code>):</p> <p>Run spark-sql with Apache Sedona</p> Spark 3.3+ and Scala 2.12 <pre><code>spark-sql --packages org.apache.sedona:sedona-spark-shaded-3.3_2.12:&lt;VERSION&gt;,org.datasyslab:geotools-wrapper:1.7.0-28.5 \\\n  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n  --conf spark.kryo.registrator=org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator \\\n  --conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions\n</code></pre> <p>Please replace the <code>3.3</code> in artifact names with the corresponding major.minor version of Spark.</p> <p>This will register all Sedona types, functions and optimizations in SedonaSQL and SedonaViz.</p>"},{"location":"tutorial/sql-pure-sql/#load-data","title":"Load data","text":"<p>Let use data from <code>examples/sql</code>. To load data from CSV file we need to execute two commands:</p> <p>Use the following code to load the data and create a raw DataFrame:</p> <pre><code>CREATE TABLE IF NOT EXISTS pointraw (_c0 string, _c1 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testpoint.csv';\n\nCREATE TABLE IF NOT EXISTS polygonraw (_c0 string, _c1 string, _c2 string, _c3 string)\nUSING csv\nOPTIONS(header='false')\nLOCATION '&lt;some path&gt;/sedona/examples/sql/src/test/resources/testenvelope.csv';\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#transform-the-data","title":"Transform the data","text":"<p>We need to transform our point and polygon data into respective types:</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointdata AS\n  SELECT ST_Point(cast(pointraw._c0 as Decimal(24,20)), cast(pointraw._c1 as Decimal(24,20))) AS pointshape\n  FROM pointraw;\n\nCREATE OR REPLACE TEMP VIEW polygondata AS\n  select ST_PolygonFromEnvelope(cast(polygonraw._c0 as Decimal(24,20)),\n        cast(polygonraw._c1 as Decimal(24,20)), cast(polygonraw._c2 as Decimal(24,20)),\n        cast(polygonraw._c3 as Decimal(24,20))) AS polygonshape\n  FROM polygonraw;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#work-with-data","title":"Work with data","text":"<p>For example, let join polygon and test data:</p> <pre><code>SELECT * from polygondata, pointdata\nWHERE ST_Contains(polygondata.polygonshape, pointdata.pointshape)\n      AND ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondata.polygonshape)\nLIMIT 5;\n</code></pre>"},{"location":"tutorial/sql-pure-sql/#geometry-data-type-support","title":"<code>GEOMETRY</code> data type support","text":"<p>Sedona has a Spark SQL parser extension to support <code>GEOMETRY</code> data type in DDL statements. For example, you can specify a schema with a geometry column when creating the table:</p> <pre><code>CREATE TABLE geom_table (id STRING, version INT, geometry GEOMETRY)\nUSING geoparquet\nLOCATION '/path/to/geoparquet_geom_table';\n\nSELECT * FROM geom_table LIMIT 10;\n</code></pre> <p>The SQL parser extension is enabled by default. If you find it conflicting with other extensions and want to disable it, please specify <code>--conf spark.sedona.enableParserExtension=false</code> when starting <code>spark-sql</code>.</p>"},{"location":"tutorial/sql/","title":"Spatial SQL app","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL.</p> <p>Note</p> <p>Since v<code>1.5.0</code>, Sedona assumes geographic coordinates to be in longitude/latitude order. If your data is lat/lon order, please use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> ScalaJavaPython <pre><code>var myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <pre><code>myDataFrame = sedona.sql(\"YOUR_SQL\")\nmyDataFrame.createOrReplaceTempView(\"spatialDf\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API. You can find example county data (i.e., <code>county_small.tsv</code>) in Sedona GitHub repo.</p>"},{"location":"tutorial/sql/#set-up-dependencies","title":"Set up dependencies","text":"Scala/JavaPython <ol> <li>Read Sedona Maven Central coordinates and add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Apache Spark core, Apache SparkSQL in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol> <ol> <li>Please read Quick start to install Sedona Python.</li> <li>This tutorial is based on Sedona SQL Jupyter Notebook example. You can interact with Sedona Python Jupyter notebook immediately on Binder. Click  to interact with Sedona Python Jupyter notebook immediately on Binder.</li> </ol>"},{"location":"tutorial/sql/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by AWS EMR/Databricks/Microsoft Fabric, please skip this step.</p> <p>Sedona &gt;= 1.4.1</p> <p>You can add additional Spark runtime config to the config builder. For example, <code>SedonaContext.builder().config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")</code></p> ScalaJavaPython <p><pre><code>import org.apache.sedona.spark.SedonaContext\n\nval config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession config = SedonaContext.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestJava\") // Change this to a proper name\n.getOrCreate()\n</code></pre> If you use SedonaViz together with SedonaSQL, please add the following line after <code>SedonaContext.builder()</code> to enable Sedona Kryo serializer: <pre><code>.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName()) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>from sedona.spark import *\n\nconfig = SedonaContext.builder() .\\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    getOrCreate()\n</code></pre> If you are using a different Spark version, please replace the <code>3.3</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.7.0</code>.</p> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> ScalaJavaPython <p><pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestScala\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaKryoRegistrator].getName)\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>SparkSession sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"readTestJava\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", KryoSerializer.class.getName()) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaKryoRegistrator.class.getName())\n.getOrCreate() // org.apache.sedona.core.serde.SedonaKryoRegistrator\n</code></pre> If you use SedonaViz together with SedonaSQL, please use the following two lines to enable Sedona Kryo serializer instead: <pre><code>.config(\"spark.serializer\", KryoSerializer.class.getName()) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", SedonaVizKryoRegistrator.class.getName()) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n</code></pre></p> <p><pre><code>sparkSession = SparkSession. \\\n    builder. \\\n    appName('readTestPython'). \\\n    config(\"spark.serializer\", KryoSerializer.getName()). \\\n    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName()). \\\n    config('spark.jars.packages',\n           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.0,'\n           'org.datasyslab:geotools-wrapper:1.7.0-28.5'). \\\n    getOrCreate()\n</code></pre> If you are using Spark versions &gt;= 3.4, please replace the <code>3.0</code> in package name of sedona-spark-shaded with the corresponding major.minor version of Spark, such as <code>sedona-spark-shaded-3.4_2.12:1.7.0</code>.</p>"},{"location":"tutorial/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by AWS EMR/Databricks/Microsoft Fabric, please call <code>sedona = SedonaContext.create(spark)</code> instead. For Databricks, the situation is more complicated, please refer to Databricks setup guide, but generally you don't need to create SedonaContext.</p> <p>Sedona &gt;= 1.4.1</p> ScalaJavaPython <pre><code>import org.apache.sedona.spark.SedonaContext\n\nval sedona = SedonaContext.create(config)\n</code></pre> <pre><code>import org.apache.sedona.spark.SedonaContext;\n\nSparkSession sedona = SedonaContext.create(config)\n</code></pre> <pre><code>from sedona.spark import *\n\nsedona = SedonaContext.create(config)\n</code></pre> <p>Sedona &lt; 1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> ScalaJavaPython <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\n</code></pre> <pre><code>from sedona.register import SedonaRegistrator\n\nSedonaRegistrator.registerAll(spark)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/sql/#load-data-from-files","title":"Load data from files","text":"<p>Assume we have a WKT file, namely <code>usa-county.tsv</code>, at Path <code>/Download/usa-county.tsv</code> as follows:</p> <pre><code>POLYGON (..., ...)  Cuming County\nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n</code></pre> <p>The file may have many other columns.</p> <p>Use the following code to load the data and create a raw DataFrame:</p> ScalaJavaPython <pre><code>var rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>Dataset&lt;Row&gt; rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <pre><code>rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(\"/Download/usa-county.tsv\")\nrawDf.createOrReplaceTempView(\"rawdf\")\nrawDf.show()\n</code></pre> <p>The output will be like this:</p> <pre><code>|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre>"},{"location":"tutorial/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <pre><code>SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n</code></pre> <p>You can select many other attributes to compose this <code>spatialdDf</code>. The output will be something like this:</p> <pre><code>|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n</code></pre> <p>Although it looks same with the input, but actually the type of column countyshape has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>spatialDf.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/sql/#load-geojson-data","title":"Load GeoJSON Data","text":"<p>Since <code>v1.6.1</code>, Sedona supports reading GeoJSON files using the <code>geojson</code> data source. It is designed to handle JSON files that use GeoJSON format for their geometries.</p> <p>This includes SpatioTemporal Asset Catalog (STAC) files, GeoJSON features, GeoJSON feature collections and other variations. The key functionality lies in the way 'geometry' fields are processed: these are specifically read as Sedona's <code>GeometryUDT</code> type, ensuring integration with Sedona's suite of spatial functions.</p>"},{"location":"tutorial/sql/#key-features","title":"Key features","text":"<ul> <li>Broad Support: The reader and writer are versatile, supporting all GeoJSON-formatted files, including STAC files, feature collections, and more.</li> <li>Geometry Transformation: When reading, fields named 'geometry' are automatically converted from GeoJSON format to Sedona's <code>GeometryUDT</code> type and vice versa when writing.</li> </ul>"},{"location":"tutorial/sql/#load-multiline-geojson-featurecollection","title":"Load MultiLine GeoJSON FeatureCollection","text":"<p>Suppose we have a GeoJSON FeatureCollection file as follows. This entire file is considered as a single GeoJSON FeatureCollection object. Multiline format is preferable for scenarios where files need to be human-readable or manually edited.</p> <pre><code>{ \"type\": \"FeatureCollection\",\n    \"features\": [\n      { \"type\": \"Feature\",\n        \"geometry\": {\"type\": \"Point\", \"coordinates\": [102.0, 0.5]},\n        \"properties\": {\"prop0\": \"value0\"}\n        },\n      { \"type\": \"Feature\",\n        \"geometry\": {\n          \"type\": \"LineString\",\n          \"coordinates\": [\n            [102.0, 0.0], [103.0, 1.0], [104.0, 0.0], [105.0, 1.0]\n            ]\n          },\n        \"properties\": {\n          \"prop0\": \"value1\",\n          \"prop1\": 0.0\n          }\n        },\n      { \"type\": \"Feature\",\n         \"geometry\": {\n           \"type\": \"Polygon\",\n           \"coordinates\": [\n             [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0],\n               [100.0, 1.0], [100.0, 0.0] ]\n             ]\n         },\n         \"properties\": {\n           \"prop0\": \"value2\",\n           \"prop1\": {\"this\": \"that\"}\n           }\n         }\n       ]\n}\n</code></pre> <p>Set the <code>multiLine</code> option to <code>True</code> to read multiline GeoJSON files.</p> PythonScalaJava <pre><code>df = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"PATH/TO/MYFILE.json\")\n .selectExpr(\"explode(features) as features\") # Explode the envelope to get one feature per row.\n .select(\"features.*\") # Unpack the features struct.\n .withColumn(\"prop0\", f.expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show()\ndf.printSchema()\n</code></pre> <pre><code>val df = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"PATH/TO/MYFILE.json\")\nval parsedDf = df.selectExpr(\"explode(features) as features\").select(\"features.*\")\n        .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\nparsedDf.show()\nparsedDf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geojson\").option(\"multiLine\", \"true\").load(\"PATH/TO/MYFILE.json\")\n .selectExpr(\"explode(features) as features\") // Explode the envelope to get one feature per row.\n .select(\"features.*\") // Unpack the features struct.\n .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show();\ndf.printSchema();\n</code></pre> <p>The output is as follows:</p> <pre><code>+--------------------+------+\n|            geometry| prop0|\n+--------------------+------+\n|     POINT (102 0.5)|value0|\n|LINESTRING (102 0...|value1|\n|POLYGON ((100 0, ...|value2|\n+--------------------+------+\n\nroot\n |-- geometry: geometry (nullable = false)\n |-- prop0: string (nullable = true)\n</code></pre>"},{"location":"tutorial/sql/#load-single-line-geojson-features","title":"Load Single Line GeoJSON Features","text":"<p>Suppose we have a single-line GeoJSON Features dataset as follows. Each line is a single GeoJSON Feature. This format is efficient for processing large datasets where each line is a separate, self-contained GeoJSON object.</p> <pre><code>{\"type\":\"Feature\",\"geometry\":{\"type\":\"Point\",\"coordinates\":[102.0,0.5]},\"properties\":{\"prop0\":\"value0\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"LineString\",\"coordinates\":[[102.0,0.0],[103.0,1.0],[104.0,0.0],[105.0,1.0]]},\"properties\":{\"prop0\":\"value1\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[100.0,0.0],[101.0,0.0],[101.0,1.0],[100.0,1.0],[100.0,0.0]]]},\"properties\":{\"prop0\":\"value2\"}}\n</code></pre> <p>By default, when <code>option</code> is not specified, Sedona reads a GeoJSON file as a single line GeoJSON.</p> PythonScalaJava <pre><code>df = sedona.read.format(\"geojson\").load(\"PATH/TO/MYFILE.json\")\n   .withColumn(\"prop0\", f.expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show()\ndf.printSchema()\n</code></pre> <pre><code>val df = sedona.read.format(\"geojson\").load(\"PATH/TO/MYFILE.json\")\n   .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show()\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geojson\").load(\"PATH/TO/MYFILE.json\")\n   .withColumn(\"prop0\", expr(\"properties['prop0']\")).drop(\"properties\").drop(\"type\")\n\ndf.show()\ndf.printSchema()\n</code></pre> <p>The output is as follows:</p> <pre><code>+--------------------+------+\n|            geometry| prop0|\n+--------------------+------+\n|     POINT (102 0.5)|value0|\n|LINESTRING (102 0...|value1|\n|POLYGON ((100 0, ...|value2|\n+--------------------+------+\n\nroot\n |-- geometry: geometry (nullable = false)\n |-- prop0: string (nullable = true)\n</code></pre>"},{"location":"tutorial/sql/#load-shapefile","title":"Load Shapefile","text":"<p>Since v<code>1.7.0</code>, Sedona supports loading Shapefile as a DataFrame.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = sedona.read.format(\"shapefile\").load(\"/path/to/shapefile\")\n</code></pre> <p>The input path can be a directory containing one or multiple shapefiles, or path to a <code>.shp</code> file.</p> <ul> <li>When the input path is a directory, all shapefiles directly under the directory will be loaded. If you want to load all shapefiles in subdirectories, please specify <code>.option(\"recursiveFileLookup\", \"true\")</code>.</li> <li>When the input path is a <code>.shp</code> file, that shapefile will be loaded. Sedona will look for sibling files (<code>.dbf</code>, <code>.shx</code>, etc.) with the same main file name and load them automatically.</li> </ul> <p>The name of the geometry column is <code>geometry</code> by default. You can change the name of the geometry column using the <code>geometry.name</code> option. If one of the non-spatial attributes is named \"geometry\", <code>geometry.name</code> must be configured to avoid conflict.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").option(\"geometry.name\", \"geom\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").option(\"geometry.name\", \"geom\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = sedona.read.format(\"shapefile\").option(\"geometry.name\", \"geom\").load(\"/path/to/shapefile\")\n</code></pre> <p>Each record in shapefile has a unique record number, that record number is not loaded by default. If you want to include record number in the loaded DataFrame, you can set the <code>key.name</code> option to the name of the record number column:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").option(\"key.name\", \"FID\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").option(\"key.name\", \"FID\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = sedona.read.format(\"shapefile\").option(\"key.name\", \"FID\").load(\"/path/to/shapefile\")\n</code></pre> <p>The character encoding of string attributes are inferred from the <code>.cpg</code> file. If you see garbled values in string fields, you can manually specify the correct charset using the <code>charset</code> option. For example:</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"shapefile\").option(\"charset\", \"UTF-8\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"shapefile\").option(\"charset\", \"UTF-8\").load(\"/path/to/shapefile\")\n</code></pre> <pre><code>df = sedona.read.format(\"shapefile\").option(\"charset\", \"UTF-8\").load(\"/path/to/shapefile\")\n</code></pre>"},{"location":"tutorial/sql/#deprecated-loading-shapefile-using-spatialrdd","title":"(Deprecated) Loading Shapefile using SpatialRDD","text":"<p>If you are using Sedona earlier than v<code>1.7.0</code>, you can load shapefiles as SpatialRDD and converted to DataFrame using Adapter. Please read Load SpatialRDD and DataFrame &lt;-&gt; RDD.</p>"},{"location":"tutorial/sql/#load-geoparquet","title":"Load GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports loading GeoParquet file. Sedona will infer geometry fields using the \"geo\" metadata in GeoParquet files.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- pop_est: long (nullable = true)\n |-- continent: string (nullable = true)\n |-- name: string (nullable = true)\n |-- iso_a3: string (nullable = true)\n |-- gdp_md_est: double (nullable = true)\n |-- geometry: geometry (nullable = true)\n</code></pre> <p>Sedona supports spatial predicate push-down for GeoParquet files, please refer to the SedonaSQL query optimizer documentation for details.</p> <p>GeoParquet file reader can also be used to read legacy Parquet files written by Apache Sedona 1.3.1-incubating or earlier. Please refer to Reading Legacy Parquet Files for details.</p> <p>Warning</p> <p>GeoParquet file reader does not work on Databricks runtime when Photon is enabled. Please disable Photon when using GeoParquet file reader on Databricks runtime.</p>"},{"location":"tutorial/sql/#inspect-geoparquet-metadata","title":"Inspect GeoParquet metadata","text":"<p>Since v<code>1.5.1</code>, Sedona provides a Spark SQL data source <code>\"geoparquet.metadata\"</code> for inspecting GeoParquet metadata. The resulting dataframe contains the \"geo\" metadata for each input file.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <pre><code>df = sedona.read.format(\"geoparquet.metadata\").load(geoparquetdatalocation1)\ndf.printSchema()\n</code></pre> <p>The output will be as follows:</p> <pre><code>root\n |-- path: string (nullable = true)\n |-- version: string (nullable = true)\n |-- primary_column: string (nullable = true)\n |-- columns: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- encoding: string (nullable = true)\n |    |    |-- geometry_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- bbox: array (nullable = true)\n |    |    |    |-- element: double (containsNull = true)\n |    |    |-- crs: string (nullable = true)\n</code></pre> <p>If the input Parquet file does not have GeoParquet metadata, the values of <code>version</code>, <code>primary_column</code> and <code>columns</code> fields of the resulting dataframe will be <code>null</code>.</p> <p>Note</p> <p><code>geoparquet.metadata</code> only supports reading GeoParquet specific metadata. Users can use G-Research/spark-extension to read comprehensive metadata of generic Parquet files.</p>"},{"location":"tutorial/sql/#load-data-from-jdbc-data-sources","title":"Load data from JDBC data sources","text":"<p>The 'query' option in Spark SQL's JDBC data source can be used to convert geometry columns to a format that Sedona can interpret. This should work for most spatial JDBC data sources. For Postgis there is no need to add a query to convert geometry types since it's already using EWKB as it's wire format.</p> ScalaJavaPython <pre><code>// For any JDBC data source, including Postgis.\nval df = sedona.read.format(\"jdbc\")\n    // Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nval df = sedona.read.format(\"jdbc\")\n    // Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code>// For any JDBC data source, including Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n    // Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n\n// This is a simplified version that works for Postgis.\nDataset&lt;Row&gt; df = sedona.read().format(\"jdbc\")\n    // Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n</code></pre> <pre><code># For any JDBC data source, including Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"query\", \"SELECT id, ST_AsBinary(geom) as geom FROM my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n\n# This is a simplified version that works for Postgis.\ndf = (sedona.read.format(\"jdbc\")\n    # Other options.\n    .option(\"dbtable\", \"my_table\")\n    .load()\n    .withColumn(\"geom\", f.expr(\"ST_GeomFromWKB(geom)\")))\n</code></pre>"},{"location":"tutorial/sql/#load-from-geopackage","title":"Load from geopackage","text":"<p>Since v1.7.0, Sedona supports loading Geopackage file format as a DataFrame.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>df = sedona.read.format(\"geopackage\").option(\"tableName\", \"tab\").load(\"/path/to/geopackage\")\n</code></pre> <p>Geopackage files can contain vector data and raster data. To show the possible options from a file you can look into the metadata table by adding parameter showMetadata and set its value as true.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geopackage\").option(\"showMetadata\", \"true\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"geopackage\").option(\"showMetadata\", \"true\").load(\"/path/to/geopackage\")\n</code></pre> <p>```python df = sedona.read.format(\"geopackage\").option(\"showMetadata\", \"true\").load(\"/path/to/geopackage\")</p> <p>Then you can see the metadata of the geopackage file like below.</p> <pre><code>+--------------------+---------+--------------------+-----------+--------------------+----------+-----------------+----------+----------+------+\n|          table_name|data_type|          identifier|description|         last_change|     min_x|            min_y|     max_x|     max_y|srs_id|\n+--------------------+---------+--------------------+-----------+--------------------+----------+-----------------+----------+----------+------+\n|gis_osm_water_a_f...| features|gis_osm_water_a_f...|           |2024-09-30 23:07:...|-9.0257084|57.96814069999999|33.4866675|80.4291867|  4326|\n+--------------------+---------+--------------------+-----------+--------------------+----------+-----------------+----------+----------+------+\n</code></pre> <p>You can also load data from raster tables in the geopackage file. To load raster data, you can use the following code.</p> Scala/JavaJavaPython <pre><code>val df = sedona.read.format(\"geopackage\").option(\"tableName\", \"raster_table\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = sedona.read().format(\"geopackage\").option(\"tableName\", \"raster_table\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>df = sedona.read.format(\"geopackage\").option(\"tableName\", \"raster_table\").load(\"/path/to/geopackage\")\n</code></pre> <pre><code>+---+----------+-----------+--------+--------------------+\n| id|zoom_level|tile_column|tile_row|           tile_data|\n+---+----------+-----------+--------+--------------------+\n|  1|        11|        428|     778|GridCoverage2D[\"c...|\n|  2|        11|        429|     778|GridCoverage2D[\"c...|\n|  3|        11|        428|     779|GridCoverage2D[\"c...|\n|  4|        11|        429|     779|GridCoverage2D[\"c...|\n|  5|        11|        427|     777|GridCoverage2D[\"c...|\n+---+----------+-----------+--------+--------------------+\n</code></pre> <p>Known limitations (v1.7.0):</p> <ul> <li>webp rasters are not supported</li> <li>ewkb geometries are not supported</li> <li>filtering based on geometries envelopes are not supported</li> </ul> <p>All points above should be resolved soon, stay tuned !</p>"},{"location":"tutorial/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>By default, this function uses lon/lat order since <code>v1.5.0</code>. Before, it used lat/lon order. You can use ST_FlipCoordinates to swap X and Y.</p> <p>For more details, please read the <code>ST_Transform</code> section in Sedona API References.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\nFROM spatialdf\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>The coordinates of polygons have been changed. The output will be like this:</p> <pre><code>+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n</code></pre>"},{"location":"tutorial/sql/#cluster-with-dbscan","title":"Cluster with DBSCAN","text":"<p>Sedona provides an implementation of the DBSCAN algorithm to cluster spatial data.</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has an additional column added containing the unique identifier of the cluster that record is a member of and a boolean column indicating if the record is a core point.</p> <p>The first parameter is the dataframe, the next two are the epsilon and min_points parameters of the DBSCAN algorithm.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.clustering.DBSCAN.dbscan\n\ndbscan(df, 0.1, 5).show()\n</code></pre> <pre><code>import org.apache.sedona.stats.clustering.DBSCAN;\n\nDBSCAN.dbscan(df, 0.1, 5).show();\n</code></pre> <pre><code>from sedona.stats.clustering.dbscan import dbscan\n\ndbscan(df, 0.1, 5).show()\n</code></pre> <p>The output will look like this:</p> <pre><code>+----------------+---+------+-------+\n|        geometry| id|isCore|cluster|\n+----------------+---+------+-------+\n|   POINT (2.5 4)|  3| false|      1|\n|     POINT (3 4)|  2| false|      1|\n|     POINT (3 5)|  5| false|      1|\n|     POINT (1 3)|  9|  true|      0|\n| POINT (2.5 4.5)|  7|  true|      1|\n|     POINT (1 2)|  1|  true|      0|\n| POINT (1.5 2.5)|  4|  true|      0|\n| POINT (1.2 2.5)|  8|  true|      0|\n|   POINT (1 2.5)| 11|  true|      0|\n|     POINT (1 5)| 10| false|     -1|\n|     POINT (5 6)| 12| false|     -1|\n|POINT (12.8 4.5)|  6| false|     -1|\n|     POINT (4 3)| 13| false|     -1|\n+----------------+---+------+-------+\n</code></pre>"},{"location":"tutorial/sql/#calculate-the-local-outlier-factor-lof","title":"Calculate the Local Outlier Factor (LOF)","text":"<p>Sedona provides an implementation of the Local Outlier Factor algorithm to identify anomalous data.</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has an additional column added containing the local outlier factor.</p> <p>The first parameter is the dataframe, the next is the number of nearest neighbors to consider use in calculating the score.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.outlierDetection.LocalOutlierFactor.localOutlierFactor\n\nlocalOutlierFactor(df, 20).show()\n</code></pre> <pre><code>import org.apache.sedona.stats.outlierDetection.LocalOutlierFactor;\n\nLocalOutlierFactor.localOutlierFactor(df, 20).show();\n</code></pre> <pre><code>from sedona.stats.outlier_detection.local_outlier_factor import local_outlier_factor\n\nlocal_outlier_factor(df, 20).show()\n</code></pre> <p>The output will look like this:</p> <pre><code>+--------------------+------------------+\n|            geometry|               lof|\n+--------------------+------------------+\n|POINT (-2.0231305...| 0.952098153363662|\n|POINT (-2.0346944...|0.9975325496668104|\n|POINT (-2.2040074...|1.0825843906411081|\n|POINT (1.61573501...|1.7367129352162634|\n|POINT (-2.1176324...|1.5714144683150393|\n|POINT (-2.2349759...|0.9167275845938276|\n|POINT (1.65470192...| 1.046231536764447|\n|POINT (0.62624112...|1.1988700676990034|\n|POINT (2.01746261...|1.1060219481067417|\n|POINT (-2.0483857...|1.0775553430145446|\n|POINT (2.43969463...|1.1129132178576646|\n|POINT (-2.2425480...| 1.104108012697006|\n|POINT (-2.7859235...|  2.86371824574529|\n|POINT (-1.9738858...|1.0398822680356794|\n|POINT (2.00153403...| 0.927409656346015|\n|POINT (2.06422812...|0.9222203762264445|\n|POINT (-1.7533819...|1.0273650471626696|\n|POINT (-2.2030766...| 0.964744555830738|\n|POINT (-1.8509857...|1.0375927869698574|\n|POINT (2.10849080...|1.0753419197322656|\n+--------------------+------------------+\n</code></pre>"},{"location":"tutorial/sql/#perform-getis-ord-gi-hot-spot-analysis","title":"Perform Getis-Ord Gi(*) Hot Spot Analysis","text":"<p>Sedona provides an implementation of the Gi and Gi* algorithms to identify local hotspots in spatial data</p> <p>The algorithm is available as a Scala and Python function called on a spatial dataframe. The returned dataframe has additional columns added containing G statistic, E[G], V[G], the Z score, and the p-value.</p> <p>Using Gi involves first generating the neighbors list for each record, then calling the g_local function.</p> ScalaJavaPython <pre><code>import org.apache.sedona.stats.Weighting.addBinaryDistanceBandColumn\nimport org.apache.sedona.stats.hotspotDetection.GetisOrd.gLocal\n\nval distanceRadius = 1.0\nval weightedDf = addBinaryDistanceBandColumn(df, distanceRadius)\ngLocal(weightedDf, \"val\").show()\n</code></pre> <pre><code>import org.apache.sedona.stats.Weighting;\nimport org.apache.sedona.stats.hotspotDetection.GetisOrd;\nimport org.apache.spark.sql.DataFrame;\n\ndouble distanceRadius = 1.0;\nDataFrame weightedDf = Weighting.addBinaryDistanceBandColumn(df, distanceRadius);\nGetisOrd.gLocal(weightedDf, \"val\").show();\n</code></pre> <pre><code>from sedona.stats.weighting import add_binary_distance_band_column\nfrom sedona.stats.hotspot_detection.getis_ord import g_local\n\ndistance_radius = 1.0\nweighted_df = addBinaryDistanceBandColumn(df, distance_radius)\ng_local(weightedDf, \"val\").show()\n</code></pre> <p>The output will look like this:</p> <pre>\n<code>\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n|   geometry|val|             weights|                  G|                 EG|                  VG|                   Z|                   P|\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n|POINT (2 2)|0.9|[{{POINT (2 3), 1...| 0.4488188976377953|0.45454545454545453| 0.00356321373799772|-0.09593402008347063|  0.4617864875295957|\n|POINT (2 3)|1.2|[{{POINT (2 2), 0...|0.35433070866141736|0.36363636363636365|0.003325666155464539|-0.16136436037034918|  0.4359032175415549|\n|POINT (3 3)|1.2|[{{POINT (2 3), 1...|0.28346456692913385| 0.2727272727272727|0.002850570990398176| 0.20110780337013057| 0.42030714022155924|\n|POINT (3 2)|1.2|[{{POINT (2 2), 0...| 0.4488188976377953|0.45454545454545453| 0.00356321373799772|-0.09593402008347063|  0.4617864875295957|\n|POINT (3 1)|1.2|[{{POINT (3 2), 3...| 0.3622047244094489| 0.2727272727272727|0.002850570990398176|  1.6758983614177538| 0.04687905137429871|\n|POINT (2 1)|2.2|[{{POINT (2 2), 0...| 0.4330708661417323|0.36363636363636365|0.003325666155464539|  1.2040263812249166| 0.11428969105925013|\n|POINT (1 1)|1.2|[{{POINT (2 1), 5...| 0.2834645669291339| 0.2727272727272727|0.002850570990398176|  0.2011078033701316|  0.4203071402215588|\n|POINT (1 2)|0.2|[{{POINT (2 2), 0...|0.35433070866141736|0.45454545454545453| 0.00356321373799772|   -1.67884535146075|0.046591093685710794|\n|POINT (1 3)|1.2|[{{POINT (2 3), 1...| 0.2047244094488189| 0.2727272727272727|0.002850570990398176| -1.2736827546774914| 0.10138793530151635|\n|POINT (0 2)|1.0|[{{POINT (1 2), 7...|0.09448818897637795|0.18181818181818182|0.002137928242798632| -1.8887168824332323|0.029464887612748458|\n|POINT (4 2)|1.2|[{{POINT (3 2), 3...| 0.1889763779527559|0.18181818181818182|0.002137928242798632| 0.15481285921583854| 0.43848442662481324|\n+-----------+---+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n</code>\n</pre>"},{"location":"tutorial/sql/#run-spatial-queries","title":"Run spatial queries","text":"<p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>SELECT *\nFROM spatialdf\nWHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n</code></pre> <p>Note</p> <p>Read SedonaSQL constructor API to learn how to create a Geometry type query window</p>"},{"location":"tutorial/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\nFROM spatialdf\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/sql/#join-query","title":"Join query","text":"<p>The details of a join query is available here Join query.</p>"},{"location":"tutorial/sql/#knn-join-query","title":"KNN join query","text":"<p>The details of a KNN join query is available here KNN join query.</p>"},{"location":"tutorial/sql/#other-queries","title":"Other queries","text":"<p>There are lots of other functions can be combined with these queries. Please read SedonaSQL functions and SedonaSQL aggregate functions.</p>"},{"location":"tutorial/sql/#visualize-query-results","title":"Visualize query results","text":"<p>Sedona provides <code>SedonaPyDeck</code> and <code>SedonaKepler</code> wrappers, both of which expose APIs to create interactive map visualizations from SedonaDataFrames in a Jupyter environment.</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler expect the default geometry order to be lon-lat. If your dataframe has geometries in the lat-lon order, please check out ST_FlipCoordinates</p> <p>Note</p> <p>Both SedonaPyDeck and SedonaKepler are designed to work with SedonaDataFrames containing only 1 geometry column. Passing dataframes with multiple geometry columns will cause errors.</p>"},{"location":"tutorial/sql/#sedonapydeck","title":"SedonaPyDeck","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaPyDeck.</p> <p>SedonaPyDeck exposes APIs to create interactive map visualizations using pydeck based on deck.gl</p> <p>Note</p> <p>To use SedonaPyDeck, install sedona with the <code>pydeck-map</code> extra: <pre><code>pip install sedona[pydeck-map]\n</code></pre></p> <p>The following tutorial showcases the various maps that can be created using SedonaPyDeck, the datasets used to create these maps are publicly available.</p> <p>Each API exposed by SedonaPyDeck offers customization via optional arguments, details on all possible arguments can be found in the API docs of SedonaPyDeck.</p>"},{"location":"tutorial/sql/#creating-a-choropleth-map-using-sedonapydeck","title":"Creating a Choropleth map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a <code>create_choropleth_map</code> API which can be used to visualize a choropleth map out of the passed SedonaDataFrame containing polygons with an observation:</p> <p>Example (referenced from example notebook available via binder):</p> <pre><code>SedonaPyDeck.create_choropleth_map(df=groupedresult, plot_col='AirportCount')\n</code></pre> <p>Note</p> <p><code>plot_col</code> is a required argument informing SedonaPyDeck of the column name used to render the choropleth effect.</p> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p>"},{"location":"tutorial/sql/#creating-a-geometry-map-using-sedonapydeck","title":"Creating a Geometry map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_geometry_map API which can be used to visualize a passed SedonaDataFrame containing any type of geometries:</p> <p>Example (referenced from overture notebook available via binder):</p> <pre><code>SedonaPyDeck.create_geometry_map(df_building, elevation_col='height')\n</code></pre> <p></p> <p>Tip</p> <p><code>elevation_col</code> is an optional argument which can be used to render a 3D map. Pass the column with 'elevation' values for the geometries here.</p>"},{"location":"tutorial/sql/#creating-a-scatterplot-map-using-sedonapydeck","title":"Creating a Scatterplot map using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_scatterplot_map API which can be used to visualize a scatterplot out of the passed SedonaDataFrame containing points:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_scatterplot_map(df=crimes_df)\n</code></pre> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#creating-a-heatmap-using-sedonapydeck","title":"Creating a heatmap using SedonaPyDeck","text":"<p>SedonaPyDeck exposes a create_heatmap API which can be used to visualize a heatmap out of the passed SedonaDataFrame containing points:</p> <p>Example:</p> <pre><code>SedonaPyDeck.create_heatmap(df=crimes_df)\n</code></pre> <p></p> <p>The dataset used here is the Chicago crimes dataset, available here</p>"},{"location":"tutorial/sql/#sedonakepler","title":"SedonaKepler","text":"<p>Spatial query results can be visualized in a Jupyter lab/notebook environment using SedonaKepler.</p> <p>SedonaKepler exposes APIs to create interactive and customizable map visualizations using KeplerGl.</p> <p>Note</p> <p>To use SedonaKepler, install sedona with the <code>kepler-map</code> extra: <pre><code>pip install sedona[kepler-map]\n</code></pre></p> <p>This tutorial showcases how simple it is to instantly visualize geospatial data using SedonaKepler.</p> <p>Example (referenced from an example notebook via the binder):</p> <pre><code>SedonaKepler.create_map(df=groupedresult, name=\"AirportCount\")\n</code></pre> <p></p> <p>The dataset used is available here and can also be found in the example notebook available here</p> <p>Details on all the APIs available by SedonaKepler are listed in the SedonaKepler API docs</p>"},{"location":"tutorial/sql/#create-a-user-defined-function-udf","title":"Create a User-Defined Function (UDF)","text":"<p>User-Defined Functions (UDFs) are user-created procedures that can perform operations on a single row of information. To cover almost all use cases, we will showcase 4 types of UDFs for a better understanding of how to use geometry with UDFs. Sedona's serializer deserializes the SQL geometry type to JTS Geometry (Scala/Java) or Shapely Geometry (Python). You can implement any custom logic using the rich ecosystem around these two libraries.</p>"},{"location":"tutorial/sql/#geometry-to-primitive","title":"Geometry to primitive","text":"<p>This UDF example takes a geometry type input and returns a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef lengthPoly(geom: Geometry): Double = {\n    geom.getLength\n}\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly _)\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_lengthPoly\",\n        (UDF1&lt;Geometry, Double&gt;) Geometry::getLength,\n        DataTypes.DoubleType);\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef lengthPoly(geom: GeometryType()):\n    return geom.length\n\nsedona.udf.register(\"udf_lengthPoly\", lengthPoly, DoubleType())\n\ndf.selectExpr(\"udf_lengthPoly(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------+\n|udf_lengthPoly(geom)|\n+--------------------+\n|   3.414213562373095|\n+--------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-to-geometry","title":"Geometry to Geometry","text":"<p>This UDF example takes a geometry type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferFixed(geom: Geometry): Geometry = {\n    geom.buffer(5.5)\n}\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed _)\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF1;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_bufferFixed\",\n        (UDF1&lt;Geometry, Geometry&gt;) geom -&gt;\n            geom.buffer(5.5),\n        new GeometryUDT());\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferFixed(geom: GeometryType()):\n    return geom.buffer(5.5)\n\nsedona.udf.register(\"udf_bufferFixed\", bufferFixed, GeometryType())\n\ndf.selectExpr(\"udf_bufferFixed(geom)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                             udf_bufferFixed(geom)|\n+--------------------------------------------------+\n|POLYGON ((1 -4.5, -0.0729967710887076 -4.394319...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry","title":"Geometry, primitive to geometry","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\n\ndef bufferIt(geom: Geometry, distance: Double): Geometry = {\n    geom.buffer(distance)\n}\n\nsedona.udf.register(\"udf_buffer\", bufferIt _)\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\n        \"udf_buffer\",\n        (UDF2&lt;Geometry, Double, Geometry&gt;) Geometry::buffer,\n        new GeometryUDT());\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import DoubleType\n\ndef bufferIt(geom: GeometryType(), distance: DoubleType()):\n    return geom.buffer(distance)\n\nsedona.udf.register(\"udf_buffer\", bufferIt, GeometryType())\n\ndf.selectExpr(\"udf_buffer(geom, distance)\").show()\n</code></pre> <p>Output:</p> <pre><code>+--------------------------------------------------+\n|                        udf_buffer(geom, distance)|\n+--------------------------------------------------+\n|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|\n+--------------------------------------------------+\n</code></pre>"},{"location":"tutorial/sql/#geometry-primitive-to-geometry-primitive","title":"Geometry, primitive to Geometry, primitive","text":"<p>This UDF example takes a geometry type input and a primitive type input and returns a geometry type and a primitive type output:</p> ScalaJavaPython <pre><code>import org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.api.java.UDF2\n\nval schemaUDF = StructType(Array(\n    StructField(\"buffed\", GeometryUDT),\n    StructField(\"length\", DoubleType)\n))\n\nval udf_bufferLength = udf(\n    new UDF2[Geometry, Double, (Geometry, Double)] {\n        def call(geom: Geometry, distance: Double): (Geometry, Double) = {\n            val buffed = geom.buffer(distance)\n            val length = geom.getLength\n            (buffed, length)\n        }\n    }, schemaUDF)\n\nsedona.udf.register(\"udf_bufferLength\", udf_bufferLength)\n\ndata.withColumn(\"bufferLength\", expr(\"udf_bufferLengths(geom, distance)\"))\n    .select(\"geom\", \"distance\", \"bufferLength.*\")\n    .show()\n</code></pre> <pre><code>import org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.StructType;\nimport scala.Tuple2;\n\nStructType schemaUDF = new StructType()\n            .add(\"buffedGeom\", new GeometryUDT())\n            .add(\"length\", DataTypes.DoubleType);\n\n// using lambda function to register the UDF\nsparkSession.udf().register(\"udf_bufferLength\",\n            (UDF2&lt;Geometry, Double, Tuple2&lt;Geometry, Double&gt;&gt;) (geom, distance) -&gt; {\n                Geometry buffed = geom.buffer(distance);\n                Double length = buffed.getLength();\n                return new Tuple2&lt;&gt;(buffed, length);\n            },\n            schemaUDF);\n\ndf.withColumn(\"bufferLength\", functions.expr(\"udf_bufferLength(geom, distance)\"))\n            .select(\"geom\", \"distance\", \"bufferLength.*\")\n            .show();\n</code></pre> <pre><code>from sedona.sql.types import GeometryType\nfrom pyspark.sql.types import *\n\nschemaUDF = StructType([\n    StructField(\"buffed\", GeometryType()),\n    StructField(\"length\", DoubleType())\n])\n\ndef bufferAndLength(geom: GeometryType(), distance: DoubleType()):\n    buffed = geom.buffer(distance)\n    length = buffed.length\n    return [buffed, length]\n\nsedona.udf.register(\"udf_bufferLength\", bufferAndLength, schemaUDF)\n\ndf.withColumn(\"bufferLength\", expr(\"udf_bufferLength(geom, buffer)\")) \\\n            .select(\"geom\", \"buffer\", \"bufferLength.*\") \\\n            .show()\n</code></pre> <p>Output:</p> <pre><code>+------------------------------+--------+--------------------------------------------------+-----------------+\n|                          geom|distance|                                        buffedGeom|           length|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n|POLYGON ((1 1, 1 2, 2 1, 1 1))|    10.0|POLYGON ((1 -9, -0.9509032201612866 -8.80785280...|66.14518337329191|\n+------------------------------+--------+--------------------------------------------------+-----------------+\n</code></pre>"},{"location":"tutorial/sql/#save-to-permanent-storage","title":"Save to permanent storage","text":"<p>To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.</p> <p>Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:</p> <pre><code>SELECT ST_AsText(countyshape)\nFROM polygondf\n</code></pre>"},{"location":"tutorial/sql/#save-as-geojson","title":"Save as GeoJSON","text":"<p>Since <code>v1.6.1</code>, the GeoJSON data source in Sedona can be used to save a Spatial DataFrame to a single-line JSON file, with geometries written in GeoJSON format.</p> <pre><code>df.write.format(\"geojson\").save(\"YOUR/PATH.json\")\n</code></pre> <p>The structure of the generated file will be like this:</p> <pre><code>{\"type\":\"Feature\",\"geometry\":{\"type\":\"Point\",\"coordinates\":[102.0,0.5]},\"properties\":{\"prop0\":\"value0\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"LineString\",\"coordinates\":[[102.0,0.0],[103.0,1.0],[104.0,0.0],[105.0,1.0]]},\"properties\":{\"prop0\":\"value1\"}}\n{\"type\":\"Feature\",\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[100.0,0.0],[101.0,0.0],[101.0,1.0],[100.0,1.0],[100.0,0.0]]]},\"properties\":{\"prop0\":\"value2\"}}\n</code></pre>"},{"location":"tutorial/sql/#save-geoparquet","title":"Save GeoParquet","text":"<p>Since v<code>1.3.0</code>, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:</p> <pre><code>df.write.format(\"geoparquet\").save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre>"},{"location":"tutorial/sql/#crs-metadata","title":"CRS Metadata","text":"<p>Since v<code>1.5.1</code>, Sedona supports writing GeoParquet files with custom GeoParquet spec version and crs. The default GeoParquet spec version is <code>1.0.0</code> and the default crs is <code>null</code>. You can specify the GeoParquet spec version and crs as follows:</p> <pre><code>val projjson = \"{...}\" // PROJJSON string for all geometry columns\ndf.write.format(\"geoparquet\")\n        .option(\"geoparquet.version\", \"1.0.0\")\n        .option(\"geoparquet.crs\", projjson)\n        .save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>If you have multiple geometry columns written to the GeoParquet file, you can specify the CRS for each column. For example, <code>g0</code> and <code>g1</code> are two geometry columns in the DataFrame <code>df</code>, and you want to specify the CRS for each column as follows:</p> <pre><code>val projjson_g0 = \"{...}\" // PROJJSON string for g0\nval projjson_g1 = \"{...}\" // PROJJSON string for g1\ndf.write.format(\"geoparquet\")\n        .option(\"geoparquet.version\", \"1.0.0\")\n        .option(\"geoparquet.crs.g0\", projjson_g0)\n        .option(\"geoparquet.crs.g1\", projjson_g1)\n        .save(geoparquetoutputlocation + \"/GeoParquet_File_Name.parquet\")\n</code></pre> <p>The value of <code>geoparquet.crs</code> and <code>geoparquet.crs.&lt;column_name&gt;</code> can be one of the following:</p> <ul> <li><code>\"null\"</code>: Explicitly setting <code>crs</code> field to <code>null</code>. This is the default behavior.</li> <li><code>\"\"</code> (empty string): Omit the <code>crs</code> field. This implies that the CRS is OGC:CRS84 for CRS-aware implementations.</li> <li><code>\"{...}\"</code> (PROJJSON string): The <code>crs</code> field will be set as the PROJJSON object representing the Coordinate Reference System (CRS) of the geometry. You can find the PROJJSON string of a specific CRS from here: https://epsg.io/ (click the JSON option at the bottom of the page). You can also customize your PROJJSON string as needed.</li> </ul> <p>Please note that Sedona currently cannot set/get a projjson string to/from a CRS. Its geoparquet reader will ignore the projjson metadata and you will have to set your CRS via <code>ST_SetSRID</code> after reading the file. Its geoparquet writer will not leverage the SRID field of a geometry so you will have to always set the <code>geoparquet.crs</code> option manually when writing the file, if you want to write a meaningful CRS field.</p> <p>Due to the same reason, Sedona geoparquet reader and writer do NOT check the axis order (lon/lat or lat/lon) and assume they are handled by the users themselves when writing / reading the files. You can always use <code>ST_FlipCoordinates</code> to swap the axis order of your geometries.</p>"},{"location":"tutorial/sql/#covering-metadata","title":"Covering Metadata","text":"<p>Since <code>v1.6.1</code>, Sedona supports writing the <code>covering</code> field to geometry column metadata. The <code>covering</code> field specifies a bounding box column to help accelerate spatial data retrieval. The bounding box column should be a top-level struct column containing <code>xmin</code>, <code>ymin</code>, <code>xmax</code>, <code>ymax</code> columns. If the DataFrame you are writing contains such columns, you can specify <code>.option(\"geoparquet.covering.&lt;geometryColumnName&gt;\", \"&lt;coveringColumnName&gt;\")</code> option to write <code>covering</code> metadata to GeoParquet files:</p> <pre><code>df.write.format(\"geoparquet\")\n        .option(\"geoparquet.covering.geometry\", \"bbox\")\n        .save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre> <p>If the DataFrame has only one geometry column, you can simply specify the <code>geoparquet.covering</code> option and omit the geometry column name:</p> <pre><code>df.write.format(\"geoparquet\")\n        .option(\"geoparquet.covering\", \"bbox\")\n        .save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre> <p>If the DataFrame does not have a covering column, you can construct one using Sedona's SQL functions:</p> <pre><code>val df_bbox = df.withColumn(\"bbox\", expr(\"struct(ST_XMin(geometry) AS xmin, ST_YMin(geometry) AS ymin, ST_XMax(geometry) AS xmax, ST_YMax(geometry) AS ymax)\"))\ndf_bbox.write.format(\"geoparquet\").option(\"geoparquet.covering.geometry\", \"bbox\").save(\"/path/to/saved_geoparquet.parquet\")\n</code></pre>"},{"location":"tutorial/sql/#sort-then-save-geoparquet","title":"Sort then Save GeoParquet","text":"<p>To maximize the performance of Sedona GeoParquet filter pushdown, we suggest that you sort the data by their geohash values (see ST_GeoHash) and then save as a GeoParquet file. An example is as follows:</p> <pre><code>SELECT col1, col2, geom, ST_GeoHash(geom, 5) as geohash\nFROM spatialDf\nORDER BY geohash\n</code></pre>"},{"location":"tutorial/sql/#save-to-postgis","title":"Save to Postgis","text":"<p>Unfortunately, the Spark SQL JDBC data source doesn't support creating geometry types in PostGIS using the 'createTableColumnTypes' option. Only the Spark built-in types are recognized. This means that you'll need to manage your PostGIS schema separately from Spark. One way to do this is to create the table with the correct geometry column before writing data to it with Spark. Alternatively, you can write your data to the table using Spark and then manually alter the column to be a geometry type afterward.</p> <p>Postgis uses EWKB to serialize geometries. If you convert your geometries to EWKB format in Sedona you don't have to do any additional conversion in Postgis.</p> <pre><code>my_postgis_db# create table my_table (id int8, geom geometry);\n\ndf.withColumn(\"geom\", expr(\"ST_AsEWKB(geom)\")\n    .write.format(\"jdbc\")\n    .option(\"truncate\",\"true\") // Don't let Spark recreate the table.\n    // Other options.\n    .save()\n\n// If you didn't create the table before writing you can change the type afterward.\nmy_postgis_db# alter table my_table alter column geom type geometry;\n</code></pre>"},{"location":"tutorial/sql/#convert-between-dataframe-and-spatialrdd","title":"Convert between DataFrame and SpatialRDD","text":""},{"location":"tutorial/sql/#dataframe-to-spatialrdd","title":"DataFrame to SpatialRDD","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>SpatialRDD spatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialRDD = Adapter.toSpatialRdd(spatialDf, \"usacounty\")\n</code></pre> <p>\"usacounty\" is the name of the geometry column</p> <p>Warning</p> <p>Only one Geometry type column is allowed per DataFrame.</p>"},{"location":"tutorial/sql/#spatialrdd-to-dataframe","title":"SpatialRDD to DataFrame","text":"<p>Use SedonaSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD. Please read Adapter Scaladoc</p> ScalaJavaPython <pre><code>var spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>Dataset&lt;Row&gt; spatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\nspatialDf = Adapter.toDf(spatialRDD, sedona)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. At least one column for the user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\n  StructField(\"county\", GeometryUDT, nullable = true),\n  StructField(\"name\", StringType, nullable = true),\n  StructField(\"price\", DoubleType, nullable = true),\n  StructField(\"age\", IntegerType, nullable = true)\n))\nval spatialDf = Adapter.toDf(spatialRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/sql/#spatialpairrdd-to-dataframe","title":"SpatialPairRDD to DataFrame","text":"<p>PairRDD is the result of a spatial join query or distance join query. SedonaSQL DataFrame-RDD Adapter can convert the result to a DataFrame. But you need to provide the name of other attributes.</p> ScalaJavaPython <pre><code>var joinResultDf = Adapter.toDf(joinResultPairRDD, Seq(\"left_attribute1\", \"left_attribute2\"), Seq(\"right_attribute1\", \"right_attribute2\"), sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters;\n\nList leftFields = new ArrayList&lt;&gt;(Arrays.asList(\"c1\", \"c2\", \"c3\"));\nList rightFields = new ArrayList&lt;&gt;(Arrays.asList(\"c4\", \"c5\", \"c6\"));\nDataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftFields).toSeq(), JavaConverters.asScalaBuffer(rightFields).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(jvm_sedona_rdd, [\"poi_from_id\", \"poi_from_name\"], [\"poi_to_id\", \"poi_to_name\"], spark))\n</code></pre> <p>or you can use the attribute names directly from the input RDD</p> ScalaJavaPython <pre><code>import scala.collection.JavaConversions._\nvar joinResultDf = Adapter.toDf(joinResultPairRDD, leftRdd.fieldNames, rightRdd.fieldNames, sedona)\n</code></pre> <pre><code>import scala.collection.JavaConverters;\nDataset joinResultDf = Adapter.toDf(joinResultPairRDD, JavaConverters.asScalaBuffer(leftRdd.fieldNames).toSeq(), JavaConverters.asScalaBuffer(rightRdd.fieldNames).toSeq(), sedona);\n</code></pre> <pre><code>from sedona.utils.adapter import Adapter\n\njoinResultDf = Adapter.toDf(result_pair_rdd, leftRdd.fieldNames, rightRdd.fieldNames, spark)\n</code></pre> <p>All other attributes such as price and age will be also brought to the DataFrame as long as you specify carryOtherAttributes (see Read other attributes in an SpatialRDD).</p> <p>You may also manually specify a schema for the resulting DataFrame in case you require different column names or data types. Note that string schemas and not all data types are supported\u2014please check the Adapter Scaladoc to confirm what is supported for your use case. Columns for the left and right user data must be provided.</p> Scala <pre><code>val schema = StructType(Array(\n  StructField(\"leftGeometry\", GeometryUDT, nullable = true),\n  StructField(\"name\", StringType, nullable = true),\n  StructField(\"price\", DoubleType, nullable = true),\n  StructField(\"age\", IntegerType, nullable = true),\n  StructField(\"rightGeometry\", GeometryUDT, nullable = true),\n  StructField(\"category\", StringType, nullable = true)\n))\nval joinResultDf = Adapter.toDf(joinResultPairRDD, schema, sedona)\n</code></pre>"},{"location":"tutorial/storing-blobs-in-parquet/","title":"Storing large raster geometries in Parquet files","text":""},{"location":"tutorial/storing-blobs-in-parquet/#storing-large-raster-geometries-in-parquet-files","title":"Storing large raster geometries in Parquet files","text":"<p>Warning</p> <p>Always convert the raster geometries to a well known format with the RS_AsXXX functions before saving them. It is possible to save the raw bytes of the raster geometries, but they will be stored in an internal Sedona format that is not guaranteed to be stable across versions.</p> <p>The default settings in Spark are not well suited for storing large binaries like raster geometries. It is very much worth the time to tune and benchmark your settings. Writing large binaries with the default settings will result in poorly structured Parquet files that are very expensive to read. Some basic tuning can increase the read performance by several magnitudes.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#background","title":"Background","text":"<p>Parquet files are divided into one or several row groups. Each column in a row group is stored in a column chunk. Each column chunk is further divided into pages. A page is conceptually an indivisible unit in terms of compression and encoding. The default size for a page is 1 MB. Data is buffered until the page is full and then written to disk. The frequency of checks of the page size limit will be between <code>parquet.page.size.row.check.min</code> and <code>parquet.page.size.row.check.max</code> (default between 100 and 10000 rows).</p> <p>If you write 5 MB image files to Parquet with the default setting the first page size check will happen after 100 rows. You will end up with pages of 500 MB instead of 1 MB. Reading such a file will require a lot of memory and will be slow.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#reading-poorly-structured-parquet-files","title":"Reading poorly structured Parquet files","text":"<p>Especially snappy compressed files are sensitive to oversized pages. More performant options are no compression or zstd compression. You can set <code>spark.buffer.size</code> to a value larger than the default of 64k to improve read performance. Increasing <code>spark.buffer.size</code> might add an io penalty for other columns in the Parquet file.</p>"},{"location":"tutorial/storing-blobs-in-parquet/#writing-better-structured-parquet-files-for-blobs","title":"Writing better structured Parquet files for blobs","text":"<p>Ideally you want to write Parquet files with a sane page size to get better and more consistent read performance across different clients. Since version 1.12.0 of parquet-hadoop, bundled with Spark 3.2, you can add Hadoop properties for controlling page size checks. Better values for writing blobs are:</p> <pre><code>spark.sql.parquet.compression.codec=zstd\nspark.hadoop.parquet.page.size.row.check.min=2\nspark.hadoop.parquet.page.size.row.check.max=10\n</code></pre> <p>Zstd performs better than snappy in general. Even more so for large pages. The first page size check will happen after 2 rows. If the page is not full after 2 rows the next check will happen after another 2-10 rows, depending on the size of the two rows already written.</p> <p>Spark will set Hadoop properties from Spark properties prefixed with \"spark.hadoop.\". For a full list of Parquet Hadoop properties see: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md</p>"},{"location":"tutorial/viz-gallery/","title":"Gallery","text":""},{"location":"tutorial/viz/","title":"Scala/Java","text":"<p>The page outlines the steps to visualize spatial data using SedonaViz. The example code is written in Scala but also works for Java.</p> <p>SedonaViz provides native support for general cartographic design by extending Sedona to process large-scale spatial data. It can visualize Spatial RDD and Spatial Queries and render super high resolution image in parallel.</p> <p>SedonaViz offers Map Visualization SQL. This gives users a more flexible way to design beautiful map visualization effects including scatter plots and heat maps. SedonaViz RDD API is also available.</p> <p>Note</p> <p>All SedonaViz SQL/DataFrame APIs are explained in SedonaViz API. Please see Viz example project</p>"},{"location":"tutorial/viz/#why-scalable-map-visualization","title":"Why scalable map visualization?","text":"<p>Data visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.</p> <p>SedonaViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.</p>"},{"location":"tutorial/viz/#visualize-spatialrdd","title":"Visualize SpatialRDD","text":"<p>This tutorial mainly focuses on explaining SQL/DataFrame API. SedonaViz RDD example can be found in Please see Viz example project</p>"},{"location":"tutorial/viz/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Apache Spark core, Apache SparkSQL, Sedona-core, Sedona-SQL, Sedona-Viz</li> </ol>"},{"location":"tutorial/viz/#create-sedona-config","title":"Create Sedona config","text":"<p>Use the following code to create your Sedona config at the beginning. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please skip this step and can use <code>spark</code> directly.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val config = SedonaContext.builder()\n        .config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n        .master(\"local[*]\") // Delete this if run in cluster mode\n        .appName(\"Sedona Viz\") // Change this to a proper name\n        .getOrCreate()\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your Sedona config.</p> <pre><code>var sparkSession = SparkSession.builder()\n.master(\"local[*]\") // Delete this if run in cluster mode\n.appName(\"Sedona Viz\") // Change this to a proper name\n// Enable Sedona custom Kryo serializer\n.config(\"spark.serializer\", classOf[KryoSerializer].getName) // org.apache.spark.serializer.KryoSerializer\n.config(\"spark.kryo.registrator\", classOf[SedonaVizKryoRegistrator].getName) // org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\n.getOrCreate()\n</code></pre>"},{"location":"tutorial/viz/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after creating Sedona config. If you already have a SparkSession (usually named <code>spark</code>) created by Wherobots/AWS EMR/Databricks, please call <code>SedonaContext.create(spark)</code> instead.</p> <p>Sedona &gt;= 1.4.1=</p> <pre><code>val sedona = SedonaContext.create(config)\nSedonaVizRegistrator.registerAll(sedona)\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaSQLRegistrator.registerAll(sparkSession)\nSedonaVizRegistrator.registerAll(sparkSession)\n</code></pre> <p>You can also register everything by passing <code>--conf spark.sql.extensions=org.apache.sedona.viz.sql.SedonaVizExtensions,org.apache.sedona.sql.SedonaSqlExtensions</code> to <code>spark-submit</code> or <code>spark-shell</code>.</p>"},{"location":"tutorial/viz/#create-spatial-dataframe","title":"Create Spatial DataFrame","text":"<p>There is a DataFrame as follows:</p> <pre><code>+----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|\n</code></pre> <p>You first need to create a Geometry type column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pointtable AS\nSELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as shape\nFROM pointtable\n</code></pre> <p>As you know, Sedona provides many different methods to load various spatial data formats. Please read Write a Spatial DataFrame application.</p>"},{"location":"tutorial/viz/#generate-a-single-image","title":"Generate a single image","text":"<p>In most cases, you just want to see a single image out of your spatial dataset.</p>"},{"location":"tutorial/viz/#pixelize-spatial-objects","title":"Pixelize spatial objects","text":"<p>To put spatial objects on a map image, you first need to convert them to pixels.</p> <p>First, compute the spatial boundary of this column.</p> <pre><code>CREATE OR REPLACE TEMP VIEW boundtable AS\nSELECT ST_Envelope_Aggr(shape) as bound FROM pointtable\n</code></pre> <p>Then use ST_Pixelize to convert them to pixels.</p> <p>This example is for Sedona before v1.0.1. ST_Pixelize extends Generator, so it can directly flatten the array without the explode function.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable)) AS pixel\n</code></pre> <p>This example is for Sedona on and after v1.0.1. ST_Pixelize returns an array of pixels. You need to use explode to flatten it.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixels AS\nSELECT pixel, shape FROM pointtable\nLATERAL VIEW explode(ST_Pixelize(ST_Transform(shape, 'epsg:4326','epsg:3857'), 256, 256, (SELECT ST_Transform(bound, 'epsg:4326','epsg:3857') FROM boundtable))) AS pixel\n</code></pre> <p>This will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.</p> <p>Warning</p> <p>We highly suggest that you should use ST_Transform to transform coordinates to a visualization-specific coordinate system such as epsg:3857, otherwise you map may look distorted.</p>"},{"location":"tutorial/viz/#aggregate-pixels","title":"Aggregate pixels","text":"<p>Many objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, count(*) as weight\nFROM pixels\nGROUP BY pixel\n</code></pre> <p>The weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.</p>"},{"location":"tutorial/viz/#colorize-pixels","title":"Colorize pixels","text":"<p>Run the following command to assign colors for pixels based on their weights.</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, ST_Colorize(weight, (SELECT max(weight) FROM pixelaggregates)) as color\nFROM pixelaggregates\n</code></pre> <p>Please read ST_Colorize for a detailed API description.</p>"},{"location":"tutorial/viz/#render-the-image","title":"Render the image","text":"<p>Use ST_Render to plot all pixels on a single image.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\nFROM pixelaggregates\n</code></pre> <p>This DataFrame will contain an Image type column which has only one image.</p>"},{"location":"tutorial/viz/#store-the-image-on-disk","title":"Store the image on disk","text":"<p>Fetch the image from the previous DataFrame</p> <pre><code>var image = sedona.table(\"images\").take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage\n</code></pre> <p>Use Sedona Viz ImageGenerator to store this image on disk.</p> <pre><code>var imageGenerator = new ImageGenerator\nimageGenerator.SaveRasterImageAsLocalFile(image, System.getProperty(\"user.dir\")+\"/target/points\", ImageType.PNG)\n</code></pre>"},{"location":"tutorial/viz/#generate-map-tiles","title":"Generate map tiles","text":"<p>If you are a map professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.</p>"},{"location":"tutorial/viz/#pixelization-and-pixel-aggregation","title":"Pixelization and pixel aggregation","text":"<p>Please first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution, such as 1000*1000. Note that, each dimension should be divisible by 2^zoom-level</p>"},{"location":"tutorial/viz/#create-tile-name","title":"Create tile name","text":"<p>Run the following command to compute the tile name for every pixels</p> <pre><code>CREATE OR REPLACE TEMP VIEW pixelaggregates AS\nSELECT pixel, weight, ST_TileName(pixel, 3) AS pid\nFROM pixelaggregates\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#colorize-pixels_1","title":"Colorize pixels","text":"<p>Use the same command explained in single image generation to assign colors.</p>"},{"location":"tutorial/viz/#render-map-tiles","title":"Render map tiles","text":"<p>You now need to group pixels by tiles and then render map tile images in parallel.</p> <pre><code>CREATE OR REPLACE TEMP VIEW images AS\nSELECT ST_Render(pixel, color, 3) AS image\nFROM pixelaggregates\nGROUP BY pid\n</code></pre> <p>\"3\" is the zoom level for these map tiles.</p>"},{"location":"tutorial/viz/#store-map-tiles-on-disk","title":"Store map tiles on disk","text":"<p>You can use the same commands in single image generation to fetch all map tiles and store them one by one.</p>"},{"location":"tutorial/zeppelin/","title":"Use Apache Zeppelin","text":"<p>Sedona provides a Helium visualization plugin tailored for Apache Zeppelin. This finally bridges the gap between Sedona and Zeppelin. Please read Install Sedona-Zeppelin to learn how to install this plugin in Zeppelin.</p> <p>Sedona-Zeppelin equips two approaches to visualize spatial data in Zeppelin. The first approach uses Zeppelin to plot all spatial objects on the map. The second one leverages SedonaViz to generate map images and overlay them on maps.</p>"},{"location":"tutorial/zeppelin/#small-scale-without-sedonaviz","title":"Small-scale without SedonaViz","text":"<p>Danger</p> <p>Zeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read SedonaViz solution.</p> <p>You can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW wktpoint AS\n    |SELECT ST_AsText(shape) as geom\n    |FROM pointtable\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data</p> <pre><code>%sql\nSELECT *\nFROM wktpoint\n</code></pre> <p>Select the geometry column to visualize:</p> <p></p>"},{"location":"tutorial/zeppelin/#large-scale-with-sedonaviz","title":"Large-scale with SedonaViz","text":"<p>SedonaViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read How to use SedonaViz.</p> <p>You can use Sedona-Zeppelin to ask Zeppelin to overlay SedonaViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).</p> <p>First, encode images of SedonaViz DataFrame in Zeppelin Spark notebook Scala paragraph,</p> <pre><code>spark.sql(\n  \"\"\"\n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n  \"\"\".stripMargin)\n</code></pre> <p>Then create an SQL paragraph to fetch the data</p> <pre><code>%sql\nSELECT *, 'I am the map center!'\nFROM images\n</code></pre> <p>Select the image and its geospatial boundary:</p> <p></p>"},{"location":"tutorial/zeppelin/#zeppelin-spark-notebook-demo","title":"Zeppelin Spark notebook demo","text":"<p>We provide a full Zeppelin Spark notebook which demonstrates all functions. Please download Sedona-Zeppelin notebook template and test data - arealm.csv.</p> <p>You need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.</p>"},{"location":"tutorial/flink/sql/","title":"Spatial SQL app (Flink)","text":"<p>The page outlines the steps to manage spatial data using SedonaSQL. The example code is written in Java but also works for Scala.</p> <p>SedonaSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:</p> <pre><code>Table myTable = tableEnv.sqlQuery(\"YOUR_SQL\")\n</code></pre> <p>Detailed SedonaSQL APIs are available here: SedonaSQL API</p>"},{"location":"tutorial/flink/sql/#set-up-dependencies","title":"Set up dependencies","text":"<ol> <li>Read Sedona Maven Central coordinates</li> <li>Add Sedona dependencies in build.sbt or pom.xml.</li> <li>Add Flink dependencies in build.sbt or pom.xml.</li> <li>Please see SQL example project</li> </ol>"},{"location":"tutorial/flink/sql/#initiate-stream-environment","title":"Initiate Stream Environment","text":"<p>Use the following code to initiate your <code>StreamExecutionEnvironment</code> at the beginning:</p> <pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);\n</code></pre>"},{"location":"tutorial/flink/sql/#initiate-sedonacontext","title":"Initiate SedonaContext","text":"<p>Add the following line after your <code>StreamExecutionEnvironment</code> and <code>StreamTableEnvironment</code> declaration</p> <p>Sedona &gt;= 1.4.1</p> <pre><code>StreamTableEnvironment sedona = SedonaContext.create(env, tableEnv);\n</code></pre> <p>Sedona &lt;1.4.1</p> <p>The following method has been deprecated since Sedona 1.4.1. Please use the method above to create your SedonaContext.</p> <pre><code>SedonaFlinkRegistrator.registerType(env);\nSedonaFlinkRegistrator.registerFunc(tableEnv);\n</code></pre> <p>Warning</p> <p>Sedona has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.</p> <p>This function will register Sedona User Defined Type and User Defined Function</p>"},{"location":"tutorial/flink/sql/#create-a-geometry-type-column","title":"Create a Geometry type column","text":"<p>All geometrical operations in SedonaSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.</p> <p>Assume you have a Flink Table <code>tbl</code> like this:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>You can create a Table with a Geometry type column as follows:</p> <pre><code>sedona.createTemporaryView(\"myTable\", tbl)\nTable geomTbl = sedona.sqlQuery(\"SELECT ST_GeomFromWKT(geom_polygon) as geom_polygon, name_polygon FROM myTable\")\ngeomTbl.execute().print()\n</code></pre> <p>The output will be:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                   geom_polygon |                   name_polygon |\n+----+--------------------------------+--------------------------------+\n| +I | POLYGON ((-0.5 -0.5, -0.5 0... |                       polygon0 |\n| +I | POLYGON ((0.5 0.5, 0.5 1.5,... |                       polygon1 |\n| +I | POLYGON ((1.5 1.5, 1.5 2.5,... |                       polygon2 |\n| +I | POLYGON ((2.5 2.5, 2.5 3.5,... |                       polygon3 |\n| +I | POLYGON ((3.5 3.5, 3.5 4.5,... |                       polygon4 |\n| +I | POLYGON ((4.5 4.5, 4.5 5.5,... |                       polygon5 |\n| +I | POLYGON ((5.5 5.5, 5.5 6.5,... |                       polygon6 |\n| +I | POLYGON ((6.5 6.5, 6.5 7.5,... |                       polygon7 |\n| +I | POLYGON ((7.5 7.5, 7.5 8.5,... |                       polygon8 |\n| +I | POLYGON ((8.5 8.5, 8.5 9.5,... |                       polygon9 |\n+----+--------------------------------+--------------------------------+\n10 rows in set\n</code></pre> <p>Although it looks same with the input, actually the type of column geom_polygon has been changed to Geometry type.</p> <p>To verify this, use the following code to print the schema of the DataFrame:</p> <pre><code>geomTbl.printSchema()\n</code></pre> <p>The output will be like this:</p> <pre><code>(\n  `geom_polygon` RAW('org.locationtech.jts.geom.Geometry', '...'),\n  `name_polygon` STRING\n)\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL constructor API.</p>"},{"location":"tutorial/flink/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use the following code:</p> <pre><code>Table geomTbl3857 = sedona.sqlQuery(\"SELECT ST_Transform(countyshape, \"epsg:4326\", \"epsg:3857\") AS geom_polygon, name_polygon FROM myTable\")\ngeomTbl3857.execute().print()\n</code></pre> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io</p> <p>Note</p> <p>Read SedonaSQL ST_Transform API to learn different spatial query predicates.</p> <p>For example, a Table that has coordinates in the US will become like this.</p> <p>Before the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                     geom_point |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I |                POINT (32 -118) |                          point |\n| +I |                POINT (33 -117) |                          point |\n| +I |                POINT (34 -116) |                          point |\n| +I |                POINT (35 -115) |                          point |\n| +I |                POINT (36 -114) |                          point |\n| +I |                POINT (37 -113) |                          point |\n| +I |                POINT (38 -112) |                          point |\n| +I |                POINT (39 -111) |                          point |\n| +I |                POINT (40 -110) |                          point |\n| +I |                POINT (41 -109) |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After the transformation:</p> <pre><code>+----+--------------------------------+--------------------------------+\n| op |                            _c0 |                     name_point |\n+----+--------------------------------+--------------------------------+\n| +I | POINT (-13135699.91360628 3... |                          point |\n| +I | POINT (-13024380.422813008 ... |                          point |\n| +I | POINT (-12913060.932019735 ... |                          point |\n| +I | POINT (-12801741.44122646 4... |                          point |\n| +I | POINT (-12690421.950433187 ... |                          point |\n| +I | POINT (-12579102.459639912 ... |                          point |\n| +I | POINT (-12467782.96884664 4... |                          point |\n| +I | POINT (-12356463.478053367 ... |                          point |\n| +I | POINT (-12245143.987260092 ... |                          point |\n| +I | POINT (-12133824.496466817 ... |                          point |\n+----+--------------------------------+--------------------------------+\n</code></pre> <p>After creating a Geometry type column, you are able to run spatial queries.</p>"},{"location":"tutorial/flink/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects and so on to run a range query over a single column.</p> <p>The following example finds all counties that are within the given polygon:</p> <pre><code>geomTable = sedona.sqlQuery(\n  \"\n    SELECT *\n    FROM spatialdf\n    WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n  \")\ngeomTable.execute().print()\n</code></pre> <p>Note</p> <p>Read SedonaSQL Predicate API to learn different spatial query predicates.</p>"},{"location":"tutorial/flink/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given polygon.</p> <pre><code>geomTable = sedona.sqlQuery(\n  \"\n    SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n    FROM geomTable\n    ORDER BY distance DESC\n    LIMIT 5\n  \")\ngeomTable.execute().print()\n</code></pre>"},{"location":"tutorial/flink/sql/#join-query","title":"Join query","text":"<p>This equi-join leverages Flink's internal equi-join algorithm. You can opt to skip the Sedona refinement step  by sacrificing query accuracy. A running example is in SQL example project.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/flink/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM lefts\n</code></pre> <pre><code>SELECT id, geom, name, ST_S2CellIDs(geom, 15) as idarray\nFROM rights\n</code></pre>"},{"location":"tutorial/flink/sql/#2-explode-id-array","title":"2. Explode id array","text":"<p>The produced S2 ids are arrays of integers. We need to explode these Ids to multiple rows, so later we can join two tables by ids.</p> <pre><code>SELECT id, geom, name, cellId\nFROM lefts CROSS JOIN UNNEST(lefts.idarray) AS tmpTbl1(cellId)\n</code></pre> <pre><code>SELECT id, geom, name, cellId\nFROM rights CROSS JOIN UNNEST(rights.idarray) AS tmpTbl2(cellId)\n</code></pre>"},{"location":"tutorial/flink/sql/#3-perform-equi-join","title":"3. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/flink/sql/#4-optional-refine-the-result","title":"4. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query as the query in Step 3.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/flink/sql/#5-optional-de-duplicate","title":"5. Optional: De-duplicate","text":"<p>Due to the explode function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, FIRST_VALUE(lcs_geom), FIRST_VALUE(lcs_name), first(rcs_geom), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>FIRST_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, first(lcs_name), first(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem, and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/flink/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>For example. run this query first on the left table before Step 1.</p> <pre><code>SELECT id, ST_Buffer(geom, DISTANCE), name\nFROM lefts\n</code></pre> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You will have to estimate the corresponding degrees based on your meter values. Please use this calculator.</p>"},{"location":"tutorial/flink/sql/#convert-spatial-table-to-spatial-datastream","title":"Convert Spatial Table to Spatial DataStream","text":""},{"location":"tutorial/flink/sql/#get-datastream","title":"Get DataStream","text":"<p>Use TableEnv's toDataStream function</p> <pre><code>DataStream&lt;Row&gt; geomStream = sedona.toDataStream(geomTable)\n</code></pre>"},{"location":"tutorial/flink/sql/#retrieve-geometries","title":"Retrieve Geometries","text":"<p>Then get the Geometry from each Row object using Map</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n            @Override\n            public Geometry map(Row value) throws Exception {\n                return (Geometry) value.getField(0);\n            }\n        });\ngeometries.print();\n</code></pre> <p>The output will be</p> <pre><code>14&gt; POLYGON ((1.5 1.5, 1.5 2.5, 2.5 2.5, 2.5 1.5, 1.5 1.5))\n2&gt; POLYGON ((5.5 5.5, 5.5 6.5, 6.5 6.5, 6.5 5.5, 5.5 5.5))\n5&gt; POLYGON ((8.5 8.5, 8.5 9.5, 9.5 9.5, 9.5 8.5, 8.5 8.5))\n16&gt; POLYGON ((3.5 3.5, 3.5 4.5, 4.5 4.5, 4.5 3.5, 3.5 3.5))\n12&gt; POLYGON ((-0.5 -0.5, -0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5 -0.5))\n13&gt; POLYGON ((0.5 0.5, 0.5 1.5, 1.5 1.5, 1.5 0.5, 0.5 0.5))\n15&gt; POLYGON ((2.5 2.5, 2.5 3.5, 3.5 3.5, 3.5 2.5, 2.5 2.5))\n3&gt; POLYGON ((6.5 6.5, 6.5 7.5, 7.5 7.5, 7.5 6.5, 6.5 6.5))\n1&gt; POLYGON ((4.5 4.5, 4.5 5.5, 5.5 5.5, 5.5 4.5, 4.5 4.5))\n4&gt; POLYGON ((7.5 7.5, 7.5 8.5, 8.5 8.5, 8.5 7.5, 7.5 7.5))\n</code></pre>"},{"location":"tutorial/flink/sql/#store-non-spatial-attributes-in-geometries","title":"Store non-spatial attributes in Geometries","text":"<p>You can concatenate other non-spatial attributes and store them in Geometry's <code>userData</code> field, so you can recover them later on. <code>userData</code> field can be any object type.</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = geomStream.map(new MapFunction&lt;Row, Geometry&gt;() {\n            @Override\n            public Geometry map(Row value) throws Exception {\n                Geometry geom = (Geometry) value.getField(0);\n                geom.setUserData(value.getField(1));\n                return geom;\n            }\n        });\ngeometries.print();\n</code></pre> <p>The <code>print</code> command will not print out <code>userData</code> field. But you can get it this way:</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\ngeometries.map(new MapFunction&lt;Geometry, String&gt;() {\n            @Override\n            public String map(Geometry value) throws Exception\n            {\n                return (String) value.getUserData();\n            }\n        }).print();\n</code></pre> <p>The output will be</p> <pre><code>13&gt; polygon9\n6&gt; polygon2\n10&gt; polygon6\n11&gt; polygon7\n5&gt; polygon1\n12&gt; polygon8\n8&gt; polygon4\n4&gt; polygon0\n7&gt; polygon3\n9&gt; polygon5\n</code></pre>"},{"location":"tutorial/flink/sql/#convert-spatial-datastream-to-spatial-table","title":"Convert Spatial DataStream to Spatial Table","text":""},{"location":"tutorial/flink/sql/#create-geometries-using-sedona-formatutils","title":"Create Geometries using Sedona FormatUtils","text":"<ul> <li>Create a Geometry from a WKT string</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                FormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\n                return formatUtils.readGeometry(value);\n            }\n        })\n</code></pre> <ul> <li>Create a Point from a String <code>1.1, 2.2</code>. Use <code>,</code> as the delimiter.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                FormatUtils&lt;Geometry&gt; formatUtils = new FormatUtils(\",\", false, GeometryType.POINT);\n                return formatUtils.readGeometry(value);\n            }\n        })\n</code></pre> <ul> <li>Create a Polygon from a String <code>1.1, 1.1, 10.1, 10.1</code>. This is a rectangle with (1.1, 1.1) and (10.1, 10.1) as their min/max corners.</li> </ul> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.GeometryFactory;\nimport org.locationtech.jts.geom.Geometry;\n\nDataStream&lt;Geometry&gt; geometries = text.map(new MapFunction&lt;String, Geometry&gt;() {\n            @Override\n            public Geometry map(String value) throws Exception\n            {\n                  // Write some code to get four double type values: minX, minY, maxX, maxY\n                  ...\n                Coordinate[] coordinates = new Coordinate[5];\n                coordinates[0] = new Coordinate(minX, minY);\n                coordinates[1] = new Coordinate(minX, maxY);\n                coordinates[2] = new Coordinate(maxX, maxY);\n                coordinates[3] = new Coordinate(maxX, minY);\n                coordinates[4] = coordinates[0];\n                GeometryFactory geometryFactory = new GeometryFactory();\n                return geometryFactory.createPolygon(coordinates);\n            }\n        })\n</code></pre>"},{"location":"tutorial/flink/sql/#create-row-objects","title":"Create Row objects","text":"<p>Put a geometry in a Flink Row to a <code>geomStream</code>. Note that you can put other attributes in Row as well. This example uses a constant value <code>myName</code> for all geometries.</p> <pre><code>import org.apache.sedona.common.utils.FormatUtils;\nimport org.locationtech.jts.geom.Geometry;\nimport org.apache.flink.types.Row;\n\nDataStream&lt;Row&gt; geomStream = text.map(new MapFunction&lt;String, Row&gt;() {\n            @Override\n            public Row map(String value) throws Exception\n            {\n                FormatUtils formatUtils = new FormatUtils(FileDataSplitter.WKT, false);\n                return Row.of(formatUtils.readGeometry(value), \"myName\");\n            }\n        })\n</code></pre>"},{"location":"tutorial/flink/sql/#get-spatial-table","title":"Get Spatial Table","text":"<p>Use TableEnv's fromDataStream function, with two column names <code>geom</code> and <code>geom_name</code>.</p> <pre><code>Table geomTable = sedona.fromDataStream(geomStream, \"geom\", \"geom_name\")\n</code></pre>"},{"location":"tutorial/snowflake/sql/","title":"Spatial SQL app (Snowflake)","text":"<p>After the installation done, you can start using Sedona functions. Please log in to Snowflake again using the user that has the privilege to access the database.</p> <p>Note</p> <p>Please always keep the schema name <code>SEDONA</code> (e.g., <code>SEDONA.ST_GeomFromWKT</code>) when you use Sedona functions to avoid conflicting with Snowflake's built-in functions.</p>"},{"location":"tutorial/snowflake/sql/#create-a-sample-table","title":"Create a sample table","text":"<p>Let's create a <code>city_tbl</code> that contains the locations and names of cities. Each location is a WKT string.</p> <pre><code>CREATE TABLE city_tbl (wkt STRING, city_name STRING);\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.33 47.61)', 'Seattle');\nINSERT INTO city_tbl(wkt, city_name) VALUES ('POINT (-122.42 37.76)', 'San Francisco');\n</code></pre> <p>Then we can show the content of this table:</p> <pre><code>SELECT *\nFROM city_tbl;\n</code></pre> <p>Output:</p> <pre><code>WKT CITY_NAME\nPOINT (-122.33 47.61)   Seattle\nPOINT (-122.42 37.76)   San Francisco\n</code></pre>"},{"location":"tutorial/snowflake/sql/#create-a-geometrygeography-column","title":"Create a Geometry/Geography column","text":"<p>All geometrical operations in SedonaSQL are on Geometry/Geography type objects. Therefore, before any kind of queries, you need to create a Geometry/Geography type column on the table.</p> <pre><code>CREATE TABLE city_tbl_geom AS\nSELECT Sedona.ST_GeomFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>The <code>geom</code> column Table <code>city_tbl_geom</code> is now in a <code>Binary</code> type and data in this column is in a format that can be understood by Sedona. The output of this query will show geometries in WKB binary format like this:</p> <pre><code>GEOM CITY_NAME\n010100000085eb51b81e955ec0ae47e17a14ce4740  Seattle\n01010000007b14ae47e19a5ec0e17a14ae47e14240  San Francisco\n</code></pre> <p>To view the content of this column in a human-readable format, you can use <code>ST_AsText</code>. For example,</p> <pre><code>SELECT Sedona.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Alternatively, you can also create Snowflake native Geometry and Geography type columns. For example, you can create a Snowflake native Geometry type column as follows (note the function has no <code>SEDONA</code> prefix):</p> <pre><code>CREATE TABLE city_tbl_geom AS\nSELECT ST_GeometryFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>The following code creates a Snowflake native Geography type column (note the function has no <code>SEDONA</code> prefix):</p> <pre><code>CREATE TABLE city_tbl_geom AS\nSELECT ST_GeographyFromWKT(wkt) AS geom, city_name\nFROM city_tbl\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to create a Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#check-the-lonlat-order","title":"Check the lon/lat order","text":"<p>In SedonaSnow <code>v1.4.1</code> and before, we use lat/lon order in the following functions:</p> <ul> <li>ST_Transform</li> <li>ST_DistanceSphere</li> <li>ST_DistanceSpheroid</li> </ul> <p>We use <code>lon/lat</code> order in the following functions:</p> <ul> <li>ST_GeomFromGeoHash</li> <li>ST_GeoHash</li> <li>ST_S2CellIDs</li> </ul> <p>In Sedona <code>v1.5.0</code> and above, all functions will be fixed to lon/lat order.</p> <p>If your original data is not in the order you want, you need to flip the coordinate using <code>ST_FlipCoordinates(geom: Geometry)</code>.</p> <p>The sample data used above is in lon/lat order, we can flip the coordinates as follows:</p> <pre><code>CREATE OR REPLACE TABLE city_tbl_geom AS\nSELECT Sedona.ST_FlipCoordinates(geom) AS geom, city_name\nFROM city_tbl_geom\n</code></pre> <p>If we show the content of this table, it is now in lat/lon order:</p> <pre><code>SELECT Sedona.ST_AsText(geom), city_name\nFROM city_tbl_geom\n</code></pre> <p>Output:</p> <pre><code>GEOM    CITY_NAME\nPOINT (47.61 -122.33)   Seattle\nPOINT (37.76 -122.42)   San Francisco\n</code></pre>"},{"location":"tutorial/snowflake/sql/#save-as-an-ordinary-column","title":"Save as an ordinary column","text":"<p>To save a table to some permanent storage, you can simply convert each geometry in the Geometry type column back to a plain String and save it anywhere you want.</p> <p>Use the following code to convert the Geometry column in a table back to a WKT string column:</p> <pre><code>SELECT ST_AsText(geom)\nFROM city_tbl_geom\n</code></pre> <p>Note</p> <p>SedonaSQL provides lots of functions to save the Geometry column, please read SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#transform-the-coordinate-reference-system","title":"Transform the Coordinate Reference System","text":"<p>Sedona doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in SedonaSQL is same as the unit of all geometries in a Geometry column.</p> <p>To convert Coordinate Reference System of the Geometry column created before, use <code>ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string</code></p> <p>The first EPSG code EPSG:4326 in <code>ST_Transform</code> is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.</p> <p>The second EPSG code EPSG:3857 in <code>ST_Transform</code> is the target CRS of the geometries. It is the most common meter-based CRS.</p> <p>This <code>ST_Transform</code> transform the CRS of these geometries from EPSG:4326 to EPSG:3857. The details CRS information can be found on EPSG.io.</p> <p>Note</p> <p>This function follows lon/order in 1.5.0+ and lat/lon order in 1.4.1 and before. You can use <code>ST_FlipCoordinates</code> to swap X and Y.</p> <p>We can transform our sample data as follows</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_Transform(geom, 'epsg:4326', 'epsg:3857')), city_name\nFROM city_tbl_geom\n</code></pre> <p>The output will be like this:</p> <pre><code>POINT (6042216.250411431 -13617713.308741156)  Seattle\nPOINT (4545577.120361927 -13627732.06291255)  San Francisco\n</code></pre> <p><code>ST_Transform</code> also supports the CRS string in OGC WKT format. For example, the following query generates the same output but with a OGC WKT CRS string.</p> <pre><code>SELECT Sedona.ST_AsText(Sedona.ST_Transform(geom, 'epsg:4326', 'PROJCS[\"WGS 84 / Pseudo-Mercator\",\n     GEOGCS[\"WGS 84\",\n         DATUM[\"WGS_1984\",\n             SPHEROID[\"WGS 84\",6378137,298.257223563,\n                 AUTHORITY[\"EPSG\",\"7030\"]],\n             AUTHORITY[\"EPSG\",\"6326\"]],\n         PRIMEM[\"Greenwich\",0,\n             AUTHORITY[\"EPSG\",\"8901\"]],\n         UNIT[\"degree\",0.0174532925199433,\n             AUTHORITY[\"EPSG\",\"9122\"]],\n         AUTHORITY[\"EPSG\",\"4326\"]],\n     PROJECTION[\"Mercator_1SP\"],\n     PARAMETER[\"central_meridian\",0],\n     PARAMETER[\"scale_factor\",1],\n     PARAMETER[\"false_easting\",0],\n     PARAMETER[\"false_northing\",0],\n     UNIT[\"metre\",1,\n         AUTHORITY[\"EPSG\",\"9001\"]],\n     AXIS[\"Easting\",EAST],\n     AXIS[\"Northing\",NORTH],\n     EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],\n     AUTHORITY[\"EPSG\",\"3857\"]]')), city_name\nFROM city_tbl_geom\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-query","title":"Range query","text":"<p>Use ST_Contains, ST_Intersects, ST_Within to run a range query over a single column.</p> <p>The following example finds all geometries that are within the given polygon:</p> <pre><code>SELECT *\nFROM city_tbl_geom\nWHERE Sedona.ST_Contains(Sedona.ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), geom)\n</code></pre> <p>Note</p> <p>Read SedonaSQL API to learn how to create a Geometry type query window.</p>"},{"location":"tutorial/snowflake/sql/#knn-query","title":"KNN query","text":"<p>Use ST_Distance, ST_DistanceSphere, ST_DistanceSpheroid to calculate the distance and rank the distance.</p> <p>The following code returns the 5 nearest neighbor of the given point.</p> <pre><code>SELECT geom, ST_Distance(Sedona.ST_Point(1.0, 1.0), geom) AS distance\nFROM city_tbl_geom\nORDER BY distance DESC\nLIMIT 5\n</code></pre>"},{"location":"tutorial/snowflake/sql/#range-join-query","title":"Range join query","text":"<p>Warning</p> <p>Sedona range join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions + native <code>Geography</code> type to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate.</p> <p>Example:</p> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM polygondf, pointdf\nWHERE ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\n</code></pre> <pre><code>SELECT *\nFROM pointdf, polygondf\nWHERE ST_Within(pointdf.pointshape, polygondf.polygonshape)\n</code></pre>"},{"location":"tutorial/snowflake/sql/#distance-join","title":"Distance join","text":"<p>Warning</p> <p>Sedona distance join in Snowflake does not trigger Sedona's optimized spatial join algorithm while Sedona Spark does. It uses Snowflake's default Cartesian join which is very slow. Therefore, it is recommended to use Sedona's S2-based join or Snowflake's native ST functions + native <code>Geography</code> type to do range join, which will trigger Snowflake's <code>GeoJoin</code> algorithm.</p> <p>Introduction: Find geometries from A and geometries from B such that the distance of each geometry pair is less or equal than a certain distance. It supports the planar Euclidean distance calculators <code>ST_Distance</code>, <code>ST_HausdorffDistance</code>, <code>ST_FrechetDistance</code> and the meter-based geodesic distance calculators <code>ST_DistanceSpheroid</code> and <code>ST_DistanceSphere</code>.</p> <p>Example for planar Euclidean distance:</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_HausdorffDistance(pointDf.pointshape, polygonDf.polygonshape, 0.3) &lt; 2\n</code></pre> <pre><code>SELECT *\nFROM pointDf, polygonDF\nWHERE ST_FrechetDistance(pointDf.pointshape, polygonDf.polygonshape) &lt; 2\n</code></pre> <p>Warning</p> <p>If you use planar Euclidean distance functions like <code>ST_Distance</code>, <code>ST_HausdorffDistance</code> or <code>ST_FrechetDistance</code> as the predicate, Sedona doesn't control the distance's unit (degree or meter). It is same with the geometry. If your coordinates are in the longitude and latitude system, the unit of <code>distance</code> should be degree instead of meter or mile. To change the geometry's unit, please either transform the coordinate reference system to a meter-based system. See ST_Transform. If you don't want to transform your data, please consider using <code>ST_DistanceSpheroid</code> or <code>ST_DistanceSphere</code>.</p> <pre><code>SELECT *\nFROM pointdf1, pointdf2\nWHERE ST_DistanceSpheroid(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\n</code></pre>"},{"location":"tutorial/snowflake/sql/#google-s2-based-approximate-equi-join","title":"Google S2 based approximate equi-join","text":"<p>You can use Sedona built-in Google S2 functions to perform an approximate equi-join. This algorithm leverages Snowflake's internal equi-join algorithm and might be performant given that you can opt to skip the refinement step  by sacrificing query accuracy.</p> <p>Please use the following steps:</p>"},{"location":"tutorial/snowflake/sql/#1-generate-s2-ids-for-both-tables","title":"1. Generate S2 ids for both tables","text":"<p>Use ST_S2CellIds to generate cell IDs. Each geometry may produce one or more IDs.</p> <pre><code>SELECT * FROM lefts, TABLE(FLATTEN(ST_S2CellIDs(lefts.geom, 15))) s1\n</code></pre> <pre><code>SELECT * FROM rights, TABLE(FLATTEN(ST_S2CellIDs(rights.geom, 15))) s2\n</code></pre>"},{"location":"tutorial/snowflake/sql/#2-perform-equi-join","title":"2. Perform equi-join","text":"<p>Join the two tables by their S2 cellId</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs JOIN rcs ON lcs.cellId = rcs.cellId\n</code></pre>"},{"location":"tutorial/snowflake/sql/#3-optional-refine-the-result","title":"3. Optional: Refine the result","text":"<p>Due to the nature of S2 Cellid, the equi-join results might have a few false-positives depending on the S2 level you choose. A smaller level indicates bigger cells, less exploded rows, but more false positives.</p> <p>To ensure the correctness, you can use one of the Spatial Predicates to filter out them. Use this query instead of the query in Step 2.</p> <pre><code>SELECT lcs.id as lcs_id, lcs.geom as lcs_geom, lcs.name as lcs_name, rcs.id as rcs_id, rcs.geom as rcs_geom, rcs.name as rcs_name\nFROM lcs, rcs\nWHERE lcs.cellId = rcs.cellId AND ST_Contains(lcs.geom, rcs.geom)\n</code></pre> <p>As you see, compared to the query in Step 2, we added one more filter, which is <code>ST_Contains</code>, to remove false positives. You can also use <code>ST_Intersects</code> and so on.</p> <p>Tip</p> <p>You can skip this step if you don't need 100% accuracy and want faster query speed.</p>"},{"location":"tutorial/snowflake/sql/#4-optional-de-duplicate","title":"4. Optional: De-duplicate","text":"<p>Due to the <code>Flatten</code> function used when we generate S2 Cell Ids, the resulting DataFrame may have several duplicate  matches. You can remove them by performing a GroupBy query. <pre><code>SELECT lcs_id, rcs_id, ANY_VALUE(lcs_geom), ANY_VALUE(lcs_name), ANY_VALUE(rcs_geom), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_id, rcs_id)\n</code></pre> <p>The <code>ANY_VALUE</code> function is to take the first value from a number of duplicate values.</p> <p>If you don't have a unique id for each geometry, you can also group by geometry itself. See below:</p> <pre><code>SELECT lcs_geom, rcs_geom, ANY_VALUE(lcs_name), ANY_VALUE(rcs_name)\nFROM joinresult\nGROUP BY (lcs_geom, rcs_geom)\n</code></pre> <p>Note</p> <p>If you are doing point-in-polygon join, this is not a problem, and you can safely discard this issue. This issue only happens when you do polygon-polygon, polygon-linestring, linestring-linestring join.</p>"},{"location":"tutorial/snowflake/sql/#s2-for-distance-join","title":"S2 for distance join","text":"<p>This also works for distance join. You first need to use <code>ST_Buffer(geometry, distance)</code> to wrap one of your original geometry column. If your original geometry column contains points, this <code>ST_Buffer</code> will make them become circles with a radius of <code>distance</code>.</p> <p>Since the coordinates are in the longitude and latitude system, so the unit of <code>distance</code> should be degree instead of meter or mile. You can get an approximation by performing <code>METER_DISTANCE/111000.0</code>, then filter out false-positives. Note that this might lead to inaccurate results if your data is close to the poles or antimeridian.</p> <p>In a nutshell, run this query first on the left table before Step 1. Please replace <code>METER_DISTANCE</code> with a meter distance. In Step 1, generate S2 IDs based on the <code>buffered_geom</code> column. Then run Step 2, 3, 4 on the original <code>geom</code> column.</p> <pre><code>SELECT id, geom, ST_Buffer(geom, METER_DISTANCE/111000.0) as buffered_geom, name\nFROM lefts\n</code></pre>"},{"location":"tutorial/snowflake/sql/#functions-that-are-only-available-in-sedona","title":"Functions that are only available in Sedona","text":"<p>Sedona implements over 200 geospatial vector and raster functions, which are much more than what Snowflake native functions offer. For example:</p> <ul> <li>ST_3DDistance</li> <li>ST_Force2D</li> <li>ST_GeometryN</li> <li>ST_MakeValid</li> <li>ST_Multi</li> <li>ST_NumGeometries</li> <li>ST_ReducePrecision</li> <li>ST_SubdivideExplode</li> </ul> <p>You can click the links above to learn more about these functions. More functions can be found in SedonaSQL API.</p>"},{"location":"tutorial/snowflake/sql/#interoperate-with-snowflake-native-functions","title":"Interoperate with Snowflake native functions","text":"<p>Sedona can interoperate with Snowflake native functions seamlessly. There are two ways to do this:</p> <ul> <li>Use <code>Sedona functions</code> to create a Geometry column, then use Snowflake native functions and Sedona functions to query it.</li> <li>Use <code>Snowflake native functions</code> to create a Geometry/Geography column, then use Snowflake native functions and Sedona functions to query it.</li> </ul> <p>Now we will show you how to do this.</p>"},{"location":"tutorial/snowflake/sql/#geometries-created-by-sedona-geometry-constructors","title":"Geometries created by Sedona Geometry constructors","text":"<p>In this case, Sedona uses EWKB type as the input/output type for geometry. If you have datasets of built-in Snowflake GEOMETRY/GEOGRAPHY type, you can easily transform them into EWKB through this function.</p>"},{"location":"tutorial/snowflake/sql/#from-snowflake-native-functions-to-sedona-functions","title":"From Snowflake native functions to Sedona functions","text":"<p>In this example, <code>SEDONA.ST_X</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AsEWKB</code> are Snowflake native functions.</p> <pre><code>SELECT SEDONA.ST_X(ST_AsEWKB(ST_GeometryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'))) FROM {{ geometry_table }};\n</code></pre>"},{"location":"tutorial/snowflake/sql/#from-sedona-functions-to-snowflake-native-functions","title":"From Sedona functions to Snowflake native functions","text":"<p>In this example, <code>SEDONA.ST_GeomFromText</code> is a Sedona function, <code>ST_AREA</code> and <code>to_geometry</code> are Snowflake native functions.</p> <pre><code>SELECT ST_AREA(to_geometry(SEDONA.ST_GeomFromText('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')));\n</code></pre>"},{"location":"tutorial/snowflake/sql/#pros","title":"Pros:","text":"<p>Sedona geometry constructors are more powerful than Snowflake native functions. It has the following advantages:</p> <ul> <li>Sedona offers more constructors especially for 3D (XYZ) geometries, but Snowflake native functions don't.</li> <li>WKB serialization is more efficient. If you need to use multiple Sedona functions, it is more efficient to use this method, which might bring in 2X performance improvement.</li> <li>SRID information of geometries is preserved. The method below will lose SRID information.</li> </ul>"},{"location":"tutorial/snowflake/sql/#geometry-geography-created-by-snowflake-geometry-geography-constructors","title":"Geometry / Geography created by Snowflake Geometry / Geography constructors","text":"<p>In this case, Sedona uses Snowflake native GEOMETRY/GEOGRAPHY type as the input/output type for geometry. The serialization format is GeoJSON string.</p> <pre><code>SELECT ST_AREA(SEDONA.ST_Buffer(ST_GeometryFromWkt('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'), 1));\n</code></pre> <p>In this example, <code>SEDONA.ST_Buffer</code> is a Sedona function, <code>ST_GeommetryFromWkt</code> and <code>ST_AREA</code> are Snowflake native functions.</p> <p>As you can see, you can use Sedona functions and Snowflake native functions together without explicit conversion.</p>"},{"location":"tutorial/snowflake/sql/#pros_1","title":"Pros:","text":"<ul> <li>You don't need to convert the geometry type, which is more convenient.</li> </ul> <p>Note that: Snowflake natively serializes Geometry type data to GeoJSON String and sends to UDF as input. GeoJSON spec does not include SRID. So the SRID information will be lost if you mix-match Snowflake functions and Sedona functions directly without using <code>WKB</code>.</p> <p>In the example below, the SRID=4326 information is lost.</p> <pre><code>SELECT ST_AsEWKT(SEDONA.ST_SetSRID(ST_GeometryFromWKT('POINT(1 2)'), 4326))\n</code></pre> <p>Output:</p> <pre><code>SRID=0;POINT(1 2)\n</code></pre>"},{"location":"tutorial/snowflake/sql/#known-issues","title":"Known issues","text":"<ol> <li>Sedona Snowflake doesn't support <code>M</code> dimension due to the limitation of WKB serialization. Sedona Spark and Sedona Flink support XYZM because it uses our in-house serialization format. Although Sedona Snowflake has functions related to <code>M</code> dimension, all <code>M</code> values will be ignored.</li> <li>Sedona H3 functions are not supported because Snowflake does not allow embedded C code in UDF.</li> <li>All User Defined Table Functions only work with geometries created by Sedona constructors due to Snowflake current limitation <code>Data type GEOMETRY is not supported in non-SQL UDTF return type</code>. This includes:<ul> <li>ST_MinimumBoundingRadius</li> <li>ST_Intersection_Aggr</li> <li>ST_SubDivideExplode</li> <li>ST_Envelope_Aggr</li> <li>ST_Union_Aggr</li> <li>ST_Collect</li> <li>ST_Dump</li> </ul> </li> <li>Only Sedona ST functions are available in Snowflake. Raster functions (RS functions) are not available in Snowflake yet.</li> </ol>"},{"location":"usecases/ApacheSedonaCore/","title":"Create SpatialRDD","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport geopandas as gpd\nimport pandas as pd\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import LongType\nfrom shapely.geometry import Point\nfrom shapely.geometry import Polygon\n\nfrom sedona.spark import *\nfrom sedona.core.geom.envelope import Envelope\n</pre> from pyspark.sql import SparkSession from pyspark import StorageLevel import geopandas as gpd import pandas as pd from pyspark.sql.types import StructType from pyspark.sql.types import StructField from pyspark.sql.types import StringType from pyspark.sql.types import LongType from shapely.geometry import Point from shapely.geometry import Polygon  from sedona.spark import * from sedona.core.geom.envelope import Envelope In\u00a0[2]: Copied! <pre>config = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"\n        \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"\n        \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n</pre> config = (     SedonaContext.builder()     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"         \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"         \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",     )     .getOrCreate() )  sedona = SedonaContext.create(config) <pre>:: loading settings :: url = jar:file:/home/jovyan/spark-3.4.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jovyan/.ivy2/cache\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\norg.apache.sedona#sedona-spark-3.4_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\nuk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-40cca92d-9ae6-46b4-9b4d-e47b53beae2e;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-spark-3.4_2.12;1.6.0 in central\n\tfound org.apache.sedona#sedona-common;1.6.0 in central\n\tfound org.apache.commons#commons-math3;3.6.1 in central\n\tfound org.locationtech.jts#jts-core;1.19.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n\tfound com.google.geometry#s2-geometry;2.0.0 in central\n\tfound com.google.guava#guava;25.1-jre in central\n\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n\tfound org.checkerframework#checker-qual;2.0.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n\tfound com.uber#h3;4.1.1 in central\n\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n\tfound org.checkerframework#checker-qual;3.10.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n\tfound org.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 in central\n\tfound commons-lang#commons-lang;2.6 in central\n\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n\tfound org.beryx#awt-color-factory;1.0.0 in central\n\tfound org.datasyslab#geotools-wrapper;1.6.0-28.2 in central\n\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n\tfound com.github.scopt#scopt_2.12;4.1.0 in central\ndownloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-3.4_2.12/1.6.0/sedona-spark-3.4_2.12-1.6.0.jar ...\n\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-3.4_2.12;1.6.0!sedona-spark-3.4_2.12.jar (79ms)\ndownloading https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.6.0-28.2/geotools-wrapper-1.6.0-28.2.jar ...\n\t[SUCCESSFUL ] org.datasyslab#geotools-wrapper;1.6.0-28.2!geotools-wrapper.jar (2033ms)\ndownloading https://repo1.maven.org/maven2/uk/co/gresearch/spark/spark-extension_2.12/2.11.0-3.4/spark-extension_2.12-2.11.0-3.4.jar ...\n\t[SUCCESSFUL ] uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4!spark-extension_2.12.jar (190ms)\ndownloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-common/1.6.0/sedona-common-1.6.0.jar ...\n\t[SUCCESSFUL ] org.apache.sedona#sedona-common;1.6.0!sedona-common.jar (50ms)\ndownloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-common-3.4_2.12/1.6.0/sedona-spark-common-3.4_2.12-1.6.0.jar ...\n\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0!sedona-spark-common-3.4_2.12.jar (154ms)\ndownloading https://repo1.maven.org/maven2/org/locationtech/jts/jts-core/1.19.0/jts-core-1.19.0.jar ...\n\t[SUCCESSFUL ] org.locationtech.jts#jts-core;1.19.0!jts-core.jar(bundle) (137ms)\ndownloading https://repo1.maven.org/maven2/org/wololo/jts2geojson/0.16.1/jts2geojson-0.16.1.jar ...\n\t[SUCCESSFUL ] org.wololo#jts2geojson;0.16.1!jts2geojson.jar (84ms)\ndownloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.5.0/scala-collection-compat_2.12-2.5.0.jar ...\n\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.5.0!scala-collection-compat_2.12.jar (25ms)\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar ...\n\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.6.1!commons-math3.jar (66ms)\ndownloading https://repo1.maven.org/maven2/org/locationtech/spatial4j/spatial4j/0.8/spatial4j-0.8.jar ...\n\t[SUCCESSFUL ] org.locationtech.spatial4j#spatial4j;0.8!spatial4j.jar(bundle) (79ms)\ndownloading https://repo1.maven.org/maven2/com/google/geometry/s2-geometry/2.0.0/s2-geometry-2.0.0.jar ...\n\t[SUCCESSFUL ] com.google.geometry#s2-geometry;2.0.0!s2-geometry.jar (35ms)\ndownloading https://repo1.maven.org/maven2/com/uber/h3/4.1.1/h3-4.1.1.jar ...\n\t[SUCCESSFUL ] com.uber#h3;4.1.1!h3.jar (98ms)\ndownloading https://repo1.maven.org/maven2/net/sf/geographiclib/GeographicLib-Java/1.52/GeographicLib-Java-1.52.jar ...\n\t[SUCCESSFUL ] net.sf.geographiclib#GeographicLib-Java;1.52!GeographicLib-Java.jar (253ms)\ndownloading https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/2.9.2/caffeine-2.9.2.jar ...\n\t[SUCCESSFUL ] com.github.ben-manes.caffeine#caffeine;2.9.2!caffeine.jar (36ms)\ndownloading https://repo1.maven.org/maven2/com/google/guava/guava/25.1-jre/guava-25.1-jre.jar ...\n\t[SUCCESSFUL ] com.google.guava#guava;25.1-jre!guava.jar(bundle) (163ms)\ndownloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (76ms)\ndownloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...\n\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (19ms)\ndownloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar ...\n\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.14!animal-sniffer-annotations.jar (18ms)\ndownloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.10.0/checker-qual-3.10.0.jar ...\n\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.10.0!checker-qual.jar (24ms)\ndownloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.5.1/error_prone_annotations-2.5.1.jar ...\n\t[SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.5.1!error_prone_annotations.jar (28ms)\ndownloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar ...\n\t[SUCCESSFUL ] commons-lang#commons-lang;2.6!commons-lang.jar (25ms)\ndownloading https://repo1.maven.org/maven2/org/beryx/awt-color-factory/1.0.0/awt-color-factory-1.0.0.jar ...\n\t[SUCCESSFUL ] org.beryx#awt-color-factory;1.0.0!awt-color-factory.jar (47ms)\ndownloading https://repo1.maven.org/maven2/com/github/scopt/scopt_2.12/4.1.0/scopt_2.12-4.1.0.jar ...\n\t[SUCCESSFUL ] com.github.scopt#scopt_2.12;4.1.0!scopt_2.12.jar (24ms)\n:: resolution report :: resolve 12085ms :: artifacts dl 3779ms\n\t:: modules in use:\n\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n\tcom.google.guava#guava;25.1-jre from central in [default]\n\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n\tcom.uber#h3;4.1.1 from central in [default]\n\tcommons-lang#commons-lang;2.6 from central in [default]\n\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n\torg.apache.sedona#sedona-common;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-3.4_2.12;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 from central in [default]\n\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n\torg.datasyslab#geotools-wrapper;1.6.0-28.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n\t:: evicted modules:\n\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   25  |   25  |   25  |   2   ||   23  |   23  |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-40cca92d-9ae6-46b4-9b4d-e47b53beae2e\n\tconfs: [default]\n\t23 artifacts copied, 0 already retrieved (42692kB/281ms)\n24/05/22 17:56:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[3]: Copied! <pre>sc = sedona.sparkContext\n</pre> sc = sedona.sparkContext <p>Suppose we want load the CSV file into Apache Sedona PointRDD</p> <pre><code>testattribute0,-88.331492,32.324142,testattribute1,testattribute2\ntestattribute0,-88.175933,32.360763,testattribute1,testattribute2\ntestattribute0,-88.388954,32.357073,testattribute1,testattribute2\ntestattribute0,-88.221102,32.35078,testattribute1,testattribute2\ntestattribute0,-88.323995,32.950671,testattribute1,testattribute2\ntestattribute0,-88.231077,32.700812,testattribute1,testattribute2\n</code></pre> In\u00a0[4]: Copied! <pre>point_rdd = PointRDD(sc, \"data/arealm-small.csv\", 1, FileDataSplitter.CSV, True, 10)\n</pre> point_rdd = PointRDD(sc, \"data/arealm-small.csv\", 1, FileDataSplitter.CSV, True, 10) <pre>                                                                                \r</pre> In\u00a0[5]: Copied! <pre>## Getting approximate total count\npoint_rdd.approximateTotalCount\n</pre> ## Getting approximate total count point_rdd.approximateTotalCount Out[5]: <pre>3000</pre> In\u00a0[6]: Copied! <pre># getting boundary for PointRDD or any other SpatialRDD, it returns Envelope object which inherits from\n# shapely.geometry.Polygon\npoint_rdd.boundary()\n</pre> # getting boundary for PointRDD or any other SpatialRDD, it returns Envelope object which inherits from # shapely.geometry.Polygon point_rdd.boundary() Out[6]: In\u00a0[7]: Copied! <pre># To run analyze please use function analyze\npoint_rdd.analyze()\n</pre> # To run analyze please use function analyze point_rdd.analyze() Out[7]: <pre>True</pre> In\u00a0[8]: Copied! <pre># Finding boundary envelope for PointRDD or any other SpatialRDD, it returns Envelope object which inherits from\n# shapely.geometry.Polygon\npoint_rdd.boundaryEnvelope\n</pre> # Finding boundary envelope for PointRDD or any other SpatialRDD, it returns Envelope object which inherits from # shapely.geometry.Polygon point_rdd.boundaryEnvelope Out[8]: In\u00a0[9]: Copied! <pre># Calculate number of records without duplicates\npoint_rdd.countWithoutDuplicates()\n</pre> # Calculate number of records without duplicates point_rdd.countWithoutDuplicates() Out[9]: <pre>2996</pre> In\u00a0[10]: Copied! <pre># Getting source epsg code\npoint_rdd.getSourceEpsgCode()\n</pre> # Getting source epsg code point_rdd.getSourceEpsgCode() Out[10]: <pre>''</pre> In\u00a0[11]: Copied! <pre># Getting target epsg code\npoint_rdd.getTargetEpsgCode()\n</pre> # Getting target epsg code point_rdd.getTargetEpsgCode() Out[11]: <pre>''</pre> In\u00a0[12]: Copied! <pre># Spatial partitioning data\npoint_rdd.spatialPartitioning(GridType.KDBTREE)\n</pre> # Spatial partitioning data point_rdd.spatialPartitioning(GridType.KDBTREE) <pre>                                                                                \r</pre> Out[12]: <pre>True</pre> <p>rawSpatialRDD method returns RDD which consists of GeoData objects which has 2 attributes</p> <li> geom: shapely.geometry.BaseGeometry </li> <li> userData: str </li> <p>You can use any operations on those objects and spread across machines</p> In\u00a0[13]: Copied! <pre># take firs element\npoint_rdd.rawSpatialRDD.take(1)\n</pre> # take firs element point_rdd.rawSpatialRDD.take(1) <pre>                                                                                \r</pre> Out[13]: <pre>[Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2]</pre> In\u00a0[14]: Copied! <pre># collect to Python list\npoint_rdd.rawSpatialRDD.collect()[:5]\n</pre> # collect to Python list point_rdd.rawSpatialRDD.collect()[:5] Out[14]: <pre>[Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2,\n Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2,\n Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2,\n Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2,\n Geometry: Point userData: testattribute0\ttestattribute1\ttestattribute2]</pre> In\u00a0[15]: Copied! <pre># apply map functions, for example distance to Point(52 21)\npoint_rdd.rawSpatialRDD.map(lambda x: x.geom.distance(Point(21, 52))).take(5)\n</pre> # apply map functions, for example distance to Point(52 21) point_rdd.rawSpatialRDD.map(lambda x: x.geom.distance(Point(21, 52))).take(5) <pre>                                                                                \r</pre> Out[15]: <pre>[111.08786851399313,\n 110.92828303170774,\n 111.1385974283527,\n 110.97450594034112,\n 110.97122518072091]</pre> In\u00a0[16]: Copied! <pre>point_rdd_to_geo = point_rdd.rawSpatialRDD.map(\n    lambda x: [x.geom, *x.getUserData().split(\"\\t\")]\n)\n</pre> point_rdd_to_geo = point_rdd.rawSpatialRDD.map(     lambda x: [x.geom, *x.getUserData().split(\"\\t\")] ) In\u00a0[17]: Copied! <pre>point_gdf = gpd.GeoDataFrame(\n    point_rdd_to_geo.collect(),\n    columns=[\"geom\", \"attr1\", \"attr2\", \"attr3\"],\n    geometry=\"geom\",\n)\n</pre> point_gdf = gpd.GeoDataFrame(     point_rdd_to_geo.collect(),     columns=[\"geom\", \"attr1\", \"attr2\", \"attr3\"],     geometry=\"geom\", ) <pre>                                                                                \r</pre> In\u00a0[18]: Copied! <pre>point_gdf[:5]\n</pre> point_gdf[:5] Out[18]: geom attr1 attr2 attr3 0 POINT (-88.33149 32.32414) testattribute0 testattribute1 testattribute2 1 POINT (-88.17593 32.36076) testattribute0 testattribute1 testattribute2 2 POINT (-88.38895 32.35707) testattribute0 testattribute1 testattribute2 3 POINT (-88.22110 32.35078) testattribute0 testattribute1 testattribute2 4 POINT (-88.32399 32.95067) testattribute0 testattribute1 testattribute2 In\u00a0[19]: Copied! <pre># Adapter allows you to convert geospatial data types introduced with sedona to other ones\n</pre> # Adapter allows you to convert geospatial data types introduced with sedona to other ones In\u00a0[20]: Copied! <pre>spatial_df = Adapter.toDf(\n    point_rdd, [\"attr1\", \"attr2\", \"attr3\"], sedona\n).createOrReplaceTempView(\"spatial_df\")\n\nspatial_gdf = sedona.sql(\"Select attr1, attr2, attr3, geometry as geom from spatial_df\")\n</pre> spatial_df = Adapter.toDf(     point_rdd, [\"attr1\", \"attr2\", \"attr3\"], sedona ).createOrReplaceTempView(\"spatial_df\")  spatial_gdf = sedona.sql(\"Select attr1, attr2, attr3, geometry as geom from spatial_df\") In\u00a0[21]: Copied! <pre>spatial_gdf.show(5, False)\n</pre> spatial_gdf.show(5, False) <pre>[Stage 12:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------------+--------------+--------------+----------------------------+\n|attr1         |attr2         |attr3         |geom                        |\n+--------------+--------------+--------------+----------------------------+\n|testattribute0|testattribute1|testattribute2|POINT (-88.331492 32.324142)|\n|testattribute0|testattribute1|testattribute2|POINT (-88.175933 32.360763)|\n|testattribute0|testattribute1|testattribute2|POINT (-88.388954 32.357073)|\n|testattribute0|testattribute1|testattribute2|POINT (-88.221102 32.35078) |\n|testattribute0|testattribute1|testattribute2|POINT (-88.323995 32.950671)|\n+--------------+--------------+--------------+----------------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[22]: Copied! <pre>gpd.GeoDataFrame(spatial_gdf.toPandas(), geometry=\"geom\")[:5]\n</pre> gpd.GeoDataFrame(spatial_gdf.toPandas(), geometry=\"geom\")[:5] <pre>                                                                                \r</pre> Out[22]: attr1 attr2 attr3 geom 0 testattribute0 testattribute1 testattribute2 POINT (-88.33149 32.32414) 1 testattribute0 testattribute1 testattribute2 POINT (-88.17593 32.36076) 2 testattribute0 testattribute1 testattribute2 POINT (-88.38895 32.35707) 3 testattribute0 testattribute1 testattribute2 POINT (-88.22110 32.35078) 4 testattribute0 testattribute1 testattribute2 POINT (-88.32399 32.95067) In\u00a0[23]: Copied! <pre>schema = StructType(\n    [\n        StructField(\"geometry\", GeometryType(), False),\n        StructField(\"attr1\", StringType(), False),\n        StructField(\"attr2\", StringType(), False),\n        StructField(\"attr3\", StringType(), False),\n    ]\n)\n</pre> schema = StructType(     [         StructField(\"geometry\", GeometryType(), False),         StructField(\"attr1\", StringType(), False),         StructField(\"attr2\", StringType(), False),         StructField(\"attr3\", StringType(), False),     ] ) In\u00a0[24]: Copied! <pre>geo_df = sedona.createDataFrame(point_rdd_to_geo, schema, verifySchema=False)\n</pre> geo_df = sedona.createDataFrame(point_rdd_to_geo, schema, verifySchema=False) In\u00a0[25]: Copied! <pre>gpd.GeoDataFrame(geo_df.toPandas(), geometry=\"geometry\")[:5]\n</pre> gpd.GeoDataFrame(geo_df.toPandas(), geometry=\"geometry\")[:5] <pre>                                                                                \r</pre> Out[25]: geometry attr1 attr2 attr3 0 POINT (-88.33149 32.32414) testattribute0 testattribute1 testattribute2 1 POINT (-88.17593 32.36076) testattribute0 testattribute1 testattribute2 2 POINT (-88.38895 32.35707) testattribute0 testattribute1 testattribute2 3 POINT (-88.22110 32.35078) testattribute0 testattribute1 testattribute2 4 POINT (-88.32399 32.95067) testattribute0 testattribute1 testattribute2 <p>Currently The library supports 5 typed SpatialRDDs:</p> <li> RectangleRDD </li> <li> PointRDD </li> <li> PolygonRDD </li> <li> LineStringRDD </li> <li> CircleRDD </li> In\u00a0[26]: Copied! <pre>rectangle_rdd = RectangleRDD(\n    sc, \"data/zcta510-small.csv\", FileDataSplitter.CSV, True, 11\n)\npoint_rdd = PointRDD(sc, \"data/arealm-small.csv\", 1, FileDataSplitter.CSV, False, 11)\npolygon_rdd = PolygonRDD(\n    sc, \"data/primaryroads-polygon.csv\", FileDataSplitter.CSV, True, 11\n)\nlinestring_rdd = LineStringRDD(\n    sc, \"data/primaryroads-linestring.csv\", FileDataSplitter.CSV, True\n)\n</pre> rectangle_rdd = RectangleRDD(     sc, \"data/zcta510-small.csv\", FileDataSplitter.CSV, True, 11 ) point_rdd = PointRDD(sc, \"data/arealm-small.csv\", 1, FileDataSplitter.CSV, False, 11) polygon_rdd = PolygonRDD(     sc, \"data/primaryroads-polygon.csv\", FileDataSplitter.CSV, True, 11 ) linestring_rdd = LineStringRDD(     sc, \"data/primaryroads-linestring.csv\", FileDataSplitter.CSV, True ) In\u00a0[27]: Copied! <pre>rectangle_rdd.analyze()\npoint_rdd.analyze()\npolygon_rdd.analyze()\nlinestring_rdd.analyze()\n</pre> rectangle_rdd.analyze() point_rdd.analyze() polygon_rdd.analyze() linestring_rdd.analyze() Out[27]: <pre>True</pre> <p>Apache Sedona spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.</p> In\u00a0[28]: Copied! <pre>point_rdd.spatialPartitioning(GridType.KDBTREE)\n</pre> point_rdd.spatialPartitioning(GridType.KDBTREE) Out[28]: <pre>True</pre> <p>Apache Sedona provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, Apache Sedona will build a local tree index on each of the SpatialRDD partition.</p> In\u00a0[29]: Copied! <pre>point_rdd.buildIndex(IndexType.RTREE, True)\n</pre> point_rdd.buildIndex(IndexType.RTREE, True) <p>Spatial join is operation which combines data based on spatial relations like:</p> <li> intersects </li> <li> touches </li> <li> within </li> <li> etc </li> <p>To Use Spatial Join in GeoPyspark library please use JoinQuery object, which has implemented below methods:</p> <pre>SpatialJoinQuery(spatialRDD: SpatialRDD, queryRDD: SpatialRDD, useIndex: bool, considerBoundaryIntersection: bool) -&gt; RDD\n\nDistanceJoinQuery(spatialRDD: SpatialRDD, queryRDD: SpatialRDD, useIndex: bool, considerBoundaryIntersection: bool) -&gt; RDD\n\nspatialJoin(queryWindowRDD: SpatialRDD, objectRDD: SpatialRDD, joinParams: JoinParams) -&gt; RDD\n\nDistanceJoinQueryFlat(spatialRDD: SpatialRDD, queryRDD: SpatialRDD, useIndex: bool, considerBoundaryIntersection: bool) -&gt; RDD\n\nSpatialJoinQueryFlat(spatialRDD: SpatialRDD, queryRDD: SpatialRDD, useIndex: bool, considerBoundaryIntersection: bool) -&gt; RDD\n</pre> In\u00a0[30]: Copied! <pre># partitioning the data\npoint_rdd.spatialPartitioning(GridType.KDBTREE)\nrectangle_rdd.spatialPartitioning(point_rdd.getPartitioner())\n# building an index\npoint_rdd.buildIndex(IndexType.RTREE, True)\n# Perform Spatial Join Query\nresult = JoinQuery.SpatialJoinQueryFlat(point_rdd, rectangle_rdd, False, True)\n</pre> # partitioning the data point_rdd.spatialPartitioning(GridType.KDBTREE) rectangle_rdd.spatialPartitioning(point_rdd.getPartitioner()) # building an index point_rdd.buildIndex(IndexType.RTREE, True) # Perform Spatial Join Query result = JoinQuery.SpatialJoinQueryFlat(point_rdd, rectangle_rdd, False, True) <p>As result we will get RDD[GeoData, GeoData] It can be used like any other Python RDD. You can use map, take, collect and other functions</p> In\u00a0[31]: Copied! <pre>result\n</pre> result Out[31]: <pre>MapPartitionsRDD[63] at map at FlatPairRddConverter.scala:30</pre> In\u00a0[32]: Copied! <pre>result.take(2)\n</pre> result.take(2) <pre>                                                                                \r</pre> Out[32]: <pre>[[Geometry: Polygon userData: , Geometry: Point userData: ],\n [Geometry: Polygon userData: , Geometry: Point userData: ]]</pre> In\u00a0[33]: Copied! <pre>result.collect()[:3]\n</pre> result.collect()[:3] <pre>                                                                                \r</pre> Out[33]: <pre>[[Geometry: Polygon userData: , Geometry: Point userData: ],\n [Geometry: Polygon userData: , Geometry: Point userData: ],\n [Geometry: Polygon userData: , Geometry: Point userData: ]]</pre> In\u00a0[34]: Copied! <pre># getting distance using SpatialObjects\nresult.map(lambda x: x[0].geom.distance(x[1].geom)).take(5)\n</pre> # getting distance using SpatialObjects result.map(lambda x: x[0].geom.distance(x[1].geom)).take(5) <pre>                                                                                \r</pre> Out[34]: <pre>[0.0, 0.0, 0.0, 0.0, 0.0]</pre> In\u00a0[35]: Copied! <pre># getting area of polygon data\nresult.map(lambda x: x[0].geom.area).take(5)\n</pre> # getting area of polygon data result.map(lambda x: x[0].geom.area).take(5) <pre>                                                                                \r</pre> Out[35]: <pre>[0.026651558685001447,\n 0.026651558685001447,\n 0.026651558685001447,\n 0.057069904940998895,\n 0.057069904940998895]</pre> In\u00a0[36]: Copied! <pre># Base on result you can create DataFrame object, using map function and build DataFrame from RDD\n</pre> # Base on result you can create DataFrame object, using map function and build DataFrame from RDD In\u00a0[37]: Copied! <pre>schema = StructType(\n    [\n        StructField(\"geom_left\", GeometryType(), False),\n        StructField(\"geom_right\", GeometryType(), False),\n    ]\n)\n</pre> schema = StructType(     [         StructField(\"geom_left\", GeometryType(), False),         StructField(\"geom_right\", GeometryType(), False),     ] ) In\u00a0[38]: Copied! <pre># Set verifySchema to False\nspatial_join_result = result.map(lambda x: [x[0].geom, x[1].geom])\nsedona.createDataFrame(spatial_join_result, schema, verifySchema=False).show(5, True)\n</pre> # Set verifySchema to False spatial_join_result = result.map(lambda x: [x[0].geom, x[1].geom]) sedona.createDataFrame(spatial_join_result, schema, verifySchema=False).show(5, True) <pre>[Stage 62:===================================&gt;                    (10 + 1) / 16]\r</pre> <pre>+--------------------+--------------------+\n|           geom_left|          geom_right|\n+--------------------+--------------------+\n|POLYGON ((-87.229...|POINT (-87.204033...|\n|POLYGON ((-87.229...|POINT (-87.204299...|\n|POLYGON ((-87.229...|POINT (-87.19351 ...|\n|POLYGON ((-87.285...|POINT (-87.28468 ...|\n|POLYGON ((-87.285...|POINT (-87.215491...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[39]: Copied! <pre># Above code produces DataFrame with geometry Data type\n</pre> # Above code produces DataFrame with geometry Data type In\u00a0[40]: Copied! <pre>sedona.createDataFrame(spatial_join_result, schema, verifySchema=False).printSchema()\n</pre> sedona.createDataFrame(spatial_join_result, schema, verifySchema=False).printSchema() <pre>root\n |-- geom_left: geometry (nullable = false)\n |-- geom_right: geometry (nullable = false)\n\n</pre> <p>We can create DataFrame object from Spatial Pair RDD using Adapter object as follows</p> In\u00a0[41]: Copied! <pre>Adapter.toDf(result, [\"attr1\"], [\"attr2\"], sedona).show(5, True)\n</pre> Adapter.toDf(result, [\"attr1\"], [\"attr2\"], sedona).show(5, True) <pre>[Stage 80:===================================&gt;                    (10 + 1) / 16]\r</pre> <pre>+--------------------+-----+--------------------+-----+\n|              geom_1|attr1|              geom_2|attr2|\n+--------------------+-----+--------------------+-----+\n|POLYGON ((-87.229...|     |POINT (-87.204033...|     |\n|POLYGON ((-87.229...|     |POINT (-87.204299...|     |\n|POLYGON ((-87.229...|     |POINT (-87.19351 ...|     |\n|POLYGON ((-87.285...|     |POINT (-87.28468 ...|     |\n|POLYGON ((-87.285...|     |POINT (-87.215491...|     |\n+--------------------+-----+--------------------+-----+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> <p>This also produce DataFrame with geometry DataType</p> In\u00a0[42]: Copied! <pre>Adapter.toDf(result, [\"attr1\"], [\"attr2\"], sedona).printSchema()\n</pre> Adapter.toDf(result, [\"attr1\"], [\"attr2\"], sedona).printSchema() <pre>[Stage 89:==========================================&gt;             (12 + 1) / 16]\r</pre> <pre>root\n |-- geom_1: geometry (nullable = true)\n |-- attr1: string (nullable = true)\n |-- geom_2: geometry (nullable = true)\n |-- attr2: string (nullable = true)\n\n</pre> <pre>                                                                                \r</pre> <p>We can create RDD which will be of type RDD[GeoData, List[GeoData]] We can for example calculate number of Points within some polygon data</p> <p>To do that we can use code specified below</p> In\u00a0[43]: Copied! <pre>point_rdd.spatialPartitioning(GridType.KDBTREE)\nrectangle_rdd.spatialPartitioning(point_rdd.getPartitioner())\n</pre> point_rdd.spatialPartitioning(GridType.KDBTREE) rectangle_rdd.spatialPartitioning(point_rdd.getPartitioner()) In\u00a0[44]: Copied! <pre>spatial_join_result_non_flat = JoinQuery.SpatialJoinQuery(\n    point_rdd, rectangle_rdd, False, True\n)\n</pre> spatial_join_result_non_flat = JoinQuery.SpatialJoinQuery(     point_rdd, rectangle_rdd, False, True ) In\u00a0[45]: Copied! <pre># number of point for each polygon\nnumber_of_points = spatial_join_result_non_flat.map(\n    lambda x: [x[0].geom, x[1].__len__()]\n)\n</pre> # number of point for each polygon number_of_points = spatial_join_result_non_flat.map(     lambda x: [x[0].geom, x[1].__len__()] ) In\u00a0[46]: Copied! <pre>schema = StructType(\n    [\n        StructField(\"geometry\", GeometryType(), False),\n        StructField(\"number_of_points\", LongType(), False),\n    ]\n)\n</pre> schema = StructType(     [         StructField(\"geometry\", GeometryType(), False),         StructField(\"number_of_points\", LongType(), False),     ] ) In\u00a0[47]: Copied! <pre>sedona.createDataFrame(number_of_points, schema, verifySchema=False).show()\n</pre> sedona.createDataFrame(number_of_points, schema, verifySchema=False).show() <pre>                                                                                \r</pre> <pre>+--------------------+----------------+\n|            geometry|number_of_points|\n+--------------------+----------------+\n|POLYGON ((-86.749...|               4|\n|POLYGON ((-87.229...|               7|\n|POLYGON ((-87.114...|              15|\n|POLYGON ((-87.082...|              12|\n|POLYGON ((-86.697...|               1|\n|POLYGON ((-86.816...|               6|\n|POLYGON ((-87.285...|              26|\n|POLYGON ((-87.105...|              15|\n|POLYGON ((-86.860...|              12|\n|POLYGON ((-87.092...|               5|\n+--------------------+----------------+\n\n</pre> <pre>                                                                                \r</pre> <p>Spatial KNNQuery is operation which help us find answer which k number of geometries lays closest to other geometry.</p> <p>For Example: 5 closest Shops to your home. To use Spatial KNNQuery please use object  KNNQuery  which has one method:</p> <pre>SpatialKnnQuery(spatialRDD: SpatialRDD, originalQueryPoint: BaseGeometry, k: int,  useIndex: bool)-&gt; List[GeoData]\n</pre> In\u00a0[48]: Copied! <pre>result = KNNQuery.SpatialKnnQuery(point_rdd, Point(-84.01, 34.01), 5, False)\n</pre> result = KNNQuery.SpatialKnnQuery(point_rdd, Point(-84.01, 34.01), 5, False) In\u00a0[49]: Copied! <pre>result\n</pre> result Out[49]: <pre>[Geometry: Point userData: ,\n Geometry: Point userData: ,\n Geometry: Point userData: ,\n Geometry: Point userData: ,\n Geometry: Point userData: ]</pre> <p>As Reference geometry you can also use Polygon or LineString object</p> In\u00a0[50]: Copied! <pre>polygon = Polygon(\n    [\n        (-84.237756, 33.904859),\n        (-84.237756, 34.090426),\n        (-83.833011, 34.090426),\n        (-83.833011, 33.904859),\n        (-84.237756, 33.904859),\n    ]\n)\npolygons_nearby = KNNQuery.SpatialKnnQuery(polygon_rdd, polygon, 5, False)\n</pre> polygon = Polygon(     [         (-84.237756, 33.904859),         (-84.237756, 34.090426),         (-83.833011, 34.090426),         (-83.833011, 33.904859),         (-84.237756, 33.904859),     ] ) polygons_nearby = KNNQuery.SpatialKnnQuery(polygon_rdd, polygon, 5, False) <pre>                                                                                \r</pre> In\u00a0[51]: Copied! <pre>polygons_nearby\n</pre> polygons_nearby Out[51]: <pre>[Geometry: Polygon userData: ,\n Geometry: Polygon userData: ,\n Geometry: Polygon userData: ,\n Geometry: Polygon userData: ,\n Geometry: Polygon userData: ]</pre> In\u00a0[52]: Copied! <pre>polygons_nearby[0].geom.wkt\n</pre> polygons_nearby[0].geom.wkt Out[52]: <pre>'POLYGON ((-83.993559 34.087259, -83.993559 34.131247, -83.959903 34.131247, -83.959903 34.087259, -83.993559 34.087259))'</pre> <p>A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that intersect / are fully covered by the query window. RangeQuery has one method:</p> <pre>SpatialRangeQuery(self, spatialRDD: SpatialRDD, rangeQueryWindow: BaseGeometry, considerBoundaryIntersection: bool, usingIndex: bool) -&gt; RDD\n</pre> In\u00a0[53]: Copied! <pre>from sedona.core.geom.envelope import Envelope\n</pre> from sedona.core.geom.envelope import Envelope In\u00a0[54]: Copied! <pre>query_envelope = Envelope(-85.01, -60.01, 34.01, 50.01)\n\nresult_range_query = RangeQuery.SpatialRangeQuery(\n    linestring_rdd, query_envelope, False, False\n)\n</pre> query_envelope = Envelope(-85.01, -60.01, 34.01, 50.01)  result_range_query = RangeQuery.SpatialRangeQuery(     linestring_rdd, query_envelope, False, False ) In\u00a0[55]: Copied! <pre>result_range_query\n</pre> result_range_query Out[55]: <pre>MapPartitionsRDD[127] at map at GeometryRddConverter.scala:30</pre> In\u00a0[56]: Copied! <pre>result_range_query.take(6)\n</pre> result_range_query.take(6) Out[56]: <pre>[Geometry: LineString userData: ,\n Geometry: LineString userData: ,\n Geometry: LineString userData: ,\n Geometry: LineString userData: ,\n Geometry: LineString userData: ,\n Geometry: LineString userData: ]</pre> In\u00a0[57]: Copied! <pre># Creating DataFrame from result\n</pre> # Creating DataFrame from result In\u00a0[58]: Copied! <pre>schema = StructType([StructField(\"geometry\", GeometryType(), False)])\n</pre> schema = StructType([StructField(\"geometry\", GeometryType(), False)]) In\u00a0[59]: Copied! <pre>sedona.createDataFrame(\n    result_range_query.map(lambda x: [x.geom]), schema, verifySchema=False\n).show(5, True)\n</pre> sedona.createDataFrame(     result_range_query.map(lambda x: [x.geom]), schema, verifySchema=False ).show(5, True) <pre>+--------------------+\n|            geometry|\n+--------------------+\n|LINESTRING (-72.1...|\n|LINESTRING (-72.4...|\n|LINESTRING (-72.4...|\n|LINESTRING (-73.4...|\n|LINESTRING (-73.6...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> <p>GeoPyspark allows to load the data from other Data formats like:</p> <li> GeoJSON </li> <li> Shapefile </li> <li> WKB </li> <li> WKT </li> In\u00a0[60]: Copied! <pre>## ShapeFile - load to SpatialRDD\n</pre> ## ShapeFile - load to SpatialRDD In\u00a0[61]: Copied! <pre>shape_rdd = ShapefileReader.readToGeometryRDD(sc, \"data/polygon\")\n</pre> shape_rdd = ShapefileReader.readToGeometryRDD(sc, \"data/polygon\") In\u00a0[62]: Copied! <pre>shape_rdd\n</pre> shape_rdd Out[62]: <pre>&lt;sedona.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f468e4bacd0&gt;</pre> In\u00a0[63]: Copied! <pre>Adapter.toDf(shape_rdd, sedona).show(5, True)\n</pre> Adapter.toDf(shape_rdd, sedona).show(5, True) <pre>+--------------------+\n|            geometry|\n+--------------------+\n|MULTIPOLYGON (((1...|\n|MULTIPOLYGON (((-...|\n|MULTIPOLYGON (((1...|\n|POLYGON ((118.362...|\n|MULTIPOLYGON (((-...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[64]: Copied! <pre>## GeoJSON - load to SpatialRDD\n</pre> ## GeoJSON - load to SpatialRDD <pre><code>{ \"type\": \"Feature\", \"properties\": { \"STATEFP\": \"01\", \"COUNTYFP\": \"077\", \"TRACTCE\": \"011501\", \"BLKGRPCE\": \"5\", \"AFFGEOID\": \"1500000US010770115015\", \"GEOID\": \"010770115015\", \"NAME\": \"5\", \"LSAD\": \"BG\", \"ALAND\": 6844991, \"AWATER\": 32636 }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n</code></pre> In\u00a0[65]: Copied! <pre>geo_json_rdd = GeoJsonReader.readToGeometryRDD(sc, \"data/testPolygon.json\")\n</pre> geo_json_rdd = GeoJsonReader.readToGeometryRDD(sc, \"data/testPolygon.json\") In\u00a0[66]: Copied! <pre>geo_json_rdd\n</pre> geo_json_rdd Out[66]: <pre>&lt;sedona.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f468e4c94f0&gt;</pre> In\u00a0[67]: Copied! <pre>Adapter.toDf(geo_json_rdd, sedona).drop(\"AWATER\").show(5, True)\n</pre> Adapter.toDf(geo_json_rdd, sedona).drop(\"AWATER\").show(5, True) <pre>[Stage 111:&gt;                                                        (0 + 1) / 1]\r</pre> <pre>+--------------------+-------+--------+-------+--------+--------------------+------------+----+----+--------+\n|            geometry|STATEFP|COUNTYFP|TRACTCE|BLKGRPCE|            AFFGEOID|       GEOID|NAME|LSAD|   ALAND|\n+--------------------+-------+--------+-------+--------+--------------------+------------+----+----+--------+\n|POLYGON ((-87.621...|     01|     077| 011501|       5|1500000US01077011...|010770115015|   5|  BG| 6844991|\n|POLYGON ((-85.719...|     01|     045| 021102|       4|1500000US01045021...|010450211024|   4|  BG|11360854|\n|POLYGON ((-86.000...|     01|     055| 001300|       3|1500000US01055001...|010550013003|   3|  BG| 1378742|\n|POLYGON ((-86.574...|     01|     089| 001700|       2|1500000US01089001...|010890017002|   2|  BG| 1040641|\n|POLYGON ((-85.382...|     01|     069| 041400|       1|1500000US01069041...|010690414001|   1|  BG| 8243574|\n+--------------------+-------+--------+-------+--------+--------------------+------------+----+----+--------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[68]: Copied! <pre>## WKT - loading to SpatialRDD\n</pre> ## WKT - loading to SpatialRDD In\u00a0[69]: Copied! <pre>wkt_rdd = WktReader.readToGeometryRDD(sc, \"data/county_small.tsv\", 0, True, False)\n</pre> wkt_rdd = WktReader.readToGeometryRDD(sc, \"data/county_small.tsv\", 0, True, False) In\u00a0[70]: Copied! <pre>wkt_rdd\n</pre> wkt_rdd Out[70]: <pre>&lt;sedona.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f468e2fd280&gt;</pre> In\u00a0[71]: Copied! <pre>Adapter.toDf(wkt_rdd, sedona).printSchema()\n</pre> Adapter.toDf(wkt_rdd, sedona).printSchema() <pre>root\n |-- geometry: geometry (nullable = true)\n\n</pre> In\u00a0[72]: Copied! <pre>Adapter.toDf(wkt_rdd, sedona).show(5, True)\n</pre> Adapter.toDf(wkt_rdd, sedona).show(5, True) <pre>[Stage 113:&gt;                                                        (0 + 1) / 1]\r</pre> <pre>+--------------------+\n|            geometry|\n+--------------------+\n|POLYGON ((-97.019...|\n|POLYGON ((-123.43...|\n|POLYGON ((-104.56...|\n|POLYGON ((-96.910...|\n|POLYGON ((-98.273...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[73]: Copied! <pre>## WKB - load to SpatialRDD\n</pre> ## WKB - load to SpatialRDD In\u00a0[74]: Copied! <pre>wkb_rdd = WkbReader.readToGeometryRDD(sc, \"data/county_small_wkb.tsv\", 0, True, False)\n</pre> wkb_rdd = WkbReader.readToGeometryRDD(sc, \"data/county_small_wkb.tsv\", 0, True, False) In\u00a0[75]: Copied! <pre>Adapter.toDf(wkb_rdd, sedona).show(5, True)\n</pre> Adapter.toDf(wkb_rdd, sedona).show(5, True) <pre>+--------------------+\n|            geometry|\n+--------------------+\n|POLYGON ((-97.019...|\n|POLYGON ((-123.43...|\n|POLYGON ((-104.56...|\n|POLYGON ((-96.910...|\n|POLYGON ((-98.273...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[76]: Copied! <pre>point_rdd.spatialPartitioning(GridType.KDBTREE)\nrectangle_rdd.spatialPartitioning(point_rdd.getPartitioner())\n# building an index\npoint_rdd.buildIndex(IndexType.RTREE, True)\n# Perform Spatial Join Query\nresult = JoinQueryRaw.SpatialJoinQueryFlat(point_rdd, rectangle_rdd, False, True)\n</pre> point_rdd.spatialPartitioning(GridType.KDBTREE) rectangle_rdd.spatialPartitioning(point_rdd.getPartitioner()) # building an index point_rdd.buildIndex(IndexType.RTREE, True) # Perform Spatial Join Query result = JoinQueryRaw.SpatialJoinQueryFlat(point_rdd, rectangle_rdd, False, True) In\u00a0[77]: Copied! <pre># without passing column names, the result will contain only two geometries columns\ngeometry_df = Adapter.toDf(result, sedona)\n</pre> # without passing column names, the result will contain only two geometries columns geometry_df = Adapter.toDf(result, sedona) In\u00a0[78]: Copied! <pre>geometry_df.printSchema()\n</pre> geometry_df.printSchema() <pre>root\n |-- leftgeometry: geometry (nullable = true)\n |-- rightgeometry: geometry (nullable = true)\n\n</pre> In\u00a0[79]: Copied! <pre>geometry_df.show(5)\n</pre> geometry_df.show(5) <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+\n|        leftgeometry|       rightgeometry|\n+--------------------+--------------------+\n|POLYGON ((-87.092...|POINT (-86.94719 ...|\n|POLYGON ((-86.749...|POINT (-86.736302...|\n|POLYGON ((-86.749...|POINT (-86.735506...|\n|POLYGON ((-86.749...|POINT (-86.68645 ...|\n|POLYGON ((-86.749...|POINT (-86.675405...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[80]: Copied! <pre>geometry_df.collect()[0]\n</pre> geometry_df.collect()[0] Out[80]: <pre>Row(leftgeometry=&lt;POLYGON ((-87.093 34.264, -87.093 34.422, -86.764 34.422, -86.764 34.264, -...&gt;, rightgeometry=&lt;POINT (-86.947 34.29)&gt;)</pre> In\u00a0[81]: Copied! <pre>geometry_df = Adapter.toDf(result, [\"left_user_data\"], [\"right_user_data\"], sedona)\n</pre> geometry_df = Adapter.toDf(result, [\"left_user_data\"], [\"right_user_data\"], sedona) In\u00a0[82]: Copied! <pre>geometry_df.show(5)\n</pre> geometry_df.show(5) <pre>+--------------------+--------------+--------------------+---------------+\n|        leftgeometry|left_user_data|       rightgeometry|right_user_data|\n+--------------------+--------------+--------------------+---------------+\n|POLYGON ((-87.092...|              |POINT (-86.94719 ...|           null|\n|POLYGON ((-86.749...|              |POINT (-86.736302...|           null|\n|POLYGON ((-86.749...|              |POINT (-86.735506...|           null|\n|POLYGON ((-86.749...|              |POINT (-86.68645 ...|           null|\n|POLYGON ((-86.749...|              |POINT (-86.675405...|           null|\n+--------------------+--------------+--------------------+---------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[83]: Copied! <pre>query_envelope = Envelope(-85.01, -60.01, 34.01, 50.01)\n\nresult_range_query = RangeQueryRaw.SpatialRangeQuery(\n    linestring_rdd, query_envelope, False, False\n)\n</pre> query_envelope = Envelope(-85.01, -60.01, 34.01, 50.01)  result_range_query = RangeQueryRaw.SpatialRangeQuery(     linestring_rdd, query_envelope, False, False ) In\u00a0[84]: Copied! <pre># converting to df\ngdf = Adapter.toDf(result_range_query, sedona)\n</pre> # converting to df gdf = Adapter.toDf(result_range_query, sedona) In\u00a0[85]: Copied! <pre>gdf.show(5)\n</pre> gdf.show(5) <pre>+--------------------+\n|            geometry|\n+--------------------+\n|LINESTRING (-72.1...|\n|LINESTRING (-72.4...|\n|LINESTRING (-72.4...|\n|LINESTRING (-73.4...|\n|LINESTRING (-73.6...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[86]: Copied! <pre>gdf.printSchema()\n</pre> gdf.printSchema() <pre>root\n |-- geometry: geometry (nullable = true)\n\n</pre> In\u00a0[87]: Copied! <pre># Passing column names\n# converting to df\ngdf_with_columns = Adapter.toDf(result_range_query, sedona, [\"_c1\"])\n</pre> # Passing column names # converting to df gdf_with_columns = Adapter.toDf(result_range_query, sedona, [\"_c1\"]) In\u00a0[88]: Copied! <pre>gdf_with_columns.show(5)\n</pre> gdf_with_columns.show(5) <pre>+--------------------+---+\n|            geometry|_c1|\n+--------------------+---+\n|LINESTRING (-72.1...|   |\n|LINESTRING (-72.4...|   |\n|LINESTRING (-72.4...|   |\n|LINESTRING (-73.4...|   |\n|LINESTRING (-73.6...|   |\n+--------------------+---+\nonly showing top 5 rows\n\n</pre> In\u00a0[89]: Copied! <pre>gdf_with_columns.printSchema()\n</pre> gdf_with_columns.printSchema() <pre>root\n |-- geometry: geometry (nullable = true)\n |-- _c1: string (nullable = true)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/ApacheSedonaCore/#create-spatialrdd","title":"Create SpatialRDD\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#reading-to-pointrdd-from-csv-file","title":"Reading to PointRDD from CSV file\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#operations-on-rawspatialrdd","title":"Operations on RawSpatialRDD\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#transforming-to-geopandas","title":"Transforming to GeoPandas\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#loaded-data-can-be-transformed-to-geopandas-dataframe-in-a-few-ways","title":"Loaded data can be transformed to GeoPandas DataFrame in a few ways\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#directly-from-rdd","title":"Directly from RDD\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#using-adapter","title":"Using Adapter\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#with-dataframe-creation","title":"With DataFrame creation\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#load-typed-spatialrdds","title":"Load Typed SpatialRDDs\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#spatial-partitioning","title":"Spatial Partitioning\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#create-index","title":"Create Index\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#spatialjoin","title":"SpatialJoin\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#example-spatialjoinqueryflat-pointrdd-with-rectanglerdd","title":"Example SpatialJoinQueryFlat PointRDD with RectangleRDD\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#knnquery","title":"KNNQuery\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#finds-5-closest-points-from-pointrdd-to-given-point","title":"Finds 5 closest points from PointRDD to given Point\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#rangequery","title":"RangeQuery\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#load-from-other-formats","title":"Load From other Formats\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#converting-rdd-spatial-join-result-to-df-directly-avoiding-jvm-python-serde","title":"Converting RDD Spatial join result to DF directly, avoiding jvm python serde\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#passing-column-names","title":"Passing column names\u00b6","text":""},{"location":"usecases/ApacheSedonaCore/#converting-rdd-spatial-join-result-to-df-directly-avoiding-jvm-python-serde","title":"Converting RDD Spatial join result to DF directly, avoiding jvm python serde\u00b6","text":""},{"location":"usecases/ApacheSedonaRaster/","title":"Raster image manipulation","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>from sedona.spark import *\nfrom IPython.display import display, HTML\n</pre> from sedona.spark import * from IPython.display import display, HTML In\u00a0[2]: Copied! <pre>config = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"\n        \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"\n        \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n\nsc = sedona.sparkContext\n</pre> config = (     SedonaContext.builder()     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"         \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"         \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",     )     .getOrCreate() )  sedona = SedonaContext.create(config)  sc = sedona.sparkContext <pre>:: loading settings :: url = jar:file:/home/jovyan/spark-3.4.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jovyan/.ivy2/cache\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\norg.apache.sedona#sedona-spark-3.4_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\nuk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-1ba43c79-c5b5-45c3-8320-61322bde74a8;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-spark-3.4_2.12;1.6.0 in central\n\tfound org.apache.sedona#sedona-common;1.6.0 in central\n\tfound org.apache.commons#commons-math3;3.6.1 in central\n\tfound org.locationtech.jts#jts-core;1.19.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n\tfound com.google.geometry#s2-geometry;2.0.0 in central\n\tfound com.google.guava#guava;25.1-jre in central\n\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n\tfound org.checkerframework#checker-qual;2.0.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n\tfound com.uber#h3;4.1.1 in central\n\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n\tfound org.checkerframework#checker-qual;3.10.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n\tfound org.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 in central\n\tfound commons-lang#commons-lang;2.6 in central\n\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n\tfound org.beryx#awt-color-factory;1.0.0 in central\n\tfound org.datasyslab#geotools-wrapper;1.6.0-28.2 in central\n\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n:: resolution report :: resolve 2131ms :: artifacts dl 92ms\n\t:: modules in use:\n\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n\tcom.google.guava#guava;25.1-jre from central in [default]\n\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n\tcom.uber#h3;4.1.1 from central in [default]\n\tcommons-lang#commons-lang;2.6 from central in [default]\n\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n\torg.apache.sedona#sedona-common;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-3.4_2.12;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 from central in [default]\n\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n\torg.datasyslab#geotools-wrapper;1.6.0-28.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n\t:: evicted modules:\n\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   25  |   0   |   0   |   2   ||   23  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-1ba43c79-c5b5-45c3-8320-61322bde74a8\n\tconfs: [default]\n\t0 artifacts copied, 23 already retrieved (0kB/13ms)\n24/05/22 17:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[3]: Copied! <pre>geotiff_df = sedona.read.format(\"binaryFile\").load(\"data/raster/test5.tiff\")\ngeotiff_df.show(2)\ngeotiff_df.createOrReplaceTempView(\"binary_raster\")\n</pre> geotiff_df = sedona.read.format(\"binaryFile\").load(\"data/raster/test5.tiff\") geotiff_df.show(2) geotiff_df.createOrReplaceTempView(\"binary_raster\") <pre>+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|file:/home/jovyan...|2024-05-22 17:49:30|209199|[4D 4D 00 2A 00 0...|\n+--------------------+-------------------+------+--------------------+\n\n</pre> In\u00a0[4]: Copied! <pre>raster_df = sedona.sql(\"SELECT RS_FromGeoTiff(content) as raster from binary_raster\")\nraster_df.show(2)\nraster_df.createOrReplaceTempView(\"raster_table\")\n</pre> raster_df = sedona.sql(\"SELECT RS_FromGeoTiff(content) as raster from binary_raster\") raster_df.show(2) raster_df.createOrReplaceTempView(\"raster_table\") <pre>[Stage 4:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+--------------------+\n|              raster|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[5]: Copied! <pre>raster_metadata = sedona.sql(\"SELECT RS_MetaData(raster) as metadata from raster_table\")\nmetadata = raster_metadata.first()[0]\nraster_srid = metadata[8]\nmetadata\n</pre> raster_metadata = sedona.sql(\"SELECT RS_MetaData(raster) as metadata from raster_table\") metadata = raster_metadata.first()[0] raster_srid = metadata[8] metadata Out[5]: <pre>[-180.0, 90.0, 1440.0, 720.0, 0.25, -0.25, 0.0, 0.0, 4326.0, 1.0]</pre> In\u00a0[6]: Copied! <pre>SedonaUtils.display_image(raster_df.selectExpr(\"RS_AsImage(raster, 500)\"))\n</pre> SedonaUtils.display_image(raster_df.selectExpr(\"RS_AsImage(raster, 500)\")) rs_asimage(raster, 500) 0 In\u00a0[7]: Copied! <pre>(width, height) = sedona.sql(\n    \"SELECT RS_Width(raster) as width, RS_Height(raster) as height from raster_table\"\n).first()\n(p1X, p1Y) = sedona.sql(\n    f\"SELECT RS_RasterToWorldCoordX(raster, {width / 2}, {height / 2}) \\\n                  as pX, RS_RasterToWorldCoordY(raster, {width / 2}, {height / 2}) as pY from raster_table\"\n).first()\n(p2X, p2Y) = sedona.sql(\n    f\"SELECT RS_RasterToWorldCoordX(raster, {(width / 2) + 2}, {height / 2}) \\\n                  as pX, RS_RasterToWorldCoordY(raster, {(width / 2) + 2}, {height / 2}) as pY from raster_table\"\n).first()\n(p3X, p3Y) = sedona.sql(\n    f\"SELECT RS_RasterToWorldCoordX(raster, {width / 2}, {(height / 2) + 2}) \\\n                  as pX, RS_RasterToWorldCoordY(raster, {width / 2}, {(height / 2) + 2}) as pY from raster_table\"\n).first()\n(p4X, p4Y) = sedona.sql(\n    f\"SELECT RS_RasterToWorldCoordX(raster, {(width / 2) + 2}, {(height / 2) + 2}) \\\n                  as pX, RS_RasterToWorldCoordY(raster, {(width / 2) + 2}, {(height / 2) + 2}) as pY from raster_table\"\n).first()\n</pre> (width, height) = sedona.sql(     \"SELECT RS_Width(raster) as width, RS_Height(raster) as height from raster_table\" ).first() (p1X, p1Y) = sedona.sql(     f\"SELECT RS_RasterToWorldCoordX(raster, {width / 2}, {height / 2}) \\                   as pX, RS_RasterToWorldCoordY(raster, {width / 2}, {height / 2}) as pY from raster_table\" ).first() (p2X, p2Y) = sedona.sql(     f\"SELECT RS_RasterToWorldCoordX(raster, {(width / 2) + 2}, {height / 2}) \\                   as pX, RS_RasterToWorldCoordY(raster, {(width / 2) + 2}, {height / 2}) as pY from raster_table\" ).first() (p3X, p3Y) = sedona.sql(     f\"SELECT RS_RasterToWorldCoordX(raster, {width / 2}, {(height / 2) + 2}) \\                   as pX, RS_RasterToWorldCoordY(raster, {width / 2}, {(height / 2) + 2}) as pY from raster_table\" ).first() (p4X, p4Y) = sedona.sql(     f\"SELECT RS_RasterToWorldCoordX(raster, {(width / 2) + 2}, {(height / 2) + 2}) \\                   as pX, RS_RasterToWorldCoordY(raster, {(width / 2) + 2}, {(height / 2) + 2}) as pY from raster_table\" ).first() <pre>                                                                                \r</pre> In\u00a0[8]: Copied! <pre>geom_wkt = f\"SRID={int(raster_srid)};POLYGON (({p1X} {p1Y}, {p2X} {p2Y}, {p3X} {p3Y}, {p4X} {p4Y}, {p1X} {p1Y}))\"\n</pre> geom_wkt = f\"SRID={int(raster_srid)};POLYGON (({p1X} {p1Y}, {p2X} {p2Y}, {p3X} {p3Y}, {p4X} {p4Y}, {p1X} {p1Y}))\" In\u00a0[9]: Copied! <pre>geom_df = sedona.sql(f\"SELECT ST_GeomFromEWKT('{geom_wkt}') as geom\")\ngeom_df.createOrReplaceTempView(\"geom_table\")\n</pre> geom_df = sedona.sql(f\"SELECT ST_GeomFromEWKT('{geom_wkt}') as geom\") geom_df.createOrReplaceTempView(\"geom_table\") In\u00a0[10]: Copied! <pre>joined_df = sedona.sql(\n    \"SELECT g.geom from raster_table r, geom_table g where RS_Intersects(r.raster, g.geom)\"\n)\njoined_df.show()\n</pre> joined_df = sedona.sql(     \"SELECT g.geom from raster_table r, geom_table g where RS_Intersects(r.raster, g.geom)\" ) joined_df.show() <pre>+--------------------+\n|                geom|\n+--------------------+\n|POLYGON ((-0.25 0...|\n+--------------------+\n\n</pre> In\u00a0[11]: Copied! <pre>raster_convex_hull = sedona.sql(\n    \"SELECT RS_ConvexHull(raster) as convex_hull from raster_table\"\n)\nraster_min_convex_hull = sedona.sql(\n    \"SELECT RS_MinConvexHull(raster) as min_convex_hull from raster_table\"\n)\nraster_convex_hull.show(truncate=False)\nraster_min_convex_hull.show(truncate=False)\n</pre> raster_convex_hull = sedona.sql(     \"SELECT RS_ConvexHull(raster) as convex_hull from raster_table\" ) raster_min_convex_hull = sedona.sql(     \"SELECT RS_MinConvexHull(raster) as min_convex_hull from raster_table\" ) raster_convex_hull.show(truncate=False) raster_min_convex_hull.show(truncate=False) <pre>+-------------------------------------------------------+\n|convex_hull                                            |\n+-------------------------------------------------------+\n|POLYGON ((-180 90, 180 90, 180 -90, -180 -90, -180 90))|\n+-------------------------------------------------------+\n\n</pre> <pre>[Stage 15:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+-------------------------------------------------------+\n|min_convex_hull                                        |\n+-------------------------------------------------------+\n|POLYGON ((-180 90, 180 90, 180 -90, -180 -90, -180 90))|\n+-------------------------------------------------------+\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[12]: Copied! <pre>rasterized_geom_df = sedona.sql(\n    \"SELECT RS_AsRaster(ST_GeomFromWKT('POLYGON((150 150, 220 260, 190 300, 300 220, 150 150))'), r.raster, 'b', 230) as rasterized_geom from raster_table r\"\n)\nrasterized_geom_df.show()\n</pre> rasterized_geom_df = sedona.sql(     \"SELECT RS_AsRaster(ST_GeomFromWKT('POLYGON((150 150, 220 260, 190 300, 300 220, 150 150))'), r.raster, 'b', 230) as rasterized_geom from raster_table r\" ) rasterized_geom_df.show() <pre>24/05/22 17:59:36 WARN VectorToRasterProcess: coercing double feature values to float raster values\n</pre> <pre>+--------------------+\n|     rasterized_geom|\n+--------------------+\n|GridCoverage2D[\"g...|\n+--------------------+\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[13]: Copied! <pre>SedonaUtils.display_image(\n    rasterized_geom_df.selectExpr(\"RS_AsImage(rasterized_geom, 250) as rasterized_geom\")\n)\n</pre> SedonaUtils.display_image(     rasterized_geom_df.selectExpr(\"RS_AsImage(rasterized_geom, 250) as rasterized_geom\") ) <pre>24/05/22 17:59:36 WARN VectorToRasterProcess: coercing double feature values to float raster values\n</pre> rasterized_geom 0 In\u00a0[14]: Copied! <pre>raster_white_bg = rasterized_geom_df.selectExpr(\n    \"RS_MapAlgebra(rasterized_geom, NULL, 'out[0] = rast[0] == 0 ? 230 : 0;') as raster\"\n)\n</pre> raster_white_bg = rasterized_geom_df.selectExpr(     \"RS_MapAlgebra(rasterized_geom, NULL, 'out[0] = rast[0] == 0 ? 230 : 0;') as raster\" ) In\u00a0[15]: Copied! <pre>SedonaUtils.display_image(\n    raster_white_bg.selectExpr(\"RS_AsImage(raster, 250) as resampled_raster\")\n)\n</pre> SedonaUtils.display_image(     raster_white_bg.selectExpr(\"RS_AsImage(raster, 250) as resampled_raster\") ) <pre>24/05/22 17:59:37 WARN VectorToRasterProcess: coercing double feature values to float raster values\nANTLR Tool version 4.7.1 used for code generation does not match the current runtime version 4.9.3\nANTLR Runtime version 4.7.1 used for parser compilation does not match the current runtime version 4.9.3\nANTLR Tool version 4.7.1 used for code generation does not match the current runtime version 4.9.3\nANTLR Runtime version 4.7.1 used for parser compilation does not match the current runtime version 4.9.3\n                                                                                \r</pre> resampled_raster 0 In\u00a0[16]: Copied! <pre>resampled_raster_df = sedona.sql(\n    \"SELECT RS_Resample(raster, 1000, 1000, false, 'NearestNeighbor') as resampled_raster from raster_table\"\n)\n</pre> resampled_raster_df = sedona.sql(     \"SELECT RS_Resample(raster, 1000, 1000, false, 'NearestNeighbor') as resampled_raster from raster_table\" ) In\u00a0[17]: Copied! <pre>SedonaUtils.display_image(\n    resampled_raster_df.selectExpr(\n        \"RS_AsImage(resampled_raster, 500) as resampled_raster\"\n    )\n)\n</pre> SedonaUtils.display_image(     resampled_raster_df.selectExpr(         \"RS_AsImage(resampled_raster, 500) as resampled_raster\"     ) ) <pre>                                                                                \r</pre> resampled_raster 0 In\u00a0[18]: Copied! <pre>resampled_raster_df.selectExpr(\n    \"RS_MetaData(resampled_raster) as resampled_raster_metadata\"\n).show(truncate=False)\n</pre> resampled_raster_df.selectExpr(     \"RS_MetaData(resampled_raster) as resampled_raster_metadata\" ).show(truncate=False) <pre>+------------------------------------------------------------------+\n|resampled_raster_metadata                                         |\n+------------------------------------------------------------------+\n|[-180.0, 90.0, 1000.0, 1000.0, 0.36, -0.18, 0.0, 0.0, 4326.0, 1.0]|\n+------------------------------------------------------------------+\n\n</pre> In\u00a0[19]: Copied! <pre># Load another raster for some more examples\nelevation_raster_df = sedona.read.format(\"binaryFile\").load(\"data/raster/test1.tiff\")\nelevation_raster_df.createOrReplaceTempView(\"elevation_raster_binary\")\n</pre> # Load another raster for some more examples elevation_raster_df = sedona.read.format(\"binaryFile\").load(\"data/raster/test1.tiff\") elevation_raster_df.createOrReplaceTempView(\"elevation_raster_binary\") In\u00a0[20]: Copied! <pre>elevation_raster_df = sedona.sql(\n    \"SELECT RS_FromGeoTiff(content) as raster from elevation_raster_binary\"\n)\nelevation_raster_df.createOrReplaceTempView(\"elevation_raster\")\n</pre> elevation_raster_df = sedona.sql(     \"SELECT RS_FromGeoTiff(content) as raster from elevation_raster_binary\" ) elevation_raster_df.createOrReplaceTempView(\"elevation_raster\") In\u00a0[21]: Copied! <pre>point_wkt_1 = \"SRID=3857;POINT (-13095600.809482181 4021100.7487925636)\"\npoint_wkt_2 = \"SRID=3857;POINT (-13095500.809482181 4021000.7487925636)\"\npoint_df = sedona.sql(\n    \"SELECT ST_GeomFromEWKT('{}') as point_1, ST_GeomFromEWKT('{}') as point_2\".format(\n        point_wkt_1, point_wkt_2\n    )\n)\npoint_df.createOrReplaceTempView(\"point_table\")\ntest_df = sedona.sql(\n    \"SELECT RS_Values(raster, Array(point_1, point_2)) as raster_values from elevation_raster, point_table\"\n)\ntest_df.show()\n</pre> point_wkt_1 = \"SRID=3857;POINT (-13095600.809482181 4021100.7487925636)\" point_wkt_2 = \"SRID=3857;POINT (-13095500.809482181 4021000.7487925636)\" point_df = sedona.sql(     \"SELECT ST_GeomFromEWKT('{}') as point_1, ST_GeomFromEWKT('{}') as point_2\".format(         point_wkt_1, point_wkt_2     ) ) point_df.createOrReplaceTempView(\"point_table\") test_df = sedona.sql(     \"SELECT RS_Values(raster, Array(point_1, point_2)) as raster_values from elevation_raster, point_table\" ) test_df.show() <pre>[Stage 22:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------------+\n| raster_values|\n+--------------+\n|[115.0, 148.0]|\n+--------------+\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[22]: Copied! <pre>band = elevation_raster_df.selectExpr(\"RS_BandAsArray(raster, 1)\").first()[0]\nprint(\n    band[500:520],\n)  # Print a part of a band as an array horizontally\n</pre> band = elevation_raster_df.selectExpr(\"RS_BandAsArray(raster, 1)\").first()[0] print(     band[500:520], )  # Print a part of a band as an array horizontally <pre>[123.0, 107.0, 156.0, 173.0, 115.0, 82.0, 165.0, 222.0, 115.0, 82.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</pre> In\u00a0[23]: Copied! <pre># Convert raster to its convex hull and transform it to EPSG:4326 to be able to visualize\nraster_mbr_df = elevation_raster_df.selectExpr(\n    \"ST_Transform(RS_ConvexHull(raster), 'EPSG:3857', 'EPSG:4326') as raster_mbr\"\n)\n</pre> # Convert raster to its convex hull and transform it to EPSG:4326 to be able to visualize raster_mbr_df = elevation_raster_df.selectExpr(     \"ST_Transform(RS_ConvexHull(raster), 'EPSG:3857', 'EPSG:4326') as raster_mbr\" ) In\u00a0[\u00a0]: Copied! <pre>sedona_kepler_map_elevation = SedonaKepler.create_map(\n    df=raster_mbr_df, name=\"RasterMBR\"\n)\nsedona_kepler_map_elevation\n</pre> sedona_kepler_map_elevation = SedonaKepler.create_map(     df=raster_mbr_df, name=\"RasterMBR\" ) sedona_kepler_map_elevation In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/ApacheSedonaRaster/#import-sedona","title":"Import Sedona\u00b6","text":""},{"location":"usecases/ApacheSedonaRaster/#create-a-sedona-context-object","title":"Create a Sedona Context object.\u00b6","text":"<p>If you already have a spark instance available, simply use <code>SedonaContext.create(spark)</code>.</p>"},{"location":"usecases/ApacheSedonaRaster/#read-geotiff-files","title":"Read GeoTiff files\u00b6","text":""},{"location":"usecases/ApacheSedonaRaster/#create-raster-columns-from-the-read-binary-data","title":"Create raster columns from the read binary data\u00b6","text":""},{"location":"usecases/ApacheSedonaRaster/#operate-on-rasters-using-sedona","title":"Operate on rasters using Sedona\u00b6","text":"<p>Once a raster column is created, you're now free to use the entire catalog of Sedona's raster functions. The following part of notebook contains a few examples.</p>"},{"location":"usecases/ApacheSedonaRaster/#access-raster-metadata","title":"Access raster metadata\u00b6","text":"<p>RS_MetaData can be used to view the loaded raster's metadata (orientation and georeferencing attributes).</p>"},{"location":"usecases/ApacheSedonaRaster/#visualize-rasters","title":"Visualize rasters\u00b6","text":"<p>Sedona 1.5.0 provides multiple ways to be able to visualize rasters. Throughout this notebook, RS_AsImage will be used to visualize any changes to the rasters.</p>"},{"location":"usecases/ApacheSedonaRaster/#join-based-on-raster-predicates","title":"Join based on raster predicates\u00b6","text":"<p>Sedona 1.5.0 now supports join predicates between raster and geometry columns.</p> <p>Below is a simple example that carves a small rectangle from the existing raster and attempts to join it with the original raster</p>"},{"location":"usecases/ApacheSedonaRaster/#interoperability-between-raster-and-vector-data-types","title":"Interoperability between raster and vector data types\u00b6","text":"<p>Sedona allows for conversions from raster to geometry and vice-versa.</p>"},{"location":"usecases/ApacheSedonaRaster/#convert-a-raster-to-vector-using-convex-hull","title":"Convert a raster to vector using convex hull\u00b6","text":"<p>A convex hull geometry can be created out of a raster using RS_ConvexHull</p> <p>Additionally, if a raster has noDataValue specified, and you wish to tighten the convexhull to exclude noDataValue boundaries, RS_MinConvexHull can be used.</p>"},{"location":"usecases/ApacheSedonaRaster/#convert-a-geometry-to-raster-rasterize-a-geometry","title":"Convert a geometry to raster (Rasterize a geometry)\u00b6","text":"<p>A geometry can be converted to a raster using RS_AsRaster</p>"},{"location":"usecases/ApacheSedonaRaster/#perform-map-algebra-operations","title":"Perform Map Algebra operations\u00b6","text":"<p>Sedona provides two ways to perform Map Algebra on rasters:</p> <ol> <li>Using RS_MapAlgebra (preferred for simpler algebraic functions)</li> <li>Using RS_BandAsArray and array based map algebra functions such as RS_Add, RS_Multiply (Useful for complex algebriac functions involving mutating each grid value differently.)</li> </ol> <p>The following example illustrates how RS_MapAlgebra can be used. This example uses jiffle script to invert the colors of the above illustrated rasterized geometry.</p>"},{"location":"usecases/ApacheSedonaRaster/#resample-a-raster","title":"Resample a raster.\u00b6","text":"<p>Sedona 1.5.0 supports resampling a raster to different height/width or scale. It also supports changing the pivot of the raster.</p> <p>Refer to RS_Resample documentation for more details.</p> <p>This simple example changes the resolution of the loaded raster to 1000*1000</p>"},{"location":"usecases/ApacheSedonaRaster/#access-individual-values-from-rasters","title":"Access individual values from rasters\u00b6","text":"<p>Sedona provides RS_Value and RS_Values that allow accessing raster values at given geometrical point(s).</p> <p>The following example extracts raster values at specific geographical points.</p>"},{"location":"usecases/ApacheSedonaRaster/#extract-individual-bands-from-rasters","title":"Extract individual bands from rasters\u00b6","text":"<p>RS_BandAsArray can be used to extract entire band values from a given raster</p>"},{"location":"usecases/ApacheSedonaRaster/#visualize-raster-mbrs","title":"Visualize Raster MBRs\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/","title":"ApacheSedonaSQL","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>import os\n\nimport geopandas as gpd\nfrom pyspark.sql import SparkSession\n\nfrom sedona.spark import *\n</pre> import os  import geopandas as gpd from pyspark.sql import SparkSession  from sedona.spark import * In\u00a0[2]: Copied! <pre>config = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"\n        \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"\n        \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n</pre> config = (     SedonaContext.builder()     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"         \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"         \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",     )     .getOrCreate() )  sedona = SedonaContext.create(config) <pre>:: loading settings :: url = jar:file:/home/jovyan/spark-3.4.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jovyan/.ivy2/cache\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\norg.apache.sedona#sedona-spark-3.4_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\nuk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-713da225-9e04-45e3-9233-485d4b212b3c;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-spark-3.4_2.12;1.6.0 in central\n\tfound org.apache.sedona#sedona-common;1.6.0 in central\n\tfound org.apache.commons#commons-math3;3.6.1 in central\n\tfound org.locationtech.jts#jts-core;1.19.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n\tfound com.google.geometry#s2-geometry;2.0.0 in central\n\tfound com.google.guava#guava;25.1-jre in central\n\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n\tfound org.checkerframework#checker-qual;2.0.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n\tfound com.uber#h3;4.1.1 in central\n\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n\tfound org.checkerframework#checker-qual;3.10.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n\tfound org.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 in central\n\tfound commons-lang#commons-lang;2.6 in central\n\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n\tfound org.beryx#awt-color-factory;1.0.0 in central\n\tfound org.datasyslab#geotools-wrapper;1.6.0-28.2 in central\n\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n:: resolution report :: resolve 2280ms :: artifacts dl 96ms\n\t:: modules in use:\n\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n\tcom.google.guava#guava;25.1-jre from central in [default]\n\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n\tcom.uber#h3;4.1.1 from central in [default]\n\tcommons-lang#commons-lang;2.6 from central in [default]\n\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n\torg.apache.sedona#sedona-common;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-3.4_2.12;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 from central in [default]\n\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n\torg.datasyslab#geotools-wrapper;1.6.0-28.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n\t:: evicted modules:\n\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   25  |   0   |   0   |   2   ||   23  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-713da225-9e04-45e3-9233-485d4b212b3c\n\tconfs: [default]\n\t0 artifacts copied, 23 already retrieved (0kB/88ms)\n24/05/22 18:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[3]: Copied! <pre>point_csv_df = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \",\")\n    .option(\"header\", \"false\")\n    .load(\"data/testpoint.csv\")\n)\n\npoint_csv_df.createOrReplaceTempView(\"pointtable\")\n\npoint_df = sedona.sql(\n    \"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\"\n)\npoint_df.show(5)\n</pre> point_csv_df = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \",\")     .option(\"header\", \"false\")     .load(\"data/testpoint.csv\") )  point_csv_df.createOrReplaceTempView(\"pointtable\")  point_df = sedona.sql(     \"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\" ) point_df.show(5) <pre>[Stage 4:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+-----------------+\n|     arealandmark|\n+-----------------+\n|POINT (1.1 101.1)|\n|POINT (2.1 102.1)|\n|POINT (3.1 103.1)|\n|POINT (4.1 104.1)|\n|POINT (5.1 105.1)|\n+-----------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[4]: Copied! <pre>polygon_wkt_df = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \"\\t\")\n    .option(\"header\", \"false\")\n    .load(\"data/county_small.tsv\")\n)\n\npolygon_wkt_df.createOrReplaceTempView(\"polygontable\")\npolygon_df = sedona.sql(\n    \"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\"\n)\npolygon_df.show(5)\n</pre> polygon_wkt_df = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \"\\t\")     .option(\"header\", \"false\")     .load(\"data/county_small.tsv\") )  polygon_wkt_df.createOrReplaceTempView(\"polygontable\") polygon_df = sedona.sql(     \"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\" ) polygon_df.show(5) <pre>+----------------+--------------------+\n|            name|         countyshape|\n+----------------+--------------------+\n|   Cuming County|POLYGON ((-97.019...|\n|Wahkiakum County|POLYGON ((-123.43...|\n|  De Baca County|POLYGON ((-104.56...|\n|Lancaster County|POLYGON ((-96.910...|\n| Nuckolls County|POLYGON ((-98.273...|\n+----------------+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[5]: Copied! <pre>polygon_wkb_df = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \"\\t\")\n    .option(\"header\", \"false\")\n    .load(\"data/county_small_wkb.tsv\")\n)\n\npolygon_wkb_df.createOrReplaceTempView(\"polygontable\")\npolygon_df = sedona.sql(\n    \"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\"\n)\npolygon_df.show(5)\n</pre> polygon_wkb_df = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \"\\t\")     .option(\"header\", \"false\")     .load(\"data/county_small_wkb.tsv\") )  polygon_wkb_df.createOrReplaceTempView(\"polygontable\") polygon_df = sedona.sql(     \"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\" ) polygon_df.show(5) <pre>+----------------+--------------------+\n|            name|         countyshape|\n+----------------+--------------------+\n|   Cuming County|POLYGON ((-97.019...|\n|Wahkiakum County|POLYGON ((-123.43...|\n|  De Baca County|POLYGON ((-104.56...|\n|Lancaster County|POLYGON ((-96.910...|\n| Nuckolls County|POLYGON ((-98.273...|\n+----------------+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[6]: Copied! <pre>polygon_json_df = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \"\\t\")\n    .option(\"header\", \"false\")\n    .load(\"data/testPolygon.json\")\n)\n\npolygon_json_df.createOrReplaceTempView(\"polygontable\")\npolygon_df = sedona.sql(\n    \"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\"\n)\npolygon_df.show(5)\n</pre> polygon_json_df = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \"\\t\")     .option(\"header\", \"false\")     .load(\"data/testPolygon.json\") )  polygon_json_df.createOrReplaceTempView(\"polygontable\") polygon_df = sedona.sql(     \"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\" ) polygon_df.show(5) <pre>+--------------------+\n|         countyshape|\n+--------------------+\n|POLYGON ((-87.621...|\n|POLYGON ((-85.719...|\n|POLYGON ((-86.000...|\n|POLYGON ((-86.574...|\n|POLYGON ((-85.382...|\n+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[7]: Copied! <pre>point_csv_df_1 = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \",\")\n    .option(\"header\", \"false\")\n    .load(\"data/testpoint.csv\")\n)\n\npoint_csv_df_1.createOrReplaceTempView(\"pointtable\")\n\npoint_df1 = sedona.sql(\n    \"SELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1, 'abc' as name1 from pointtable\"\n)\npoint_df1.createOrReplaceTempView(\"pointdf1\")\n\npoint_csv_df2 = (\n    sedona.read.format(\"csv\")\n    .option(\"delimiter\", \",\")\n    .option(\"header\", \"false\")\n    .load(\"data/testpoint.csv\")\n)\n\npoint_csv_df2.createOrReplaceTempView(\"pointtable\")\npoint_df2 = sedona.sql(\n    \"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2, 'def' as name2 from pointtable\"\n)\npoint_df2.createOrReplaceTempView(\"pointdf2\")\n\ndistance_join_df = sedona.sql(\n    \"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\"\n)\ndistance_join_df.explain()\ndistance_join_df.show(5)\n</pre> point_csv_df_1 = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \",\")     .option(\"header\", \"false\")     .load(\"data/testpoint.csv\") )  point_csv_df_1.createOrReplaceTempView(\"pointtable\")  point_df1 = sedona.sql(     \"SELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1, 'abc' as name1 from pointtable\" ) point_df1.createOrReplaceTempView(\"pointdf1\")  point_csv_df2 = (     sedona.read.format(\"csv\")     .option(\"delimiter\", \",\")     .option(\"header\", \"false\")     .load(\"data/testpoint.csv\") )  point_csv_df2.createOrReplaceTempView(\"pointtable\") point_df2 = sedona.sql(     \"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2, 'def' as name2 from pointtable\" ) point_df2.createOrReplaceTempView(\"pointdf2\")  distance_join_df = sedona.sql(     \"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) &lt; 2\" ) distance_join_df.explain() distance_join_df.show(5) <pre>== Physical Plan ==\nBroadcastIndexJoin pointshape2#257: geometry, LeftSide, LeftSide, Inner, INTERSECTS, ( **org.apache.spark.sql.sedona_sql.expressions.ST_Distance**   &lt; 2.0) ST_INTERSECTS(pointshape1#232, pointshape2#257)\n:- SpatialIndex pointshape1#232: geometry, RTREE, false, false, 2.0\n:  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pointshape1#232, abc AS name1#233]\n:     +- FileScan csv [_c0#228,_c1#229] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/docs/usecases/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;_c0:string,_c1:string&gt;\n+- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pointshape2#257, def AS name2#258]\n   +- FileScan csv [_c0#253,_c1#254] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/docs/usecases/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;_c0:string,_c1:string&gt;\n\n\n</pre> <pre>                                                                                \r</pre> <pre>+-----------------+-----+-----------------+-----+\n|      pointshape1|name1|      pointshape2|name2|\n+-----------------+-----+-----------------+-----+\n|POINT (1.1 101.1)|  abc|POINT (1.1 101.1)|  def|\n|POINT (2.1 102.1)|  abc|POINT (1.1 101.1)|  def|\n|POINT (1.1 101.1)|  abc|POINT (2.1 102.1)|  def|\n|POINT (2.1 102.1)|  abc|POINT (2.1 102.1)|  def|\n|POINT (3.1 103.1)|  abc|POINT (2.1 102.1)|  def|\n+-----------------+-----+-----------------+-----+\nonly showing top 5 rows\n\n</pre> In\u00a0[8]: Copied! <pre>import pandas as pd\n\ngdf = gpd.read_file(\"data/gis_osm_pois_free_1.shp\")\ngdf = gdf.replace(pd.NA, \"\")\nosm_points = sedona.createDataFrame(gdf)\n</pre> import pandas as pd  gdf = gpd.read_file(\"data/gis_osm_pois_free_1.shp\") gdf = gdf.replace(pd.NA, \"\") osm_points = sedona.createDataFrame(gdf) In\u00a0[9]: Copied! <pre>osm_points.printSchema()\n</pre> osm_points.printSchema() <pre>root\n |-- osm_id: string (nullable = true)\n |-- code: long (nullable = true)\n |-- fclass: string (nullable = true)\n |-- name: string (nullable = true)\n |-- geometry: geometry (nullable = true)\n\n</pre> In\u00a0[10]: Copied! <pre>osm_points.show(5)\n</pre> osm_points.show(5) <pre>[Stage 15:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------+----+---------+--------------+--------------------+\n|  osm_id|code|   fclass|          name|            geometry|\n+--------+----+---------+--------------+--------------------+\n|26860257|2422|camp_site|      de Kroon|POINT (15.3393145...|\n|26860294|2406|   chalet|Le\u015bne Ustronie|POINT (14.8709625...|\n|29947493|2402|    motel|              |POINT (15.0946636...|\n|29947498|2602|      atm|              |POINT (15.0732014...|\n|29947499|2401|    hotel|              |POINT (15.0696777...|\n+--------+----+---------+--------------+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[11]: Copied! <pre>osm_points.createOrReplaceTempView(\"points\")\n</pre> osm_points.createOrReplaceTempView(\"points\") In\u00a0[12]: Copied! <pre>transformed_df = sedona.sql(\n    \"\"\"\n        SELECT osm_id,\n               code,\n               fclass,\n               name,\n               ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom \n        FROM points\n    \"\"\"\n)\n</pre> transformed_df = sedona.sql(     \"\"\"         SELECT osm_id,                code,                fclass,                name,                ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom          FROM points     \"\"\" ) In\u00a0[13]: Copied! <pre>transformed_df.show(5)\n</pre> transformed_df.show(5) <pre>[Stage 16:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------+----+---------+--------------+--------------------+\n|  osm_id|code|   fclass|          name|                geom|\n+--------+----+---------+--------------+--------------------+\n|26860257|2422|camp_site|      de Kroon|POINT (250776.778...|\n|26860294|2406|   chalet|Le\u015bne Ustronie|POINT (221076.709...|\n|29947493|2402|    motel|              |POINT (233902.541...|\n|29947498|2602|      atm|              |POINT (232447.203...|\n|29947499|2401|    hotel|              |POINT (232208.377...|\n+--------+----+---------+--------------+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[14]: Copied! <pre>transformed_df.createOrReplaceTempView(\"points_2180\")\n</pre> transformed_df.createOrReplaceTempView(\"points_2180\") In\u00a0[15]: Copied! <pre>neighbours_within_1000m = sedona.sql(\n    \"\"\"\n        SELECT a.osm_id AS id_1,\n               b.osm_id AS id_2,\n               a.geom \n        FROM points_2180 AS a, points_2180 AS b \n        WHERE ST_Distance(a.geom,b.geom) &lt; 50\n    \"\"\"\n)\n</pre> neighbours_within_1000m = sedona.sql(     \"\"\"         SELECT a.osm_id AS id_1,                b.osm_id AS id_2,                a.geom          FROM points_2180 AS a, points_2180 AS b          WHERE ST_Distance(a.geom,b.geom) &lt; 50     \"\"\" ) In\u00a0[16]: Copied! <pre>neighbours_within_1000m.show()\n</pre> neighbours_within_1000m.show() <pre>24/05/22 18:07:13 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n[Stage 20:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+----------+---------+--------------------+\n|      id_1|     id_2|                geom|\n+----------+---------+--------------------+\n|  26860257| 26860257|POINT (250776.778...|\n|  26860294| 26860294|POINT (221076.709...|\n|  29947493| 29947493|POINT (233902.541...|\n|3241834852| 29947493|POINT (233866.098...|\n|5964811085| 29947493|POINT (233861.172...|\n|5818905324| 29947498|POINT (232446.535...|\n|4165181885| 29947498|POINT (232449.441...|\n|5846858758| 29947498|POINT (232407.167...|\n|  29947498| 29947498|POINT (232447.203...|\n|  29947499| 29947499|POINT (232208.377...|\n|  30077461| 29947499|POINT (232185.872...|\n|  29947505| 29947505|POINT (228595.321...|\n|  29947499| 30077461|POINT (232208.377...|\n|  30077461| 30077461|POINT (232185.872...|\n|  30079117| 30079117|POINT (273599.241...|\n| 197624402|197624402|POINT (203703.035...|\n| 197663196|197663196|POINT (203936.327...|\n| 197953474|197953474|POINT (203724.746...|\n|1074233127|262310516|POINT (203524.110...|\n| 262310516|262310516|POINT (203507.730...|\n+----------+---------+--------------------+\nonly showing top 20 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[17]: Copied! <pre>df = neighbours_within_1000m.toPandas()\n</pre> df = neighbours_within_1000m.toPandas() <pre>24/05/22 18:07:17 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n                                                                                \r</pre> In\u00a0[18]: Copied! <pre>gdf = gpd.GeoDataFrame(df, geometry=\"geom\")\n</pre> gdf = gpd.GeoDataFrame(df, geometry=\"geom\") In\u00a0[19]: Copied! <pre>gdf\n</pre> gdf Out[19]: id_1 id_2 geom 0 26860257 26860257 POINT (250776.778 504581.332) 1 26860294 26860294 POINT (221076.710 544222.650) 2 29947493 29947493 POINT (233902.541 501298.382) 3 3241834852 29947493 POINT (233866.099 501323.801) 4 5964811085 29947493 POINT (233861.173 501326.441) ... ... ... ... 65670 6823696220 6823696220 POINT (234310.474 465790.364) 65671 6823721834 6823721834 POINT (234313.567 465869.023) 65672 6823721838 6823721838 POINT (234425.224 465829.994) 65673 6823772928 6823772928 POINT (234582.864 465875.142) 65674 6823823445 6823823445 POINT (236859.419 428547.388) <p>65675 rows \u00d7 3 columns</p>"},{"location":"usecases/ApacheSedonaSQL/#geometry-constructors","title":"Geometry Constructors\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#st_point","title":"ST_Point\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#st_geomfromtext","title":"ST_GeomFromText\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#st_geomfromwkb","title":"ST_GeomFromWKB\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#st_geomfromgeojson","title":"ST_GeomFromGeoJSON\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#spatial-operations","title":"Spatial Operations\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#spatial-join-distance-join","title":"Spatial Join - Distance Join\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#spatial-join-range-join-and-rdd-api-join","title":"Spatial Join - Range Join and RDD API Join\u00b6","text":"<p>Please refer to the example - airports per country: https://github.com/apache/sedona/blob/master/docs/usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry.ipynb</p>"},{"location":"usecases/ApacheSedonaSQL/#converting-geopandas-to-apache-sedona","title":"Converting GeoPandas to Apache Sedona\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL/#converting-apache-sedona-to-geopandas","title":"Converting Apache Sedona to GeoPandas\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/","title":"Spatially aggregate airports per country","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>import os\n\nimport geopandas as gpd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, expr, when, explode, hex\n\n\nfrom sedona.spark import *\nfrom utilities import getConfig\n</pre> import os  import geopandas as gpd from pyspark.sql import SparkSession from pyspark.sql.functions import col, expr, when, explode, hex   from sedona.spark import * from utilities import getConfig In\u00a0[2]: Copied! <pre>config = (\n    SedonaContext.builder()\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-shaded-3.4_2.12:1.6.0,\"\n        \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"\n        \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\nsc = sedona.sparkContext\nsc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n</pre> config = (     SedonaContext.builder()     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-spark-shaded-3.4_2.12:1.6.0,\"         \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"         \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",     )     .getOrCreate() )  sedona = SedonaContext.create(config) sc = sedona.sparkContext sc.setSystemProperty(\"sedona.global.charset\", \"utf8\") <pre>:: loading settings :: url = jar:file:/home/jovyan/spark-3.4.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jovyan/.ivy2/cache\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\norg.apache.sedona#sedona-spark-shaded-3.4_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\nuk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-42c276ec-386f-421b-9fd0-00abbab81649;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-spark-shaded-3.4_2.12;1.6.0 in central\n\tfound org.datasyslab#geotools-wrapper;1.6.0-28.2 in central\n\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n\tfound com.github.scopt#scopt_2.12;4.1.0 in central\ndownloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-shaded-3.4_2.12/1.6.0/sedona-spark-shaded-3.4_2.12-1.6.0.jar ...\n\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-shaded-3.4_2.12;1.6.0!sedona-spark-shaded-3.4_2.12.jar (3668ms)\n:: resolution report :: resolve 1816ms :: artifacts dl 3681ms\n\t:: modules in use:\n\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n\torg.apache.sedona#sedona-spark-shaded-3.4_2.12;1.6.0 from central in [default]\n\torg.datasyslab#geotools-wrapper;1.6.0-28.2 from central in [default]\n\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   4   |   1   |   1   |   0   ||   4   |   1   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-42c276ec-386f-421b-9fd0-00abbab81649\n\tconfs: [default]\n\t1 artifacts copied, 3 already retrieved (21486kB/106ms)\n24/05/22 18:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[3]: Copied! <pre>countries = ShapefileReader.readToGeometryRDD(\n    sc, \"data/ne_50m_admin_0_countries_lakes/\"\n)\ncountries_df = Adapter.toDf(countries, sedona)\ncountries_df.createOrReplaceTempView(\"country\")\ncountries_df.printSchema()\n</pre> countries = ShapefileReader.readToGeometryRDD(     sc, \"data/ne_50m_admin_0_countries_lakes/\" ) countries_df = Adapter.toDf(countries, sedona) countries_df.createOrReplaceTempView(\"country\") countries_df.printSchema() <pre>root\n |-- geometry: geometry (nullable = true)\n |-- featurecla: string (nullable = true)\n |-- scalerank: string (nullable = true)\n |-- LABELRANK: string (nullable = true)\n |-- SOVEREIGNT: string (nullable = true)\n |-- SOV_A3: string (nullable = true)\n |-- ADM0_DIF: string (nullable = true)\n |-- LEVEL: string (nullable = true)\n |-- TYPE: string (nullable = true)\n |-- ADMIN: string (nullable = true)\n |-- ADM0_A3: string (nullable = true)\n |-- GEOU_DIF: string (nullable = true)\n |-- GEOUNIT: string (nullable = true)\n |-- GU_A3: string (nullable = true)\n |-- SU_DIF: string (nullable = true)\n |-- SUBUNIT: string (nullable = true)\n |-- SU_A3: string (nullable = true)\n |-- BRK_DIFF: string (nullable = true)\n |-- NAME: string (nullable = true)\n |-- NAME_LONG: string (nullable = true)\n |-- BRK_A3: string (nullable = true)\n |-- BRK_NAME: string (nullable = true)\n |-- BRK_GROUP: string (nullable = true)\n |-- ABBREV: string (nullable = true)\n |-- POSTAL: string (nullable = true)\n |-- FORMAL_EN: string (nullable = true)\n |-- FORMAL_FR: string (nullable = true)\n |-- NAME_CIAWF: string (nullable = true)\n |-- NOTE_ADM0: string (nullable = true)\n |-- NOTE_BRK: string (nullable = true)\n |-- NAME_SORT: string (nullable = true)\n |-- NAME_ALT: string (nullable = true)\n |-- MAPCOLOR7: string (nullable = true)\n |-- MAPCOLOR8: string (nullable = true)\n |-- MAPCOLOR9: string (nullable = true)\n |-- MAPCOLOR13: string (nullable = true)\n |-- POP_EST: string (nullable = true)\n |-- POP_RANK: string (nullable = true)\n |-- GDP_MD_EST: string (nullable = true)\n |-- POP_YEAR: string (nullable = true)\n |-- LASTCENSUS: string (nullable = true)\n |-- GDP_YEAR: string (nullable = true)\n |-- ECONOMY: string (nullable = true)\n |-- INCOME_GRP: string (nullable = true)\n |-- WIKIPEDIA: string (nullable = true)\n |-- FIPS_10_: string (nullable = true)\n |-- ISO_A2: string (nullable = true)\n |-- ISO_A3: string (nullable = true)\n |-- ISO_A3_EH: string (nullable = true)\n |-- ISO_N3: string (nullable = true)\n |-- UN_A3: string (nullable = true)\n |-- WB_A2: string (nullable = true)\n |-- WB_A3: string (nullable = true)\n |-- WOE_ID: string (nullable = true)\n |-- WOE_ID_EH: string (nullable = true)\n |-- WOE_NOTE: string (nullable = true)\n |-- ADM0_A3_IS: string (nullable = true)\n |-- ADM0_A3_US: string (nullable = true)\n |-- ADM0_A3_UN: string (nullable = true)\n |-- ADM0_A3_WB: string (nullable = true)\n |-- CONTINENT: string (nullable = true)\n |-- REGION_UN: string (nullable = true)\n |-- SUBREGION: string (nullable = true)\n |-- REGION_WB: string (nullable = true)\n |-- NAME_LEN: string (nullable = true)\n |-- LONG_LEN: string (nullable = true)\n |-- ABBREV_LEN: string (nullable = true)\n |-- TINY: string (nullable = true)\n |-- HOMEPART: string (nullable = true)\n |-- MIN_ZOOM: string (nullable = true)\n |-- MIN_LABEL: string (nullable = true)\n |-- MAX_LABEL: string (nullable = true)\n |-- NE_ID: string (nullable = true)\n |-- WIKIDATAID: string (nullable = true)\n |-- NAME_AR: string (nullable = true)\n |-- NAME_BN: string (nullable = true)\n |-- NAME_DE: string (nullable = true)\n |-- NAME_EN: string (nullable = true)\n |-- NAME_ES: string (nullable = true)\n |-- NAME_FR: string (nullable = true)\n |-- NAME_EL: string (nullable = true)\n |-- NAME_HI: string (nullable = true)\n |-- NAME_HU: string (nullable = true)\n |-- NAME_ID: string (nullable = true)\n |-- NAME_IT: string (nullable = true)\n |-- NAME_JA: string (nullable = true)\n |-- NAME_KO: string (nullable = true)\n |-- NAME_NL: string (nullable = true)\n |-- NAME_PL: string (nullable = true)\n |-- NAME_PT: string (nullable = true)\n |-- NAME_RU: string (nullable = true)\n |-- NAME_SV: string (nullable = true)\n |-- NAME_TR: string (nullable = true)\n |-- NAME_VI: string (nullable = true)\n |-- NAME_ZH: string (nullable = true)\n\n</pre> <pre>24/05/22 18:02:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n</pre> In\u00a0[4]: Copied! <pre>airports = ShapefileReader.readToGeometryRDD(sc, \"data/ne_50m_airports/\")\nairports_df = Adapter.toDf(airports, sedona)\nairports_df.createOrReplaceTempView(\"airport\")\nairports_df.printSchema()\n</pre> airports = ShapefileReader.readToGeometryRDD(sc, \"data/ne_50m_airports/\") airports_df = Adapter.toDf(airports, sedona) airports_df.createOrReplaceTempView(\"airport\") airports_df.printSchema() <pre>root\n |-- geometry: geometry (nullable = true)\n |-- scalerank: string (nullable = true)\n |-- featurecla: string (nullable = true)\n |-- type: string (nullable = true)\n |-- name: string (nullable = true)\n |-- abbrev: string (nullable = true)\n |-- location: string (nullable = true)\n |-- gps_code: string (nullable = true)\n |-- iata_code: string (nullable = true)\n |-- wikipedia: string (nullable = true)\n |-- natlscale: string (nullable = true)\n\n</pre> In\u00a0[5]: Copied! <pre>result = sedona.sql(\n    \"SELECT c.geometry as country_geom, c.NAME_EN, a.geometry as airport_geom, a.name FROM country c, airport a WHERE ST_Contains(c.geometry, a.geometry)\"\n)\n</pre> result = sedona.sql(     \"SELECT c.geometry as country_geom, c.NAME_EN, a.geometry as airport_geom, a.name FROM country c, airport a WHERE ST_Contains(c.geometry, a.geometry)\" ) In\u00a0[6]: Copied! <pre>airports_rdd = Adapter.toSpatialRdd(airports_df, \"geometry\")\n# Drop the duplicate name column in countries_df\ncountries_df = countries_df.drop(\"NAME\")\ncountries_rdd = Adapter.toSpatialRdd(countries_df, \"geometry\")\n\nairports_rdd.analyze()\ncountries_rdd.analyze()\n\n# 4 is the num partitions used in spatial partitioning. This is an optional parameter\nairports_rdd.spatialPartitioning(GridType.KDBTREE, 4)\ncountries_rdd.spatialPartitioning(airports_rdd.getPartitioner())\n\nbuildOnSpatialPartitionedRDD = True\nusingIndex = True\nconsiderBoundaryIntersection = True\nairports_rdd.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n\nresult_pair_rdd = JoinQueryRaw.SpatialJoinQueryFlat(\n    airports_rdd, countries_rdd, usingIndex, considerBoundaryIntersection\n)\n\nresult2 = Adapter.toDf(\n    result_pair_rdd, countries_rdd.fieldNames, airports.fieldNames, sedona\n)\n\nresult2.createOrReplaceTempView(\"join_result_with_all_cols\")\n# Select the columns needed in the join\nresult2 = sedona.sql(\n    \"SELECT leftgeometry as country_geom, NAME_EN, rightgeometry as airport_geom, name FROM join_result_with_all_cols\"\n)\n</pre> airports_rdd = Adapter.toSpatialRdd(airports_df, \"geometry\") # Drop the duplicate name column in countries_df countries_df = countries_df.drop(\"NAME\") countries_rdd = Adapter.toSpatialRdd(countries_df, \"geometry\")  airports_rdd.analyze() countries_rdd.analyze()  # 4 is the num partitions used in spatial partitioning. This is an optional parameter airports_rdd.spatialPartitioning(GridType.KDBTREE, 4) countries_rdd.spatialPartitioning(airports_rdd.getPartitioner())  buildOnSpatialPartitionedRDD = True usingIndex = True considerBoundaryIntersection = True airports_rdd.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)  result_pair_rdd = JoinQueryRaw.SpatialJoinQueryFlat(     airports_rdd, countries_rdd, usingIndex, considerBoundaryIntersection )  result2 = Adapter.toDf(     result_pair_rdd, countries_rdd.fieldNames, airports.fieldNames, sedona )  result2.createOrReplaceTempView(\"join_result_with_all_cols\") # Select the columns needed in the join result2 = sedona.sql(     \"SELECT leftgeometry as country_geom, NAME_EN, rightgeometry as airport_geom, name FROM join_result_with_all_cols\" ) <pre>                                                                                \r</pre> In\u00a0[7]: Copied! <pre># The result of SQL API\nresult.show()\n# The result of RDD API\nresult2.show()\n</pre> # The result of SQL API result.show() # The result of RDD API result2.show() <pre>24/05/22 18:03:06 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n                                                                                \r</pre> <pre>+--------------------+--------------------+--------------------+--------------------+\n|        country_geom|             NAME_EN|        airport_geom|                name|\n+--------------------+--------------------+--------------------+--------------------+\n|MULTIPOLYGON (((1...|Taiwan           ...|POINT (121.231370...|Taoyuan          ...|\n|MULTIPOLYGON (((5...|Netherlands      ...|POINT (4.76437693...|Schiphol         ...|\n|POLYGON ((103.969...|Singapore        ...|POINT (103.986413...|Singapore Changi ...|\n|MULTIPOLYGON (((-...|United Kingdom   ...|POINT (-0.4531566...|London Heathrow  ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-149.98172...|Anchorage Int'l  ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-84.425397...|Hartsfield-Jackso...|\n|MULTIPOLYGON (((1...|People's Republic...|POINT (116.588174...|Beijing Capital  ...|\n|MULTIPOLYGON (((-...|Colombia         ...|POINT (-74.143371...|Eldorado Int'l   ...|\n|MULTIPOLYGON (((6...|India            ...|POINT (72.8745639...|Chhatrapati Shiva...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-71.016406...|Gen E L Logan Int...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-76.668642...|Baltimore-Washing...|\n|POLYGON ((36.8713...|Egypt            ...|POINT (31.3997430...|Cairo Int'l      ...|\n|POLYGON ((-2.2196...|Morocco          ...|POINT (-7.6632188...|Casablanca-Anfa  ...|\n|MULTIPOLYGON (((-...|Venezuela        ...|POINT (-67.005748...|Simon Bolivar Int...|\n|MULTIPOLYGON (((2...|South Africa     ...|POINT (18.5976565...|Cape Town Int'l  ...|\n|MULTIPOLYGON (((1...|People's Republic...|POINT (103.956136...|Chengdushuang Liu...|\n|MULTIPOLYGON (((6...|India            ...|POINT (77.0878362...|Indira Gandhi Int...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-104.67379...|Denver Int'l     ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-97.040371...|Dallas-Ft. Worth ...|\n|MULTIPOLYGON (((1...|Thailand         ...|POINT (100.602578...|Don Muang Int'l  ...|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</pre> <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+--------------------+--------------------+\n|        country_geom|             NAME_EN|        airport_geom|                name|\n+--------------------+--------------------+--------------------+--------------------+\n|MULTIPOLYGON (((-...|United States of ...|POINT (-80.145258...|Fort Lauderdale H...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-80.278971...|Miami Int'l      ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-95.333704...|George Bush Inter...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-90.256693...|New Orleans Int'l...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-81.307371...|Orlando Int'l    ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-82.534824...|Tampa Int'l      ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-112.01363...|Sky Harbor Int'l ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-118.40246...|Los Angeles Int'l...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-116.97547...|General Abelardo ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-97.040371...|Dallas-Ft. Worth ...|\n|MULTIPOLYGON (((-...|United States of ...|POINT (-84.425397...|Hartsfield-Jackso...|\n|POLYGON ((-69.965...|Peru             ...|POINT (-77.107565...|Jorge Chavez     ...|\n|MULTIPOLYGON (((-...|Panama           ...|POINT (-79.387134...|Tocumen Int'l    ...|\n|POLYGON ((-83.157...|Nicaragua        ...|POINT (-86.171284...|Augusto Cesar San...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-96.183570...|Gen. Heriberto Ja...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-106.27001...|General Rafael Bu...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-99.754508...|General Juan N Al...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-99.570649...|Jose Maria Morelo...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-98.375759...|Puebla           ...|\n|MULTIPOLYGON (((-...|Mexico           ...|POINT (-99.082607...|Lic Benito Juarez...|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</pre> In\u00a0[8]: Copied! <pre># result.createOrReplaceTempView(\"result\")\nresult2.createOrReplaceTempView(\"result\")\ngroupedresult = sedona.sql(\n    \"SELECT c.NAME_EN, c.country_geom, count(*) as AirportCount FROM result c GROUP BY c.NAME_EN, c.country_geom\"\n)\ngroupedresult.show()\ngroupedresult.createOrReplaceTempView(\"grouped_result\")\n</pre> # result.createOrReplaceTempView(\"result\") result2.createOrReplaceTempView(\"result\") groupedresult = sedona.sql(     \"SELECT c.NAME_EN, c.country_geom, count(*) as AirportCount FROM result c GROUP BY c.NAME_EN, c.country_geom\" ) groupedresult.show() groupedresult.createOrReplaceTempView(\"grouped_result\") <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+------------+\n|             NAME_EN|        country_geom|AirportCount|\n+--------------------+--------------------+------------+\n|Cuba             ...|MULTIPOLYGON (((-...|           1|\n|Mexico           ...|MULTIPOLYGON (((-...|          12|\n|Panama           ...|MULTIPOLYGON (((-...|           1|\n|Nicaragua        ...|POLYGON ((-83.157...|           1|\n|Honduras         ...|MULTIPOLYGON (((-...|           1|\n|Colombia         ...|MULTIPOLYGON (((-...|           4|\n|United States of ...|MULTIPOLYGON (((-...|          35|\n|Ecuador          ...|MULTIPOLYGON (((-...|           1|\n|The Bahamas      ...|MULTIPOLYGON (((-...|           1|\n|Peru             ...|POLYGON ((-69.965...|           1|\n|Guatemala        ...|POLYGON ((-92.235...|           1|\n|Canada           ...|MULTIPOLYGON (((-...|          15|\n|Venezuela        ...|MULTIPOLYGON (((-...|           3|\n|Argentina        ...|MULTIPOLYGON (((-...|           3|\n|Bolivia          ...|MULTIPOLYGON (((-...|           2|\n|Paraguay         ...|POLYGON ((-58.159...|           1|\n|Benin            ...|POLYGON ((1.62265...|           1|\n|Guinea           ...|POLYGON ((-10.283...|           1|\n|Chile            ...|MULTIPOLYGON (((-...|           5|\n|Nigeria          ...|MULTIPOLYGON (((7...|           3|\n+--------------------+--------------------+------------+\nonly showing top 20 rows\n\n</pre> In\u00a0[9]: Copied! <pre>sedona_kepler_map = SedonaKepler.create_map(\n    df=groupedresult, name=\"AirportCount\", config=getConfig()\n)\nsedona_kepler_map\n</pre> sedona_kepler_map = SedonaKepler.create_map(     df=groupedresult, name=\"AirportCount\", config=getConfig() ) sedona_kepler_map <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>                                                                                \r</pre> <pre>KeplerGl(config={'version': 'v1', 'config': {'visState': {'filters': [], 'layers': [{'id': 'ikzru0t', 'type': \u2026</pre> In\u00a0[10]: Copied! <pre>sedona_pydeck_map = SedonaPyDeck.create_choropleth_map(\n    df=groupedresult, plot_col=\"AirportCount\"\n)\nsedona_pydeck_map\n</pre> sedona_pydeck_map = SedonaPyDeck.create_choropleth_map(     df=groupedresult, plot_col=\"AirportCount\" ) sedona_pydeck_map Out[10]: In\u00a0[11]: Copied! <pre>h3_df = sedona.sql(\n    \"SELECT g.NAME_EN, g.country_geom, ST_H3CellIDs(g.country_geom, 3, false) as h3_cellID from grouped_result g\"\n)\nh3_df.show(2)\n</pre> h3_df = sedona.sql(     \"SELECT g.NAME_EN, g.country_geom, ST_H3CellIDs(g.country_geom, 3, false) as h3_cellID from grouped_result g\" ) h3_df.show(2) <pre>[Stage 42:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------------------+--------------------+--------------------+\n|             NAME_EN|        country_geom|           h3_cellID|\n+--------------------+--------------------+--------------------+\n|Cuba             ...|MULTIPOLYGON (((-...|[5911955825051566...|\n|Mexico           ...|MULTIPOLYGON (((-...|[5918915733655388...|\n+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[12]: Copied! <pre>exploded_h3 = h3_df.select(\n    h3_df.NAME_EN, h3_df.country_geom, explode(h3_df.h3_cellID).alias(\"h3\")\n)\nexploded_h3.show(2)\n</pre> exploded_h3 = h3_df.select(     h3_df.NAME_EN, h3_df.country_geom, explode(h3_df.h3_cellID).alias(\"h3\") ) exploded_h3.show(2) <pre>[Stage 45:=================================================&gt;        (6 + 1) / 7]\r</pre> <pre>+--------------------+--------------------+------------------+\n|             NAME_EN|        country_geom|                h3|\n+--------------------+--------------------+------------------+\n|Cuba             ...|MULTIPOLYGON (((-...|591195582505156607|\n|Cuba             ...|MULTIPOLYGON (((-...|591195513785679871|\n+--------------------+--------------------+------------------+\nonly showing top 2 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[13]: Copied! <pre>exploded_h3 = exploded_h3.sample(0.3)\nexploded_h3.createOrReplaceTempView(\"exploded_h3\")\nhex_exploded_h3 = exploded_h3.select(\n    exploded_h3.NAME_EN, hex(exploded_h3.h3).alias(\"ex_h3\")\n)\nhex_exploded_h3.show(2)\nhex_exploded_h3.printSchema()\n</pre> exploded_h3 = exploded_h3.sample(0.3) exploded_h3.createOrReplaceTempView(\"exploded_h3\") hex_exploded_h3 = exploded_h3.select(     exploded_h3.NAME_EN, hex(exploded_h3.h3).alias(\"ex_h3\") ) hex_exploded_h3.show(2) hex_exploded_h3.printSchema() <pre>[Stage 52:=================================================&gt;        (6 + 1) / 7]\r</pre> <pre>+--------------------+---------------+\n|             NAME_EN|          ex_h3|\n+--------------------+---------------+\n|Cuba             ...|83459EFFFFFFFFF|\n|Cuba             ...|83459DFFFFFFFFF|\n+--------------------+---------------+\nonly showing top 2 rows\n\nroot\n |-- NAME_EN: string (nullable = true)\n |-- ex_h3: string (nullable = true)\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[14]: Copied! <pre>sedona_kepler_h3 = SedonaKepler.create_map(df=hex_exploded_h3, name=\"h3\")\nsedona_kepler_h3\n</pre> sedona_kepler_h3 = SedonaKepler.create_map(df=hex_exploded_h3, name=\"h3\") sedona_kepler_h3 <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>                                                                                \r</pre> <pre>KeplerGl(data={'h3': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, \u2026</pre>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#setup-sedona-environment","title":"Setup Sedona environment\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#read-countries-shapefile-into-a-sedona-dataframe","title":"Read countries shapefile into a Sedona DataFrame\u00b6","text":"<p>Data link: https://www.naturalearthdata.com/downloads/50m-cultural-vectors/</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#read-airports-shapefile-into-a-sedona-dataframe","title":"Read airports shapefile into a Sedona DataFrame\u00b6","text":"<p>Data link: https://www.naturalearthdata.com/downloads/50m-cultural-vectors/</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#run-spatial-join-using-sql-api","title":"Run Spatial Join using SQL API\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#run-spatial-join-using-rdd-api","title":"Run Spatial Join using RDD API\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#print-spatial-join-results","title":"Print spatial join results\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#group-airports-by-country","title":"Group airports by country\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#visualize-the-number-of-airports-in-each-country","title":"Visualize the number of airports in each country\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#visualize-using-sedonakepler","title":"Visualize using SedonaKepler\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#visualize-using-sedonapydeck","title":"Visualize using SedonaPyDeck\u00b6","text":"<p>The above visualization is generated by a pre-set config informing SedonaKepler that the map to be rendered has to be a choropleth map with choropleth of the <code>AirportCount</code> column value.</p> <p>This can be also be achieved using SedonaPyDeck and its <code>create_choropleth_map</code> API.</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#visualize-uber-h3-cells-using-sedonakepler","title":"Visualize Uber H3 cells using SedonaKepler\u00b6","text":"<p>The following tutorial depicts how Uber H3 cells can be generated using Sedona and visualized using SedonaKepler.</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#generate-h3-cell-ids","title":"Generate H3 cell IDs\u00b6","text":"<p>ST_H3CellIDs can be used to generated cell IDs for given geometries</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#since-each-geometry-can-have-multiple-h3-cell-ids-lets-explode-the-generated-h3-cell-id-array-to-get-individual-cells","title":"Since each geometry can have multiple H3 cell IDs, let's explode the generated H3 cell ID array to get individual cells\u00b6","text":""},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#convert-generated-long-h3-cell-id-to-a-hex-cell-id","title":"Convert generated long H3 cell ID to a hex cell ID\u00b6","text":"<p>SedonaKepler accepts each H3 cell ID as a hexadecimal to automatically visualize them. Also, let us sample the data to be able to visualize sparse cells on the map.</p>"},{"location":"usecases/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry/#visualize-using-sedonakepler","title":"Visualize using SedonaKepler\u00b6","text":"<p>Now, simply provide the final df to SedonaKepler.create_map and you can automagically visualize the H3 cells on the map!</p>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/","title":"Understand Overture Map data","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>from sedona.spark import *\nimport os\nimport time\nimport gresearch.spark.parquet\nimport geopandas as gpd\n</pre> from sedona.spark import * import os import time import gresearch.spark.parquet import geopandas as gpd In\u00a0[2]: Copied! <pre>DATA_LINK = (\n    \"s3a://wherobots-examples/data/overturemaps-us-west-2/release/2023-07-26-alpha.0/\"\n)\n</pre> DATA_LINK = (     \"s3a://wherobots-examples/data/overturemaps-us-west-2/release/2023-07-26-alpha.0/\" ) In\u00a0[3]: Copied! <pre># DATA_LINK = \"s3a://overturemaps-us-west-2/release/2023-11-14-alpha.0/\"\n# DATA_LINK = \"s3a://overturemaps-us-west-2/release/2023-12-14-alpha.0/\"\n# DATA_LINK = \"s3a://overturemaps-us-west-2/release/2024-01-17-alpha.0/\"\n</pre> # DATA_LINK = \"s3a://overturemaps-us-west-2/release/2023-11-14-alpha.0/\" # DATA_LINK = \"s3a://overturemaps-us-west-2/release/2023-12-14-alpha.0/\" # DATA_LINK = \"s3a://overturemaps-us-west-2/release/2024-01-17-alpha.0/\" In\u00a0[4]: Copied! <pre>config = (\n    SedonaContext.builder()\n    .config(\n        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n    )\n    .config(\n        \"fs.s3a.aws.credentials.provider\",\n        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n    )\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"\n        \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"\n        \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",\n    )\n    .getOrCreate()\n)\n\nsedona = SedonaContext.create(config)\n</pre> config = (     SedonaContext.builder()     .config(         \"spark.hadoop.fs.s3a.aws.credentials.provider\",         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",     )     .config(         \"fs.s3a.aws.credentials.provider\",         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",     )     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-spark-3.4_2.12:1.6.0,\"         \"org.datasyslab:geotools-wrapper:1.6.0-28.2,\"         \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4\",     )     .getOrCreate() )  sedona = SedonaContext.create(config) <pre>Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n</pre> <pre>:: loading settings :: url = jar:file:/home/jovyan/spark-3.4.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jovyan/.ivy2/cache\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\norg.apache.sedona#sedona-spark-3.4_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\nuk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-32c6a229-d04d-446c-b16d-58a8e96cbc3f;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-spark-3.4_2.12;1.6.0 in central\n\tfound org.apache.sedona#sedona-common;1.6.0 in central\n\tfound org.apache.commons#commons-math3;3.6.1 in central\n\tfound org.locationtech.jts#jts-core;1.19.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n\tfound com.google.geometry#s2-geometry;2.0.0 in central\n\tfound com.google.guava#guava;25.1-jre in central\n\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n\tfound org.checkerframework#checker-qual;2.0.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n\tfound com.uber#h3;4.1.1 in central\n\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n\tfound org.checkerframework#checker-qual;3.10.0 in central\n\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n\tfound org.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 in central\n\tfound commons-lang#commons-lang;2.6 in central\n\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n\tfound org.beryx#awt-color-factory;1.0.0 in central\n\tfound org.datasyslab#geotools-wrapper;1.6.0-28.2 in central\n\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n:: resolution report :: resolve 2297ms :: artifacts dl 105ms\n\t:: modules in use:\n\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n\tcom.google.guava#guava;25.1-jre from central in [default]\n\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n\tcom.uber#h3;4.1.1 from central in [default]\n\tcommons-lang#commons-lang;2.6 from central in [default]\n\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n\torg.apache.sedona#sedona-common;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-3.4_2.12;1.6.0 from central in [default]\n\torg.apache.sedona#sedona-spark-common-3.4_2.12;1.6.0 from central in [default]\n\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n\torg.datasyslab#geotools-wrapper;1.6.0-28.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n\t:: evicted modules:\n\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   25  |   0   |   0   |   2   ||   23  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-32c6a229-d04d-446c-b16d-58a8e96cbc3f\n\tconfs: [default]\n\t0 artifacts copied, 23 already retrieved (0kB/81ms)\n24/05/22 18:08:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[5]: Copied! <pre># Washington state boundary\n# spatial_filter = \"POLYGON((-123.3208 49.0023,-123.0338 49.0027,-122.0650 49.0018,-121.7491 48.9973,-121.5912 48.9991,-119.6082 49.0009,-118.0378 49.0005,-117.0319 48.9996,-117.0415 47.9614,-117.0394 46.5060,-117.0394 46.4274,-117.0621 46.3498,-117.0277 46.3384,-116.9879 46.2848,-116.9577 46.2388,-116.9659 46.2022,-116.9254 46.1722,-116.9357 46.1432,-116.9584 46.1009,-116.9762 46.0785,-116.9433 46.0537,-116.9165 45.9960,-118.0330 46.0008,-118.9867 45.9998,-119.1302 45.9320,-119.1708 45.9278,-119.2559 45.9402,-119.3047 45.9354,-119.3644 45.9220,-119.4386 45.9172,-119.4894 45.9067,-119.5724 45.9249,-119.6013 45.9196,-119.6700 45.8565,-119.8052 45.8479,-119.9096 45.8278,-119.9652 45.8245,-120.0710 45.7852,-120.1705 45.7623,-120.2110 45.7258,-120.3628 45.7057,-120.4829 45.6951,-120.5942 45.7469,-120.6340 45.7460,-120.6924 45.7143,-120.8558 45.6721,-120.9142 45.6409,-120.9471 45.6572,-120.9787 45.6419,-121.0645 45.6529,-121.1469 45.6078,-121.1847 45.6083,-121.2177 45.6721,-121.3392 45.7057,-121.4010 45.6932,-121.5328 45.7263,-121.6145 45.7091,-121.7361 45.6947,-121.8095 45.7067,-121.9338 45.6452,-122.0451 45.6088,-122.1089 45.5833,-122.1426 45.5838,-122.2009 45.5660,-122.2641 45.5439,-122.3321 45.5482,-122.3795 45.5756,-122.4392 45.5636,-122.5676 45.6006,-122.6891 45.6236,-122.7647 45.6582,-122.7750 45.6817,-122.7619 45.7613,-122.7962 45.8106,-122.7839 45.8642,-122.8114 45.9120,-122.8148 45.9612,-122.8587 46.0160,-122.8848 46.0604,-122.9034 46.0832,-122.9597 46.1028,-123.0579 46.1556,-123.1210 46.1865,-123.1664 46.1893,-123.2810 46.1446,-123.3703 46.1470,-123.4314 46.1822,-123.4287 46.2293,-123.4946 46.2691,-123.5557 46.2582,-123.6209 46.2573,-123.6875 46.2497,-123.7404 46.2691,-123.8729 46.2350,-123.9292 46.2383,-123.9711 46.2677,-124.0212 46.2924,-124.0329 46.2653,-124.2444 46.2596,-124.2691 46.4312,-124.3529 46.8386,-124.4380 47.1832,-124.5616 47.4689,-124.7566 47.8012,-124.8679 48.0423,-124.8679 48.2457,-124.8486 48.3727,-124.7539 48.4984,-124.4174 48.4096,-124.2389 48.3599,-124.0116 48.2964,-123.9141 48.2795,-123.5413 48.2247,-123.3998 48.2539,-123.2501 48.2841,-123.1169 48.4233,-123.1609 48.4533,-123.2220 48.5548,-123.2336 48.5902,-123.2721 48.6901,-123.0084 48.7675,-123.0084 48.8313,-123.3215 49.0023,-123.3208 49.0023))\"\n\n# Bellevue city boundary\nspatial_filter = \"POLYGON ((-122.235128 47.650163, -122.233796 47.65162, -122.231581 47.653287, -122.228514 47.65482, -122.227526 47.655204, -122.226175 47.655729, -122.222039 47.656743999999996, -122.218428 47.657464, -122.217026 47.657506, -122.21437399999999 47.657588, -122.212091 47.657464, -122.212135 47.657320999999996, -122.21092999999999 47.653552, -122.209834 47.650121, -122.209559 47.648976, -122.209642 47.648886, -122.21042 47.648658999999995, -122.210897 47.64864, -122.211005 47.648373, -122.21103099999999 47.648320999999996, -122.211992 47.64644, -122.212457 47.646426, -122.212469 47.646392, -122.212469 47.646088999999996, -122.212471 47.645213, -122.213115 47.645212, -122.213123 47.644576, -122.21352999999999 47.644576, -122.213768 47.644560999999996, -122.21382 47.644560999999996, -122.21382 47.644456999999996, -122.21373299999999 47.644455, -122.213748 47.643102999999996, -122.213751 47.642790999999995, -122.213753 47.642716, -122.213702 47.642697999999996, -122.213679 47.642689999999995, -122.21364 47.642678, -122.213198 47.642541, -122.213065 47.642500000000005, -122.212918 47.642466, -122.21275 47.642441, -122.212656 47.642433, -122.21253899999999 47.642429, -122.212394 47.64243, -122.212182 47.642444999999995, -122.211957 47.642488, -122.211724 47.642551999999995, -122.21143599999999 47.642647, -122.210906 47.642834, -122.210216 47.643099, -122.209858 47.643215, -122.20973000000001 47.643248, -122.20973599999999 47.643105, -122.209267 47.643217, -122.208832 47.643302, -122.208391 47.643347999999996, -122.207797 47.643414, -122.207476 47.643418, -122.20701199999999 47.643397, -122.206795 47.643387999999995, -122.205742 47.643246, -122.20549 47.643201999999995, -122.20500200000001 47.643119, -122.204802 47.643085, -122.204641 47.643066, -122.204145 47.643012, -122.203547 47.643012, -122.203097 47.643107, -122.20275699999999 47.643283, -122.202507 47.643496999999996, -122.202399 47.643653, -122.202111 47.643771, -122.201668 47.643767, -122.201363 47.643665, -122.20133 47.643648999999996, -122.201096 47.643536, -122.200744 47.64328, -122.200568 47.64309, -122.200391 47.642849, -122.200162 47.642539, -122.199896 47.642500000000005, -122.19980799999999 47.642424, -122.199755 47.642376999999996, -122.199558 47.642227999999996, -122.199439 47.642157, -122.199293 47.642078999999995, -122.199131 47.642004, -122.198928 47.641925, -122.19883 47.641892, -122.19856300000001 47.641811999999994, -122.198203 47.641731, -122.197662 47.641619999999996, -122.196819 47.641436, -122.196294 47.641309, -122.196294 47.642314, -122.19628 47.642855, -122.196282 47.642897999999995, -122.196281 47.643111, -122.196283 47.643415, -122.196283 47.643508999999995, -122.19628399999999 47.643739, -122.196287 47.644203999999995, -122.196287 47.644262999999995, -122.19629 47.644937999999996, -122.19629 47.644954999999996, -122.196292 47.645271, -122.196291 47.645426, -122.19629499999999 47.646315, -122.19629499999999 47.646432, -122.195925 47.646432, -122.195251 47.646432, -122.190853 47.646429999999995, -122.187649 47.646428, -122.187164 47.646426, -122.18683 47.646426, -122.185547 47.646409, -122.185546 47.646316, -122.185537 47.645599, -122.185544 47.644197, -122.185537 47.643294999999995, -122.185544 47.642733, -122.185541 47.641757, -122.185555 47.640681, -122.185561 47.63972, -122.185557 47.638228999999995, -122.185591 47.635419, -122.185611 47.634750999999994, -122.18562299999999 47.634484, -122.18561700000001 47.634375999999996, -122.185592 47.634311, -122.185549 47.634232999999995, -122.185504 47.634181999999996, -122.185426 47.634119, -122.184371 47.633424999999995, -122.18400000000001 47.633198, -122.183896 47.633134, -122.1838 47.633067, -122.18375499999999 47.633019999999995, -122.183724 47.632959, -122.183695 47.632858, -122.183702 47.632675, -122.182757 47.632622999999995, -122.182365 47.63259, -122.18220600000001 47.632562, -122.181984 47.632504999999995, -122.18163799999999 47.632363, -122.18142 47.632262999999995, -122.181229 47.632165, -122.181612 47.632172999999995, -122.18271899999999 47.632151, -122.183138 47.632135, -122.18440000000001 47.632081, -122.184743 47.632065999999995, -122.185312 47.63205, -122.185624 47.632047, -122.185625 47.631873999999996, -122.184618 47.63187, -122.184291 47.631878, -122.184278 47.631817999999996, -122.183882 47.629942, -122.182689 47.623548, -122.182594 47.622789999999995, -122.182654 47.622155, -122.183135 47.622372999999996, -122.183471 47.622506, -122.18360200000001 47.622552, -122.183893 47.622637999999995, -122.184244 47.62272, -122.184618 47.622777, -122.184741 47.622727999999995, -122.184605 47.622679, -122.18424 47.622622, -122.183985 47.622569, -122.183717 47.622501, -122.183506 47.622439, -122.18327 47.622357, -122.18305699999999 47.622271999999995, -122.182669 47.622088999999995, -122.182796 47.621545, -122.18347 47.619628999999996, -122.18365 47.619098, -122.183859 47.6184, -122.183922 47.617793999999996, -122.183956 47.617292, -122.183792 47.616388, -122.183261 47.614391999999995, -122.183202 47.613802, -122.183209 47.613155, -122.183436 47.612384999999996, -122.18395100000001 47.610445999999996, -122.184338 47.60924, -122.184657 47.609116, -122.18481 47.609051, -122.18491900000001 47.608987, -122.184974 47.608942, -122.185047 47.608846, -122.185082 47.608743999999994, -122.185109 47.608526999999995, -122.185116 47.608359, -122.18513 47.608315999999995, -122.185157 47.608273999999994, -122.185183 47.608247, -122.185246 47.608214, -122.185354 47.608196, -122.185475 47.608191999999995, -122.185472 47.606697, -122.185472 47.606373999999995, -122.185521 47.606272, -122.185528 47.606210999999995, -122.185506 47.606037, -122.185451 47.605872999999995, -122.185411 47.605781, -122.185358 47.605681999999995, -122.185248 47.605509999999995, -122.185127 47.605365, -122.185058 47.605292, -122.184772 47.605038, -122.184428 47.604834, -122.184122 47.604693999999995, -122.183775 47.604574, -122.183644 47.604546, -122.183708 47.604400999999996, -122.183749 47.604223999999995, -122.18376 47.604037, -122.183707 47.603778, -122.183619 47.603556999999995, -122.183559 47.603406, -122.183488 47.603303, -122.183824 47.603167, -122.184108 47.603052, -122.184478 47.602902, -122.18543 47.602495, -122.186669 47.601957, -122.186433 47.601220999999995, -122.186341 47.601127999999996, -122.18874199999999 47.593742999999996, -122.188434 47.592338999999996, -122.188479 47.591786, -122.188217 47.591269999999994, -122.18795399999999 47.590871, -122.186822 47.589228, -122.187421 47.589228999999996, -122.18848299999999 47.589228999999996, -122.188433 47.587922999999996, -122.18990000000001 47.588547, -122.191368 47.589169999999996, -122.19158 47.589222, -122.191779 47.589254999999994, -122.192117 47.589289, -122.191569 47.587478999999995, -122.191323 47.586628999999995, -122.191295 47.586554, -122.191268 47.586479, -122.191192 47.586318, -122.191163 47.586268999999994, -122.1911 47.586164, -122.19099 47.586011, -122.19067 47.585668999999996, -122.1905 47.585515, -122.190301 47.58531, -122.190143 47.585152, -122.189573 47.584576999999996, -122.188702 47.583735999999995, -122.188646 47.583679, -122.188239 47.583258, -122.188037 47.583005, -122.187832 47.582657, -122.187726 47.582164999999996, -122.18769499999999 47.581964, -122.18768299999999 47.581781, -122.187678 47.581592, -122.18766099999999 47.581455, -122.187674 47.581311, -122.18768 47.581146, -122.187722 47.580877, -122.187817 47.580569999999994, -122.187932 47.580301999999996, -122.188047 47.580087, -122.188161 47.579933999999994, -122.188399 47.579660999999994, -122.18851699999999 47.579547, -122.188621 47.579454, -122.188042 47.579493, -122.18762 47.579527, -122.187806 47.579358, -122.188009 47.579175, -122.18814499999999 47.579051, -122.188177 47.579021, -122.18842000000001 47.5788, -122.188638 47.578461, -122.188895 47.57806, -122.189791 47.577281, -122.190008 47.577103, -122.190372 47.576805, -122.19119 47.576358, -122.191877 47.576087, -122.193025 47.57566, -122.194317 47.575185999999995, -122.196061 47.574664, -122.197239 47.574386999999994, -122.197873 47.574267, -122.198286 47.574189999999994, -122.199091 47.574044, -122.199067 47.574574999999996, -122.199007 47.575921, -122.200335 47.578222, -122.20057299999999 47.578345999999996, -122.2009 47.578517999999995, -122.201095 47.578621999999996, -122.20138399999999 47.578776999999995, -122.201465 47.57882, -122.201516 47.578846999999996, -122.205753 47.581112, -122.209515 47.583124, -122.210634 47.583721, -122.21473399999999 47.587021, -122.21538699999999 47.588254, -122.21580399999999 47.589042, -122.216534 47.590421, -122.220092 47.596261, -122.220434 47.596821, -122.22041899999999 47.597837999999996, -122.220289 47.606455, -122.220234 47.610121, -122.22048 47.615221999999996, -122.220359 47.615379, -122.220283 47.615477999999996, -122.21999 47.615854999999996, -122.219993 47.61597, -122.22023300000001 47.616634, -122.220356 47.616687999999996, -122.220409 47.616712, -122.221401 47.618538, -122.22142 47.618573, -122.221456 47.618635, -122.221791 47.619222, -122.222492 47.619682999999995, -122.222799 47.619886, -122.222083 47.620368, -122.222046 47.620407, -122.222028 47.620449, -122.222025 47.620483, -122.22203999999999 47.620523999999996, -122.222079 47.620557999999996, -122.222156 47.620594999999994, -122.222458 47.620629, -122.222454 47.620673, -122.222454 47.620711, -122.22244599999999 47.621041999999996, -122.223056 47.621041, -122.223129 47.62104, -122.223153 47.62104, -122.223574 47.621041, -122.22377900000001 47.621041, -122.223857 47.621041, -122.22467499999999 47.621041, -122.224712 47.62104, -122.224958 47.62104, -122.225167 47.621049, -122.226882 47.621037, -122.227565 47.621032, -122.228002 47.621029, -122.22797800000001 47.621300999999995, -122.227919 47.626574999999995, -122.227914 47.627085, -122.227901 47.6283, -122.227881 47.630069, -122.227869 47.631177, -122.227879 47.631952999999996, -122.22789 47.633879, -122.227886 47.63409, -122.227871 47.635534, -122.227918 47.635565, -122.228953 47.635624, -122.22895199999999 47.635571999999996, -122.231018 47.635574999999996, -122.233276 47.635588999999996, -122.233287 47.63617, -122.233273 47.63639, -122.233272 47.636469999999996, -122.23327 47.636578, -122.233266 47.636827, -122.233263 47.636851, -122.233262 47.637014, -122.23322999999999 47.638110999999995, -122.233239 47.638219, -122.233262 47.638279, -122.233313 47.638324999999995, -122.233255 47.638359, -122.233218 47.638380999999995, -122.233153 47.638450999999996, -122.233136 47.638552999999995, -122.233137 47.638692, -122.232715 47.639348999999996, -122.232659 47.640093, -122.232704 47.641375, -122.233821 47.645111, -122.234906 47.648874, -122.234924 47.648938, -122.235128 47.650163))\"\n</pre> # Washington state boundary # spatial_filter = \"POLYGON((-123.3208 49.0023,-123.0338 49.0027,-122.0650 49.0018,-121.7491 48.9973,-121.5912 48.9991,-119.6082 49.0009,-118.0378 49.0005,-117.0319 48.9996,-117.0415 47.9614,-117.0394 46.5060,-117.0394 46.4274,-117.0621 46.3498,-117.0277 46.3384,-116.9879 46.2848,-116.9577 46.2388,-116.9659 46.2022,-116.9254 46.1722,-116.9357 46.1432,-116.9584 46.1009,-116.9762 46.0785,-116.9433 46.0537,-116.9165 45.9960,-118.0330 46.0008,-118.9867 45.9998,-119.1302 45.9320,-119.1708 45.9278,-119.2559 45.9402,-119.3047 45.9354,-119.3644 45.9220,-119.4386 45.9172,-119.4894 45.9067,-119.5724 45.9249,-119.6013 45.9196,-119.6700 45.8565,-119.8052 45.8479,-119.9096 45.8278,-119.9652 45.8245,-120.0710 45.7852,-120.1705 45.7623,-120.2110 45.7258,-120.3628 45.7057,-120.4829 45.6951,-120.5942 45.7469,-120.6340 45.7460,-120.6924 45.7143,-120.8558 45.6721,-120.9142 45.6409,-120.9471 45.6572,-120.9787 45.6419,-121.0645 45.6529,-121.1469 45.6078,-121.1847 45.6083,-121.2177 45.6721,-121.3392 45.7057,-121.4010 45.6932,-121.5328 45.7263,-121.6145 45.7091,-121.7361 45.6947,-121.8095 45.7067,-121.9338 45.6452,-122.0451 45.6088,-122.1089 45.5833,-122.1426 45.5838,-122.2009 45.5660,-122.2641 45.5439,-122.3321 45.5482,-122.3795 45.5756,-122.4392 45.5636,-122.5676 45.6006,-122.6891 45.6236,-122.7647 45.6582,-122.7750 45.6817,-122.7619 45.7613,-122.7962 45.8106,-122.7839 45.8642,-122.8114 45.9120,-122.8148 45.9612,-122.8587 46.0160,-122.8848 46.0604,-122.9034 46.0832,-122.9597 46.1028,-123.0579 46.1556,-123.1210 46.1865,-123.1664 46.1893,-123.2810 46.1446,-123.3703 46.1470,-123.4314 46.1822,-123.4287 46.2293,-123.4946 46.2691,-123.5557 46.2582,-123.6209 46.2573,-123.6875 46.2497,-123.7404 46.2691,-123.8729 46.2350,-123.9292 46.2383,-123.9711 46.2677,-124.0212 46.2924,-124.0329 46.2653,-124.2444 46.2596,-124.2691 46.4312,-124.3529 46.8386,-124.4380 47.1832,-124.5616 47.4689,-124.7566 47.8012,-124.8679 48.0423,-124.8679 48.2457,-124.8486 48.3727,-124.7539 48.4984,-124.4174 48.4096,-124.2389 48.3599,-124.0116 48.2964,-123.9141 48.2795,-123.5413 48.2247,-123.3998 48.2539,-123.2501 48.2841,-123.1169 48.4233,-123.1609 48.4533,-123.2220 48.5548,-123.2336 48.5902,-123.2721 48.6901,-123.0084 48.7675,-123.0084 48.8313,-123.3215 49.0023,-123.3208 49.0023))\"  # Bellevue city boundary spatial_filter = \"POLYGON ((-122.235128 47.650163, -122.233796 47.65162, -122.231581 47.653287, -122.228514 47.65482, -122.227526 47.655204, -122.226175 47.655729, -122.222039 47.656743999999996, -122.218428 47.657464, -122.217026 47.657506, -122.21437399999999 47.657588, -122.212091 47.657464, -122.212135 47.657320999999996, -122.21092999999999 47.653552, -122.209834 47.650121, -122.209559 47.648976, -122.209642 47.648886, -122.21042 47.648658999999995, -122.210897 47.64864, -122.211005 47.648373, -122.21103099999999 47.648320999999996, -122.211992 47.64644, -122.212457 47.646426, -122.212469 47.646392, -122.212469 47.646088999999996, -122.212471 47.645213, -122.213115 47.645212, -122.213123 47.644576, -122.21352999999999 47.644576, -122.213768 47.644560999999996, -122.21382 47.644560999999996, -122.21382 47.644456999999996, -122.21373299999999 47.644455, -122.213748 47.643102999999996, -122.213751 47.642790999999995, -122.213753 47.642716, -122.213702 47.642697999999996, -122.213679 47.642689999999995, -122.21364 47.642678, -122.213198 47.642541, -122.213065 47.642500000000005, -122.212918 47.642466, -122.21275 47.642441, -122.212656 47.642433, -122.21253899999999 47.642429, -122.212394 47.64243, -122.212182 47.642444999999995, -122.211957 47.642488, -122.211724 47.642551999999995, -122.21143599999999 47.642647, -122.210906 47.642834, -122.210216 47.643099, -122.209858 47.643215, -122.20973000000001 47.643248, -122.20973599999999 47.643105, -122.209267 47.643217, -122.208832 47.643302, -122.208391 47.643347999999996, -122.207797 47.643414, -122.207476 47.643418, -122.20701199999999 47.643397, -122.206795 47.643387999999995, -122.205742 47.643246, -122.20549 47.643201999999995, -122.20500200000001 47.643119, -122.204802 47.643085, -122.204641 47.643066, -122.204145 47.643012, -122.203547 47.643012, -122.203097 47.643107, -122.20275699999999 47.643283, -122.202507 47.643496999999996, -122.202399 47.643653, -122.202111 47.643771, -122.201668 47.643767, -122.201363 47.643665, -122.20133 47.643648999999996, -122.201096 47.643536, -122.200744 47.64328, -122.200568 47.64309, -122.200391 47.642849, -122.200162 47.642539, -122.199896 47.642500000000005, -122.19980799999999 47.642424, -122.199755 47.642376999999996, -122.199558 47.642227999999996, -122.199439 47.642157, -122.199293 47.642078999999995, -122.199131 47.642004, -122.198928 47.641925, -122.19883 47.641892, -122.19856300000001 47.641811999999994, -122.198203 47.641731, -122.197662 47.641619999999996, -122.196819 47.641436, -122.196294 47.641309, -122.196294 47.642314, -122.19628 47.642855, -122.196282 47.642897999999995, -122.196281 47.643111, -122.196283 47.643415, -122.196283 47.643508999999995, -122.19628399999999 47.643739, -122.196287 47.644203999999995, -122.196287 47.644262999999995, -122.19629 47.644937999999996, -122.19629 47.644954999999996, -122.196292 47.645271, -122.196291 47.645426, -122.19629499999999 47.646315, -122.19629499999999 47.646432, -122.195925 47.646432, -122.195251 47.646432, -122.190853 47.646429999999995, -122.187649 47.646428, -122.187164 47.646426, -122.18683 47.646426, -122.185547 47.646409, -122.185546 47.646316, -122.185537 47.645599, -122.185544 47.644197, -122.185537 47.643294999999995, -122.185544 47.642733, -122.185541 47.641757, -122.185555 47.640681, -122.185561 47.63972, -122.185557 47.638228999999995, -122.185591 47.635419, -122.185611 47.634750999999994, -122.18562299999999 47.634484, -122.18561700000001 47.634375999999996, -122.185592 47.634311, -122.185549 47.634232999999995, -122.185504 47.634181999999996, -122.185426 47.634119, -122.184371 47.633424999999995, -122.18400000000001 47.633198, -122.183896 47.633134, -122.1838 47.633067, -122.18375499999999 47.633019999999995, -122.183724 47.632959, -122.183695 47.632858, -122.183702 47.632675, -122.182757 47.632622999999995, -122.182365 47.63259, -122.18220600000001 47.632562, -122.181984 47.632504999999995, -122.18163799999999 47.632363, -122.18142 47.632262999999995, -122.181229 47.632165, -122.181612 47.632172999999995, -122.18271899999999 47.632151, -122.183138 47.632135, -122.18440000000001 47.632081, -122.184743 47.632065999999995, -122.185312 47.63205, -122.185624 47.632047, -122.185625 47.631873999999996, -122.184618 47.63187, -122.184291 47.631878, -122.184278 47.631817999999996, -122.183882 47.629942, -122.182689 47.623548, -122.182594 47.622789999999995, -122.182654 47.622155, -122.183135 47.622372999999996, -122.183471 47.622506, -122.18360200000001 47.622552, -122.183893 47.622637999999995, -122.184244 47.62272, -122.184618 47.622777, -122.184741 47.622727999999995, -122.184605 47.622679, -122.18424 47.622622, -122.183985 47.622569, -122.183717 47.622501, -122.183506 47.622439, -122.18327 47.622357, -122.18305699999999 47.622271999999995, -122.182669 47.622088999999995, -122.182796 47.621545, -122.18347 47.619628999999996, -122.18365 47.619098, -122.183859 47.6184, -122.183922 47.617793999999996, -122.183956 47.617292, -122.183792 47.616388, -122.183261 47.614391999999995, -122.183202 47.613802, -122.183209 47.613155, -122.183436 47.612384999999996, -122.18395100000001 47.610445999999996, -122.184338 47.60924, -122.184657 47.609116, -122.18481 47.609051, -122.18491900000001 47.608987, -122.184974 47.608942, -122.185047 47.608846, -122.185082 47.608743999999994, -122.185109 47.608526999999995, -122.185116 47.608359, -122.18513 47.608315999999995, -122.185157 47.608273999999994, -122.185183 47.608247, -122.185246 47.608214, -122.185354 47.608196, -122.185475 47.608191999999995, -122.185472 47.606697, -122.185472 47.606373999999995, -122.185521 47.606272, -122.185528 47.606210999999995, -122.185506 47.606037, -122.185451 47.605872999999995, -122.185411 47.605781, -122.185358 47.605681999999995, -122.185248 47.605509999999995, -122.185127 47.605365, -122.185058 47.605292, -122.184772 47.605038, -122.184428 47.604834, -122.184122 47.604693999999995, -122.183775 47.604574, -122.183644 47.604546, -122.183708 47.604400999999996, -122.183749 47.604223999999995, -122.18376 47.604037, -122.183707 47.603778, -122.183619 47.603556999999995, -122.183559 47.603406, -122.183488 47.603303, -122.183824 47.603167, -122.184108 47.603052, -122.184478 47.602902, -122.18543 47.602495, -122.186669 47.601957, -122.186433 47.601220999999995, -122.186341 47.601127999999996, -122.18874199999999 47.593742999999996, -122.188434 47.592338999999996, -122.188479 47.591786, -122.188217 47.591269999999994, -122.18795399999999 47.590871, -122.186822 47.589228, -122.187421 47.589228999999996, -122.18848299999999 47.589228999999996, -122.188433 47.587922999999996, -122.18990000000001 47.588547, -122.191368 47.589169999999996, -122.19158 47.589222, -122.191779 47.589254999999994, -122.192117 47.589289, -122.191569 47.587478999999995, -122.191323 47.586628999999995, -122.191295 47.586554, -122.191268 47.586479, -122.191192 47.586318, -122.191163 47.586268999999994, -122.1911 47.586164, -122.19099 47.586011, -122.19067 47.585668999999996, -122.1905 47.585515, -122.190301 47.58531, -122.190143 47.585152, -122.189573 47.584576999999996, -122.188702 47.583735999999995, -122.188646 47.583679, -122.188239 47.583258, -122.188037 47.583005, -122.187832 47.582657, -122.187726 47.582164999999996, -122.18769499999999 47.581964, -122.18768299999999 47.581781, -122.187678 47.581592, -122.18766099999999 47.581455, -122.187674 47.581311, -122.18768 47.581146, -122.187722 47.580877, -122.187817 47.580569999999994, -122.187932 47.580301999999996, -122.188047 47.580087, -122.188161 47.579933999999994, -122.188399 47.579660999999994, -122.18851699999999 47.579547, -122.188621 47.579454, -122.188042 47.579493, -122.18762 47.579527, -122.187806 47.579358, -122.188009 47.579175, -122.18814499999999 47.579051, -122.188177 47.579021, -122.18842000000001 47.5788, -122.188638 47.578461, -122.188895 47.57806, -122.189791 47.577281, -122.190008 47.577103, -122.190372 47.576805, -122.19119 47.576358, -122.191877 47.576087, -122.193025 47.57566, -122.194317 47.575185999999995, -122.196061 47.574664, -122.197239 47.574386999999994, -122.197873 47.574267, -122.198286 47.574189999999994, -122.199091 47.574044, -122.199067 47.574574999999996, -122.199007 47.575921, -122.200335 47.578222, -122.20057299999999 47.578345999999996, -122.2009 47.578517999999995, -122.201095 47.578621999999996, -122.20138399999999 47.578776999999995, -122.201465 47.57882, -122.201516 47.578846999999996, -122.205753 47.581112, -122.209515 47.583124, -122.210634 47.583721, -122.21473399999999 47.587021, -122.21538699999999 47.588254, -122.21580399999999 47.589042, -122.216534 47.590421, -122.220092 47.596261, -122.220434 47.596821, -122.22041899999999 47.597837999999996, -122.220289 47.606455, -122.220234 47.610121, -122.22048 47.615221999999996, -122.220359 47.615379, -122.220283 47.615477999999996, -122.21999 47.615854999999996, -122.219993 47.61597, -122.22023300000001 47.616634, -122.220356 47.616687999999996, -122.220409 47.616712, -122.221401 47.618538, -122.22142 47.618573, -122.221456 47.618635, -122.221791 47.619222, -122.222492 47.619682999999995, -122.222799 47.619886, -122.222083 47.620368, -122.222046 47.620407, -122.222028 47.620449, -122.222025 47.620483, -122.22203999999999 47.620523999999996, -122.222079 47.620557999999996, -122.222156 47.620594999999994, -122.222458 47.620629, -122.222454 47.620673, -122.222454 47.620711, -122.22244599999999 47.621041999999996, -122.223056 47.621041, -122.223129 47.62104, -122.223153 47.62104, -122.223574 47.621041, -122.22377900000001 47.621041, -122.223857 47.621041, -122.22467499999999 47.621041, -122.224712 47.62104, -122.224958 47.62104, -122.225167 47.621049, -122.226882 47.621037, -122.227565 47.621032, -122.228002 47.621029, -122.22797800000001 47.621300999999995, -122.227919 47.626574999999995, -122.227914 47.627085, -122.227901 47.6283, -122.227881 47.630069, -122.227869 47.631177, -122.227879 47.631952999999996, -122.22789 47.633879, -122.227886 47.63409, -122.227871 47.635534, -122.227918 47.635565, -122.228953 47.635624, -122.22895199999999 47.635571999999996, -122.231018 47.635574999999996, -122.233276 47.635588999999996, -122.233287 47.63617, -122.233273 47.63639, -122.233272 47.636469999999996, -122.23327 47.636578, -122.233266 47.636827, -122.233263 47.636851, -122.233262 47.637014, -122.23322999999999 47.638110999999995, -122.233239 47.638219, -122.233262 47.638279, -122.233313 47.638324999999995, -122.233255 47.638359, -122.233218 47.638380999999995, -122.233153 47.638450999999996, -122.233136 47.638552999999995, -122.233137 47.638692, -122.232715 47.639348999999996, -122.232659 47.640093, -122.232704 47.641375, -122.233821 47.645111, -122.234906 47.648874, -122.234924 47.648938, -122.235128 47.650163))\" <p>Inspect the parquet metadata of the building dataset using spark-extension. Detailed usage can be found here: https://github.com/G-Research/spark-extension/blob/master/PARQUET.md</p> In\u00a0[6]: Copied! <pre>sedona.read.parquet_blocks(DATA_LINK + \"theme=places/type=place\").show()\n</pre> sedona.read.parquet_blocks(DATA_LINK + \"theme=places/type=place\").show() <pre>24/05/22 18:09:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n[Stage 5:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+--------------------+-----+----------+---------------+-----------------+------+-------+--------+-------+\n|            filename|block|blockStart|compressedBytes|uncompressedBytes|  rows|columns|  values|  nulls|\n+--------------------+-----+----------+---------------+-----------------+------+-------+--------+-------+\n|s3a://wherobots-e...|    1|         4|      133401158|        212026494|672280|     26|25498425|3949559|\n|s3a://wherobots-e...|    2| 133401162|      117481155|        186628211|592107|     26|22456535|3480966|\n|s3a://wherobots-e...|    1|         4|      132009298|        217901239|739959|     26|28171336|4659055|\n|s3a://wherobots-e...|    2| 132009302|       27865148|         44972145|155610|     26| 5925253| 980001|\n|s3a://wherobots-e...|    1|         4|      131924175|        210390680|679198|     26|26835822|4204847|\n|s3a://wherobots-e...|    2| 131924179|      100376907|        159779330|516904|     26|20420533|3199442|\n|s3a://wherobots-e...|    1|         4|      133368917|        212391474|710100|     26|26798558|4480776|\n|s3a://wherobots-e...|    2| 133368921|       68844967|        109127771|366446|     26|13827983|2312016|\n|s3a://wherobots-e...|    1|         4|      133624141|        224974977|671721|     26|28791936|4047492|\n|s3a://wherobots-e...|    2| 133624145|       78353690|        131116926|393520|     26|16855422|2370775|\n|s3a://wherobots-e...|    1|         4|      132531884|        212087729|673983|     26|27189443|4181539|\n|s3a://wherobots-e...|    2| 132531888|       85813847|        136921776|436523|     26|17610252|2708624|\n|s3a://wherobots-e...|    1|         4|      132240373|        228506339|679198|     26|24892057|4556778|\n|s3a://wherobots-e...|    2| 132240377|       77948324|        134442494|400011|     26|14661020|2682371|\n|s3a://wherobots-e...|    1|         4|      133097687|        217681358|690100|     26|25177484|4427618|\n|s3a://wherobots-e...|    2| 133097691|       10383388|         16254438| 53363|     26| 1946918| 342463|\n|s3a://wherobots-e...|    1|         4|      132977557|        221242036|673983|     26|24519195|4518407|\n|s3a://wherobots-e...|    2| 132977561|       23099608|         37806057|116767|     26| 4246990| 782901|\n|s3a://wherobots-e...|    1|         4|      131748222|        207244630|658924|     26|26686490|4121025|\n|s3a://wherobots-e...|    2| 131748226|      131665990|        207128595|658924|     26|26681616|4123667|\n+--------------------+-----+----------+---------------+-----------------+------+-------+--------+-------+\nonly showing top 20 rows\n\n</pre> <pre>                                                                                \r</pre> <p>Inspect the GeoParquet metadata using Sedona <code>geoparquet.metadata</code></p> In\u00a0[7]: Copied! <pre>sedona.read.format(\"geoparquet.metadata\").load(\n    DATA_LINK + \"theme=places/type=place\"\n).drop(\"path\").printSchema()\n</pre> sedona.read.format(\"geoparquet.metadata\").load(     DATA_LINK + \"theme=places/type=place\" ).drop(\"path\").printSchema() <pre>                                                                                \r</pre> <pre>root\n |-- version: string (nullable = true)\n |-- primary_column: string (nullable = true)\n |-- columns: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- encoding: string (nullable = true)\n |    |    |-- geometry_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- bbox: array (nullable = true)\n |    |    |    |-- element: double (containsNull = true)\n |    |    |-- crs: string (nullable = true)\n |-- geohash: string (nullable = true)\n\n</pre> In\u00a0[8]: Copied! <pre>sedona.read.format(\"geoparquet.metadata\").load(\n    DATA_LINK + \"theme=places/type=place\"\n).drop(\"path\").show(truncate=False)\n</pre> sedona.read.format(\"geoparquet.metadata\").load(     DATA_LINK + \"theme=places/type=place\" ).drop(\"path\").show(truncate=False) <pre>[Stage 8:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+------------+--------------+-----------------------------------------------------------------------------------------+-------+\n|version     |primary_column|columns                                                                                  |geohash|\n+------------+--------------+-----------------------------------------------------------------------------------------+-------+\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-45.0, -16.8737144, -33.7740679, -11.2501301], null}}       |7j     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-135.0, -83.8333, -124.4478565, -78.7677918], null}}        |11     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-145.546875, -66.4255372, -135.3419066, -62.6119407], null}}|0u     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-168.75, -27.8467506, -158.4935007, -22.5328537], null}}    |27     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [68.203125, -44.3395652, 78.3984375, -39.5638609], null}}    |m8     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [33.75, -16.8686782, 44.9838638, -11.2558997], null}}        |kv     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [78.75, -22.4083013, 89.99897, -16.968308], null}}           |mu     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [80.1560848, 73.5, 89.0321735, 78.1825871], null}}           |vv     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [158.3080636, 0.0, 168.7045937, 5.6247198], null}}           |x8     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [146.65, 67.614791, 156.863333, 72.0234111], null}}          |zk     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [56.25, -67.5, 67.2499575, -61.9483887], null}}              |jk     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [67.5, 67.518, 78.046875, 72.96], null}}                     |vs     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [135.0, 16.953941, 146.0742188, 22.4856433], null}}          |x5     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [90.0, 39.376772, 101.2495333, 44.9817729], null}}           |wp     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [101.25, 45.0, 112.493593, 50.6131607], null}}               |y2     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-67.5, -73.1167, -56.5362777, -67.5167], null}}             |4e     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-78.4263447, 61.90054, -67.6226755, 67.41667], null}}       |f7     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [-56.25, 22.5402227, -45.2120876, 28.1104459], null}}        |du     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [136.7578125, -84.8460811, 142.3828125, -84.3999669], null}} |p0     |\n|1.0.0-beta.1|geometry      |{geometry -&gt; {WKB, [Point], [90.8843994, -56.1702103, 101.16154, -50.6342525], null}}    |nn     |\n+------------+--------------+-----------------------------------------------------------------------------------------+-------+\nonly showing top 20 rows\n\n</pre> <pre>                                                                                \r</pre> <p>Inspect the schema of GeoParquet files</p> In\u00a0[9]: Copied! <pre>sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=places/type=place\"\n).printSchema()\n</pre> sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=places/type=place\" ).printSchema() <pre>                                                                                \r</pre> <pre>root\n |-- id: string (nullable = true)\n |-- updatetime: string (nullable = true)\n |-- version: integer (nullable = true)\n |-- names: map (nullable = true)\n |    |-- key: string\n |    |-- value: array (valueContainsNull = true)\n |    |    |-- element: map (containsNull = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |-- categories: struct (nullable = true)\n |    |-- main: string (nullable = true)\n |    |-- alternate: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- confidence: double (nullable = true)\n |-- websites: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- socials: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- emails: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- phones: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- brand: struct (nullable = true)\n |    |-- names: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: array (valueContainsNull = true)\n |    |    |    |-- element: map (containsNull = true)\n |    |    |    |    |-- key: string\n |    |    |    |    |-- value: string (valueContainsNull = true)\n |    |-- wikidata: string (nullable = true)\n |-- addresses: array (nullable = true)\n |    |-- element: map (containsNull = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- sources: array (nullable = true)\n |    |-- element: map (containsNull = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- bbox: struct (nullable = true)\n |    |-- minx: double (nullable = true)\n |    |-- maxx: double (nullable = true)\n |    |-- miny: double (nullable = true)\n |    |-- maxy: double (nullable = true)\n |-- geometry: geometry (nullable = true)\n |-- geohash: string (nullable = true)\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[10]: Copied! <pre>%%time\n\ndf_place = sedona.read.format(\"geoparquet\").load(DATA_LINK + \"theme=places/type=place\")\n\ndf_place = df_place.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n).cache()\n</pre> %%time  df_place = sedona.read.format(\"geoparquet\").load(DATA_LINK + \"theme=places/type=place\")  df_place = df_place.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" ).cache() <pre>                                                                                \r</pre> <pre>CPU times: user 861 ms, sys: 214 ms, total: 1.08 s\nWall time: 3min 22s\n</pre> In\u00a0[11]: Copied! <pre>%%time\n\nmap_place = SedonaKepler.create_map(df_place, \"Place\")\n\nmap_place\n</pre> %%time  map_place = SedonaKepler.create_map(df_place, \"Place\")  map_place <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>                                                                                \r</pre> <pre>CPU times: user 1.9 s, sys: 244 ms, total: 2.14 s\nWall time: 14min 52s\n</pre> <pre>KeplerGl(data={'Place': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2\u2026</pre> In\u00a0[12]: Copied! <pre>df_place.select(\"id\", \"geometry\", \"categories.main\").limit(1000).repartition(\n    1\n).write.format(\"geoparquet\").option(\"geoparquet.version\", \"1.0.0\").option(\n    \"geoparquet.crs\", \"\"\n).mode(\n    \"overwrite\"\n).save(\n    \"places.parquet\"\n)\n</pre> df_place.select(\"id\", \"geometry\", \"categories.main\").limit(1000).repartition(     1 ).write.format(\"geoparquet\").option(\"geoparquet.version\", \"1.0.0\").option(     \"geoparquet.crs\", \"\" ).mode(     \"overwrite\" ).save(     \"places.parquet\" ) <pre>                                                                                \r</pre> In\u00a0[13]: Copied! <pre>gdf = gpd.GeoDataFrame(\n    df_place.select(\"id\", \"geometry\", \"categories.main\").limit(1000).toPandas(),\n    geometry=\"geometry\",\n)\ngdf.to_file(\"places.geojson\", driver=\"GeoJSON\")\ngdf.to_file(\"places.shp\")\ngdf\n</pre> gdf = gpd.GeoDataFrame(     df_place.select(\"id\", \"geometry\", \"categories.main\").limit(1000).toPandas(),     geometry=\"geometry\", ) gdf.to_file(\"places.geojson\", driver=\"GeoJSON\") gdf.to_file(\"places.shp\") gdf <pre>                                                                                \r</pre> Out[13]: id geometry main 0 tmp_1C7D119020F10C096A27DBAD487578E3 POINT (-122.22225 47.63574) cafe 1 tmp_CABF2AA2D28C364087CA75322925EF3B POINT (-122.19133 47.63664) dentist 2 tmp_4DF5EDF25453A5B26C67DB7DCF4AADEA POINT (-122.18555 47.63417) park 3 tmp_2F03B1439285DF7ED6B85BDB817B38F5 POINT (-122.18565 47.61602) None 4 tmp_A07F048103476EE43B98CD623E0939C7 POINT (-122.20394 47.61775) fast_food_restaurant ... ... ... ... 995 tmp_0F88FA5D31AADC237ECC31C029737EB0 POINT (-122.20033 47.61743) None 996 tmp_341F7756223EFDC7FCB60CB22276F67B POINT (-122.19403 47.61779) mortgage_broker 997 tmp_26900E583436FD0BA86722A6A78309B3 POINT (-122.19101 47.61840) bakery 998 tmp_FF38B028F3C5ED0C3919C357BAF09315 POINT (-122.19332 47.62042) bubble_tea 999 tmp_683827FB15AFF93F5530B3096DD86828 POINT (-122.18405 47.62007) lighting_store <p>1000 rows \u00d7 3 columns</p> In\u00a0[14]: Copied! <pre>%%time\n\ndf_building = sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=buildings/type=building\"\n)\n\ndf_building = df_building.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n)\n\ndf_building = df_building.limit(200_000)\n</pre> %%time  df_building = sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=buildings/type=building\" )  df_building = df_building.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" )  df_building = df_building.limit(200_000) <pre>                                                                                \r</pre> <pre>CPU times: user 474 ms, sys: 92.9 ms, total: 567 ms\nWall time: 1min 48s\n</pre> In\u00a0[15]: Copied! <pre>%%time\n\nmap_building = SedonaKepler.create_map(df_building, \"Building\")\nmap_building\n</pre> %%time  map_building = SedonaKepler.create_map(df_building, \"Building\") map_building <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>/srv/conda/envs/notebook/lib/python3.9/site-packages/jupyter_client/session.py:718: UserWarning: Message serialization failed with:\nOut of range float values are not JSON compliant\nSupporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n  content = self.pack(content)\n</pre> <pre>CPU times: user 3.91 s, sys: 590 ms, total: 4.5 s\nWall time: 25min 30s\n</pre> <pre>KeplerGl(data={'Building': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20\u2026</pre> In\u00a0[16]: Copied! <pre>%%time\n\ndf_admin = sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=admins/type=administrativeBoundary\"\n)\n\ndf_admin = df_admin.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n)\n</pre> %%time  df_admin = sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=admins/type=administrativeBoundary\" )  df_admin = df_admin.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" ) <pre>                                                                                \r</pre> <pre>CPU times: user 272 ms, sys: 57.9 ms, total: 330 ms\nWall time: 1min 1s\n</pre> In\u00a0[17]: Copied! <pre>%%time\n\nmap_admin = SedonaKepler.create_map(df_admin, \"Admin\")\n\nmap_admin\n</pre> %%time  map_admin = SedonaKepler.create_map(df_admin, \"Admin\")  map_admin <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>[Stage 34:==================================================&gt;     (10 + 1) / 11]\r</pre> <pre>CPU times: user 96.4 ms, sys: 37.9 ms, total: 134 ms\nWall time: 4min 3s\n</pre> <pre>                                                                                \r</pre> <pre>KeplerGl(data={'Admin': {'index': [], 'columns': ['id', 'updatetime', 'version', 'names', 'adminlevel', 'marit\u2026</pre> In\u00a0[18]: Copied! <pre>%%time\n\ndf_locality = sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=admins/type=locality\"\n)\n\ndf_locality = df_locality.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n)\n</pre> %%time  df_locality = sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=admins/type=locality\" )  df_locality = df_locality.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" ) <pre>                                                                                \r</pre> <pre>CPU times: user 280 ms, sys: 59 ms, total: 339 ms\nWall time: 1min 5s\n</pre> In\u00a0[19]: Copied! <pre>%%time\n\nmap_locality = SedonaKepler.create_map(df_locality, \"Locality\")\n\nmap_locality\n</pre> %%time  map_locality = SedonaKepler.create_map(df_locality, \"Locality\")  map_locality <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>[Stage 37:====================================================&gt;   (14 + 1) / 15]\r</pre> <pre>CPU times: user 130 ms, sys: 28.7 ms, total: 158 ms\nWall time: 4min 47s\n</pre> <pre>                                                                                \r</pre> <pre>KeplerGl(data={'Locality': {'index': [], 'columns': ['id', 'updatetime', 'version', 'names', 'adminlevel', 'ma\u2026</pre> In\u00a0[20]: Copied! <pre>%%time\n\ndf_connector = sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=transportation/type=connector\"\n)\n\ndf_connector = df_connector.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n)\n</pre> %%time  df_connector = sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=transportation/type=connector\" )  df_connector = df_connector.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" ) <pre>                                                                                \r</pre> <pre>CPU times: user 409 ms, sys: 93.6 ms, total: 502 ms\nWall time: 1min 42s\n</pre> In\u00a0[25]: Copied! <pre>%%time\n\nmap_connector = SedonaKepler.create_map(df_connector.drop(\"updatetime\"), \"Connector\")\n\nmap_connector\n</pre> %%time  map_connector = SedonaKepler.create_map(df_connector.drop(\"updatetime\"), \"Connector\")  map_connector <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>                                                                                \r</pre> <pre>CPU times: user 1.92 s, sys: 139 ms, total: 2.06 s\nWall time: 9min 52s\n</pre> <pre>/srv/conda/envs/notebook/lib/python3.9/site-packages/jupyter_client/session.py:718: UserWarning: Message serialization failed with:\nOut of range float values are not JSON compliant\nSupporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n  content = self.pack(content)\n</pre> <pre>KeplerGl(data={'Connector': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2\u2026</pre> In\u00a0[22]: Copied! <pre>%%time\n\ndf_segment = sedona.read.format(\"geoparquet\").load(\n    DATA_LINK + \"theme=transportation/type=segment\"\n)\n\ndf_segment = df_segment.filter(\n    \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\"\n)\n\ndf_segment = df_segment.limit(200000)\n</pre> %%time  df_segment = sedona.read.format(\"geoparquet\").load(     DATA_LINK + \"theme=transportation/type=segment\" )  df_segment = df_segment.filter(     \"ST_Contains(ST_GeomFromWKT('\" + spatial_filter + \"'), geometry) = true\" )  df_segment = df_segment.limit(200000) <pre>                                                                                \r</pre> <pre>CPU times: user 502 ms, sys: 79.8 ms, total: 582 ms\nWall time: 1min 45s\n</pre> In\u00a0[24]: Copied! <pre>%%time\n\nmap_segment = SedonaKepler.create_map(df_segment.drop(\"updatetime\"), \"Segment\")\n\nmap_segment\n</pre> %%time  map_segment = SedonaKepler.create_map(df_segment.drop(\"updatetime\"), \"Segment\")  map_segment <pre>User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n</pre> <pre>                                                                                \r</pre> <pre>CPU times: user 2.63 s, sys: 335 ms, total: 2.96 s\nWall time: 15min 50s\n</pre> <pre>/srv/conda/envs/notebook/lib/python3.9/site-packages/jupyter_client/session.py:718: UserWarning: Message serialization failed with:\nOut of range float values are not JSON compliant\nSupporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n  content = self.pack(content)\n</pre> <pre>KeplerGl(data={'Segment': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\u2026</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#understand-overture-map-data","title":"Understand Overture Map data\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#wherobots-version","title":"Wherobots version\u00b6","text":"<p>Wherobots only releases a version for overturemaps-us-west-2/release/2023-07-26-alpha.0/.</p> <p>This data is in GeoParquet format and data is clustered by their spatial proximity to ensure efficient filter pushdown performance</p>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#omf-versions","title":"OMF versions\u00b6","text":"<p>The following files are official OMF releases. They are GeoParquet files generated by Apache Sedona.</p> <p>However, unlike the Wherobots version, these data might not be in the same file structures and hence the spatial query might be a bit slower.</p>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#create-sedona-context","title":"Create Sedona Context\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#spatial-filter-by-boundary","title":"Spatial filter by boundary\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#pick-a-boundary","title":"Pick a boundary\u00b6","text":"<p>Bellevue city is selected</p> <p>Click here for boundaries of other states</p>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#visualizing-overture-maps","title":"Visualizing Overture Maps\u00b6","text":"<p>Explanation to the each step is similar across the different datasets. Click here to learn more about Overture Maps.</p> <ol> <li><p><code>df = sedona.read.format(\"geoparquet\").load(DATA_LINK+\"theme=XX/type=YY\")</code></p> <p>It reads the dataset mentioned by theme and type, that's stored in GeoParquet format.</p> </li> <li><p><code>df = df.filter(\"ST_Contains(ST_GeomFromWKT('\"+state_boundary+\"'), geometry) = true\")</code></p> <p>This filters out all the data that is not in the mentioned <code>state_boundary</code> string. Please select a state as you wish.</p> <p>ST_GeomFromWKT() - constructs a geometry from WKT (Well Known Text)</p> <p>ST_Contains(A, B) - checks if A fully contains B and returns True</p> </li> <li><p><code>XX_geom = df.selectExpr(\"geometry\")</code></p> <p>Storing geometry column for SedonaKepler.</p> </li> <li><p><code>map = SedonaKepler.create_map(XX_geom, 'XX')</code></p> <p>Creating a map object using SedonaKepler with inputs geometry column and the name of dataset.</p> </li> </ol>"},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#place-dataset","title":"Place Dataset\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#inspect-the-metadata-of-geoparquet-files","title":"Inspect the metadata of GeoParquet files\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#run-a-spatial-range-query","title":"Run a spatial range query\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#write-the-result-back-to-a-small-geoparquet-file","title":"Write the result back to a small GeoParquet file\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#convert-a-part-of-the-result-to-geopandas-and-save-to-different-file-formats","title":"Convert a part of the result to GeoPandas and save to different file formats\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#building-dataset","title":"Building Dataset\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#admins-theme-datasets","title":"Admins Theme Datasets\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#administrative-boundary-dataset","title":"Administrative Boundary Dataset\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#locality-dataset","title":"Locality Dataset\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#transportation-theme-datasets","title":"Transportation Theme Datasets\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#connector-dataset","title":"Connector Dataset\u00b6","text":""},{"location":"usecases/Sedona_OvertureMaps_GeoParquet/#segment-dataset","title":"Segment Dataset\u00b6","text":""},{"location":"usecases/utilities/","title":"Utilities","text":"<p>Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</p> In\u00a0[\u00a0]: Copied! <pre>def getConfig():\n    config = {\n        \"version\": \"v1\",\n        \"config\": {\n            \"visState\": {\n                \"filters\": [],\n                \"layers\": [\n                    {\n                        \"id\": \"ikzru0t\",\n                        \"type\": \"geojson\",\n                        \"config\": {\n                            \"dataId\": \"AirportCount\",\n                            \"label\": \"AirportCount\",\n                            \"color\": [218, 112, 191],\n                            \"highlightColor\": [252, 242, 26, 255],\n                            \"columns\": {\"geojson\": \"geometry\"},\n                            \"isVisible\": True,\n                            \"visConfig\": {\n                                \"opacity\": 0.8,\n                                \"strokeOpacity\": 0.8,\n                                \"thickness\": 0.5,\n                                \"strokeColor\": [18, 92, 119],\n                                \"colorRange\": {\n                                    \"name\": \"Uber Viz Sequential 6\",\n                                    \"type\": \"sequential\",\n                                    \"category\": \"Uber\",\n                                    \"colors\": [\n                                        \"#E6FAFA\",\n                                        \"#C1E5E6\",\n                                        \"#9DD0D4\",\n                                        \"#75BBC1\",\n                                        \"#4BA7AF\",\n                                        \"#00939C\",\n                                        \"#108188\",\n                                        \"#0E7077\",\n                                    ],\n                                },\n                                \"strokeColorRange\": {\n                                    \"name\": \"Global Warming\",\n                                    \"type\": \"sequential\",\n                                    \"category\": \"Uber\",\n                                    \"colors\": [\n                                        \"#5A1846\",\n                                        \"#900C3F\",\n                                        \"#C70039\",\n                                        \"#E3611C\",\n                                        \"#F1920E\",\n                                        \"#FFC300\",\n                                    ],\n                                },\n                                \"radius\": 10,\n                                \"sizeRange\": [0, 10],\n                                \"radiusRange\": [0, 50],\n                                \"heightRange\": [0, 500],\n                                \"elevationScale\": 5,\n                                \"enableElevationZoomFactor\": True,\n                                \"stroked\": False,\n                                \"filled\": True,\n                                \"enable3d\": False,\n                                \"wireframe\": False,\n                            },\n                            \"hidden\": False,\n                            \"textLabel\": [\n                                {\n                                    \"field\": None,\n                                    \"color\": [255, 255, 255],\n                                    \"size\": 18,\n                                    \"offset\": [0, 0],\n                                    \"anchor\": \"start\",\n                                    \"alignment\": \"center\",\n                                }\n                            ],\n                        },\n                        \"visualChannels\": {\n                            \"colorField\": {\"name\": \"AirportCount\", \"type\": \"integer\"},\n                            \"colorScale\": \"quantize\",\n                            \"strokeColorField\": None,\n                            \"strokeColorScale\": \"quantile\",\n                            \"sizeField\": None,\n                            \"sizeScale\": \"linear\",\n                            \"heightField\": None,\n                            \"heightScale\": \"linear\",\n                            \"radiusField\": None,\n                            \"radiusScale\": \"linear\",\n                        },\n                    }\n                ],\n                \"interactionConfig\": {\n                    \"tooltip\": {\n                        \"fieldsToShow\": {\n                            \"AirportCount\": [\n                                {\"name\": \"NAME_EN\", \"format\": None},\n                                {\"name\": \"AirportCount\", \"format\": None},\n                            ]\n                        },\n                        \"compareMode\": False,\n                        \"compareType\": \"absolute\",\n                        \"enabled\": True,\n                    },\n                    \"brush\": {\"size\": 0.5, \"enabled\": False},\n                    \"geocoder\": {\"enabled\": False},\n                    \"coordinate\": {\"enabled\": False},\n                },\n                \"layerBlending\": \"normal\",\n                \"splitMaps\": [],\n                \"animationConfig\": {\"currentTime\": None, \"speed\": 1},\n            },\n            \"mapState\": {\n                \"bearing\": 0,\n                \"dragRotate\": False,\n                \"latitude\": 56.422456606624316,\n                \"longitude\": 9.778836615231771,\n                \"pitch\": 0,\n                \"zoom\": 0.4214991225736964,\n                \"isSplit\": False,\n            },\n            \"mapStyle\": {\n                \"styleType\": \"dark\",\n                \"topLayerGroups\": {},\n                \"visibleLayerGroups\": {\n                    \"label\": True,\n                    \"road\": True,\n                    \"border\": False,\n                    \"building\": True,\n                    \"water\": True,\n                    \"land\": True,\n                    \"3d building\": False,\n                },\n                \"threeDBuildingColor\": [\n                    9.665468314072013,\n                    17.18305478057247,\n                    31.1442867897876,\n                ],\n                \"mapStyles\": {},\n            },\n        },\n    }\n    return config\n</pre> def getConfig():     config = {         \"version\": \"v1\",         \"config\": {             \"visState\": {                 \"filters\": [],                 \"layers\": [                     {                         \"id\": \"ikzru0t\",                         \"type\": \"geojson\",                         \"config\": {                             \"dataId\": \"AirportCount\",                             \"label\": \"AirportCount\",                             \"color\": [218, 112, 191],                             \"highlightColor\": [252, 242, 26, 255],                             \"columns\": {\"geojson\": \"geometry\"},                             \"isVisible\": True,                             \"visConfig\": {                                 \"opacity\": 0.8,                                 \"strokeOpacity\": 0.8,                                 \"thickness\": 0.5,                                 \"strokeColor\": [18, 92, 119],                                 \"colorRange\": {                                     \"name\": \"Uber Viz Sequential 6\",                                     \"type\": \"sequential\",                                     \"category\": \"Uber\",                                     \"colors\": [                                         \"#E6FAFA\",                                         \"#C1E5E6\",                                         \"#9DD0D4\",                                         \"#75BBC1\",                                         \"#4BA7AF\",                                         \"#00939C\",                                         \"#108188\",                                         \"#0E7077\",                                     ],                                 },                                 \"strokeColorRange\": {                                     \"name\": \"Global Warming\",                                     \"type\": \"sequential\",                                     \"category\": \"Uber\",                                     \"colors\": [                                         \"#5A1846\",                                         \"#900C3F\",                                         \"#C70039\",                                         \"#E3611C\",                                         \"#F1920E\",                                         \"#FFC300\",                                     ],                                 },                                 \"radius\": 10,                                 \"sizeRange\": [0, 10],                                 \"radiusRange\": [0, 50],                                 \"heightRange\": [0, 500],                                 \"elevationScale\": 5,                                 \"enableElevationZoomFactor\": True,                                 \"stroked\": False,                                 \"filled\": True,                                 \"enable3d\": False,                                 \"wireframe\": False,                             },                             \"hidden\": False,                             \"textLabel\": [                                 {                                     \"field\": None,                                     \"color\": [255, 255, 255],                                     \"size\": 18,                                     \"offset\": [0, 0],                                     \"anchor\": \"start\",                                     \"alignment\": \"center\",                                 }                             ],                         },                         \"visualChannels\": {                             \"colorField\": {\"name\": \"AirportCount\", \"type\": \"integer\"},                             \"colorScale\": \"quantize\",                             \"strokeColorField\": None,                             \"strokeColorScale\": \"quantile\",                             \"sizeField\": None,                             \"sizeScale\": \"linear\",                             \"heightField\": None,                             \"heightScale\": \"linear\",                             \"radiusField\": None,                             \"radiusScale\": \"linear\",                         },                     }                 ],                 \"interactionConfig\": {                     \"tooltip\": {                         \"fieldsToShow\": {                             \"AirportCount\": [                                 {\"name\": \"NAME_EN\", \"format\": None},                                 {\"name\": \"AirportCount\", \"format\": None},                             ]                         },                         \"compareMode\": False,                         \"compareType\": \"absolute\",                         \"enabled\": True,                     },                     \"brush\": {\"size\": 0.5, \"enabled\": False},                     \"geocoder\": {\"enabled\": False},                     \"coordinate\": {\"enabled\": False},                 },                 \"layerBlending\": \"normal\",                 \"splitMaps\": [],                 \"animationConfig\": {\"currentTime\": None, \"speed\": 1},             },             \"mapState\": {                 \"bearing\": 0,                 \"dragRotate\": False,                 \"latitude\": 56.422456606624316,                 \"longitude\": 9.778836615231771,                 \"pitch\": 0,                 \"zoom\": 0.4214991225736964,                 \"isSplit\": False,             },             \"mapStyle\": {                 \"styleType\": \"dark\",                 \"topLayerGroups\": {},                 \"visibleLayerGroups\": {                     \"label\": True,                     \"road\": True,                     \"border\": False,                     \"building\": True,                     \"water\": True,                     \"land\": True,                     \"3d building\": False,                 },                 \"threeDBuildingColor\": [                     9.665468314072013,                     17.18305478057247,                     31.1442867897876,                 ],                 \"mapStyles\": {},             },         },     }     return config"},{"location":"usecases/contrib/ApacheSedonaImageFilter/","title":"ApacheSedonaImageFilter","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre># EXECUTAR NO TERMINAL\n# pip install pandas\n# pip install apache-sedona\n# COPIAR TIF PARA PASTA RASTER/BIG\n# EXECUTAR FORBIGRASTER para dividir a Imagem em Imagens menores\n\n## TODO - ENCONTRAR FORMA DE COPIAR DIRETO PARA O HADOOP PELO USU\u00c0RIO (PARA FAZER PELO JUPYTER OLHAR ANOTACAO NO FIM DO ARQUIVO ForBigRaster)\n# sudo docker exec -it hadoop bash\n# hadoop fs -copyFromLocal /opt/workspace/raster/* /\n</pre> # EXECUTAR NO TERMINAL # pip install pandas # pip install apache-sedona # COPIAR TIF PARA PASTA RASTER/BIG # EXECUTAR FORBIGRASTER para dividir a Imagem em Imagens menores  ## TODO - ENCONTRAR FORMA DE COPIAR DIRETO PARA O HADOOP PELO USU\u00c0RIO (PARA FAZER PELO JUPYTER OLHAR ANOTACAO NO FIM DO ARQUIVO ForBigRaster) # sudo docker exec -it hadoop bash # hadoop fs -copyFromLocal /opt/workspace/raster/* / In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    LongType,\n    IntegerType,\n    DoubleType,\n    ArrayType,\n)\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\n</pre> from IPython.display import display, HTML from pyspark.sql import SparkSession from pyspark import StorageLevel import pandas as pd from pyspark.sql.types import (     StructType,     StructField,     StringType,     LongType,     IntegerType,     DoubleType,     ArrayType, ) from pyspark.sql.functions import regexp_replace from sedona.register import SedonaRegistrator from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit In\u00a0[3]: Copied! <pre>spark = (\n    SparkSession.builder.appName(\"Demo-app\")\n    .enableHiveSupport()\n    .master(\"local[*]\")\n    .master(\"spark://spark-master:7077\")\n    .config(\"spark.executor.memory\", \"15G\")\n    .config(\"spark.driver.maxResultSize\", \"15G\")\n    .config(\"spark.serializer\", KryoSerializer.getName)\n    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",\n    )\n    .getOrCreate()\n)\n#     config(\"spark.rpc.message.maxSize\", 2047).\\\n# rdd = spark.sparkContext.parallelize(range(1000))\n# rdd.takeSample(False, 5)\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</pre> spark = (     SparkSession.builder.appName(\"Demo-app\")     .enableHiveSupport()     .master(\"local[*]\")     .master(\"spark://spark-master:7077\")     .config(\"spark.executor.memory\", \"15G\")     .config(\"spark.driver.maxResultSize\", \"15G\")     .config(\"spark.serializer\", KryoSerializer.getName)     .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",     )     .getOrCreate() ) #     config(\"spark.rpc.message.maxSize\", 2047).\\ # rdd = spark.sparkContext.parallelize(range(1000)) # rdd.takeSample(False, 5)  SedonaRegistrator.registerAll(spark) sc = spark.sparkContext <pre>Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\norg.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-158e8878-0532-4bc3-b6b9-016f34becad3;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating in central\n\tfound org.locationtech.jts#jts-core;1.18.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n\tfound org.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating in central\n\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating in central\n\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n:: resolution report :: resolve 594ms :: artifacts dl 5ms\n\t:: modules in use:\n\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n\torg.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\t:: evicted modules:\n\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-158e8878-0532-4bc3-b6b9-016f34becad3\n\tconfs: [default]\n\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n21/12/29 16:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \r</pre> In\u00a0[4]: Copied! <pre># Path to directory of geotiff images\nDATA_DIR = \"hdfs://776faf4d6a1e:8020/tmp/\"\ndf = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR)\n</pre> # Path to directory of geotiff images DATA_DIR = \"hdfs://776faf4d6a1e:8020/tmp/\" df = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR) In\u00a0[5]: Copied! <pre>df.cache()\ndf.printSchema()\n</pre> df.cache() df.printSchema() <pre>root\n |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- wkt: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nBands: integer (nullable = true)\n |    |-- data: array (nullable = true)\n |    |    |-- element: double (containsNull = true)\n\n</pre> In\u00a0[6]: Copied! <pre>df.is_cached\n</pre> df.is_cached Out[6]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># Java Heap Out Of Memory  =&gt; Ir nas m\u00e1quinas e aumentar o export _JAVA_OPTIONS=\"-Xmx15g\"\n# Java lang Assertion Error image is too large =&gt;\ndf = df.selectExpr(\n    \"image.origin as origin\",\n    \"ST_GeomFromWkt(image.wkt) as Geom\",\n    \"image.height as height\",\n    \"image.width as width\",\n    \"image.data as data\",\n    \"image.nBands as bands\",\n).cache()\ndf.show(5)\n# df.count()\n</pre> # Java Heap Out Of Memory  =&gt; Ir nas m\u00e1quinas e aumentar o export _JAVA_OPTIONS=\"-Xmx15g\" # Java lang Assertion Error image is too large =&gt; df = df.selectExpr(     \"image.origin as origin\",     \"ST_GeomFromWkt(image.wkt) as Geom\",     \"image.height as height\",     \"image.width as width\",     \"image.data as data\",     \"image.nBands as bands\", ).cache() df.show(5) # df.count() <pre>[Stage 2:&gt;                                                          (0 + 1) / 1]\r</pre> In\u00a0[\u00a0]: Copied! <pre># ,\"RS_GetBand(data, 2,bands) as Band2\",\"RS_GetBand(data, 3,bands) as Band3\", \"RS_GetBand(data, 4,bands) as Band4\"\ndf = df.selectExpr(\n    \"Geom\",\n    \"RS_GetBand(data, 1,bands) as Band1\",\n    \"RS_GetBand(data, 2,bands) as Band2\",\n    \"RS_GetBand(data, 3,bands) as Band3\",\n    \"RS_GetBand(data, 4,bands) as Band4\",\n).cache()\ndf.createOrReplaceTempView(\"allbands\")\ndf.show(5)\n</pre> # ,\"RS_GetBand(data, 2,bands) as Band2\",\"RS_GetBand(data, 3,bands) as Band3\", \"RS_GetBand(data, 4,bands) as Band4\" df = df.selectExpr(     \"Geom\",     \"RS_GetBand(data, 1,bands) as Band1\",     \"RS_GetBand(data, 2,bands) as Band2\",     \"RS_GetBand(data, 3,bands) as Band3\",     \"RS_GetBand(data, 4,bands) as Band4\", ).cache() df.createOrReplaceTempView(\"allbands\") df.show(5) In\u00a0[\u00a0]: Copied! <pre># spark.catalog.cacheTable('df')\n# spark.catalog.isCached(tableName='df')\n</pre> # spark.catalog.cacheTable('df') # spark.catalog.isCached(tableName='df') In\u00a0[\u00a0]: Copied! <pre>NomalizedDifference = df.selectExpr(\n    \"RS_NormalizedDifference(Band1, Band2) as normDiff\"\n).cache()\nNomalizedDifference.show(5)\n</pre> NomalizedDifference = df.selectExpr(     \"RS_NormalizedDifference(Band1, Band2) as normDiff\" ).cache() NomalizedDifference.show(5) In\u00a0[\u00a0]: Copied! <pre>meanDF = df.selectExpr(\"RS_Mean(Band1) as mean\").cache()\nmeanDF.show(5)\n</pre> meanDF = df.selectExpr(\"RS_Mean(Band1) as mean\").cache() meanDF.show(5) In\u00a0[\u00a0]: Copied! <pre>modeDF = df.selectExpr(\"RS_Mode(Band1) as mode\").cache()\nmodeDF.show(5)\n</pre> modeDF = df.selectExpr(\"RS_Mode(Band1) as mode\").cache() modeDF.show(5) In\u00a0[\u00a0]: Copied! <pre>greaterthanDF = spark.sql(\n    \"Select RS_GreaterThan(Band1,1000.0) as greaterthan from allbands\"\n).cache()\ngreaterthanDF.show()\n</pre> greaterthanDF = spark.sql(     \"Select RS_GreaterThan(Band1,1000.0) as greaterthan from allbands\" ).cache() greaterthanDF.show() In\u00a0[\u00a0]: Copied! <pre>greaterthanEqualDF = spark.sql(\n    \"Select RS_GreaterThanEqual(Band1,360.0) as greaterthanEqual from allbands\"\n).cache()\ngreaterthanEqualDF.show()\n</pre> greaterthanEqualDF = spark.sql(     \"Select RS_GreaterThanEqual(Band1,360.0) as greaterthanEqual from allbands\" ).cache() greaterthanEqualDF.show() In\u00a0[\u00a0]: Copied! <pre>lessthanDF = spark.sql(\n    \"Select RS_LessThan(Band1,1000.0) as lessthan from allbands\"\n).cache()\nlessthanDF.show()\n</pre> lessthanDF = spark.sql(     \"Select RS_LessThan(Band1,1000.0) as lessthan from allbands\" ).cache() lessthanDF.show() In\u00a0[\u00a0]: Copied! <pre>lessthanEqualDF = spark.sql(\n    \"Select RS_LessThanEqual(Band1,2890.0) as lessthanequal from allbands\"\n).cache()\nlessthanEqualDF.show()\n</pre> lessthanEqualDF = spark.sql(     \"Select RS_LessThanEqual(Band1,2890.0) as lessthanequal from allbands\" ).cache() lessthanEqualDF.show() In\u00a0[\u00a0]: Copied! <pre>sumDF = df.selectExpr(\"RS_AddBands(Band1, Band2) as sumOfBand\").cache()\nsumDF.show(5)\n</pre> sumDF = df.selectExpr(\"RS_AddBands(Band1, Band2) as sumOfBand\").cache() sumDF.show(5) In\u00a0[\u00a0]: Copied! <pre>subtractDF = df.selectExpr(\"RS_SubtractBands(Band1, Band2) as diffOfBand\").cache()\nsubtractDF.show(5)\n</pre> subtractDF = df.selectExpr(\"RS_SubtractBands(Band1, Band2) as diffOfBand\").cache() subtractDF.show(5) In\u00a0[\u00a0]: Copied! <pre>multiplyDF = df.selectExpr(\"RS_MultiplyBands(Band1, Band2) as productOfBand\").cache()\nmultiplyDF.show(5)\n</pre> multiplyDF = df.selectExpr(\"RS_MultiplyBands(Band1, Band2) as productOfBand\").cache() multiplyDF.show(5) In\u00a0[\u00a0]: Copied! <pre>divideDF = df.selectExpr(\"RS_DivideBands(Band1, Band2) as divisionOfBand\").cache()\ndivideDF.show(5)\n</pre> divideDF = df.selectExpr(\"RS_DivideBands(Band1, Band2) as divisionOfBand\").cache() divideDF.show(5) In\u00a0[\u00a0]: Copied! <pre>mulfacDF = df.selectExpr(\"RS_MultiplyFactor(Band2, 2) as target\").cache()\nmulfacDF.show(5)\n</pre> mulfacDF = df.selectExpr(\"RS_MultiplyFactor(Band2, 2) as target\").cache() mulfacDF.show(5) In\u00a0[\u00a0]: Copied! <pre>bitwiseAND = df.selectExpr(\"RS_BitwiseAND(Band1, Band2) as AND\").cache()\nbitwiseAND.show(5)\n</pre> bitwiseAND = df.selectExpr(\"RS_BitwiseAND(Band1, Band2) as AND\").cache() bitwiseAND.show(5) In\u00a0[\u00a0]: Copied! <pre>bitwiseOR = df.selectExpr(\"RS_BitwiseOR(Band1, Band2) as OR\").cache()\nbitwiseOR.show(5)\n</pre> bitwiseOR = df.selectExpr(\"RS_BitwiseOR(Band1, Band2) as OR\").cache() bitwiseOR.show(5) In\u00a0[\u00a0]: Copied! <pre>countDF = df.selectExpr(\"RS_Count(RS_GreaterThan(Band1,1000.0), 1.0) as count\").cache()\ncountDF.show(5)\n</pre> countDF = df.selectExpr(\"RS_Count(RS_GreaterThan(Band1,1000.0), 1.0) as count\").cache() countDF.show(5) In\u00a0[\u00a0]: Copied! <pre>moduloDF = df.selectExpr(\"RS_Modulo(Band1, 21.0) as modulo \").cache()\nmoduloDF.show(5)\n</pre> moduloDF = df.selectExpr(\"RS_Modulo(Band1, 21.0) as modulo \").cache() moduloDF.show(5) In\u00a0[\u00a0]: Copied! <pre>rootDF = df.selectExpr(\"RS_SquareRoot(Band1) as root\").cache()\nrootDF.show(5)\n</pre> rootDF = df.selectExpr(\"RS_SquareRoot(Band1) as root\").cache() rootDF.show(5) In\u00a0[\u00a0]: Copied! <pre>logDiff = df.selectExpr(\"RS_LogicalDifference(Band1, Band2) as loggDifference\").cache()\nlogDiff.show(5)\n</pre> logDiff = df.selectExpr(\"RS_LogicalDifference(Band1, Band2) as loggDifference\").cache() logDiff.show(5) In\u00a0[\u00a0]: Copied! <pre>logOver = df.selectExpr(\"RS_LogicalOver(Band3, Band2) as logicalOver\").cache()\nlogOver.show(5)\n</pre> logOver = df.selectExpr(\"RS_LogicalOver(Band3, Band2) as logicalOver\").cache() logOver.show(5) In\u00a0[\u00a0]: Copied! <pre>df = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR)\ndf = df.selectExpr(\n    \"image.origin as origin\",\n    \"ST_GeomFromWkt(image.wkt) as Geom\",\n    \"image.height as height\",\n    \"image.width as width\",\n    \"image.data as data\",\n    \"image.nBands as bands\",\n).cache()\n\ndf = df.selectExpr(\n    \"RS_GetBand(data,1,bands) as targetband\", \"height\", \"width\", \"bands\", \"Geom\"\n)\ndf_base64 = df.selectExpr(\n    \"Geom\",\n    \"RS_Base64(height,width,RS_Normalize(targetBand), RS_Array(height*width,0.0), RS_Array(height*width, 0.0)) as red\",\n    \"RS_Base64(height,width,RS_Array(height*width, 0.0), RS_Normalize(targetBand), RS_Array(height*width, 0.0)) as green\",\n    \"RS_Base64(height,width,RS_Array(height*width, 0.0),  RS_Array(height*width, 0.0), RS_Normalize(targetBand)) as blue\",\n    \"RS_Base64(height,width,RS_Normalize(targetBand), RS_Normalize(targetBand),RS_Normalize(targetBand)) as RGB\",\n).cache()\ndf_HTML = df_base64.selectExpr(\n    \"Geom\",\n    \"RS_HTML(red) as RedBand\",\n    \"RS_HTML(blue) as BlueBand\",\n    \"RS_HTML(green) as GreenBand\",\n    \"RS_HTML(RGB) as CombinedBand\",\n).cache()\ndf_HTML.show(5)\n</pre> df = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR) df = df.selectExpr(     \"image.origin as origin\",     \"ST_GeomFromWkt(image.wkt) as Geom\",     \"image.height as height\",     \"image.width as width\",     \"image.data as data\",     \"image.nBands as bands\", ).cache()  df = df.selectExpr(     \"RS_GetBand(data,1,bands) as targetband\", \"height\", \"width\", \"bands\", \"Geom\" ) df_base64 = df.selectExpr(     \"Geom\",     \"RS_Base64(height,width,RS_Normalize(targetBand), RS_Array(height*width,0.0), RS_Array(height*width, 0.0)) as red\",     \"RS_Base64(height,width,RS_Array(height*width, 0.0), RS_Normalize(targetBand), RS_Array(height*width, 0.0)) as green\",     \"RS_Base64(height,width,RS_Array(height*width, 0.0),  RS_Array(height*width, 0.0), RS_Normalize(targetBand)) as blue\",     \"RS_Base64(height,width,RS_Normalize(targetBand), RS_Normalize(targetBand),RS_Normalize(targetBand)) as RGB\", ).cache() df_HTML = df_base64.selectExpr(     \"Geom\",     \"RS_HTML(red) as RedBand\",     \"RS_HTML(blue) as BlueBand\",     \"RS_HTML(green) as GreenBand\",     \"RS_HTML(RGB) as CombinedBand\", ).cache() df_HTML.show(5) In\u00a0[\u00a0]: Copied! <pre>display(HTML(df_HTML.limit(2).toPandas().to_html(escape=False)))\n</pre> display(HTML(df_HTML.limit(2).toPandas().to_html(escape=False))) In\u00a0[\u00a0]: Copied! <pre>def SumOfValues(band):\n    total = 0.0\n    for num in band:\n        if num &gt; 1000.0:\n            total += 1\n    return total\n\n\ncalculateSum = udf(SumOfValues, DoubleType())\nspark.udf.register(\"RS_Sum\", calculateSum)\n\nsumDF = df.selectExpr(\"RS_Sum(targetband) as sum\").cache()\nsumDF.show()\n</pre> def SumOfValues(band):     total = 0.0     for num in band:         if num &gt; 1000.0:             total += 1     return total   calculateSum = udf(SumOfValues, DoubleType()) spark.udf.register(\"RS_Sum\", calculateSum)  sumDF = df.selectExpr(\"RS_Sum(targetband) as sum\").cache() sumDF.show() In\u00a0[\u00a0]: Copied! <pre>def generatemask(band, width, height):\n    for i, val in enumerate(band):\n        if (i % width &gt;= 12 and i % width &lt; 26) and (\n            i % height &gt;= 12 and i % height &lt; 26\n        ):\n            band[i] = 255.0\n        else:\n            band[i] = 0.0\n    return band\n\n\nmaskValues = udf(generatemask, ArrayType(DoubleType()))\nspark.udf.register(\"RS_MaskValues\", maskValues)\n\n\ndf_base64 = df.selectExpr(\n    \"Geom\",\n    \"RS_Base64(height,width,RS_Normalize(targetband), RS_Array(height*width,0.0), RS_Array(height*width, 0.0), RS_MaskValues(targetband,width,height)) as region\",\n).cache()\ndf_HTML = df_base64.selectExpr(\"Geom\", \"RS_HTML(region) as selectedregion\").cache()\ndisplay(HTML(df_HTML.limit(2).toPandas().to_html(escape=False)))\n</pre> def generatemask(band, width, height):     for i, val in enumerate(band):         if (i % width &gt;= 12 and i % width &lt; 26) and (             i % height &gt;= 12 and i % height &lt; 26         ):             band[i] = 255.0         else:             band[i] = 0.0     return band   maskValues = udf(generatemask, ArrayType(DoubleType())) spark.udf.register(\"RS_MaskValues\", maskValues)   df_base64 = df.selectExpr(     \"Geom\",     \"RS_Base64(height,width,RS_Normalize(targetband), RS_Array(height*width,0.0), RS_Array(height*width, 0.0), RS_MaskValues(targetband,width,height)) as region\", ).cache() df_HTML = df_base64.selectExpr(\"Geom\", \"RS_HTML(region) as selectedregion\").cache() display(HTML(df_HTML.limit(2).toPandas().to_html(escape=False))) In\u00a0[\u00a0]: Copied! <pre>spark.stop()\n</pre> spark.stop() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/contrib/DownloadImageFromGEE/","title":"DownloadImageFromGEE","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre># NO TERMINAL\n# pip install earthengine-api\n# pip install geemap\n# pip install pywebhdfs\n</pre> # NO TERMINAL # pip install earthengine-api # pip install geemap # pip install pywebhdfs In\u00a0[2]: Copied! <pre># DOWNLOAD FROM GEE\n</pre> # DOWNLOAD FROM GEE In\u00a0[8]: Copied! <pre>import ee\nimport geemap\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date, timedelta\nimport os\n</pre> import ee import geemap from pywebhdfs.webhdfs import PyWebHdfsClient from datetime import date, timedelta import os In\u00a0[4]: Copied! <pre># Inicia o fluxo de autentica\u00e7\u00e3o\nee.Authenticate()\n\n# Inicializa a biblioteca\nee.Initialize()\n</pre> # Inicia o fluxo de autentica\u00e7\u00e3o ee.Authenticate()  # Inicializa a biblioteca ee.Initialize() <p>To authorize access needed by Earth Engine, open the following         URL in a web browser and follow the instructions:</p> <p>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;response_type=code&amp;code_challenge=K-IM1uXz0IQDC8jBaT1Wg1f70o4hf6h3k9KgLmAZalI&amp;code_challenge_method=S256</p> <p>The authorization workflow will generate a code, which you         should paste in the box below</p> <pre>\nSuccessfully saved authorization token.\n</pre> In\u00a0[5]: Copied! <pre># VARIAVEIS GLOBAIS (MUDAR AQUI!)\nend_date = date.today()\npercent_cloud = 100\n# box = [[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin], [xmin, ymin]]\n# https://boundingbox.klokantech.com/\nbox = [\n    [\n        [-54.6306579887, -25.5892766534],\n        [-54.5393341362, -25.5892766534],\n        [-54.5393341362, -25.4046299874],\n        [-54.6306579887, -25.4046299874],\n        [-54.6306579887, -25.5892766534],\n    ]\n]\nboundary = ee.Geometry.Polygon(box, None, False)\ncollection_name = \"COPERNICUS/S2_SR\"\nscale = 10\ncrs = \"EPSG:4326\"\n# CRIE UMA PASTA VAZIA NO DIRETORIO RASTER E COLOQUE O CAMINHO DELA AQUI EM BAIXO\nout_dir = \"raster/sentinel2_tmp\"\nhdfs_dir = \"sentinel2_tmp\"\n</pre> # VARIAVEIS GLOBAIS (MUDAR AQUI!) end_date = date.today() percent_cloud = 100 # box = [[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin], [xmin, ymin]] # https://boundingbox.klokantech.com/ box = [     [         [-54.6306579887, -25.5892766534],         [-54.5393341362, -25.5892766534],         [-54.5393341362, -25.4046299874],         [-54.6306579887, -25.4046299874],         [-54.6306579887, -25.5892766534],     ] ] boundary = ee.Geometry.Polygon(box, None, False) collection_name = \"COPERNICUS/S2_SR\" scale = 10 crs = \"EPSG:4326\" # CRIE UMA PASTA VAZIA NO DIRETORIO RASTER E COLOQUE O CAMINHO DELA AQUI EM BAIXO out_dir = \"raster/sentinel2_tmp\" hdfs_dir = \"sentinel2_tmp\" In\u00a0[9]: Copied! <pre>for i in range(2, 15, 2):\n    start_date = str((end_date - timedelta(days=i)))\n    collection = (\n        ee.ImageCollection(collection_name)\n        .select([\"B2\", \"B3\", \"B4\", \"B8\"])\n        .filterBounds(boundary)\n        .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", percent_cloud)\n        .filterDate(start_date, str(end_date))\n    )\n    geemap.ee_export_image_collection(\n        collection, scale=scale, crs=crs, region=boundary, out_dir=out_dir\n    )\n    #     NOT SURE WHAT F.. THIS BELOW DOES\n    for root, directory, files in os.walk(out_dir):\n        if len(files) == 0:\n            geemap.ee_export_image_collection(\n                collection, scale=scale, crs=crs, region=boundary, out_dir=out_dir\n            )\n        else:\n            print(\"Existem imagens na pasta\")\n</pre> for i in range(2, 15, 2):     start_date = str((end_date - timedelta(days=i)))     collection = (         ee.ImageCollection(collection_name)         .select([\"B2\", \"B3\", \"B4\", \"B8\"])         .filterBounds(boundary)         .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", percent_cloud)         .filterDate(start_date, str(end_date))     )     geemap.ee_export_image_collection(         collection, scale=scale, crs=crs, region=boundary, out_dir=out_dir     )     #     NOT SURE WHAT F.. THIS BELOW DOES     for root, directory, files in os.walk(out_dir):         if len(files) == 0:             geemap.ee_export_image_collection(                 collection, scale=scale, crs=crs, region=boundary, out_dir=out_dir             )         else:             print(\"Existem imagens na pasta\") <pre>Total number of images: 0\n\nTotal number of images: 0\n\nTotal number of images: 1\n\nExporting 1/1: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b15a6ed8d284266b94147d85f741ed13-1fcd749687233dd2cb56e7828e6d1bb8:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\nTotal number of images: 1\n\nExporting 1/1: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b15a6ed8d284266b94147d85f741ed13-cb8dd97c1aeb7dc713e018e86fc54600:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\nTotal number of images: 1\n\nExporting 1/1: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b15a6ed8d284266b94147d85f741ed13-f400c1a106bdb4b10e0a979634ebd16f:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\nTotal number of images: 2\n\nExporting 1/2: 20211221T134209_20211221T134205_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/3aa9a4dd0980782c6450b743424311da-d17433801f5712f60c47b17b71b4281c:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211221T134209_20211221T134205_T21JYM.tif\n\n\nExporting 2/2: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b15a6ed8d284266b94147d85f741ed13-859f18c37de9efdab4faa18bd55e652d:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\nTotal number of images: 2\n\nExporting 1/2: 20211221T134209_20211221T134205_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/3aa9a4dd0980782c6450b743424311da-cef8e1649f70d212459136a58286fce1:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211221T134209_20211221T134205_T21JYM.tif\n\n\nExporting 2/2: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/be5e1a1d63296b074d488e4130f08578-58380cb69d77be8f9c45ae87ec164be5:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\nTotal number of images: 3\n\nExporting 1/3: 20211216T134211_20211216T134211_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b63ccf2edf826be5501daa568c1fc35a-5fc6a742b7115b3978355a433962997c:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211216T134211_20211216T134211_T21JYM.tif\n\n\nExporting 2/3: 20211221T134209_20211221T134205_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/67c944bd3653197b6d1a48a8b7cd669f-01955ac97af9f3e738a80e65c7870921:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211221T134209_20211221T134205_T21JYM.tif\n\n\nExporting 3/3: 20211226T134211_20211226T134212_T21JYM.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/7b96db8dad91ced9e4ca0e746ab92377-ddb37508d7bc43278abca3a0c610cc81:getPixels\nPlease wait ...\nData downloaded to /opt/workspace/raster/sentinel2_tmp/20211226T134211_20211226T134212_T21JYM.tif\n\n\nExistem imagens na pasta\n</pre> In\u00a0[\u00a0]: Copied! <pre>hdfs = PyWebHdfsClient(host=\"179.106.229.159\", port=\"50070\", user_name=\"root\")\nhdfs.delete_file_dir(hdfs_dir, recursive=True)\nhdfs.make_dir(hdfs_dir)\n</pre> hdfs = PyWebHdfsClient(host=\"179.106.229.159\", port=\"50070\", user_name=\"root\") hdfs.delete_file_dir(hdfs_dir, recursive=True) hdfs.make_dir(hdfs_dir) In\u00a0[\u00a0]: Copied! <pre># Adicionar imagens no HDFS\n\n# No terminal do JupyterLab\n# sudo docker exec -itu 0 hadoop passwd (primeira vez) - hoje \u00e9 admin123\n# ssh hadoop\n# cd ..\n# cd hadoop/bin\n# ./hadoop fs -copyFromLocal /opt/workspace/raster/sentinel2_tmp/* /sentinel2_tmp\n</pre> # Adicionar imagens no HDFS  # No terminal do JupyterLab # sudo docker exec -itu 0 hadoop passwd (primeira vez) - hoje \u00e9 admin123 # ssh hadoop # cd .. # cd hadoop/bin # ./hadoop fs -copyFromLocal /opt/workspace/raster/sentinel2_tmp/* /sentinel2_tmp"},{"location":"usecases/contrib/NdviSentinelApacheSedona/","title":"NdviSentinelApacheSedona","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre># pip install sklearn\n# pip install pyarrow\n# pip install fsspec\n</pre> # pip install sklearn # pip install pyarrow # pip install fsspec In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    LongType,\n    IntegerType,\n    DoubleType,\n    ArrayType,\n)\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit, flatten\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date\n</pre> from IPython.display import display, HTML from pyspark.sql import SparkSession from pyspark import StorageLevel import pandas as pd from pyspark.sql.types import (     StructType,     StructField,     StringType,     LongType,     IntegerType,     DoubleType,     ArrayType, ) from pyspark.sql.functions import regexp_replace from sedona.register import SedonaRegistrator from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit, flatten from pywebhdfs.webhdfs import PyWebHdfsClient from datetime import date In\u00a0[3]: Copied! <pre>analise_folder = \"analise_teste_\" + str(date.today())\nhdfs = PyWebHdfsClient(host=\"179.106.229.159\", port=\"50070\", user_name=\"root\")\nhdfs.delete_file_dir(analise_folder, recursive=True)\n</pre> analise_folder = \"analise_teste_\" + str(date.today()) hdfs = PyWebHdfsClient(host=\"179.106.229.159\", port=\"50070\", user_name=\"root\") hdfs.delete_file_dir(analise_folder, recursive=True) Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># spark.scheduler.mode', 'FAIR'\nspark = (\n    SparkSession.builder.appName(\"Sentinel-app\")\n    .enableHiveSupport()\n    .master(\"local[*]\")\n    .master(\"spark://spark-master:7077\")\n    .config(\"spark.executor.memory\", \"15G\")\n    .config(\"spark.driver.maxResultSize\", \"135G\")\n    .config(\"spark.sql.shuffle.partitions\", \"500\")\n    .config(\" spark.sql.adaptive.coalescePartitions.enabled\", True)\n    .config(\"spark.sql.adaptive.enabled\", True)\n    .config(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", 125)\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n    .config(\"spark.sql.execution.arrow.fallback.enabled\", True)\n    .config(\"spark.kryoserializer.buffer.max\", 2047)\n    .config(\"spark.serializer\", KryoSerializer.getName)\n    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",\n    )\n    .enableHiveSupport()\n    .getOrCreate()\n)\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</pre> # spark.scheduler.mode', 'FAIR' spark = (     SparkSession.builder.appName(\"Sentinel-app\")     .enableHiveSupport()     .master(\"local[*]\")     .master(\"spark://spark-master:7077\")     .config(\"spark.executor.memory\", \"15G\")     .config(\"spark.driver.maxResultSize\", \"135G\")     .config(\"spark.sql.shuffle.partitions\", \"500\")     .config(\" spark.sql.adaptive.coalescePartitions.enabled\", True)     .config(\"spark.sql.adaptive.enabled\", True)     .config(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", 125)     .config(\"spark.sql.execution.arrow.pyspark.enabled\", True)     .config(\"spark.sql.execution.arrow.fallback.enabled\", True)     .config(\"spark.kryoserializer.buffer.max\", 2047)     .config(\"spark.serializer\", KryoSerializer.getName)     .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",     )     .enableHiveSupport()     .getOrCreate() )  SedonaRegistrator.registerAll(spark) sc = spark.sparkContext <pre>Warning: Ignoring non-Spark config property:  spark.sql.adaptive.coalescePartitions.enabled\nIvy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\norg.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-475a8539-e626-41a7-8cca-cb3c72ad1694;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating in central\n\tfound org.locationtech.jts#jts-core;1.18.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n\tfound org.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating in central\n\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating in central\n\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n:: resolution report :: resolve 503ms :: artifacts dl 8ms\n\t:: modules in use:\n\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n\torg.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\t:: evicted modules:\n\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-475a8539-e626-41a7-8cca-cb3c72ad1694\n\tconfs: [default]\n\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n22/01/06 20:04:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/01/06 20:04:45 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:04:45 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n                                                                                \r</pre> In\u00a0[5]: Copied! <pre># Path to directory of geotiff images\nDATA_DIR = \"hdfs://776faf4d6a1e:8020/sentinel2_tmp/*\"\ndf = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR)\n</pre> # Path to directory of geotiff images DATA_DIR = \"hdfs://776faf4d6a1e:8020/sentinel2_tmp/*\" df = spark.read.format(\"geotiff\").option(\"dropInvalid\", True).load(DATA_DIR) <pre>                                                                                \r</pre> In\u00a0[6]: Copied! <pre># SUPER IMPORTANT ULTRA MEGA POWER FOR MEMORY PROBLENS SOLVE\nrdd = spark.sparkContext.parallelize((0, 20))\nprint(\"From local[5]\" + str(rdd.getNumPartitions()))\n</pre> # SUPER IMPORTANT ULTRA MEGA POWER FOR MEMORY PROBLENS SOLVE rdd = spark.sparkContext.parallelize((0, 20)) print(\"From local[5]\" + str(rdd.getNumPartitions())) <pre>From local[5]4\n</pre> In\u00a0[7]: Copied! <pre>df.cache()\ndf.printSchema()\n</pre> df.cache() df.printSchema() <pre>root\n |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- wkt: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nBands: integer (nullable = true)\n |    |-- data: array (nullable = true)\n |    |    |-- element: double (containsNull = true)\n\n</pre> <pre>22/01/06 20:04:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n</pre> In\u00a0[8]: Copied! <pre>from pyspark.sql.functions import monotonically_increasing_id\n\n# add ID\ndf_index = df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\ndf_index.explain()\ndf_index.show(5)\n</pre> from pyspark.sql.functions import monotonically_increasing_id  # add ID df_index = df.select(\"*\").withColumn(\"id\", monotonically_increasing_id()) df_index.explain() df_index.show(5) <pre>== Physical Plan ==\n*(1) Project [image#14, monotonically_increasing_id() AS id#22L]\n+- InMemoryTableScan [image#14]\n      +- InMemoryRelation [image#14], StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- FileScan geotiff [image#14] Batched: false, DataFilters: [], Format: org.apache.spark.sql.sedona_sql.io.GeotiffFileFormat@5f9f8a31, Location: InMemoryFileIndex[hdfs://776faf4d6a1e:8020/sentinel2_tmp/1, hdfs://776faf4d6a1e:8020/sentinel2_tm..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;image:struct&lt;origin:string,wkt:string,height:int,width:int,nBands:int,data:array&lt;double&gt;&gt;&gt;\n\n\n</pre> <pre>[Stage 4:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+--------------------+---+\n|               image| id|\n+--------------------+---+\n|[hdfs://776faf4d6...|  0|\n|[hdfs://776faf4d6...|  1|\n|[hdfs://776faf4d6...|  2|\n|[hdfs://776faf4d6...|  3|\n|[hdfs://776faf4d6...|  4|\n+--------------------+---+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[9]: Copied! <pre># \"image.wkt as Geom\",\ndf_export = df_index.selectExpr(\n    \"id\",\n    \"image.origin as origin\",\n    \"image.height as height\",\n    \"image.width as width\",\n    \"cast(image.data as string) as data\",\n    \"image.nBands as bands\",\n)\nprint(df_export.dtypes)\ndf_export.explain()\ndf_export.createOrReplaceTempView(\"df_export\")\n</pre> # \"image.wkt as Geom\", df_export = df_index.selectExpr(     \"id\",     \"image.origin as origin\",     \"image.height as height\",     \"image.width as width\",     \"cast(image.data as string) as data\",     \"image.nBands as bands\", ) print(df_export.dtypes) df_export.explain() df_export.createOrReplaceTempView(\"df_export\") <pre>[('id', 'bigint'), ('origin', 'string'), ('height', 'int'), ('width', 'int'), ('data', 'string'), ('bands', 'int')]\n== Physical Plan ==\n*(1) Project [id#22L, image#14.origin AS origin#65, image#14.height AS height#66, image#14.width AS width#67, cast(image#14.data as string) AS data#68, image#14.nBands AS bands#69]\n+- *(1) Project [image#14, monotonically_increasing_id() AS id#22L]\n   +- InMemoryTableScan [image#14]\n         +- InMemoryRelation [image#14], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- FileScan geotiff [image#14] Batched: false, DataFilters: [], Format: org.apache.spark.sql.sedona_sql.io.GeotiffFileFormat@5f9f8a31, Location: InMemoryFileIndex[hdfs://776faf4d6a1e:8020/sentinel2_tmp/1, hdfs://776faf4d6a1e:8020/sentinel2_tm..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;image:struct&lt;origin:string,wkt:string,height:int,width:int,nBands:int,data:array&lt;double&gt;&gt;&gt;\n\n\n</pre> In\u00a0[10]: Copied! <pre># df_export.repartition(\"origin\").write.format('csv').option('header', True).partitionBy(\"origin\").mode('overwrite').option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder)\n# df_export.write.format('csv').option('header', True).option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder)\n# start = 0\n# end = 10\n# part_df_export =  spark.sql('select * from df_export where id between '+str(start)+' and '+str(end))\n# part_df_export.show(7)\n</pre> # df_export.repartition(\"origin\").write.format('csv').option('header', True).partitionBy(\"origin\").mode('overwrite').option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder) # df_export.write.format('csv').option('header', True).option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder) # start = 0 # end = 10 # part_df_export =  spark.sql('select * from df_export where id between '+str(start)+' and '+str(end)) # part_df_export.show(7) In\u00a0[11]: Copied! <pre># df_writer = part_df_export.write.format('csv').option('header', True).option('sep', ',')\n# df_writer.save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder)\n</pre> # df_writer = part_df_export.write.format('csv').option('header', True).option('sep', ',') # df_writer.save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder) In\u00a0[12]: Copied! <pre># POR 1 LINHA SER GRANDE O SUFICIENTE PARA ESTOURO DE MEMORIA O COLLECT N\u00c2O FUNCIONA E NEM SALVAR O DF_SPARK DIRETO\n# (NECESS\u00c0RIO TRANFORMAR PARA PANDAS LINHA A LINHA)\n# part_df_export.take(3)\npart_df_export = df_export.take(1)\n# print(part_df_export)\npd.DataFrame(part_df_export).to_csv(\"teste.csv\", sep=\",\", encoding=\"utf-8\")\n</pre> # POR 1 LINHA SER GRANDE O SUFICIENTE PARA ESTOURO DE MEMORIA O COLLECT N\u00c2O FUNCIONA E NEM SALVAR O DF_SPARK DIRETO # (NECESS\u00c0RIO TRANFORMAR PARA PANDAS LINHA A LINHA) # part_df_export.take(3) part_df_export = df_export.take(1) # print(part_df_export) pd.DataFrame(part_df_export).to_csv(\"teste.csv\", sep=\",\", encoding=\"utf-8\") In\u00a0[13]: Copied! <pre>df = df.selectExpr(\n    \"image.origin as origin\",\n    \"ST_GeomFromWkt(image.wkt) as Geom\",\n    \"image.height as height\",\n    \"image.width as width\",\n    \"image.data as data\",\n    \"image.nBands as bands\",\n).cache()\ndf.show(5)\nprint(df.dtypes)\ndf.explain()\n</pre> df = df.selectExpr(     \"image.origin as origin\",     \"ST_GeomFromWkt(image.wkt) as Geom\",     \"image.height as height\",     \"image.width as width\",     \"image.data as data\",     \"image.nBands as bands\", ).cache() df.show(5) print(df.dtypes) df.explain() <pre>22/01/06 20:05:02 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n</pre> <pre>+--------------------+--------------------+------+-----+--------------------+-----+\n|              origin|                Geom|height|width|                data|bands|\n+--------------------+--------------------+------+-----+--------------------+-----+\n|hdfs://776faf4d6a...|POLYGON ((-54.546...|   186|  300|[409.0, 404.0, 41...|    4|\n|hdfs://776faf4d6a...|POLYGON ((-54.546...|   186|  300|[1838.0, 1778.0, ...|    4|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|   199|  257|[931.0, 971.0, 95...|    4|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|   199|  257|[957.0, 995.0, 97...|    4|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|   199|  257|[428.0, 428.0, 43...|    4|\n+--------------------+--------------------+------+-----+--------------------+-----+\nonly showing top 5 rows\n\n[('origin', 'string'), ('Geom', 'udt'), ('height', 'int'), ('width', 'int'), ('data', 'array&lt;double&gt;'), ('bands', 'int')]\n== Physical Plan ==\nInMemoryTableScan [origin#111, Geom#112, height#113, width#114, data#115, bands#116]\n   +- InMemoryRelation [origin#111, Geom#112, height#113, width#114, data#115, bands#116], StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- Project [image#14.origin AS origin#111, st_geomfromwkt(image#14.wkt) AS Geom#112, image#14.height AS height#113, image#14.width AS width#114, image#14.data AS data#115, image#14.nBands AS bands#116]\n            +- InMemoryTableScan [image#14]\n                  +- InMemoryRelation [image#14], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- FileScan geotiff [image#14] Batched: false, DataFilters: [], Format: org.apache.spark.sql.sedona_sql.io.GeotiffFileFormat@5f9f8a31, Location: InMemoryFileIndex[hdfs://776faf4d6a1e:8020/sentinel2_tmp/1, hdfs://776faf4d6a1e:8020/sentinel2_tm..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;image:struct&lt;origin:string,wkt:string,height:int,width:int,nBands:int,data:array&lt;double&gt;&gt;&gt;\n\n\n</pre> In\u00a0[14]: Copied! <pre>df = df.selectExpr(\n    \"origin\",\n    \"Geom\",\n    \"RS_GetBand(data, 1,bands) as B2\",\n    \"RS_GetBand(data, 2,bands) as B3\",\n    \"RS_GetBand(data, 3,bands) as B4\",\n    \"RS_GetBand(data, 4,bands) as B8\",\n    \"RS_Array(height * width, 2.4) as constant_evi_2\",\n    \"RS_Array(height * width, 2.5) as constant_evi_1\",\n    \"RS_Array(height * width, 1.0) as constant_evi_3\",\n    \"RS_Array(height * width, -0.5) as constant_tgi_1\",\n    \"RS_Array(height * width, 120.0) as constant_tgi_2\",\n    \"RS_Array(height * width, 0.001) as corrector\",\n).cache()\ndf.createOrReplaceTempView(\"allbands\")\ndf.show(5)\n</pre> df = df.selectExpr(     \"origin\",     \"Geom\",     \"RS_GetBand(data, 1,bands) as B2\",     \"RS_GetBand(data, 2,bands) as B3\",     \"RS_GetBand(data, 3,bands) as B4\",     \"RS_GetBand(data, 4,bands) as B8\",     \"RS_Array(height * width, 2.4) as constant_evi_2\",     \"RS_Array(height * width, 2.5) as constant_evi_1\",     \"RS_Array(height * width, 1.0) as constant_evi_3\",     \"RS_Array(height * width, -0.5) as constant_tgi_1\",     \"RS_Array(height * width, 120.0) as constant_tgi_2\",     \"RS_Array(height * width, 0.001) as corrector\", ).cache() df.createOrReplaceTempView(\"allbands\") df.show(5) <pre>22/01/06 20:05:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n[Stage 7:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|              origin|                Geom|                  B2|                  B3|                  B4|                  B8|      constant_evi_2|      constant_evi_1|      constant_evi_3|      constant_tgi_1|      constant_tgi_2|           corrector|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|hdfs://776faf4d6a...|POLYGON ((-54.546...|[409.0, 404.0, 41...|[713.0, 673.0, 70...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|hdfs://776faf4d6a...|POLYGON ((-54.546...|[1838.0, 1778.0, ...|[1074.0, 1026.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|[931.0, 971.0, 95...|[1282.0, 1356.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|[957.0, 995.0, 97...|[1282.0, 1354.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|hdfs://776faf4d6a...|POLYGON ((-54.274...|[428.0, 428.0, 43...|[880.0, 874.0, 79...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[15]: Copied! <pre># N\u00e3o tem data da imagem\n# N\u00e3o tem parte a qual ela se refere\n# Necess\u00e1rio adicionar\norigin = df.selectExpr(\"origin\")\nsplit_origin = origin.select(split(col(\"origin\"), \"/\"))\nsplit_origin.head()\n# 20211226T134212\nsplit_origin = spark.sql(\n    \"select to_timestamp(REPLACE(SPLIT(SPLIT(origin,'/')[5], '_')[1],'T',' '),'yyyyMMdd HHmmss') as image_date, SPLIT(origin,'/')[4] as feature_name, * from allbands\"\n)\nsplit_origin.show(5)\n</pre> # N\u00e3o tem data da imagem # N\u00e3o tem parte a qual ela se refere # Necess\u00e1rio adicionar origin = df.selectExpr(\"origin\") split_origin = origin.select(split(col(\"origin\"), \"/\")) split_origin.head() # 20211226T134212 split_origin = spark.sql(     \"select to_timestamp(REPLACE(SPLIT(SPLIT(origin,'/')[5], '_')[1],'T',' '),'yyyyMMdd HHmmss') as image_date, SPLIT(origin,'/')[4] as feature_name, * from allbands\" ) split_origin.show(5) <pre>[Stage 9:&gt;                                                          (0 + 1) / 1]\r</pre> <pre>+-------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|         image_date|feature_name|              origin|                Geom|                  B2|                  B3|                  B4|                  B8|      constant_evi_2|      constant_evi_1|      constant_evi_3|      constant_tgi_1|      constant_tgi_2|           corrector|\n+-------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|2021-12-26 13:42:12|          70|hdfs://776faf4d6a...|POLYGON ((-54.546...|[409.0, 404.0, 41...|[713.0, 673.0, 70...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|2021-12-21 13:42:05|          70|hdfs://776faf4d6a...|POLYGON ((-54.546...|[1838.0, 1778.0, ...|[1074.0, 1026.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|2021-12-26 13:42:12|           3|hdfs://776faf4d6a...|POLYGON ((-54.274...|[931.0, 971.0, 95...|[1282.0, 1356.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|2021-12-26 13:42:12|           3|hdfs://776faf4d6a...|POLYGON ((-54.274...|[957.0, 995.0, 97...|[1282.0, 1354.0, ...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n|2021-12-16 13:42:11|           3|hdfs://776faf4d6a...|POLYGON ((-54.274...|[428.0, 428.0, 43...|[880.0, 874.0, 79...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[2.4, 2.4, 2.4, 2...|[2.5, 2.5, 2.5, 2...|[1.0, 1.0, 1.0, 1...|[-0.5, -0.5, -0.5...|[120.0, 120.0, 12...|[0.001, 0.001, 0....|\n+-------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[16]: Copied! <pre># Fator de correcao da banda para ficar com valores entre 0 e 1\ncorrect_origin = split_origin.selectExpr(\n    \"RS_MultiplyBands(B2, corrector) as bluen\",\n    \"RS_MultiplyBands(B3, corrector) as greenn\",\n    \"RS_MultiplyBands(B4, corrector) as redn\",\n    \"RS_MultiplyBands(B8, corrector) as nirn\",\n    \"*\",\n).cache()\ncorrect_origin = correct_origin.selectExpr(\n    \"RS_NormalizedDifference(nirn, redn) as gndvi\",\n    \"RS_SubtractBands(nirn, redn) as sub_nirn_redn\",\n    \"RS_AddBands(nirn,constant_evi_2) as add_nirn_contant_evi_2\",\n    \"RS_AddBands(redn, constant_evi_3) as add_redn_contant_evi_3\",\n    \"RS_DivideBands(nirn, greenn) as div_nirn_greenn\",\n    \"RS_SubtractBands(greenn, redn) as sub_greenn_redn\",\n    \"RS_SubtractBands(redn, greenn) as sub_redn_greenn\",\n    \"RS_SubtractBands(redn, bluen) as sub_redn_bluen\",\n    \"RS_AddBands(greenn, redn) as add_greenn_redn\",\n    \"*\",\n).cache()\n\ncorrect_origin = correct_origin.selectExpr(\n    \"RS_SubtractBands(add_greenn_redn, bluen) as greenn_redn_sub_bluen\",\n    \"RS_AddBands(add_greenn_redn, bluen) as greenn_redn_add_bluen\",\n    \"RS_SubtractBands(sub_greenn_redn, bluen) as sub_greenn_redn_bluen\",\n    \"RS_SubtractBands(sub_redn_greenn, constant_tgi_2) as sub_red_gren_tgi_2\",\n    \"*\",\n).cache()\ncorrect_origin = correct_origin.selectExpr(\n    \"RS_MultiplyFactor(sub_redn_bluen,120) as ms_redn_bluen_120\", \"*\"\n).cache()\ncorrect_origin = correct_origin.selectExpr(\n    \"RS_MultiplyFactor(sub_redn_greenn,190) as ms_redn_greenn_190\", \"*\"\n).cache()\ncorrect_origin = correct_origin.selectExpr(\n    \"RS_SubtractBands(ms_redn_greenn_190,ms_redn_bluen_120) as sub_msrg_190_msrb_120\",\n    \"*\",\n).cache()\n</pre> # Fator de correcao da banda para ficar com valores entre 0 e 1 correct_origin = split_origin.selectExpr(     \"RS_MultiplyBands(B2, corrector) as bluen\",     \"RS_MultiplyBands(B3, corrector) as greenn\",     \"RS_MultiplyBands(B4, corrector) as redn\",     \"RS_MultiplyBands(B8, corrector) as nirn\",     \"*\", ).cache() correct_origin = correct_origin.selectExpr(     \"RS_NormalizedDifference(nirn, redn) as gndvi\",     \"RS_SubtractBands(nirn, redn) as sub_nirn_redn\",     \"RS_AddBands(nirn,constant_evi_2) as add_nirn_contant_evi_2\",     \"RS_AddBands(redn, constant_evi_3) as add_redn_contant_evi_3\",     \"RS_DivideBands(nirn, greenn) as div_nirn_greenn\",     \"RS_SubtractBands(greenn, redn) as sub_greenn_redn\",     \"RS_SubtractBands(redn, greenn) as sub_redn_greenn\",     \"RS_SubtractBands(redn, bluen) as sub_redn_bluen\",     \"RS_AddBands(greenn, redn) as add_greenn_redn\",     \"*\", ).cache()  correct_origin = correct_origin.selectExpr(     \"RS_SubtractBands(add_greenn_redn, bluen) as greenn_redn_sub_bluen\",     \"RS_AddBands(add_greenn_redn, bluen) as greenn_redn_add_bluen\",     \"RS_SubtractBands(sub_greenn_redn, bluen) as sub_greenn_redn_bluen\",     \"RS_SubtractBands(sub_redn_greenn, constant_tgi_2) as sub_red_gren_tgi_2\",     \"*\", ).cache() correct_origin = correct_origin.selectExpr(     \"RS_MultiplyFactor(sub_redn_bluen,120) as ms_redn_bluen_120\", \"*\" ).cache() correct_origin = correct_origin.selectExpr(     \"RS_MultiplyFactor(sub_redn_greenn,190) as ms_redn_greenn_190\", \"*\" ).cache() correct_origin = correct_origin.selectExpr(     \"RS_SubtractBands(ms_redn_greenn_190,ms_redn_bluen_120) as sub_msrg_190_msrb_120\",     \"*\", ).cache() <pre>22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:05:06 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n</pre> In\u00a0[17]: Copied! <pre># bluen = src.read(1, masked=True) / 10000\n# greenn = src.read(2, masked=True) / 10000\n# redn = src.read(3, masked=True) / 10000\n# nirn = src.read(4, masked=True) / 10000\n# evi = 2.5 * (nirn - redn) / (nirn + 2.4 * redn + 1)\n# gci = (nirn / greenn) - 1\n# gli = (2 * greenn - redn - bluen) / (2 * greenn + redn + bluen)\n# gndvi = (nirn - greenn) / (nirn + greenn)\n# tgi = (-0.5) * (190 * (redn - greenn) - 120 * (redn - bluen))\n# vari = (greenn - redn) / (greenn + redn - bluen)\n\n\ncalculated = correct_origin.selectExpr(\n    \"RS_NormalizedDifference(nirn, redn) as gndvi\",\n    \"RS_DivideBands(RS_MultiplyBands(constant_evi_1, sub_nirn_redn), RS_MultiplyBands(add_nirn_contant_evi_2, add_redn_contant_evi_3)) as evi\",\n    \"RS_SubtractBands(div_nirn_greenn, constant_evi_3) as gci\",\n    \"RS_DivideBands(sub_greenn_redn, greenn_redn_sub_bluen) as vari\",\n    \"RS_DivideBands(RS_MultiplyFactor(sub_greenn_redn_bluen,2),RS_MultiplyFactor(greenn_redn_add_bluen, 2)) as gli\",\n    \"RS_MultiplyBands(constant_tgi_1,sub_msrg_190_msrb_120)  as tgi\",\n    \"origin\",\n    \"image_date\",\n    \"feature_name\",\n).cache()\ncalculated.show(5)\ncalculated.printSchema()\n</pre> # bluen = src.read(1, masked=True) / 10000 # greenn = src.read(2, masked=True) / 10000 # redn = src.read(3, masked=True) / 10000 # nirn = src.read(4, masked=True) / 10000 # evi = 2.5 * (nirn - redn) / (nirn + 2.4 * redn + 1) # gci = (nirn / greenn) - 1 # gli = (2 * greenn - redn - bluen) / (2 * greenn + redn + bluen) # gndvi = (nirn - greenn) / (nirn + greenn) # tgi = (-0.5) * (190 * (redn - greenn) - 120 * (redn - bluen)) # vari = (greenn - redn) / (greenn + redn - bluen)   calculated = correct_origin.selectExpr(     \"RS_NormalizedDifference(nirn, redn) as gndvi\",     \"RS_DivideBands(RS_MultiplyBands(constant_evi_1, sub_nirn_redn), RS_MultiplyBands(add_nirn_contant_evi_2, add_redn_contant_evi_3)) as evi\",     \"RS_SubtractBands(div_nirn_greenn, constant_evi_3) as gci\",     \"RS_DivideBands(sub_greenn_redn, greenn_redn_sub_bluen) as vari\",     \"RS_DivideBands(RS_MultiplyFactor(sub_greenn_redn_bluen,2),RS_MultiplyFactor(greenn_redn_add_bluen, 2)) as gli\",     \"RS_MultiplyBands(constant_tgi_1,sub_msrg_190_msrb_120)  as tgi\",     \"origin\",     \"image_date\",     \"feature_name\", ).cache() calculated.show(5) calculated.printSchema() <pre>22/01/06 20:05:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n[Stage 10:&gt;                                                         (0 + 1) / 1]\r</pre> <pre>+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+\n|               gndvi|                 evi|                 gci|                vari|                 gli|                 tgi|              origin|         image_date|feature_name|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+\n|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[1.0, 1.0, 1.0, 1...|[2.35, 2.5, 2.45,...|[1.0, 1.0, 1.0, 1...|[43.1949999999999...|hdfs://776faf4d6a...|2021-12-26 13:42:12|          70|\n|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[1.0, 1.0, 1.0, 1...|[-1.41, -1.36, -1...|[1.0, 1.0, 1.0, 1...|[-8.25, -9.210000...|hdfs://776faf4d6a...|2021-12-21 13:42:05|          70|\n|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[1.0, 1.0, 1.0, 1...|[3.65, 3.52, 3.9,...|[1.0, 1.0, 1.0, 1...|[65.93, 70.560000...|hdfs://776faf4d6a...|2021-12-26 13:42:12|           3|\n|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[1.0, 1.0, 1.0, 1...|[3.94, 3.77, 4.17...|[1.0, 1.0, 1.0, 1...|[64.37, 68.929999...|hdfs://776faf4d6a...|2021-12-26 13:42:12|           3|\n|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|[1.0, 1.0, 1.0, 1...|[1.95, 1.96, 2.19...|[1.0, 1.0, 1.0, 1...|[57.9199999999999...|hdfs://776faf4d6a...|2021-12-16 13:42:11|           3|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+\nonly showing top 5 rows\n\nroot\n |-- gndvi: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- evi: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- gci: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- vari: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- gli: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- tgi: array (nullable = false)\n |    |-- element: double (containsNull = true)\n |-- origin: string (nullable = true)\n |-- image_date: timestamp (nullable = true)\n |-- feature_name: string (nullable = true)\n\n</pre> <pre>                                                                                \r</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[18]: Copied! <pre>calculated_mean = calculated.selectExpr(\n    \"RS_Mean(gndvi) as gndvi\",\n    \"RS_Mean(evi) as evi\",\n    \"RS_Mean(gci) as gci\",\n    \"RS_Mean(vari) as vari\",\n    \"RS_Mean(gli) as gli\",\n    \"RS_Mean(tgi) as tgi\",\n    \"origin\",\n    \"image_date\",\n    \"feature_name\",\n).cache()\ncalculated_mean.show(5)\ncalculated_mean.printSchema()\ncalculated_mean.createOrReplaceTempView(\"all_mean\")\n</pre> calculated_mean = calculated.selectExpr(     \"RS_Mean(gndvi) as gndvi\",     \"RS_Mean(evi) as evi\",     \"RS_Mean(gci) as gci\",     \"RS_Mean(vari) as vari\",     \"RS_Mean(gli) as gli\",     \"RS_Mean(tgi) as tgi\",     \"origin\",     \"image_date\",     \"feature_name\", ).cache() calculated_mean.show(5) calculated_mean.printSchema() calculated_mean.createOrReplaceTempView(\"all_mean\") <pre>22/01/06 20:05:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n</pre> <pre>+-----+---+---+----+---+------+--------------------+-------------------+------------+\n|gndvi|evi|gci|vari|gli|   tgi|              origin|         image_date|feature_name|\n+-----+---+---+----+---+------+--------------------+-------------------+------------+\n|  0.0|0.0|1.0| 0.0|1.0| -64.0|hdfs://776faf4d6a...|2021-12-26 13:42:12|          70|\n|  0.0|0.0|1.0| 0.0|1.0|-94.53|hdfs://776faf4d6a...|2021-12-21 13:42:05|          70|\n|  0.0|0.0|1.0| 0.0|1.0|-49.08|hdfs://776faf4d6a...|2021-12-26 13:42:12|           3|\n|  0.0|0.0|1.0| 0.0|1.0|-50.85|hdfs://776faf4d6a...|2021-12-26 13:42:12|           3|\n|  0.0|0.0|1.0| 0.0|1.0| -23.8|hdfs://776faf4d6a...|2021-12-16 13:42:11|           3|\n+-----+---+---+----+---+------+--------------------+-------------------+------------+\nonly showing top 5 rows\n\nroot\n |-- gndvi: double (nullable = false)\n |-- evi: double (nullable = false)\n |-- gci: double (nullable = false)\n |-- vari: double (nullable = false)\n |-- gli: double (nullable = false)\n |-- tgi: double (nullable = false)\n |-- origin: string (nullable = true)\n |-- image_date: timestamp (nullable = true)\n |-- feature_name: string (nullable = true)\n\n</pre> In\u00a0[35]: Copied! <pre># POR 1 LINHA SER GRANDE O SUFICIENTE PARA ESTOURO DE MEMORIA O COLLECT N\u00c2O FUNCIONA E NEM SALVAR O DF_SPARK DIRETO\n# (NECESS\u00c0RIO TRANFORMAR PARA PANDAS LINHA A LINHA)\n# part_df_export.take(3)\npart_df_export = calculated_mean.limit(10).collect()\nprint(part_df_export)\npd.DataFrame(part_df_export).to_csv(\"teste.csv\", sep=\",\", encoding=\"utf-8\")\n</pre> # POR 1 LINHA SER GRANDE O SUFICIENTE PARA ESTOURO DE MEMORIA O COLLECT N\u00c2O FUNCIONA E NEM SALVAR O DF_SPARK DIRETO # (NECESS\u00c0RIO TRANFORMAR PARA PANDAS LINHA A LINHA) # part_df_export.take(3) part_df_export = calculated_mean.limit(10).collect() print(part_df_export) pd.DataFrame(part_df_export).to_csv(\"teste.csv\", sep=\",\", encoding=\"utf-8\") <pre>[Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-64.0, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/70/20211226T134211_20211226T134212_T21JYM.tif', image_date=datetime.datetime(2021, 12, 26, 13, 42, 12), feature_name='70'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-94.53, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/70/20211221T134209_20211221T134205_T21JYM.tif', image_date=datetime.datetime(2021, 12, 21, 13, 42, 5), feature_name='70'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-49.08, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211226T134211_20211226T134212_T21KYP.tif', image_date=datetime.datetime(2021, 12, 26, 13, 42, 12), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-50.85, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211226T134211_20211226T134212_T21JYN.tif', image_date=datetime.datetime(2021, 12, 26, 13, 42, 12), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-23.8, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211216T134211_20211216T134211_T21KYP.tif', image_date=datetime.datetime(2021, 12, 16, 13, 42, 11), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-24.49, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211216T134211_20211216T134211_T21JYN.tif', image_date=datetime.datetime(2021, 12, 16, 13, 42, 11), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-18.95, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/70/20211216T134211_20211216T134211_T21JYM.tif', image_date=datetime.datetime(2021, 12, 16, 13, 42, 11), feature_name='70'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-374.07, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211221T134209_20211221T134205_T21JYN.tif', image_date=datetime.datetime(2021, 12, 21, 13, 42, 5), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-375.15, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/3/20211221T134209_20211221T134205_T21KYP.tif', image_date=datetime.datetime(2021, 12, 21, 13, 42, 5), feature_name='3'), Row(gndvi=0.0, evi=0.0, gci=1.0, vari=0.0, gli=1.0, tgi=-35.97, origin='hdfs://776faf4d6a1e:8020/sentinel2_tmp/1021/20211226T134211_20211226T134212_T21JYN.tif', image_date=datetime.datetime(2021, 12, 26, 13, 42, 12), feature_name='1021')]\n</pre> In\u00a0[20]: Copied! <pre># SAVE COPY TO HDFS\n# d\u00e1 o mesmo problema de threadshod unsuficiente que ocorre no fit\nimport gc\n\ncollected = gc.collect()\nprint(\"Garbage collector: collected %d objects.\" % collected)\n</pre> # SAVE COPY TO HDFS # d\u00e1 o mesmo problema de threadshod unsuficiente que ocorre no fit import gc  collected = gc.collect() print(\"Garbage collector: collected %d objects.\" % collected) <pre>Garbage collector: collected 199 objects.\n</pre> In\u00a0[21]: Copied! <pre># calculated_mean.repartition(\"origin\").write.format('csv').option('header', True).partitionBy(\"origin\").mode('overwrite').option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder)\n</pre> # calculated_mean.repartition(\"origin\").write.format('csv').option('header', True).partitionBy(\"origin\").mode('overwrite').option('sep', ',').save(\"hdfs://776faf4d6a1e:8020/\"+analise_folder) In\u00a0[22]: Copied! <pre># Random Forest\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import (\n    IndexToString,\n    StringIndexer,\n    VectorIndexer,\n    VectorAssembler,\n)\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder\nimport numpy as np\nfrom pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import OneHotEncoder\n</pre> # Random Forest from pyspark.ml import Pipeline from pyspark.ml.classification import RandomForestClassifier from pyspark.ml.linalg import Vectors from pyspark.ml.feature import (     IndexToString,     StringIndexer,     VectorIndexer,     VectorAssembler, ) from pyspark.ml.evaluation import MulticlassClassificationEvaluator from pyspark.ml.tuning import ParamGridBuilder import numpy as np from pyspark.ml.tuning import CrossValidator from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import OneHotEncoder In\u00a0[23]: Copied! <pre>vari = calculated_mean.select(\"vari\")\nvari.printSchema()\nvari.show(5)\ndf_rf_assembler = calculated_mean.selectExpr(\n    \"vari\", \"gndvi\", \"evi\", \"tgi\", \"gli\", \"cast(feature_name as long) as labels\"\n)\n# FORMATO NECESSARIO PARA O FIT\nfeature_list = [col for col in df_rf_assembler.columns if col != \"labels\"]\nassembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n# rf = RandomForestClassifier(labelCol=\"labels\", featuresCol=\"features\")\ndf_rf_assembler = assembler.transform(df_rf_assembler)\ndf_rf_assembler.show(5)\n# (trainingData, testData) = df_rf_assembler.randomSplit([0.8, 0.2])\n# trainingData.show(5)\n# testData.show(5)\n</pre> vari = calculated_mean.select(\"vari\") vari.printSchema() vari.show(5) df_rf_assembler = calculated_mean.selectExpr(     \"vari\", \"gndvi\", \"evi\", \"tgi\", \"gli\", \"cast(feature_name as long) as labels\" ) # FORMATO NECESSARIO PARA O FIT feature_list = [col for col in df_rf_assembler.columns if col != \"labels\"] assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\") # rf = RandomForestClassifier(labelCol=\"labels\", featuresCol=\"features\") df_rf_assembler = assembler.transform(df_rf_assembler) df_rf_assembler.show(5) # (trainingData, testData) = df_rf_assembler.randomSplit([0.8, 0.2]) # trainingData.show(5) # testData.show(5) <pre>root\n |-- vari: double (nullable = false)\n\n+----+\n|vari|\n+----+\n| 0.0|\n| 0.0|\n| 0.0|\n| 0.0|\n| 0.0|\n+----+\nonly showing top 5 rows\n\n+----+-----+---+------+---+------+--------------------+\n|vari|gndvi|evi|   tgi|gli|labels|            features|\n+----+-----+---+------+---+------+--------------------+\n| 0.0|  0.0|0.0| -64.0|1.0|    70|(5,[3,4],[-64.0,1...|\n| 0.0|  0.0|0.0|-94.53|1.0|    70|(5,[3,4],[-94.53,...|\n| 0.0|  0.0|0.0|-49.08|1.0|     3|(5,[3,4],[-49.08,...|\n| 0.0|  0.0|0.0|-50.85|1.0|     3|(5,[3,4],[-50.85,...|\n| 0.0|  0.0|0.0| -23.8|1.0|     3|(5,[3,4],[-23.8,1...|\n+----+-----+---+------+---+------+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[24]: Copied! <pre>hdfs.delete_file_dir(\"teste\", recursive=True)\n</pre> hdfs.delete_file_dir(\"teste\", recursive=True) Out[24]: <pre>True</pre> In\u00a0[25]: Copied! <pre>import numpy\nfrom numpy import allclose\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.classification import (\n    RandomForestClassifier,\n    RandomForestClassificationModel,\n)\n\n# df = spark.createDataFrame([\n#     (1.0, Vectors.dense(1.0)),\n#     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n\n# stringIndexer = StringIndexer(inputCol=\"labels\", outputCol=\"indexed\")\n# si_model = stringIndexer.fit(df)\n# td = si_model.transform(df)\n# rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42,\n#     leafCol=\"leafId\")\n# rf.getMinWeightFractionPerNode()\n\n# model = rf.fit(td)\n# model.getLabelCol()\n\n# model.setFeaturesCol(\"features\")\n\n# model.setRawPredictionCol(\"newRawPrediction\")\n\n# model.getBootstrap()\n\n# model.getRawPredictionCol()\n\n# model.featureImportances\n\n# allclose(model.treeWeights, [1.0, 1.0, 1.0])\n\n# test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n# model.predict(test0.head().features)\n\n# model.predictRaw(test0.head().features)\n\n# model.predictProbability(test0.head().features)\n\n# result = model.transform(test0).head()\n# result.prediction\n\n# numpy.argmax(result.probability)\n\n# numpy.argmax(result.newRawPrediction)\n\n# result.leafId\n\n# test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n# model.transform(test1).head().prediction\n\n# model.trees\n# temp_path= 'hdfs://776faf4d6a1e:8020/teste'\n# rfc_path = temp_path + \"/rfc\"\n# rf.save(rfc_path)\n# rf2 = RandomForestClassifier.load(rfc_path)\n# rf2.getNumTrees()\n\n# model_path = temp_path + \"/rfc_model\"\n# model.save(model_path)\n# model2 = RandomForestClassificationModel.load(model_path)\n# model.featureImportances == model2.featureImportances\n\n# model.transform(test0).take(1) == model2.transform(test0).take(1)\n</pre> import numpy from numpy import allclose from pyspark.ml.linalg import Vectors from pyspark.ml.feature import StringIndexer from pyspark.ml.classification import (     RandomForestClassifier,     RandomForestClassificationModel, )  # df = spark.createDataFrame([ #     (1.0, Vectors.dense(1.0)), #     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])  # stringIndexer = StringIndexer(inputCol=\"labels\", outputCol=\"indexed\") # si_model = stringIndexer.fit(df) # td = si_model.transform(df) # rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42, #     leafCol=\"leafId\") # rf.getMinWeightFractionPerNode()  # model = rf.fit(td) # model.getLabelCol()  # model.setFeaturesCol(\"features\")  # model.setRawPredictionCol(\"newRawPrediction\")  # model.getBootstrap()  # model.getRawPredictionCol()  # model.featureImportances  # allclose(model.treeWeights, [1.0, 1.0, 1.0])  # test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"]) # model.predict(test0.head().features)  # model.predictRaw(test0.head().features)  # model.predictProbability(test0.head().features)  # result = model.transform(test0).head() # result.prediction  # numpy.argmax(result.probability)  # numpy.argmax(result.newRawPrediction)  # result.leafId  # test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"]) # model.transform(test1).head().prediction  # model.trees # temp_path= 'hdfs://776faf4d6a1e:8020/teste' # rfc_path = temp_path + \"/rfc\" # rf.save(rfc_path) # rf2 = RandomForestClassifier.load(rfc_path) # rf2.getNumTrees()  # model_path = temp_path + \"/rfc_model\" # model.save(model_path) # model2 = RandomForestClassificationModel.load(model_path) # model.featureImportances == model2.featureImportances  # model.transform(test0).take(1) == model2.transform(test0).take(1) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[26]: Copied! <pre># spark.stop()\n</pre> # spark.stop()"},{"location":"usecases/contrib/PostgresqlConnectionApacheSedona/","title":"PostgresqlConnectionApacheSedona","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName(\"db-connection-2\")\n    .master(\"spark://spark-master:7077\")\n    .config(\"spark.executor.memory\", \"10gb\")\n    .config(\"spark.jars\", \"postgresql-42.2.24.jar\")\n    .getOrCreate()\n)\n</pre> from pyspark.sql import SparkSession  spark = (     SparkSession.builder.appName(\"db-connection-2\")     .master(\"spark://spark-master:7077\")     .config(\"spark.executor.memory\", \"10gb\")     .config(\"spark.jars\", \"postgresql-42.2.24.jar\")     .getOrCreate() ) <pre>21/11/22 14:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n</pre> In\u00a0[\u00a0]: Copied! <pre>properties = {\"user\": \"\", \"password\": \"\", \"host\": \"\", \"port\": \"\", \"database\": \"\"}\nproperties[\"url\"] = (\n    \"jdbc:postgresql://\"\n    + properties[\"host\"]\n    + \":\"\n    + properties[\"port\"]\n    + \"/\"\n    + properties[\"database\"]\n)\n</pre> properties = {\"user\": \"\", \"password\": \"\", \"host\": \"\", \"port\": \"\", \"database\": \"\"} properties[\"url\"] = (     \"jdbc:postgresql://\"     + properties[\"host\"]     + \":\"     + properties[\"port\"]     + \"/\"     + properties[\"database\"] ) In\u00a0[\u00a0]: Copied! <pre>jdbcDF = (\n    spark.read.format(\"jdbc\")\n    .options(\n        url=properties[\"url\"],  # jdbc:postgresql://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;\n        dbtable=\"clima.t_indices_prec_cpc\",\n        user=properties[\"user\"],\n        password=properties[\"password\"],\n        driver=\"org.postgresql.Driver\",\n    )\n    .load()\n)\n</pre> jdbcDF = (     spark.read.format(\"jdbc\")     .options(         url=properties[\"url\"],  # jdbc:postgresql://:/         dbtable=\"clima.t_indices_prec_cpc\",         user=properties[\"user\"],         password=properties[\"password\"],         driver=\"org.postgresql.Driver\",     )     .load() ) In\u00a0[\u00a0]: Copied! <pre>jdbcDF.printSchema()\n</pre> jdbcDF.printSchema() <pre>root\n |-- id_pk: integer (nullable = true)\n |-- codigo: integer (nullable = true)\n |-- ano: integer (nullable = true)\n |-- cdd: decimal(10,2) (nullable = true)\n |-- prcptot: decimal(10,2) (nullable = true)\n |-- sdii: decimal(10,2) (nullable = true)\n |-- r20mm: decimal(10,2) (nullable = true)\n |-- r30mm: decimal(10,2) (nullable = true)\n |-- r50mm: decimal(10,2) (nullable = true)\n |-- r80mm: decimal(10,2) (nullable = true)\n |-- r100mm: decimal(10,2) (nullable = true)\n |-- r150mm: decimal(10,2) (nullable = true)\n |-- rx1day: decimal(10,2) (nullable = true)\n |-- rx2day: decimal(10,2) (nullable = true)\n |-- rx5day: decimal(10,2) (nullable = true)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\njdbcDF.filter(\"r100mm &gt; 2000\")\n</pre> %%timeit jdbcDF.filter(\"r100mm &gt; 2000\") <pre>1.15 ms \u00b1 313 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>spark.stop()\n</pre> spark.stop() In\u00a0[\u00a0]: Copied! <pre>! pip install psycopg2-binary\n</pre> ! pip install psycopg2-binary <pre>Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.9/dist-packages (2.9.2)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import psycopg2\n\n\nconnection = psycopg2.connect(\n    user=properties[\"user\"],\n    password=properties[\"password\"],\n    host=properties[\"host\"],\n    port=properties[\"port\"],\n    database=properties[\"database\"],\n)\n</pre> import psycopg2   connection = psycopg2.connect(     user=properties[\"user\"],     password=properties[\"password\"],     host=properties[\"host\"],     port=properties[\"port\"],     database=properties[\"database\"], ) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nfrom pandas import DataFrame\n\ncursor = connection.cursor()\ncursor.execute(\"SELECT * FROM clima.t_indices_prec_cpc t Where r100mm &gt; 2000\")\nnames = [x[0] for x in cursor.description]\nresult = cursor.fetchall()\ndf = DataFrame(result, columns=names)\n</pre> %%timeit from pandas import DataFrame  cursor = connection.cursor() cursor.execute(\"SELECT * FROM clima.t_indices_prec_cpc t Where r100mm &gt; 2000\") names = [x[0] for x in cursor.description] result = cursor.fetchall() df = DataFrame(result, columns=names) <pre>2.57 ms \u00b1 125 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>connection.close()\n</pre> connection.close() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/contrib/VectorAnalisisApacheSedona/","title":"VectorAnalisisApacheSedona","text":"<pre><code>Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n</code></pre> In\u00a0[1]: Copied! <pre>from IPython.display import display, HTML\nfrom pyspark.sql import SparkSession\nfrom pyspark import StorageLevel\nimport pandas as pd\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    LongType,\n    IntegerType,\n    DoubleType,\n    ArrayType,\n)\nfrom pyspark.sql.functions import regexp_replace\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\nfrom pyspark.sql.functions import col, split, expr\nfrom pyspark.sql.functions import udf, lit, flatten\nfrom pywebhdfs.webhdfs import PyWebHdfsClient\nfrom datetime import date\nfrom pyspark.sql.functions import monotonically_increasing_id\nimport json\n</pre> from IPython.display import display, HTML from pyspark.sql import SparkSession from pyspark import StorageLevel import pandas as pd from pyspark.sql.types import (     StructType,     StructField,     StringType,     LongType,     IntegerType,     DoubleType,     ArrayType, ) from pyspark.sql.functions import regexp_replace from sedona.register import SedonaRegistrator from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit from sedona.utils import SedonaKryoRegistrator, KryoSerializer from pyspark.sql.functions import col, split, expr from pyspark.sql.functions import udf, lit, flatten from pywebhdfs.webhdfs import PyWebHdfsClient from datetime import date from pyspark.sql.functions import monotonically_increasing_id import json In\u00a0[2]: Copied! <pre># spark.scheduler.mode', 'FAIR'\nspark = (\n    SparkSession.builder.appName(\"Overpass-API\")\n    .enableHiveSupport()\n    .master(\"local[*]\")\n    .master(\"spark://spark-master:7077\")\n    .config(\"spark.executor.memory\", \"15G\")\n    .config(\"spark.driver.maxResultSize\", \"135G\")\n    .config(\"spark.sql.shuffle.partitions\", \"500\")\n    .config(\" spark.sql.adaptive.coalescePartitions.enabled\", True)\n    .config(\"spark.sql.adaptive.enabled\", True)\n    .config(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", 125)\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n    .config(\"spark.sql.execution.arrow.fallback.enabled\", True)\n    .config(\"spark.kryoserializer.buffer.max\", 2047)\n    .config(\"spark.serializer\", KryoSerializer.getName)\n    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",\n    )\n    .enableHiveSupport()\n    .getOrCreate()\n)\n\nSedonaRegistrator.registerAll(spark)\nsc = spark.sparkContext\n</pre> # spark.scheduler.mode', 'FAIR' spark = (     SparkSession.builder.appName(\"Overpass-API\")     .enableHiveSupport()     .master(\"local[*]\")     .master(\"spark://spark-master:7077\")     .config(\"spark.executor.memory\", \"15G\")     .config(\"spark.driver.maxResultSize\", \"135G\")     .config(\"spark.sql.shuffle.partitions\", \"500\")     .config(\" spark.sql.adaptive.coalescePartitions.enabled\", True)     .config(\"spark.sql.adaptive.enabled\", True)     .config(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", 125)     .config(\"spark.sql.execution.arrow.pyspark.enabled\", True)     .config(\"spark.sql.execution.arrow.fallback.enabled\", True)     .config(\"spark.kryoserializer.buffer.max\", 2047)     .config(\"spark.serializer\", KryoSerializer.getName)     .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\",     )     .enableHiveSupport()     .getOrCreate() )  SedonaRegistrator.registerAll(spark) sc = spark.sparkContext <pre>Warning: Ignoring non-Spark config property:  spark.sql.adaptive.coalescePartitions.enabled\nIvy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\norg.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-c20579c1-d499-4129-ae06-0876b2e0525a;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating in central\n\tfound org.locationtech.jts#jts-core;1.18.0 in central\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n\tfound org.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating in central\n\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating in central\n\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n:: resolution report :: resolve 227ms :: artifacts dl 4ms\n\t:: modules in use:\n\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n\torg.apache.sedona#sedona-core-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.apache.sedona#sedona-sql-3.0_2.12;1.1.0-incubating from central in [default]\n\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\t:: evicted modules:\n\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-c20579c1-d499-4129-ae06-0876b2e0525a\n\tconfs: [default]\n\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n22/02/01 12:43:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/02/01 12:43:56 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n22/02/01 12:43:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.fallback.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.fallback.enabled' instead of it.\n                                                                                \r</pre> In\u00a0[3]: Copied! <pre>import requests\nimport json\n\noverpass_url = \"http://overpass-api.de/api/interpreter\"\n# overpass_query = \"\"\"\n# [out:json];\n# area[\"ISO3166-1\"=\"DE\"][admin_level=2];\n# (node[\"amenity\"=\"biergarten\"](area);\n#  way[\"amenity\"=\"biergarten\"](area);\n#  rel[\"amenity\"=\"biergarten\"](area);\n# );\n# out center;\n# \"\"\"\n\n# overpass_query = \"\"\"\n# [out:json];\n# area[name = \"Foz do Igua\u00e7u\"];\n# (way(area)[\"highway\"~\"^(private|cycleway|footway|bus_guideway|elevator|construction|proposed|bridleway|steps|raceway|motorway_link|path|secondary|motorway|trunk|primary)$\"];&gt;;);\n# out center;\n# \"\"\"\n\noverpass_query = \"\"\"\n[out:json];\narea[name = \"Foz do Igua\u00e7u\"];\nway(area)[\"highway\"~\"\"];\nout geom;\n&gt;;\nout skel qt;\n\"\"\"\n\n# response = requests.get(overpass_url,\n#                         params={'data': overpass_query})\n# data = response.json()\n# hdfs = PyWebHdfsClient(host='179.106.229.159',port='50070', user_name='root')\nfile_name = \"foz_roads_osm.json\"\n# hdfs.delete_file_dir(file_name)\n# hdfs.create_file(file_name, json.dumps(data))\n</pre> import requests import json  overpass_url = \"http://overpass-api.de/api/interpreter\" # overpass_query = \"\"\" # [out:json]; # area[\"ISO3166-1\"=\"DE\"][admin_level=2]; # (node[\"amenity\"=\"biergarten\"](area); #  way[\"amenity\"=\"biergarten\"](area); #  rel[\"amenity\"=\"biergarten\"](area); # ); # out center; # \"\"\"  # overpass_query = \"\"\" # [out:json]; # area[name = \"Foz do Igua\u00e7u\"]; # (way(area)[\"highway\"~\"^(private|cycleway|footway|bus_guideway|elevator|construction|proposed|bridleway|steps|raceway|motorway_link|path|secondary|motorway|trunk|primary)$\"];&gt;;); # out center; # \"\"\"  overpass_query = \"\"\" [out:json]; area[name = \"Foz do Igua\u00e7u\"]; way(area)[\"highway\"~\"\"]; out geom; &gt;; out skel qt; \"\"\"  # response = requests.get(overpass_url, #                         params={'data': overpass_query}) # data = response.json() # hdfs = PyWebHdfsClient(host='179.106.229.159',port='50070', user_name='root') file_name = \"foz_roads_osm.json\" # hdfs.delete_file_dir(file_name) # hdfs.create_file(file_name, json.dumps(data)) In\u00a0[4]: Copied! <pre>path = \"hdfs://776faf4d6a1e:8020/\" + file_name\ndf = spark.read.json(path, multiLine=\"true\")\n</pre> path = \"hdfs://776faf4d6a1e:8020/\" + file_name df = spark.read.json(path, multiLine=\"true\") <pre>22/02/01 12:44:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n</pre> In\u00a0[5]: Copied! <pre>df.printSchema()\n</pre> df.printSchema() <pre>root\n |-- elements: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- bounds: struct (nullable = true)\n |    |    |    |-- maxlat: double (nullable = true)\n |    |    |    |-- maxlon: double (nullable = true)\n |    |    |    |-- minlat: double (nullable = true)\n |    |    |    |-- minlon: double (nullable = true)\n |    |    |-- geometry: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- lat: double (nullable = true)\n |    |    |    |    |-- lon: double (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- lat: double (nullable = true)\n |    |    |-- lon: double (nullable = true)\n |    |    |-- nodes: array (nullable = true)\n |    |    |    |-- element: long (containsNull = true)\n |    |    |-- tags: struct (nullable = true)\n |    |    |    |-- FIXME: string (nullable = true)\n |    |    |    |-- access: string (nullable = true)\n |    |    |    |-- addr:city: string (nullable = true)\n |    |    |    |-- addr:postcode: string (nullable = true)\n |    |    |    |-- alt_name: string (nullable = true)\n |    |    |    |-- area: string (nullable = true)\n |    |    |    |-- barrier: string (nullable = true)\n |    |    |    |-- bicycle: string (nullable = true)\n |    |    |    |-- bridge: string (nullable = true)\n |    |    |    |-- bridge:structure: string (nullable = true)\n |    |    |    |-- bus: string (nullable = true)\n |    |    |    |-- covered: string (nullable = true)\n |    |    |    |-- crossing: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- destination: string (nullable = true)\n |    |    |    |-- destination:ref: string (nullable = true)\n |    |    |    |-- embankment: string (nullable = true)\n |    |    |    |-- fixme: string (nullable = true)\n |    |    |    |-- foot: string (nullable = true)\n |    |    |    |-- footway: string (nullable = true)\n |    |    |    |-- height: string (nullable = true)\n |    |    |    |-- hgv: string (nullable = true)\n |    |    |    |-- highway: string (nullable = true)\n |    |    |    |-- horse: string (nullable = true)\n |    |    |    |-- incline: string (nullable = true)\n |    |    |    |-- junction: string (nullable = true)\n |    |    |    |-- kerb: string (nullable = true)\n |    |    |    |-- lanes: string (nullable = true)\n |    |    |    |-- lanes:backward: string (nullable = true)\n |    |    |    |-- lanes:forward: string (nullable = true)\n |    |    |    |-- layer: string (nullable = true)\n |    |    |    |-- lit: string (nullable = true)\n |    |    |    |-- maxspeed: string (nullable = true)\n |    |    |    |-- motor_vehicle: string (nullable = true)\n |    |    |    |-- motorcar: string (nullable = true)\n |    |    |    |-- motorroad: string (nullable = true)\n |    |    |    |-- mtb:scale: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- name:en: string (nullable = true)\n |    |    |    |-- name:es: string (nullable = true)\n |    |    |    |-- name:etymology:wikidata: string (nullable = true)\n |    |    |    |-- name:pt: string (nullable = true)\n |    |    |    |-- noname: string (nullable = true)\n |    |    |    |-- note: string (nullable = true)\n |    |    |    |-- old_name: string (nullable = true)\n |    |    |    |-- oneway: string (nullable = true)\n |    |    |    |-- operator: string (nullable = true)\n |    |    |    |-- owner: string (nullable = true)\n |    |    |    |-- postal_code: string (nullable = true)\n |    |    |    |-- ref: string (nullable = true)\n |    |    |    |-- sac_scale: string (nullable = true)\n |    |    |    |-- segregated: string (nullable = true)\n |    |    |    |-- service: string (nullable = true)\n |    |    |    |-- sidewalk: string (nullable = true)\n |    |    |    |-- source: string (nullable = true)\n |    |    |    |-- source:name: string (nullable = true)\n |    |    |    |-- sport: string (nullable = true)\n |    |    |    |-- surface: string (nullable = true)\n |    |    |    |-- trail_visibility: string (nullable = true)\n |    |    |    |-- tunnel: string (nullable = true)\n |    |    |    |-- turn:lanes: string (nullable = true)\n |    |    |    |-- turn:lanes:forward: string (nullable = true)\n |    |    |    |-- vehicle: string (nullable = true)\n |    |    |    |-- wheelchair: string (nullable = true)\n |    |    |    |-- width: string (nullable = true)\n |    |    |    |-- wikidata: string (nullable = true)\n |    |    |    |-- wikipedia: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- generator: string (nullable = true)\n |-- osm3s: struct (nullable = true)\n |    |-- copyright: string (nullable = true)\n |    |-- timestamp_areas_base: timestamp (nullable = true)\n |    |-- timestamp_osm_base: timestamp (nullable = true)\n |-- version: double (nullable = true)\n\n</pre> In\u00a0[6]: Copied! <pre>from pyspark.sql.functions import explode, arrays_zip\n\ndf.createOrReplaceTempView(\"df\")\ntb = spark.sql(\"select *, size(elements) total_nodes from df\")\ntb.show(5)\n\nisolate_total_nodes = tb.select(\"total_nodes\").toPandas()\ntotal_nodes = isolate_total_nodes[\"total_nodes\"].iloc[0]\nprint(total_nodes)\n\nisolate_ids = tb.select(\"elements.id\").toPandas()\nids = pd.DataFrame(isolate_ids[\"id\"].iloc[0]).drop_duplicates()\nprint(ids[0].iloc[1])\n\nformatted_df = tb.withColumn(\"id\", explode(\"elements.id\"))\n\nformatted_df.show(5)\n\nformatted_df = tb.withColumn(\n    \"new\",\n    arrays_zip(\"elements.id\", \"elements.geometry\", \"elements.nodes\", \"elements.tags\"),\n).withColumn(\"new\", explode(\"new\"))\n\nformatted_df.show(5)\n\n# formatted_df.printSchema()\n\nformatted_df = formatted_df.select(\n    \"new.0\",\n    \"new.1\",\n    \"new.2\",\n    \"new.3.maxspeed\",\n    \"new.3.incline\",\n    \"new.3.surface\",\n    \"new.3.name\",\n    \"total_nodes\",\n)\nformatted_df = (\n    formatted_df.withColumnRenamed(\"0\", \"id\")\n    .withColumnRenamed(\"1\", \"geom\")\n    .withColumnRenamed(\"2\", \"nodes\")\n    .withColumnRenamed(\"3\", \"tags\")\n)\nformatted_df.createOrReplaceTempView(\"formatted_df\")\nformatted_df.show(5)\n# TODO atualizar daqui para baixo para considerar a linha inteira na l\u00f3gica\npoints_tb = spark.sql(\"select geom, id from formatted_df where geom IS NOT NULL\")\npoints_tb = points_tb.withColumn(\"new\", arrays_zip(\"geom.lat\", \"geom.lon\")).withColumn(\n    \"new\", explode(\"new\")\n)\n\npoints_tb = points_tb.select(\"new.0\", \"new.1\", \"id\")\n\npoints_tb = points_tb.withColumnRenamed(\"0\", \"lat\").withColumnRenamed(\"1\", \"lon\")\npoints_tb.printSchema()\n\npoints_tb.createOrReplaceTempView(\"points_tb\")\n\npoints_tb.show(5)\n\n# teste = spark.sql(\"select st_point(lat, lon) as geom, id from points_tb\")\n\n# teste = spark.sql(\"select collect_list(array(p1.lat,p1.lon)) as line from points_tb p1 where p1.id = 25835738 group by p1.id\")\n# teste.show(5)\n# print(teste.take(1))\n\n# SELECT ST_AsText(ST_Envelope(\n# \t\tST_Collect(\n# \t\t\tST_GeomFromText('LINESTRING(55 75,125 150)'),\n# \t\t\t\tST_Point(20, 80))\n# \t\t\t\t)) As wktenv;\n\ncoordinates_tb = spark.sql(\n    \"select (select collect_list(CONCAT(p1.lat,',',p1.lon)) from points_tb p1 where p1.id = p2.id group by p1.id) as coordinates, p2.id, p2.maxspeed, p2.incline, p2.surface, p2.name, p2.nodes, p2.total_nodes from formatted_df p2\"\n)\ncoordinates_tb.createOrReplaceTempView(\"coordinates_tb\")\ncoordinates_tb.show(5)\n\nroads_tb = spark.sql(\n    \"SELECT ST_LineStringFromText(REPLACE(REPLACE(CAST(coordinates as string),'[',''),']',''), ',') as geom, id, maxspeed, incline, surface, name, nodes, total_nodes FROM coordinates_tb WHERE coordinates IS NOT NULL\"\n)\nroads_tb.createOrReplaceTempView(\"roads_tb\")\nroads_tb.show(5)\n</pre> from pyspark.sql.functions import explode, arrays_zip  df.createOrReplaceTempView(\"df\") tb = spark.sql(\"select *, size(elements) total_nodes from df\") tb.show(5)  isolate_total_nodes = tb.select(\"total_nodes\").toPandas() total_nodes = isolate_total_nodes[\"total_nodes\"].iloc[0] print(total_nodes)  isolate_ids = tb.select(\"elements.id\").toPandas() ids = pd.DataFrame(isolate_ids[\"id\"].iloc[0]).drop_duplicates() print(ids[0].iloc[1])  formatted_df = tb.withColumn(\"id\", explode(\"elements.id\"))  formatted_df.show(5)  formatted_df = tb.withColumn(     \"new\",     arrays_zip(\"elements.id\", \"elements.geometry\", \"elements.nodes\", \"elements.tags\"), ).withColumn(\"new\", explode(\"new\"))  formatted_df.show(5)  # formatted_df.printSchema()  formatted_df = formatted_df.select(     \"new.0\",     \"new.1\",     \"new.2\",     \"new.3.maxspeed\",     \"new.3.incline\",     \"new.3.surface\",     \"new.3.name\",     \"total_nodes\", ) formatted_df = (     formatted_df.withColumnRenamed(\"0\", \"id\")     .withColumnRenamed(\"1\", \"geom\")     .withColumnRenamed(\"2\", \"nodes\")     .withColumnRenamed(\"3\", \"tags\") ) formatted_df.createOrReplaceTempView(\"formatted_df\") formatted_df.show(5) # TODO atualizar daqui para baixo para considerar a linha inteira na l\u00f3gica points_tb = spark.sql(\"select geom, id from formatted_df where geom IS NOT NULL\") points_tb = points_tb.withColumn(\"new\", arrays_zip(\"geom.lat\", \"geom.lon\")).withColumn(     \"new\", explode(\"new\") )  points_tb = points_tb.select(\"new.0\", \"new.1\", \"id\")  points_tb = points_tb.withColumnRenamed(\"0\", \"lat\").withColumnRenamed(\"1\", \"lon\") points_tb.printSchema()  points_tb.createOrReplaceTempView(\"points_tb\")  points_tb.show(5)  # teste = spark.sql(\"select st_point(lat, lon) as geom, id from points_tb\")  # teste = spark.sql(\"select collect_list(array(p1.lat,p1.lon)) as line from points_tb p1 where p1.id = 25835738 group by p1.id\") # teste.show(5) # print(teste.take(1))  # SELECT ST_AsText(ST_Envelope( # \t\tST_Collect( # \t\t\tST_GeomFromText('LINESTRING(55 75,125 150)'), # \t\t\t\tST_Point(20, 80)) # \t\t\t\t)) As wktenv;  coordinates_tb = spark.sql(     \"select (select collect_list(CONCAT(p1.lat,',',p1.lon)) from points_tb p1 where p1.id = p2.id group by p1.id) as coordinates, p2.id, p2.maxspeed, p2.incline, p2.surface, p2.name, p2.nodes, p2.total_nodes from formatted_df p2\" ) coordinates_tb.createOrReplaceTempView(\"coordinates_tb\") coordinates_tb.show(5)  roads_tb = spark.sql(     \"SELECT ST_LineStringFromText(REPLACE(REPLACE(CAST(coordinates as string),'[',''),']',''), ',') as geom, id, maxspeed, incline, surface, name, nodes, total_nodes FROM coordinates_tb WHERE coordinates IS NOT NULL\" ) roads_tb.createOrReplaceTempView(\"roads_tb\") roads_tb.show(5) <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+--------------------+-------+-----------+\n|            elements|           generator|               osm3s|version|total_nodes|\n+--------------------+--------------------+--------------------+-------+-----------+\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|\n+--------------------+--------------------+--------------------+-------+-----------+\n\n36560\n26122619\n</pre> <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+--------------------+-------+-----------+--------+\n|            elements|           generator|               osm3s|version|total_nodes|      id|\n+--------------------+--------------------+--------------------+-------+-----------+--------+\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|25835738|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|26122619|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|26122631|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|26122645|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|26122801|\n+--------------------+--------------------+--------------------+-------+-----------+--------+\nonly showing top 5 rows\n\n</pre> <pre>                                                                                \r</pre> <pre>+--------------------+--------------------+--------------------+-------+-----------+--------------------+\n|            elements|           generator|               osm3s|version|total_nodes|                 new|\n+--------------------+--------------------+--------------------+-------+-----------+--------------------+\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|[25835738, [[-25....|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|[26122619, [[-25....|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|[26122631, [[-25....|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|[26122645, [[-25....|\n|[[[-25.5267745, -...|Overpass API 0.7....|[The data include...|    0.6|      36560|[26122801, [[-25....|\n+--------------------+--------------------+--------------------+-------+-----------+--------------------+\nonly showing top 5 rows\n\n+--------+--------------------+--------------------+--------+-------+--------+--------------------+-----------+\n|      id|                geom|               nodes|maxspeed|incline| surface|                name|total_nodes|\n+--------+--------------------+--------------------+--------+-------+--------+--------------------+-----------+\n|25835738|[[-25.5343718, -5...|[362528326, 28597...|    null|   null|    null|Avenida Costa e S...|      36560|\n|26122619|[[-25.5868602, -5...|[285975617, 59202...|      60|   null|   paved|    Avenida Mercosul|      36560|\n|26122631|[[-25.5868602, -5...|[285975617, 69310...|      60|   null|concrete|Ponte Internacion...|      36560|\n|26122645|[[-25.691851, -54...|[307420349, 16982...|    null|   null|   paved|Rodovia das Catar...|      36560|\n|26122801|[[-25.5637579, -5...|[5514364259, 5514...|    null|   null|    null| Rua Natal Graciotin|      36560|\n+--------+--------------------+--------------------+--------+-------+--------+--------------------+-----------+\nonly showing top 5 rows\n\nroot\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n |-- id: long (nullable = true)\n\n+-----------+-----------+--------+\n|        lat|        lon|      id|\n+-----------+-----------+--------+\n|-25.5343718|-54.5761722|25835738|\n|-25.5343227|-54.5760531|25835738|\n|-25.5341375|-54.5756818|25835738|\n|-25.5337506|-54.5750471|25835738|\n|-25.5334046|-54.5746139|25835738|\n+-----------+-----------+--------+\nonly showing top 5 rows\n\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\n|         coordinates|      id|maxspeed|incline| surface|                name|               nodes|total_nodes|\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\n|[-25.5343718,-54....|25835738|    null|   null|    null|Avenida Costa e S...|[362528326, 28597...|      36560|\n|[-25.5868602,-54....|26122619|      60|   null|   paved|    Avenida Mercosul|[285975617, 59202...|      36560|\n|[-25.5868602,-54....|26122631|      60|   null|concrete|Ponte Internacion...|[285975617, 69310...|      36560|\n|[-25.691851,-54.4...|26122645|    null|   null|   paved|Rodovia das Catar...|[307420349, 16982...|      36560|\n|[-25.5637579,-54....|26122801|    null|   null|    null| Rua Natal Graciotin|[5514364259, 5514...|      36560|\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\nonly showing top 5 rows\n\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\n|                geom|      id|maxspeed|incline| surface|                name|               nodes|total_nodes|\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\n|LINESTRING (-25.5...|25835738|    null|   null|    null|Avenida Costa e S...|[362528326, 28597...|      36560|\n|LINESTRING (-25.5...|26122619|      60|   null|   paved|    Avenida Mercosul|[285975617, 59202...|      36560|\n|LINESTRING (-25.5...|26122631|      60|   null|concrete|Ponte Internacion...|[285975617, 69310...|      36560|\n|LINESTRING (-25.6...|26122645|    null|   null|   paved|Rodovia das Catar...|[307420349, 16982...|      36560|\n|LINESTRING (-25.5...|26122801|    null|   null|    null| Rua Natal Graciotin|[5514364259, 5514...|      36560|\n+--------------------+--------+--------+-------+--------+--------------------+--------------------+-----------+\nonly showing top 5 rows\n\n</pre> In\u00a0[7]: Copied! <pre>print(roads_tb.select(\"geom\").take(1))\n</pre> print(roads_tb.select(\"geom\").take(1)) <pre>[Row(geom=&lt;shapely.geometry.linestring.LineString object at 0x7f8b6c4fefd0&gt;)]\n</pre> In\u00a0[8]: Copied! <pre># N\u00e3o foi considerado que um caminha pode necessitar mais de 1 rua\n\nstart_point = \"-25.4695946,-54.5909028\"\nend_point = \"-25.4786993,-54.57938\"\n\ndistance_tb = spark.sql(\n    \"select nodes, st_distance(geom, st_point(\"\n    + end_point\n    + \")) as distance_toend, st_distance(st_point(\"\n    + start_point\n    + \"), geom) as distance, st_length(geom) * 1000 as geomsize, geom, id, maxspeed, incline, surface, name , total_nodes from roads_tb\"\n)\ndistance_tb.createOrReplaceTempView(\"distance_tb\")\ndistance_tb.show(5)\n\n# considerar dist\u00e2ncia, direcao(ex: 0 180 e etc), inclinacao(up, down, 0%), superficie(asphalt,paved, concrete), velocidade(60 80 50 40 e etc)\nfill_null_tb = spark.sql(\n    \"select nodes, IFNULL(maxspeed, 20) as maxspeed, IFNULL(incline, '0%') as incline, IFNULL(surface, 'soil') as surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from distance_tb\"\n)\nfill_null_tb.createOrReplaceTempView(\"fill_null_tb\")\nfill_null_tb.show(5)\n\nsurface_index_tb = spark.sql(\n    \"select nodes, case surface when 'asphalt'\"\n    + \"then 0.01 when 'concrete'\"\n    + \"then 0.02 when 'paved'\"\n    + \"then 0.03 when 'soil'\"\n    + \"then 0.04 when 'unpaved'\"\n    + \"then 0.04 when 'sett'\"\n    + \"then 0.03 ELSE 0.05 end as surface_index,\"\n    + \"maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from fill_null_tb\"\n)\nsurface_index_tb.createOrReplaceTempView(\"surface_index_tb\")\nsurface_index_tb.show(5)\n\nincline_index_tb = spark.sql(\n    \"select nodes, case incline when 'top' then -0.10 when 'down' then 0.10 when '0%' then 0 end as incline_index, surface_index, maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from surface_index_tb\"\n)\nincline_index_tb.createOrReplaceTempView(\"incline_index_tb\")\nincline_index_tb.show(5)\n\nweight_index_tb = spark.sql(\n    \"select nodes, (maxspeed - (maxspeed * surface_index)) + (maxspeed +(maxspeed * incline_index)) as weight, incline_index, surface_index, maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from incline_index_tb WHERE geomsize IS NOT NULL\"\n)\nweight_index_tb.createOrReplaceTempView(\"weight_index_tb\")\nweight_index_tb.show(5)\n</pre> # N\u00e3o foi considerado que um caminha pode necessitar mais de 1 rua  start_point = \"-25.4695946,-54.5909028\" end_point = \"-25.4786993,-54.57938\"  distance_tb = spark.sql(     \"select nodes, st_distance(geom, st_point(\"     + end_point     + \")) as distance_toend, st_distance(st_point(\"     + start_point     + \"), geom) as distance, st_length(geom) * 1000 as geomsize, geom, id, maxspeed, incline, surface, name , total_nodes from roads_tb\" ) distance_tb.createOrReplaceTempView(\"distance_tb\") distance_tb.show(5)  # considerar dist\u00e2ncia, direcao(ex: 0 180 e etc), inclinacao(up, down, 0%), superficie(asphalt,paved, concrete), velocidade(60 80 50 40 e etc) fill_null_tb = spark.sql(     \"select nodes, IFNULL(maxspeed, 20) as maxspeed, IFNULL(incline, '0%') as incline, IFNULL(surface, 'soil') as surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from distance_tb\" ) fill_null_tb.createOrReplaceTempView(\"fill_null_tb\") fill_null_tb.show(5)  surface_index_tb = spark.sql(     \"select nodes, case surface when 'asphalt'\"     + \"then 0.01 when 'concrete'\"     + \"then 0.02 when 'paved'\"     + \"then 0.03 when 'soil'\"     + \"then 0.04 when 'unpaved'\"     + \"then 0.04 when 'sett'\"     + \"then 0.03 ELSE 0.05 end as surface_index,\"     + \"maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from fill_null_tb\" ) surface_index_tb.createOrReplaceTempView(\"surface_index_tb\") surface_index_tb.show(5)  incline_index_tb = spark.sql(     \"select nodes, case incline when 'top' then -0.10 when 'down' then 0.10 when '0%' then 0 end as incline_index, surface_index, maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from surface_index_tb\" ) incline_index_tb.createOrReplaceTempView(\"incline_index_tb\") incline_index_tb.show(5)  weight_index_tb = spark.sql(     \"select nodes, (maxspeed - (maxspeed * surface_index)) + (maxspeed +(maxspeed * incline_index)) as weight, incline_index, surface_index, maxspeed, incline, surface, name, id, geom, geomsize, distance, distance_toend, total_nodes  from incline_index_tb WHERE geomsize IS NOT NULL\" ) weight_index_tb.createOrReplaceTempView(\"weight_index_tb\") weight_index_tb.show(5) <pre>+--------------------+-------------------+-------------------+------------------+--------------------+--------+--------+-------+--------+--------------------+-----------+\n|               nodes|     distance_toend|           distance|          geomsize|                geom|      id|maxspeed|incline| surface|                name|total_nodes|\n+--------------------+-------------------+-------------------+------------------+--------------------+--------+--------+-------+--------+--------------------+-----------+\n|[362528326, 28597...|0.04960428437877076|0.06191383729684203|11.818939546233448|LINESTRING (-25.5...|25835738|    null|   null|    null|Avenida Costa e S...|      36560|\n|[285975617, 59202...| 0.1036513492683037|0.11504041822963577|  8.00531493113018|LINESTRING (-25.5...|26122619|      60|   null|   paved|    Avenida Mercosul|      36560|\n|[285975617, 69310...|0.10941553855472455|0.12057253982300521|1.9538901610218935|LINESTRING (-25.5...|26122631|      60|   null|concrete|Ponte Internacion...|      36560|\n|[307420349, 16982...|0.24791409977758808| 0.2619862588434014|11.873439986567648|LINESTRING (-25.6...|26122645|    null|   null|   paved|Rodovia das Catar...|      36560|\n|[5514364259, 5514...|  0.079155366559698|0.08873043404159503| 6.558866347858073|LINESTRING (-25.5...|26122801|    null|   null|    null| Rua Natal Graciotin|      36560|\n+--------------------+-------------------+-------------------+------------------+--------------------+--------+--------+-------+--------+--------------------+-----------+\nonly showing top 5 rows\n\n+--------------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|               nodes|maxspeed|incline| surface|                name|      id|                geom|          geomsize|           distance|     distance_toend|total_nodes|\n+--------------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|[362528326, 28597...|      20|     0%|    soil|Avenida Costa e S...|25835738|LINESTRING (-25.5...|11.818939546233448|0.06191383729684203|0.04960428437877076|      36560|\n|[285975617, 59202...|      60|     0%|   paved|    Avenida Mercosul|26122619|LINESTRING (-25.5...|  8.00531493113018|0.11504041822963577| 0.1036513492683037|      36560|\n|[285975617, 69310...|      60|     0%|concrete|Ponte Internacion...|26122631|LINESTRING (-25.5...|1.9538901610218935|0.12057253982300521|0.10941553855472455|      36560|\n|[307420349, 16982...|      20|     0%|   paved|Rodovia das Catar...|26122645|LINESTRING (-25.6...|11.873439986567648| 0.2619862588434014|0.24791409977758808|      36560|\n|[5514364259, 5514...|      20|     0%|    soil| Rua Natal Graciotin|26122801|LINESTRING (-25.5...| 6.558866347858073|0.08873043404159503|  0.079155366559698|      36560|\n+--------------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\nonly showing top 5 rows\n\n+--------------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|               nodes|surface_index|maxspeed|incline| surface|                name|      id|                geom|          geomsize|           distance|     distance_toend|total_nodes|\n+--------------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|[362528326, 28597...|         0.04|      20|     0%|    soil|Avenida Costa e S...|25835738|LINESTRING (-25.5...|11.818939546233448|0.06191383729684203|0.04960428437877076|      36560|\n|[285975617, 59202...|         0.03|      60|     0%|   paved|    Avenida Mercosul|26122619|LINESTRING (-25.5...|  8.00531493113018|0.11504041822963577| 0.1036513492683037|      36560|\n|[285975617, 69310...|         0.02|      60|     0%|concrete|Ponte Internacion...|26122631|LINESTRING (-25.5...|1.9538901610218935|0.12057253982300521|0.10941553855472455|      36560|\n|[307420349, 16982...|         0.03|      20|     0%|   paved|Rodovia das Catar...|26122645|LINESTRING (-25.6...|11.873439986567648| 0.2619862588434014|0.24791409977758808|      36560|\n|[5514364259, 5514...|         0.04|      20|     0%|    soil| Rua Natal Graciotin|26122801|LINESTRING (-25.5...| 6.558866347858073|0.08873043404159503|  0.079155366559698|      36560|\n+--------------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\nonly showing top 5 rows\n\n+--------------------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|               nodes|incline_index|surface_index|maxspeed|incline| surface|                name|      id|                geom|          geomsize|           distance|     distance_toend|total_nodes|\n+--------------------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|[362528326, 28597...|         0.00|         0.04|      20|     0%|    soil|Avenida Costa e S...|25835738|LINESTRING (-25.5...|11.818939546233448|0.06191383729684203|0.04960428437877076|      36560|\n|[285975617, 59202...|         0.00|         0.03|      60|     0%|   paved|    Avenida Mercosul|26122619|LINESTRING (-25.5...|  8.00531493113018|0.11504041822963577| 0.1036513492683037|      36560|\n|[285975617, 69310...|         0.00|         0.02|      60|     0%|concrete|Ponte Internacion...|26122631|LINESTRING (-25.5...|1.9538901610218935|0.12057253982300521|0.10941553855472455|      36560|\n|[307420349, 16982...|         0.00|         0.03|      20|     0%|   paved|Rodovia das Catar...|26122645|LINESTRING (-25.6...|11.873439986567648| 0.2619862588434014|0.24791409977758808|      36560|\n|[5514364259, 5514...|         0.00|         0.04|      20|     0%|    soil| Rua Natal Graciotin|26122801|LINESTRING (-25.5...| 6.558866347858073|0.08873043404159503|  0.079155366559698|      36560|\n+--------------------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\nonly showing top 5 rows\n\n+--------------------+------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|               nodes|weight|incline_index|surface_index|maxspeed|incline| surface|                name|      id|                geom|          geomsize|           distance|     distance_toend|total_nodes|\n+--------------------+------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\n|[362528326, 28597...|  39.2|         0.00|         0.04|      20|     0%|    soil|Avenida Costa e S...|25835738|LINESTRING (-25.5...|11.818939546233448|0.06191383729684203|0.04960428437877076|      36560|\n|[285975617, 59202...| 118.2|         0.00|         0.03|      60|     0%|   paved|    Avenida Mercosul|26122619|LINESTRING (-25.5...|  8.00531493113018|0.11504041822963577| 0.1036513492683037|      36560|\n|[285975617, 69310...| 118.8|         0.00|         0.02|      60|     0%|concrete|Ponte Internacion...|26122631|LINESTRING (-25.5...|1.9538901610218935|0.12057253982300521|0.10941553855472455|      36560|\n|[307420349, 16982...|  39.4|         0.00|         0.03|      20|     0%|   paved|Rodovia das Catar...|26122645|LINESTRING (-25.6...|11.873439986567648| 0.2619862588434014|0.24791409977758808|      36560|\n|[5514364259, 5514...|  39.2|         0.00|         0.04|      20|     0%|    soil| Rua Natal Graciotin|26122801|LINESTRING (-25.5...| 6.558866347858073|0.08873043404159503|  0.079155366559698|      36560|\n+--------------------+------+-------------+-------------+--------+-------+--------+--------------------+--------+--------------------+------------------+-------------------+-------------------+-----------+\nonly showing top 5 rows\n\n</pre> In\u00a0[9]: Copied! <pre>teste = spark.sql(\"select min(weight) from weight_index_tb\")\nteste.show(5)\n</pre> teste = spark.sql(\"select min(weight) from weight_index_tb\") teste.show(5) <pre>+-----------+\n|min(weight)|\n+-----------+\n|       39.0|\n+-----------+\n\n</pre> In\u00a0[10]: Copied! <pre>closestoend_tb = spark.sql(\n    \"select w1.id, w1.distance_toend from weight_index_tb w1 group by w1.id, w1.distance_toend having (select min(w2.distance_toend) as distance_toend  from weight_index_tb w2) = w1.distance_toend\"\n)\nclosestoend_tb.createOrReplaceTempView(\"closestoend_tb\")\nclosestoend = closestoend_tb.take(1)[0][\"id\"]\nprint(closestoend)\n\nclosestostart_tb = spark.sql(\n    \"select w1.id, w1.distance from weight_index_tb w1 group by w1.id, w1.distance having (select min(w2.distance) as distance  from weight_index_tb w2) = w1.distance\"\n)\nclosestostart_tb.createOrReplaceTempView(\"closestostart_tb\")\nclosestostart_tb.show(5)\n\nclosestostart = closestostart_tb.take(1)[0][\"id\"]\n\n# FOLIUM EM 3857 dado em 4326 st_transform(st_union_aggr(geom),'epsg:3857','epsg:4326')\njson_lines = spark.sql(\n    \"select ST_AsGeoJSON(st_envelope_aggr(geom)) AS json  from weight_index_tb where id in (\"\n    + str(closestostart)\n    + \",\"\n    + str(closestoend)\n    + \")\"\n)\njson_lines_string_teste = json_lines.take(1)[0][\"json\"]\ncoordinates_teste = json.loads(json_lines_string_teste)[\"coordinates\"]\n\n\n# st_boundary st_contains\n\n# Pegar o limite entre a uniao da geom inicial e final\n# select st_boundary(st_union_aggr(geom)) AS boundary  from weight_index_tb where id in (\"+str(closestostart)+\",\"+str(closestoend)+\")\n\nboundary_tb = spark.sql(\n    \"select st_envelope_aggr(geom) as boundary from weight_index_tb where id in (\"\n    + str(closestostart)\n    + \",\"\n    + str(closestoend)\n    + \")\"\n)\nboundary_tb.createOrReplaceTempView(\"boundary_tb\")\nboundary_tb.show(5)\n\ncontains_tb = spark.sql(\n    \"select st_intersects(boundary,geom) as contains, id from weight_index_tb, boundary_tb\"\n)\ncontains_tb.createOrReplaceTempView(\"contains_tb\")\ncontains_tb.show(5)\n\npossible_paths = spark.sql(\n    \"select id, geom from weight_index_tb group by id, geom having id in (select id from contains_tb where contains = true)\"\n)\npossible_paths.createOrReplaceTempView(\"possible_paths\")\npossible_paths.show(5)\n\npaths_collection = spark.sql(\n    \"select ST_AsGeoJSON(st_union_aggr(geom)) AS json from possible_paths\"\n)\njson_lines_string = paths_collection.take(1)[0][\"json\"]\ncoordinates = json.loads(json_lines_string)[\"coordinates\"]\n</pre> closestoend_tb = spark.sql(     \"select w1.id, w1.distance_toend from weight_index_tb w1 group by w1.id, w1.distance_toend having (select min(w2.distance_toend) as distance_toend  from weight_index_tb w2) = w1.distance_toend\" ) closestoend_tb.createOrReplaceTempView(\"closestoend_tb\") closestoend = closestoend_tb.take(1)[0][\"id\"] print(closestoend)  closestostart_tb = spark.sql(     \"select w1.id, w1.distance from weight_index_tb w1 group by w1.id, w1.distance having (select min(w2.distance) as distance  from weight_index_tb w2) = w1.distance\" ) closestostart_tb.createOrReplaceTempView(\"closestostart_tb\") closestostart_tb.show(5)  closestostart = closestostart_tb.take(1)[0][\"id\"]  # FOLIUM EM 3857 dado em 4326 st_transform(st_union_aggr(geom),'epsg:3857','epsg:4326') json_lines = spark.sql(     \"select ST_AsGeoJSON(st_envelope_aggr(geom)) AS json  from weight_index_tb where id in (\"     + str(closestostart)     + \",\"     + str(closestoend)     + \")\" ) json_lines_string_teste = json_lines.take(1)[0][\"json\"] coordinates_teste = json.loads(json_lines_string_teste)[\"coordinates\"]   # st_boundary st_contains  # Pegar o limite entre a uniao da geom inicial e final # select st_boundary(st_union_aggr(geom)) AS boundary  from weight_index_tb where id in (\"+str(closestostart)+\",\"+str(closestoend)+\")  boundary_tb = spark.sql(     \"select st_envelope_aggr(geom) as boundary from weight_index_tb where id in (\"     + str(closestostart)     + \",\"     + str(closestoend)     + \")\" ) boundary_tb.createOrReplaceTempView(\"boundary_tb\") boundary_tb.show(5)  contains_tb = spark.sql(     \"select st_intersects(boundary,geom) as contains, id from weight_index_tb, boundary_tb\" ) contains_tb.createOrReplaceTempView(\"contains_tb\") contains_tb.show(5)  possible_paths = spark.sql(     \"select id, geom from weight_index_tb group by id, geom having id in (select id from contains_tb where contains = true)\" ) possible_paths.createOrReplaceTempView(\"possible_paths\") possible_paths.show(5)  paths_collection = spark.sql(     \"select ST_AsGeoJSON(st_union_aggr(geom)) AS json from possible_paths\" ) json_lines_string = paths_collection.take(1)[0][\"json\"] coordinates = json.loads(json_lines_string)[\"coordinates\"] <pre>32344883\n</pre> <pre>22/02/01 12:44:21 ERROR Utils: Uncaught exception in thread element-tracking-store-worker\njava.util.NoSuchElementException: key not found: 4363\n\tat scala.collection.MapLike.default(MapLike.scala:235)\n\tat scala.collection.MapLike.default$(MapLike.scala:234)\n\tat scala.collection.AbstractMap.default(Map.scala:63)\n\tat scala.collection.MapLike.apply(MapLike.scala:144)\n\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n\tat scala.collection.AbstractMap.apply(Map.scala:63)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$11(SQLAppStatusListener.scala:257)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:256)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$onExecutionEnd$2(SQLAppStatusListener.scala:365)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryLog(Utils.scala:1945)\n\tat org.apache.spark.status.ElementTrackingStore$$anon$1.run(ElementTrackingStore.scala:117)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</pre> <pre>+--------+--------------------+\n|      id|            distance|\n+--------+--------------------+\n|32347167|8.330276767022637E-5|\n+--------+--------------------+\n\n</pre> <pre>22/02/01 12:44:22 ERROR Utils: Uncaught exception in thread element-tracking-store-worker\njava.util.NoSuchElementException: key not found: 5357\n\tat scala.collection.MapLike.default(MapLike.scala:235)\n\tat scala.collection.MapLike.default$(MapLike.scala:234)\n\tat scala.collection.AbstractMap.default(Map.scala:63)\n\tat scala.collection.MapLike.apply(MapLike.scala:144)\n\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n\tat scala.collection.AbstractMap.apply(Map.scala:63)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$11(SQLAppStatusListener.scala:257)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:256)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$onExecutionEnd$2(SQLAppStatusListener.scala:365)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryLog(Utils.scala:1945)\n\tat org.apache.spark.status.ElementTrackingStore$$anon$1.run(ElementTrackingStore.scala:117)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n22/02/01 12:44:23 ERROR Utils: Uncaught exception in thread element-tracking-store-worker\njava.util.NoSuchElementException: key not found: 6462\n\tat scala.collection.MapLike.default(MapLike.scala:235)\n\tat scala.collection.MapLike.default$(MapLike.scala:234)\n\tat scala.collection.AbstractMap.default(Map.scala:63)\n\tat scala.collection.MapLike.apply(MapLike.scala:144)\n\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n\tat scala.collection.AbstractMap.apply(Map.scala:63)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$11(SQLAppStatusListener.scala:257)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:256)\n\tat org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$onExecutionEnd$2(SQLAppStatusListener.scala:365)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryLog(Utils.scala:1945)\n\tat org.apache.spark.status.ElementTrackingStore$$anon$1.run(ElementTrackingStore.scala:117)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n                                                                                \r</pre> <pre>+--------------------+\n|            boundary|\n+--------------------+\n|POLYGON ((-25.479...|\n+--------------------+\n\n+--------+--------+\n|contains|      id|\n+--------+--------+\n|   false|25835738|\n|   false|26122619|\n|   false|26122631|\n|   false|26122645|\n|   false|26122801|\n+--------+--------+\nonly showing top 5 rows\n\n+---------+--------------------+\n|       id|                geom|\n+---------+--------------------+\n| 32344891|LINESTRING (-25.4...|\n|197591238|LINESTRING (-25.4...|\n|918520467|LINESTRING (-25.4...|\n| 32345349|LINESTRING (-25.4...|\n|964560305|LINESTRING (-25.4...|\n+---------+--------------------+\nonly showing top 5 rows\n\n</pre> In\u00a0[11]: Copied! <pre>path = [closestostart]\nvisited = [closestostart]\ncurrent_nodes = spark.sql(\n    \"select geom from weight_index_tb where id = \" + str(closestostart)\n)\nrow = current_nodes.rdd.collect()[0][\"geom\"]\nid_current = closestostart\n\n\ndef choose_path(row, path, id_current, visited, copy_row):\n\n    visited_frm = str(visited).replace(\"[\", \"(\").replace(\"]\", \")\")\n\n    touches_tb = spark.sql(\n        \"select st_touches(st_geomfromwkt('\"\n        + str(row)\n        + \"'),geom) as touches, * from weight_index_tb where geom IS NOT NULL and distance_toend IS NOT NULL\"\n    )\n    touches_tb.createOrReplaceTempView(\"touches_tb\")\n    # st_distance(st_geomfromwkt('\"+str(row)+\"'),geom)\n    fim_distance = spark.sql(\n        \"select distance_toend from touches_tb where id = \" + str(id_current)\n    )\n    fim_distance.show(5)\n    fim_distance_value = fim_distance.rdd.collect()[0][\"distance_toend\"]\n\n    current_distance = spark.sql(\n        \"select distance from touches_tb where id = \" + str(id_current)\n    )\n    current_distance_value = current_distance.rdd.collect()[0][\"distance\"]\n\n    #     st_distance(st_geomfromwkt('\"+str(row)+\"'),geom) =\n\n    sql = (\n        \"select geom, id, weight from touches_tb where \"\n        + \"touches = true\"\n        + \" and \"\n        + \"distance_toend &lt; \"\n        + str(fim_distance_value)\n        + \" and \"\n        + \"distance &gt; \"\n        + str(current_distance_value)\n        + \" and \"\n        + \"id NOT IN \"\n        + visited_frm\n    )\n    print(sql)\n    current_nodes = spark.sql(sql)\n    current_nodes.createOrReplaceTempView(\"current_nodes\")\n    current_nodes.show(5)\n\n    current_node = spark.sql(\n        \"select id, geom, max(weight) from current_nodes group by id, geom, weight having max(weight) = weight \"\n    )\n    current_node.show(5)\n\n    if len(current_nodes.rdd.collect()) == 0:\n        return path\n    else:\n        row = current_node.rdd.collect()[0][\"geom\"]\n        id_current = current_node.rdd.collect()[0][\"id\"]\n        path.append(id_current)\n        visited.append(id_current)\n        return choose_path(row, path, id_current, visited, copy_row)\n\n\npath_ids = choose_path(row, path, id_current, visited, row)\npath_ids.append(closestoend)\npath_ids_frm = str(path_ids).replace(\"[\", \"(\").replace(\"]\", \")\")\nprint(path_ids_frm)\n\nshort_path = spark.sql(\n    \"select ST_AsGeoJSON(st_union_aggr(geom)) AS json from weight_index_tb where id in \"\n    + path_ids_frm\n)\nshort_path_string = short_path.take(1)[0][\"json\"]\nshort_path_coordinates = json.loads(short_path_string)[\"coordinates\"]\n</pre> path = [closestostart] visited = [closestostart] current_nodes = spark.sql(     \"select geom from weight_index_tb where id = \" + str(closestostart) ) row = current_nodes.rdd.collect()[0][\"geom\"] id_current = closestostart   def choose_path(row, path, id_current, visited, copy_row):      visited_frm = str(visited).replace(\"[\", \"(\").replace(\"]\", \")\")      touches_tb = spark.sql(         \"select st_touches(st_geomfromwkt('\"         + str(row)         + \"'),geom) as touches, * from weight_index_tb where geom IS NOT NULL and distance_toend IS NOT NULL\"     )     touches_tb.createOrReplaceTempView(\"touches_tb\")     # st_distance(st_geomfromwkt('\"+str(row)+\"'),geom)     fim_distance = spark.sql(         \"select distance_toend from touches_tb where id = \" + str(id_current)     )     fim_distance.show(5)     fim_distance_value = fim_distance.rdd.collect()[0][\"distance_toend\"]      current_distance = spark.sql(         \"select distance from touches_tb where id = \" + str(id_current)     )     current_distance_value = current_distance.rdd.collect()[0][\"distance\"]      #     st_distance(st_geomfromwkt('\"+str(row)+\"'),geom) =      sql = (         \"select geom, id, weight from touches_tb where \"         + \"touches = true\"         + \" and \"         + \"distance_toend &lt; \"         + str(fim_distance_value)         + \" and \"         + \"distance &gt; \"         + str(current_distance_value)         + \" and \"         + \"id NOT IN \"         + visited_frm     )     print(sql)     current_nodes = spark.sql(sql)     current_nodes.createOrReplaceTempView(\"current_nodes\")     current_nodes.show(5)      current_node = spark.sql(         \"select id, geom, max(weight) from current_nodes group by id, geom, weight having max(weight) = weight \"     )     current_node.show(5)      if len(current_nodes.rdd.collect()) == 0:         return path     else:         row = current_node.rdd.collect()[0][\"geom\"]         id_current = current_node.rdd.collect()[0][\"id\"]         path.append(id_current)         visited.append(id_current)         return choose_path(row, path, id_current, visited, copy_row)   path_ids = choose_path(row, path, id_current, visited, row) path_ids.append(closestoend) path_ids_frm = str(path_ids).replace(\"[\", \"(\").replace(\"]\", \")\") print(path_ids_frm)  short_path = spark.sql(     \"select ST_AsGeoJSON(st_union_aggr(geom)) AS json from weight_index_tb where id in \"     + path_ids_frm ) short_path_string = short_path.take(1)[0][\"json\"] short_path_coordinates = json.loads(short_path_string)[\"coordinates\"] <pre>+-------------------+\n|     distance_toend|\n+-------------------+\n|0.01362215583892774|\n+-------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.01362215583892774 and distance &gt; 8.330276767022637e-05 and id NOT IN (32347167)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...| 32347009|  39.2|\n|LINESTRING (-25.4...|197591238|  39.2|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|197591238|LINESTRING (-25.4...|       39.2|\n| 32347009|LINESTRING (-25.4...|       39.2|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.011868762484771467|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.011868762484771467 and distance &gt; 0.001627280815937359 and id NOT IN (32347167, 197591238)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...|197591235|  39.2|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|197591235|LINESTRING (-25.4...|       39.2|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.010329729036617774|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.010329729036617774 and distance &gt; 0.0018402611895601347 and id NOT IN (32347167, 197591238, 197591235)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...|197591232|  39.2|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|197591232|LINESTRING (-25.4...|       39.2|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.007666208743570536|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.007666208743570536 and distance &gt; 0.004980378367153946 and id NOT IN (32347167, 197591238, 197591235, 197591232)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...|437818123| 117.6|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|437818123|LINESTRING (-25.4...|      117.6|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.004314646750311...|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.0043146467503117755 and distance &gt; 0.005667965772492608 and id NOT IN (32347167, 197591238, 197591235, 197591232, 437818123)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...|933800237| 117.6|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|933800237|LINESTRING (-25.4...|      117.6|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.004249921855750084|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.004249921855750084 and distance &gt; 0.01157000312230015 and id NOT IN (32347167, 197591238, 197591235, 197591232, 437818123, 933800237)\n+--------------------+---------+------+\n|                geom|       id|weight|\n+--------------------+---------+------+\n|LINESTRING (-25.4...|933800236| 117.6|\n+--------------------+---------+------+\n\n+---------+--------------------+-----------+\n|       id|                geom|max(weight)|\n+---------+--------------------+-----------+\n|933800236|LINESTRING (-25.4...|      117.6|\n+---------+--------------------+-----------+\n\n+--------------------+\n|      distance_toend|\n+--------------------+\n|0.003896125505221...|\n+--------------------+\n\nselect geom, id, weight from touches_tb where touches = true and distance_toend &lt; 0.0038961255052215697 and distance &gt; 0.011710186719691087 and id NOT IN (32347167, 197591238, 197591235, 197591232, 437818123, 933800237, 933800236)\n+----+---+------+\n|geom| id|weight|\n+----+---+------+\n+----+---+------+\n\n+---+----+-----------+\n| id|geom|max(weight)|\n+---+----+-----------+\n+---+----+-----------+\n\n(32347167, 197591238, 197591235, 197591232, 437818123, 933800237, 933800236, 32344883)\n</pre> In\u00a0[12]: Copied! <pre>#\n\nimport folium\n\nstart_point_arr = [-25.4695946, -54.5909028]\nend_point_arr = [-25.4786993, -54.57938]\ntooltip = \"Click me!\"\n# 3857\nm = folium.Map(\n    location=[-25.5172662, -54.6170038],\n    zoom_start=12,\n    tiles=\"OpenStreetMap\",\n    crs=\"EPSG3857\",\n)\nfolium.Marker(\n    start_point_arr,\n    popup=\"&lt;i&gt;Inicio&lt;/i&gt;\",\n    tooltip=tooltip,\n    icon=folium.Icon(color=\"green\"),\n).add_to(m)\nfolium.Marker(\n    end_point_arr, popup=\"&lt;b&gt;Fim&lt;/b&gt;\", tooltip=tooltip, icon=folium.Icon(color=\"red\")\n).add_to(m)\n\n# lines = folium.vector_layers.PolyLine(locations=coordinates)\n# lines.add_to(m)\n\n# polygon = folium.vector_layers.Polygon(locations=coordinates_teste)\n# polygon.add_to(m)\n\npolygon_path = folium.vector_layers.Polygon(locations=short_path_coordinates)\npolygon_path.add_to(m)\n\nm\n</pre> #  import folium  start_point_arr = [-25.4695946, -54.5909028] end_point_arr = [-25.4786993, -54.57938] tooltip = \"Click me!\" # 3857 m = folium.Map(     location=[-25.5172662, -54.6170038],     zoom_start=12,     tiles=\"OpenStreetMap\",     crs=\"EPSG3857\", ) folium.Marker(     start_point_arr,     popup=\"Inicio\",     tooltip=tooltip,     icon=folium.Icon(color=\"green\"), ).add_to(m) folium.Marker(     end_point_arr, popup=\"Fim\", tooltip=tooltip, icon=folium.Icon(color=\"red\") ).add_to(m)  # lines = folium.vector_layers.PolyLine(locations=coordinates) # lines.add_to(m)  # polygon = folium.vector_layers.Polygon(locations=coordinates_teste) # polygon.add_to(m)  polygon_path = folium.vector_layers.Polygon(locations=short_path_coordinates) polygon_path.add_to(m)  m Out[12]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[13]: Copied! <pre>spark.stop()\n</pre> spark.stop() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/contrib/foot-traffic/","title":"Understand Consumer Behavior With Verified Foot Traffic Data","text":"In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\n\nspark = (\n    SparkSession.builder.appName(\"sigspatial2021\")\n    .master(\"spark://data-ocean-lab-1:7077\")\n    .config(\"spark.serializer\", KryoSerializer.getName)\n    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n    .config(\n        \"spark.jars.packages\",\n        \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.0-incubating,\"\n        \"org.datasyslab:geotools-wrapper:1.1.0-25.2\",\n    )\n    .getOrCreate()\n)\n</pre> from pyspark.sql import SparkSession from sedona.register import SedonaRegistrator from sedona.utils import SedonaKryoRegistrator, KryoSerializer  spark = (     SparkSession.builder.appName(\"sigspatial2021\")     .master(\"spark://data-ocean-lab-1:7077\")     .config(\"spark.serializer\", KryoSerializer.getName)     .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)     .config(         \"spark.jars.packages\",         \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.0-incubating,\"         \"org.datasyslab:geotools-wrapper:1.1.0-25.2\",     )     .getOrCreate() ) <pre>WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/media/hdd1/code/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n</pre> <pre>:: loading settings :: url = jar:file:/media/hdd1/code/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n</pre> <pre>Ivy Default Cache set to: /home/jiayu/.ivy2/cache\nThe jars for the packages stored in: /home/jiayu/.ivy2/jars\norg.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\norg.datasyslab#geotools-wrapper added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-f959fc69-d6af-4223-b017-6334535799e9;1.0\n\tconfs: [default]\n\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.2.0-incubating in central\n\tfound org.locationtech.jts#jts-core;1.18.0 in local-m2-cache\n\tfound org.wololo#jts2geojson;0.16.1 in central\n\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n\tfound org.apache.sedona#sedona-core-3.0_2.12;1.2.0-incubating in central\n\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.2.0-incubating in central\n\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n:: resolution report :: resolve 322ms :: artifacts dl 7ms\n\t:: modules in use:\n\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n\torg.apache.sedona#sedona-core-3.0_2.12;1.2.0-incubating from central in [default]\n\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.2.0-incubating from central in [default]\n\torg.apache.sedona#sedona-sql-3.0_2.12;1.2.0-incubating from central in [default]\n\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n\torg.locationtech.jts#jts-core;1.18.0 from local-m2-cache in [default]\n\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n\torg.wololo#jts2geojson;0.16.1 from central in [default]\n\t:: evicted modules:\n\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   11  |   0   |   0   |   1   ||   10  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-f959fc69-d6af-4223-b017-6334535799e9\n\tconfs: [default]\n\t0 artifacts copied, 10 already retrieved (0kB/8ms)\n2022-05-22 13:07:11,104 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n</pre> In\u00a0[2]: Copied! <pre>import pyspark.sql.functions as f\nfrom pyspark.sql.types import MapType, StringType, IntegerType\n\nimport pandas as pd\n</pre> import pyspark.sql.functions as f from pyspark.sql.types import MapType, StringType, IntegerType  import pandas as pd In\u00a0[76]: Copied! <pre>sample_csv_path = \"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/seattle_coffee_monthly_patterns/\"\nsample = (\n    spark.read.option(\"header\", \"true\")\n    .option(\"escape\", '\"')\n    .csv(sample_csv_path)\n    .withColumn(\"date_range_start\", f.to_date(f.col(\"date_range_start\")))\n    .withColumn(\"date_range_end\", f.to_date(f.col(\"date_range_end\")))\n    .withColumn(\n        \"visitor_home_cbgs\",\n        f.from_json(\"visitor_home_cbgs\", schema=MapType(StringType(), IntegerType())),\n    )\n    .withColumn(\"distance_from_home\", f.col(\"distance_from_home\"))\n)\n</pre> sample_csv_path = \"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/seattle_coffee_monthly_patterns/\" sample = (     spark.read.option(\"header\", \"true\")     .option(\"escape\", '\"')     .csv(sample_csv_path)     .withColumn(\"date_range_start\", f.to_date(f.col(\"date_range_start\")))     .withColumn(\"date_range_end\", f.to_date(f.col(\"date_range_end\")))     .withColumn(         \"visitor_home_cbgs\",         f.from_json(\"visitor_home_cbgs\", schema=MapType(StringType(), IntegerType())),     )     .withColumn(\"distance_from_home\", f.col(\"distance_from_home\")) ) <pre>                                                                                \r</pre> In\u00a0[77]: Copied! <pre>print(\"Number of coffe shops patterns: \", sample.count())\nprint(\"Number of coffe shops: \", sample.select(\"placekey\").distinct().count())\nsample.limit(10).toPandas().head()\n</pre> print(\"Number of coffe shops patterns: \", sample.count()) print(\"Number of coffe shops: \", sample.select(\"placekey\").distinct().count()) sample.limit(10).toPandas().head() <pre>                                                                                \r</pre> <pre>Number of coffe shops patterns:  66607\n</pre> <pre>                                                                                \r</pre> <pre>Number of coffe shops:  1679\n</pre> Out[77]: placekey safegraph_place_id parent_placekey parent_safegraph_place_id safegraph_brand_ids location_name brands store_id top_category sub_category ... distance_from_home median_dwell bucketed_dwell_times related_same_day_brand related_same_month_brand popularity_by_hour popularity_by_day device_type carrier_name county_fips 0 226-222@5x3-t3j-ch5 sg:290a6c59dd3f4b28a1c086a510e40944 None None None Black Gold Coffee Company None None Restaurants and Other Eating Places Snack and Nonalcoholic Beverage Bars ... 11416 20.0 {\"&lt;5\":3,\"5-10\":30,\"11-20\":12,\"21-60\":14,\"61-12... {\"CENEX\":10,\"Starbucks\":9,\"Safeway Pharmacy\":9... {\"McDonald's\":44,\"Walmart\":37,\"Starbucks\":37,\"... [9,9,9,9,9,12,11,9,15,20,15,22,16,12,11,11,8,7... {\"Monday\":13,\"Tuesday\":20,\"Wednesday\":8,\"Thurs... {\"android\":25,\"ios\":14} {\"AT&amp;T\":5,\"Sprint\":4,\"T-Mobile\":9,\"Verizon\":14} 53033 1 224-222@5x4-49y-8gk sg:33d6c187ce6e419cbcb09f0b47ed6236 None None None Bite Box None None Restaurants and Other Eating Places Snack and Nonalcoholic Beverage Bars ... 2737 41.0 {\"&lt;5\":4,\"5-10\":28,\"11-20\":12,\"21-60\":30,\"61-12... {\"Safeway Pharmacy\":6,\"Ace Hardware\":5,\"ARCO\":... {\"Starbucks\":41,\"Shell Oil\":33,\"Safeway Pharma... [8,10,10,9,8,12,13,19,22,25,30,34,39,32,21,20,... {\"Monday\":17,\"Tuesday\":25,\"Wednesday\":15,\"Thur... {\"android\":20,\"ios\":51} {\"AT&amp;T\":14,\"Sprint\":1,\"T-Mobile\":14,\"Verizon\":35} 53033 2 228-222@5x2-t6c-qfz sg:442c1606a0d54d2aa017ea5a89ea8d78 zzw-223@5x2-t6c-qfz sg:a6e078c59b154cd59d8ac9e1f6f6311c SG_BRAND_f116acfe9147494063e58da666d1d57e Starbucks Starbucks 26209-243989 Restaurants and Other Eating Places Snack and Nonalcoholic Beverage Bars ... 5074 9.0 {\"&lt;5\":22,\"5-10\":165,\"11-20\":47,\"21-60\":50,\"61-... {\"Walmart\":7,\"Safeway Fuel Station\":4,\"McDonal... {\"Walmart\":55,\"McDonald's\":45,\"Costco\":35,\"She... [9,7,6,6,6,10,17,22,26,47,61,51,39,35,39,48,26... {\"Monday\":52,\"Tuesday\":60,\"Wednesday\":38,\"Thur... {\"android\":100,\"ios\":160} {\"AT&amp;T\":64,\"Sprint\":9,\"T-Mobile\":62,\"Verizon\":98} 53061 3 zzw-22b@5x4-4yr-gtv sg:5b5977ecc8814452bb5630d0b1b56ae1 None None None Cozy Bubble Tea None None Restaurants and Other Eating Places Snack and Nonalcoholic Beverage Bars ... None 90.0 {\"&lt;5\":0,\"5-10\":0,\"11-20\":0,\"21-60\":1,\"61-120\":... {\"Hallmark Cards\":20,\"76\":20} {\"Costco Gasoline\":60,\"Costco\":60,\"Starbucks\":... [1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,1,2,2,2,2] {\"Monday\":0,\"Tuesday\":1,\"Wednesday\":1,\"Thursda... {\"android\":4,\"ios\":4} {\"AT&amp;T\":1,\"T-Mobile\":2,\"Verizon\":1} 53033 4 zzw-224@5x4-4bc-d5f sg:6af014f98e9f471b85b8e3eacb58b2f7 None None SG_BRAND_f116acfe9147494063e58da666d1d57e Starbucks Starbucks 3278-4859 Restaurants and Other Eating Places Snack and Nonalcoholic Beverage Bars ... 9319 45.0 {\"&lt;5\":36,\"5-10\":201,\"11-20\":119,\"21-60\":175,\"6... {\"Potbelly Sandwich Works\":1,\"Kung Fu Tea\":1,\"... {\"Chevron\":18,\"Costco\":15,\"Shell Oil\":13,\"McDo... [202,186,187,192,187,196,235,254,272,267,259,2... {\"Monday\":181,\"Tuesday\":171,\"Wednesday\":127,\"T... {\"android\":136,\"ios\":289} {\"AT&amp;T\":41,\"Sprint\":2,\"T-Mobile\":69,\"Verizon\":91} 53033 <p>5 rows \u00d7 51 columns</p> In\u00a0[78]: Copied! <pre>from pyspark.sql.window import Window\nimport geopandas as gpd\nimport folium\n</pre> from pyspark.sql.window import Window import geopandas as gpd import folium In\u00a0[79]: Copied! <pre>w = Window().partitionBy(\"placekey\").orderBy(f.col(\"date_range_start\").desc())\n\ncafes_latest = (\n    sample\n    # as our data improves, addresses or geocodes for a given location may change over time\n    # use a window function to keep only the most recent appearance of the given cafe\n    .withColumn(\"row_num\", f.row_number().over(w)).filter(f.col(\"row_num\") == 1)\n    # select the columns we need for mapping\n    .select(\n        \"placekey\",\n        \"location_name\",\n        \"brands\",\n        \"street_address\",\n        \"city\",\n        \"region\",\n        \"postal_code\",\n        \"latitude\",\n        \"longitude\",\n        \"open_hours\",\n    )\n)\n</pre> w = Window().partitionBy(\"placekey\").orderBy(f.col(\"date_range_start\").desc())  cafes_latest = (     sample     # as our data improves, addresses or geocodes for a given location may change over time     # use a window function to keep only the most recent appearance of the given cafe     .withColumn(\"row_num\", f.row_number().over(w)).filter(f.col(\"row_num\") == 1)     # select the columns we need for mapping     .select(         \"placekey\",         \"location_name\",         \"brands\",         \"street_address\",         \"city\",         \"region\",         \"postal_code\",         \"latitude\",         \"longitude\",         \"open_hours\",     ) ) In\u00a0[80]: Copied! <pre># create a geopandas geodataframe\ncafes_gdf = cafes_latest.toPandas()\ncafes_gdf = gpd.GeoDataFrame(\n    cafes_gdf,\n    geometry=gpd.points_from_xy(cafes_gdf[\"longitude\"], cafes_gdf[\"latitude\"]),\n    crs=\"EPSG:4326\",\n)\n</pre> # create a geopandas geodataframe cafes_gdf = cafes_latest.toPandas() cafes_gdf = gpd.GeoDataFrame(     cafes_gdf,     geometry=gpd.points_from_xy(cafes_gdf[\"longitude\"], cafes_gdf[\"latitude\"]),     crs=\"EPSG:4326\", ) <pre>                                                                                \r</pre> In\u00a0[81]: Copied! <pre>def map_cafes(gdf):\n\n    # map bounds\n    sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]\n    ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]\n    folium_bounds = [sw, ne]\n\n    # map\n    x = gdf.centroid.x[0]\n    y = gdf.centroid.y[0]\n\n    map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")\n\n    for i, point in gdf.iterrows():\n\n        tooltip = f\"placekey: {point['placekey']}&lt;br&gt;location_name: {point['location_name']}&lt;br&gt;brands: {point['brands']}&lt;br&gt;street_address: {point['street_address']}&lt;br&gt;city: {point['city']}&lt;br&gt;region: {point['region']}&lt;br&gt;postal_code: {point['postal_code']}&lt;br&gt;open_hours: {point['open_hours']}\"\n\n        folium.Circle(\n            [point[\"geometry\"].y, point[\"geometry\"].x],\n            radius=40,\n            fill_color=\"blue\",\n            color=\"blue\",\n            fill_opacity=1,\n            tooltip=tooltip,\n        ).add_to(map_)\n\n    map_.fit_bounds(folium_bounds)\n\n    return map_\n</pre> def map_cafes(gdf):      # map bounds     sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]     ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]     folium_bounds = [sw, ne]      # map     x = gdf.centroid.x[0]     y = gdf.centroid.y[0]      map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")      for i, point in gdf.iterrows():          tooltip = f\"placekey: {point['placekey']}location_name: {point['location_name']}brands: {point['brands']}street_address: {point['street_address']}city: {point['city']}region: {point['region']}postal_code: {point['postal_code']}open_hours: {point['open_hours']}\"          folium.Circle(             [point[\"geometry\"].y, point[\"geometry\"].x],             radius=40,             fill_color=\"blue\",             color=\"blue\",             fill_opacity=1,             tooltip=tooltip,         ).add_to(map_)      map_.fit_bounds(folium_bounds)      return map_ In\u00a0[82]: Copied! <pre>map_ = map_cafes(cafes_gdf)\nmap_\n</pre> map_ = map_cafes(cafes_gdf) map_ <pre>/tmp/ipykernel_2685102/3619001546.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  x = gdf.centroid.x[0]\n/tmp/ipykernel_2685102/3619001546.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  y = gdf.centroid.y[0]\n</pre> Out[82]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[128]: Copied! <pre># the `distance_from_home` column tells us the median distance (as the crow flies), in meters, between the coffee shop and the visitors' homes\n# which coffee shop's visitors had the highest average median distance traveled since Jan 2018?\n\n# outlier values in this column distort the histogram\n# these outliers are likely due to a combination of (1) coffee shops in downtown areas that receive high numbers of out-of-town visitors and (2) quirks in the underlying GPS data\nfurthest_traveled = (\n    sample.groupBy(\"placekey\", \"location_name\", \"street_address\")\n    .agg(f.mean(\"distance_from_home\").alias(\"avg_median_dist_from_home\"))\n    .orderBy(\"avg_median_dist_from_home\", ascending=False)\n)\n\ndisplay(furthest_traveled)\nfurthest_traveled.filter(f.col(\"avg_median_dist_from_home\").isNotNull()).withColumn(\n    \"avg_median_dist_from_home\", f.col(\"avg_median_dist_from_home\") / 1000.0\n).show(truncate=False)\nfurthest_traveled = furthest_traveled.drop(\"street_address\")\n</pre> # the `distance_from_home` column tells us the median distance (as the crow flies), in meters, between the coffee shop and the visitors' homes # which coffee shop's visitors had the highest average median distance traveled since Jan 2018?  # outlier values in this column distort the histogram # these outliers are likely due to a combination of (1) coffee shops in downtown areas that receive high numbers of out-of-town visitors and (2) quirks in the underlying GPS data furthest_traveled = (     sample.groupBy(\"placekey\", \"location_name\", \"street_address\")     .agg(f.mean(\"distance_from_home\").alias(\"avg_median_dist_from_home\"))     .orderBy(\"avg_median_dist_from_home\", ascending=False) )  display(furthest_traveled) furthest_traveled.filter(f.col(\"avg_median_dist_from_home\").isNotNull()).withColumn(     \"avg_median_dist_from_home\", f.col(\"avg_median_dist_from_home\") / 1000.0 ).show(truncate=False) furthest_traveled = furthest_traveled.drop(\"street_address\") <pre>DataFrame[placekey: string, location_name: string, street_address: string, avg_median_dist_from_home: double]</pre> <pre>\r\n[Stage 332:=&gt;                                                     (1 + 39) / 40]\r\n\r\n[Stage 332:======================&gt;                               (17 + 23) / 40]\r</pre> <pre>+-------------------+--------------------------+-----------------------------------+-------------------------+\n|placekey           |location_name             |street_address                     |avg_median_dist_from_home|\n+-------------------+--------------------------+-----------------------------------+-------------------------+\n|zzy-222@5x4-4b4-p7q|Starbucks                 |1700 Seventh Ave Nordstrom Building|1124.5566666666668       |\n|25x-222@5x4-4b5-dvz|Storyville Coffee Company |94 Pike St Ste 34                  |757.5968378378378        |\n|zzw-22f@5x4-4b5-p7q|Starbucks                 |1912 Pike Pl                       |750.9135581395349        |\n|223-222@5x4-4b5-dvz|Ghost Alley Espresso      |1499 Post Aly                      |727.4028888888889        |\n|224-224@5x4-4b4-xt9|The Bar                   |110 6th Ave N                      |667.3632222222222        |\n|zzw-222@5x4-4b5-pvz|Cafe Opla                 |2200 Alaskan Way Ste 120           |661.2868000000001        |\n|22c-222@5x4-4b4-s5z|Starbucks                 |1124 Pike St                       |654.5950666666666        |\n|222-222@5x4-4b5-dvz|Starbucks                 |102 Pike St                        |633.7193111111111        |\n|22q-222@5x4-4b5-m8v|Limoncello Belltown       |2326 1st Ave                       |485.0474516129032        |\n|22j-222@5x4-4b5-dvz|The Crumpet Shop          |1503 1st Ave                       |431.5123333333333        |\n|zzw-22c@5x4-4b7-bkz|Starbucks                 |305 Harrison St Ste 220            |406.287                  |\n|zzy-222@5xd-hjq-yn5|Starbucks                 |1300 Station Drive                 |369.72424137931034       |\n|zzw-22b@5x4-4b5-pgk|World Spice Merchants     |1509 Western Ave                   |360.2277209302325        |\n|zzw-22f@5x4-4b5-pgk|Procopio Gelateria        |1501 Western Ave Ste 300           |344.466                  |\n|223-222@5x4-4b5-hqz|Starbucks                 |1101 Alaskan Way Ste 102           |317.55111111111114       |\n|zzw-22r@5x4-4b5-gkz|Cafe ABoDegas             |1303 6th Ave                       |316.76696000000004       |\n|22g-222@5x4-4b5-dsq|Bottega Italiana          |1425 1st Ave                       |308.5182222222222        |\n|zzw-22d@5x4-4b7-bkz|Eltana                    |305 Harrison St                    |294.919                  |\n|zzw-224@5x4-4b5-f75|MarketSpice               |85A Pike St                        |263.49886363636364       |\n|22b-222@5x4-4vs-qzz|Cherry Street Coffee House|700 1st Ave                        |210.19386666666668       |\n+-------------------+--------------------------+-----------------------------------+-------------------------+\nonly showing top 20 rows\n\n</pre> <pre>\r\n                                                                                \r</pre> In\u00a0[129]: Copied! <pre># most coffee shops' visitors' homes are &lt;10km away\ndisplay(furthest_traveled.filter(f.col(\"avg_median_dist_from_home\") &lt; 10000))\nprint(\n    furthest_traveled.filter(f.col(\"avg_median_dist_from_home\") &lt; 10000).count(),\n    \" coffee shops have vistors' home &lt;10 km away, out of \",\n    furthest_traveled.count(),\n    \"coffee shops.\",\n)\n</pre> # most coffee shops' visitors' homes are &lt;10km away display(furthest_traveled.filter(f.col(\"avg_median_dist_from_home\") &lt; 10000)) print(     furthest_traveled.filter(f.col(\"avg_median_dist_from_home\") &lt; 10000).count(),     \" coffee shops have vistors' home &lt;10 km away, out of \",     furthest_traveled.count(),     \"coffee shops.\", ) <pre>DataFrame[placekey: string, location_name: string, avg_median_dist_from_home: double]</pre> <pre>[Stage 337:========================================&gt;             (30 + 10) / 40]\r</pre> <pre>1071  coffee shops have vistors' home &lt;10 km away, out of  1688 coffee shops.\n</pre> <pre>\r\n                                                                                \r</pre> In\u00a0[\u00a0]: Copied! <pre># but `distance_from_home` takes into account ALL visitors to the coffee shop, which as we saw above can distort values. When selecting a site for a coffee shop, we likely care more about how far visitors traveled from within Seattle\n# we can compute this using Sedona\nimport sedona\nfrom sedona.register import SedonaRegistrator\nfrom sedona.utils import SedonaKryoRegistrator, KryoSerializer\n\nSedonaRegistrator.registerAll(spark)\n</pre> # but `distance_from_home` takes into account ALL visitors to the coffee shop, which as we saw above can distort values. When selecting a site for a coffee shop, we likely care more about how far visitors traveled from within Seattle # we can compute this using Sedona import sedona from sedona.register import SedonaRegistrator from sedona.utils import SedonaKryoRegistrator, KryoSerializer  SedonaRegistrator.registerAll(spark) In\u00a0[131]: Copied! <pre># load the census block groups for Washington State\n# filter it down to our three counties of interest in Seattle\nWA_cbgs = (\n    spark.read.option(\"header\", \"true\")\n    .option(\"escape\", '\"')\n    .csv(\"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/wa_cbg.csv\")\n    .filter(f.col(\"GEOID\").rlike(\"^(53033|53053|53061)\"))\n)\nWA_cbgs.head()\n</pre> # load the census block groups for Washington State # filter it down to our three counties of interest in Seattle WA_cbgs = (     spark.read.option(\"header\", \"true\")     .option(\"escape\", '\"')     .csv(\"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/wa_cbg.csv\")     .filter(f.col(\"GEOID\").rlike(\"^(53033|53053|53061)\")) ) WA_cbgs.head() Out[131]: <pre>Row(GEOID='530330033001', geometry='POLYGON ((-122.376771 47.679527, -122.376172 47.67953199999999, -122.375716 47.679533, -122.375137 47.679534, -122.374659 47.679531, -122.374092 47.679536, -122.373605 47.679537, -122.373003 47.679538, -122.37255 47.679539, -122.371964 47.679541, -122.371494 47.679539, -122.370899 47.679543, -122.370439 47.679543, -122.369828 47.679544, -122.369383 47.679545, -122.368747 47.67954599999999, -122.368328 47.679547, -122.367673 47.679548, -122.367273 47.679548, -122.366609 47.67954899999999, -122.366038 47.679551, -122.365979 47.679399, -122.365958 47.677762, -122.365957 47.676293, -122.36602 47.67616899999999, -122.366025 47.675984, -122.366197 47.675983, -122.36656 47.675982, -122.367624 47.67598, -122.368688 47.67598599999999, -122.36874 47.67598599999999, -122.369067 47.675987, -122.369779 47.675991, -122.370845 47.675993, -122.371399 47.675997, -122.371946 47.675999, -122.373158 47.675993, -122.373574 47.675991, -122.37401 47.67599, -122.37558 47.675988, -122.375645 47.675989, -122.375908 47.675992, -122.376117 47.675992, -122.376277 47.675993, -122.376513 47.675996, -122.376652 47.676203, -122.376708 47.67631799999999, -122.376749 47.676443, -122.376767 47.676553, -122.376765 47.677709, -122.376765 47.6778, -122.376771 47.679527))')</pre> In\u00a0[132]: Copied! <pre># transform the geometry column into a Geometry-type\nWA_cbgs = (\n    WA_cbgs.withColumn(\"cbg_geometry\", f.expr(\"ST_GeomFromWkt(geometry)\"))\n    # we'll just use the CBG centroid\n    .withColumn(\"cbg_geometry\", f.expr(\"ST_Centroid(cbg_geometry)\"))\n    # since we'll be doing a distance calculation, let's also use a projected CRS - epsg:3857\n    .withColumn(\n        \"cbg_geometry\",\n        f.expr(\n            \"ST_Transform(ST_FlipCoordinates(cbg_geometry), 'epsg:4326','epsg:3857', false)\"\n        ),\n    )  # ST_FlipCoordinates() necessary due to this bug: https://issues.apache.org/jira/browse/SEDONA-39\n    .withColumnRenamed(\"GEOID\", \"cbg\")\n    .withColumnRenamed(\"geometry\", \"cbg_polygon_geometry\")\n)\n</pre> # transform the geometry column into a Geometry-type WA_cbgs = (     WA_cbgs.withColumn(\"cbg_geometry\", f.expr(\"ST_GeomFromWkt(geometry)\"))     # we'll just use the CBG centroid     .withColumn(\"cbg_geometry\", f.expr(\"ST_Centroid(cbg_geometry)\"))     # since we'll be doing a distance calculation, let's also use a projected CRS - epsg:3857     .withColumn(         \"cbg_geometry\",         f.expr(             \"ST_Transform(ST_FlipCoordinates(cbg_geometry), 'epsg:4326','epsg:3857', false)\"         ),     )  # ST_FlipCoordinates() necessary due to this bug: https://issues.apache.org/jira/browse/SEDONA-39     .withColumnRenamed(\"GEOID\", \"cbg\")     .withColumnRenamed(\"geometry\", \"cbg_polygon_geometry\") ) In\u00a0[133]: Copied! <pre>WA_cbgs.limit(10).toPandas().head()\n</pre> WA_cbgs.limit(10).toPandas().head() Out[133]: cbg cbg_polygon_geometry cbg_geometry 0 530330033001 POLYGON ((-122.376771 47.679527, -122.376172 4... POINT (-13622316.63058369 6053413.257523819) 1 530530729031 POLYGON ((-122.643503 47.134887, -122.643502 4... POINT (-13647989.55994223 5963208.486137308) 2 530530730015 POLYGON ((-122.567775 46.970114, -122.566807 4... POINT (-13641020.87626943 5941126.596682728) 3 530530730013 POLYGON ((-122.533868 46.996149, -122.532109 4... POINT (-13637080.70343767 5938242.363543665) 4 530530730014 POLYGON ((-122.484146 46.938446, -122.484146 4... POINT (-13632564.01655458 5936624.370012998) In\u00a0[134]: Copied! <pre># Next let's prep our sample data\nsample_seattle_visitors = (\n    sample.select(\"placekey\", f.explode(\"visitor_home_cbgs\"))\n    .withColumnRenamed(\"key\", \"cbg\")\n    .withColumnRenamed(\"value\", \"visitors\")\n    #   # filter out CBGs with low visitor counts\n    .filter(f.col(\"visitors\") &gt; 4)\n    # filter down to only the visitors from Seattle CBGs\n    .filter(f.col(\"cbg\").rlike(\"^(53033|53053|53061)\"))\n    # aggregate up all the visitors over time from each CBG to each Cafe\n    .groupBy(\"placekey\", \"cbg\")\n    .agg(f.sum(\"visitors\").alias(\"visitors\"))\n    # join back with most up-to-date POI information\n    .join(cafes_latest.select(\"placekey\", \"latitude\", \"longitude\"), \"placekey\")\n    # transform geometry column\n    .withColumn(\n        \"cafe_geometry\",\n        f.expr(\n            \"ST_Point(CAST(longitude AS Decimal(24, 20)), CAST(latitude AS Decimal(24, 20)))\"\n        ),\n    )\n    .withColumn(\n        \"cafe_geometry\",\n        f.expr(\n            \"ST_Transform(ST_FlipCoordinates(cafe_geometry), 'epsg:4326','epsg:3857', false)\"\n        ),\n    )\n    # join with CBG geometries\n    .join(WA_cbgs, \"cbg\")\n)\n</pre> # Next let's prep our sample data sample_seattle_visitors = (     sample.select(\"placekey\", f.explode(\"visitor_home_cbgs\"))     .withColumnRenamed(\"key\", \"cbg\")     .withColumnRenamed(\"value\", \"visitors\")     #   # filter out CBGs with low visitor counts     .filter(f.col(\"visitors\") &gt; 4)     # filter down to only the visitors from Seattle CBGs     .filter(f.col(\"cbg\").rlike(\"^(53033|53053|53061)\"))     # aggregate up all the visitors over time from each CBG to each Cafe     .groupBy(\"placekey\", \"cbg\")     .agg(f.sum(\"visitors\").alias(\"visitors\"))     # join back with most up-to-date POI information     .join(cafes_latest.select(\"placekey\", \"latitude\", \"longitude\"), \"placekey\")     # transform geometry column     .withColumn(         \"cafe_geometry\",         f.expr(             \"ST_Point(CAST(longitude AS Decimal(24, 20)), CAST(latitude AS Decimal(24, 20)))\"         ),     )     .withColumn(         \"cafe_geometry\",         f.expr(             \"ST_Transform(ST_FlipCoordinates(cafe_geometry), 'epsg:4326','epsg:3857', false)\"         ),     )     # join with CBG geometries     .join(WA_cbgs, \"cbg\") ) In\u00a0[135]: Copied! <pre>sample_seattle_visitors.limit(10).toPandas().head()\n</pre> sample_seattle_visitors.limit(10).toPandas().head() <pre>                                                                                \r</pre> Out[135]: cbg placekey visitors latitude longitude cafe_geometry cbg_polygon_geometry cbg_geometry 0 530330002006 zzw-222@5x4-4d5-mrk 77 47.742573 -122.349879 POINT (-13619926.22889864 6064134.508228292) POLYGON ((-122.308696 47.733904, -122.308045 4... POINT (-13614894.7487456 6061924.520392932) 1 530330002006 223-222@5x4-48d-f4v 5 47.701524 -122.323637 POINT (-13617004.98282124 6057341.933516146) POLYGON ((-122.308696 47.733904, -122.308045 4... POINT (-13614894.7487456 6061924.520392932) 2 530330002006 zzw-226@5x4-48g-t5f 5 47.680164 -122.323755 POINT (-13617018.11852115 6053809.507537933) POLYGON ((-122.308696 47.733904, -122.308045 4... POINT (-13614894.7487456 6061924.520392932) 3 530330002006 223-222@5x4-4nz-pqf 5 47.71778 -122.313042 POINT (-13615825.55281628 6060031.251671663) POLYGON ((-122.308696 47.733904, -122.308045 4... POINT (-13614894.7487456 6061924.520392932) 4 530330002006 225-222@5x4-4d6-789 10 47.724882 -122.343856 POINT (-13619255.75160559 6061206.437371126) POLYGON ((-122.308696 47.733904, -122.308045 4... POINT (-13614894.7487456 6061924.520392932) In\u00a0[136]: Copied! <pre>distance_traveled_SEA = (\n    sample_seattle_visitors\n    # calculate the distance from home in meters\n    .withColumn(\n        \"distance_from_home\", f.expr(\"ST_Distance(cafe_geometry, cbg_geometry)\")\n    )\n)\ndistance_traveled_SEA.createOrReplaceTempView(\"distance_traveled_SEA\")\n</pre> distance_traveled_SEA = (     sample_seattle_visitors     # calculate the distance from home in meters     .withColumn(         \"distance_from_home\", f.expr(\"ST_Distance(cafe_geometry, cbg_geometry)\")     ) ) distance_traveled_SEA.createOrReplaceTempView(\"distance_traveled_SEA\") In\u00a0[137]: Copied! <pre>q = \"\"\"\nSELECT *\nFROM (\n  SELECT DISTINCT \n    placekey, \n    cbg,\n    visitors,\n    distance_from_home,\n    posexplode(split(repeat(\",\", visitors), \",\"))\n    FROM distance_traveled_SEA\n)\nWHERE pos &gt; 0\n\"\"\"\n\nweighted_median_tmp = spark.sql(q)\n</pre> q = \"\"\" SELECT * FROM (   SELECT DISTINCT      placekey,      cbg,     visitors,     distance_from_home,     posexplode(split(repeat(\",\", visitors), \",\"))     FROM distance_traveled_SEA ) WHERE pos &gt; 0 \"\"\"  weighted_median_tmp = spark.sql(q) In\u00a0[138]: Copied! <pre>grp_window = Window.partitionBy(\"placekey\")\nmedian_percentile = f.expr(\"percentile_approx(distance_from_home, 0.5)\")\n</pre> grp_window = Window.partitionBy(\"placekey\") median_percentile = f.expr(\"percentile_approx(distance_from_home, 0.5)\") In\u00a0[139]: Copied! <pre>weighted_median_tmp.limit(10).toPandas().head()\n</pre> weighted_median_tmp.limit(10).toPandas().head() <pre>                                                                                \r</pre> Out[139]: placekey cbg visitors distance_from_home pos col 0 zzw-222@5x4-4d5-mrk 530330002006 77 5495.437996 1 1 zzw-222@5x4-4d5-mrk 530330002006 77 5495.437996 2 2 zzw-222@5x4-4d5-mrk 530330002006 77 5495.437996 3 3 zzw-222@5x4-4d5-mrk 530330002006 77 5495.437996 4 4 zzw-222@5x4-4d5-mrk 530330002006 77 5495.437996 5 In\u00a0[140]: Copied! <pre>median_dist_traveled_SEA = weighted_median_tmp.groupBy(\"placekey\").agg(\n    median_percentile.alias(\"median_dist_traveled_SEA\")\n)\n</pre> median_dist_traveled_SEA = weighted_median_tmp.groupBy(\"placekey\").agg(     median_percentile.alias(\"median_dist_traveled_SEA\") ) In\u00a0[141]: Copied! <pre>median_dist_traveled_SEA.filter(f.col(\"median_dist_traveled_SEA\") &lt; 25000).limit(\n    10\n).toPandas().head()\n</pre> median_dist_traveled_SEA.filter(f.col(\"median_dist_traveled_SEA\") &lt; 25000).limit(     10 ).toPandas().head() <pre>                                                                                \r</pre> Out[141]: placekey median_dist_traveled_SEA 0 zzw-225@5x4-8xs-h3q 5373.004786 1 zzw-222@5x4-49x-jqf 1337.200428 2 224-222@5x4-4mg-9vf 2707.661156 3 222-222@5x4-4fr-mrk 9124.893780 4 zzw-223@5x4-8pj-b49 2364.791999 In\u00a0[142]: Copied! <pre>total_visits = sample.groupBy(\"placekey\").agg(\n    f.sum(\"raw_visit_counts\").alias(\"total_visits\"),\n    f.sum(\"raw_visitor_counts\").alias(\"total_visitors\"),\n)\n</pre> total_visits = sample.groupBy(\"placekey\").agg(     f.sum(\"raw_visit_counts\").alias(\"total_visits\"),     f.sum(\"raw_visitor_counts\").alias(\"total_visitors\"), ) In\u00a0[143]: Copied! <pre>distance_traveled_final = (\n    furthest_traveled.join(median_dist_traveled_SEA, \"placekey\")\n    .join(cafes_latest, [\"placekey\", \"location_name\"])\n    .withColumn(\n        \"distance_traveled_diff\",\n        f.col(\"avg_median_dist_from_home\") - f.col(\"median_dist_traveled_SEA\"),\n    )\n    # keep only the cafes with a meaningful sample - at least 1000 visits since 2018\n    .join(total_visits, \"placekey\")\n    .filter(f.col(\"total_visits\") &gt; 1000)\n    .orderBy(\"distance_traveled_diff\", ascending=False)\n)\n</pre> distance_traveled_final = (     furthest_traveled.join(median_dist_traveled_SEA, \"placekey\")     .join(cafes_latest, [\"placekey\", \"location_name\"])     .withColumn(         \"distance_traveled_diff\",         f.col(\"avg_median_dist_from_home\") - f.col(\"median_dist_traveled_SEA\"),     )     # keep only the cafes with a meaningful sample - at least 1000 visits since 2018     .join(total_visits, \"placekey\")     .filter(f.col(\"total_visits\") &gt; 1000)     .orderBy(\"distance_traveled_diff\", ascending=False) ) In\u00a0[144]: Copied! <pre>distance_traveled_final.limit(10).toPandas().head()\n</pre> distance_traveled_final.limit(10).toPandas().head() <pre>                                                                                \r</pre> Out[144]: placekey location_name avg_median_dist_from_home median_dist_traveled_SEA brands street_address city region postal_code latitude longitude open_hours distance_traveled_diff total_visits total_visitors 0 zzw-22f@5x4-4b5-p7q Starbucks 750913.558140 23663.589737 Starbucks 1912 Pike Pl Seattle WA 98101 47.6101 -122.342482 { \"Mon\": [[\"6:30\", \"20:30\"]], \"Tue\": [[\"6:30\",... 727249.968402 5195.0 4938.0 1 223-222@5x4-4b5-dvz Ghost Alley Espresso 727402.888889 26369.553927 None 1499 Post Aly Seattle WA 98101 47.608608 -122.340546 { \"Mon\": [[\"7:30\", \"17:30\"]], \"Tue\": [[\"7:30\",... 701033.334962 17286.0 16489.0 2 224-224@5x4-4b4-xt9 The Bar 667363.222222 290.137631 None 110 6th Ave N Seattle WA 98109 47.618842 -122.344648 None 667073.084592 15564.0 7781.0 3 22c-222@5x4-4b4-s5z Starbucks 654595.066667 1881.813510 Starbucks 1124 Pike St Seattle WA 98101 47.613994 -122.328201 { \"Mon\": [[\"7:00\", \"21:00\"]], \"Tue\": [[\"7:00\",... 652713.253157 52085.0 43177.0 4 zzw-222@5x4-4b5-pvz Cafe Opla 661286.800000 24168.052053 None 2200 Alaskan Way Ste 120 Seattle WA 98121 47.611012 -122.34752 None 637118.747947 14462.0 9118.0 In\u00a0[145]: Copied! <pre>distance_traveled_final.select(\"placekey\").distinct().count()\n</pre> distance_traveled_final.select(\"placekey\").distinct().count() <pre>                                                                                \r</pre> Out[145]: <pre>1429</pre> In\u00a0[146]: Copied! <pre>most_tourists = distance_traveled_final.limit(500).withColumn(\n    \"visitor_type\", f.lit(\"tourist\")\n)\nmost_locals = (\n    distance_traveled_final.orderBy(\"distance_traveled_diff\")\n    .limit(500)\n    .withColumn(\"visitor_type\", f.lit(\"local\"))\n)\n\nvisitor_type = most_tourists.unionByName(most_locals)\n\n# create a geopandas geodataframe\nvisitor_type_gdf = visitor_type.toPandas()\nvisitor_type_gdf = gpd.GeoDataFrame(\n    visitor_type_gdf,\n    geometry=gpd.points_from_xy(\n        visitor_type_gdf[\"longitude\"], visitor_type_gdf[\"latitude\"]\n    ),\n    crs=\"EPSG:4326\",\n)\n</pre> most_tourists = distance_traveled_final.limit(500).withColumn(     \"visitor_type\", f.lit(\"tourist\") ) most_locals = (     distance_traveled_final.orderBy(\"distance_traveled_diff\")     .limit(500)     .withColumn(\"visitor_type\", f.lit(\"local\")) )  visitor_type = most_tourists.unionByName(most_locals)  # create a geopandas geodataframe visitor_type_gdf = visitor_type.toPandas() visitor_type_gdf = gpd.GeoDataFrame(     visitor_type_gdf,     geometry=gpd.points_from_xy(         visitor_type_gdf[\"longitude\"], visitor_type_gdf[\"latitude\"]     ),     crs=\"EPSG:4326\", ) <pre>                                                                                \r</pre> In\u00a0[147]: Copied! <pre>def map_cafe_visitor_type(gdf):\n\n    # map bounds\n    sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]\n    ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]\n    folium_bounds = [sw, ne]\n\n    # map\n    x = gdf.centroid.x[0]\n    y = gdf.centroid.y[0]\n\n    map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")\n\n    for i, point in gdf.iterrows():\n\n        tooltip = f\"placekey: {point['placekey']}&lt;br&gt;location_name: {point['location_name']}&lt;br&gt;brands: {point['brands']}&lt;br&gt;street_address: {point['street_address']}&lt;br&gt;city: {point['city']}&lt;br&gt;region: {point['region']}&lt;br&gt;postal_code: {point['postal_code']}&lt;br&gt;visitor_type: {point['visitor_type']}&lt;br&gt;avg_median_dist_from_home: {point['avg_median_dist_from_home']}\"\n\n        folium.Circle(\n            [point[\"geometry\"].y, point[\"geometry\"].x],\n            radius=40,\n            fill_color=\"blue\" if point[\"visitor_type\"] == \"tourist\" else \"red\",\n            color=\"blue\" if point[\"visitor_type\"] == \"tourist\" else \"red\",\n            fill_opacity=1,\n            tooltip=tooltip,\n        ).add_to(map_)\n\n    map_.fit_bounds(folium_bounds)\n\n    return map_\n</pre> def map_cafe_visitor_type(gdf):      # map bounds     sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]     ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]     folium_bounds = [sw, ne]      # map     x = gdf.centroid.x[0]     y = gdf.centroid.y[0]      map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")      for i, point in gdf.iterrows():          tooltip = f\"placekey: {point['placekey']}location_name: {point['location_name']}brands: {point['brands']}street_address: {point['street_address']}city: {point['city']}region: {point['region']}postal_code: {point['postal_code']}visitor_type: {point['visitor_type']}avg_median_dist_from_home: {point['avg_median_dist_from_home']}\"          folium.Circle(             [point[\"geometry\"].y, point[\"geometry\"].x],             radius=40,             fill_color=\"blue\" if point[\"visitor_type\"] == \"tourist\" else \"red\",             color=\"blue\" if point[\"visitor_type\"] == \"tourist\" else \"red\",             fill_opacity=1,             tooltip=tooltip,         ).add_to(map_)      map_.fit_bounds(folium_bounds)      return map_ In\u00a0[148]: Copied! <pre># blue = touristy, red = locals\nmap_cafe_visitor_type(visitor_type_gdf)\n</pre> # blue = touristy, red = locals map_cafe_visitor_type(visitor_type_gdf) <pre>/tmp/ipykernel_2685102/2617676160.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  x = gdf.centroid.x[0]\n/tmp/ipykernel_2685102/2617676160.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  y = gdf.centroid.y[0]\n</pre> Out[148]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[149]: Copied! <pre>WA_neighbs = (\n    spark.read.option(\"header\", \"true\")\n    .option(\"escape\", '\"')\n    .csv(\n        \"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/seattle_neighborhoods.csv\"\n    )\n    # transform the geometry column into a Geometry-type\n    .withColumn(\"geometry\", f.expr(\"ST_GeomFromWkt(geometry)\"))\n)\nWA_neighbs.createOrReplaceTempView(\"WA_neighbs\")\n\nWA_neighbs.limit(10).toPandas().head()\n</pre> WA_neighbs = (     spark.read.option(\"header\", \"true\")     .option(\"escape\", '\"')     .csv(         \"file:///media/hdd1/code/sigspatial-2021-cafe-analysis/data/seattle_neighborhoods.csv\"     )     # transform the geometry column into a Geometry-type     .withColumn(\"geometry\", f.expr(\"ST_GeomFromWkt(geometry)\")) ) WA_neighbs.createOrReplaceTempView(\"WA_neighbs\")  WA_neighbs.limit(10).toPandas().head() Out[149]: S_HOOD L_HOOD geometry 0 OOO POLYGON ((-122.2739789529401 47.69522647266365... 1 OOO POLYGON ((-122.2875597861967 47.64522740482133... 2 OOO POLYGON ((-122.3952908582123 47.6651350445393,... 3 OOO POLYGON ((-122.3983207858678 47.66608770690774... 4 OOO POLYGON ((-122.2885127664106 47.65630022774357... In\u00a0[150]: Copied! <pre>cafes_geo = cafes_latest.withColumn(\n    \"cafe_geometry\",\n    f.expr(\n        \"ST_Point(CAST(longitude AS Decimal(24,20)), CAST(latitude AS Decimal(24,20)))\"\n    ),\n).select(\"placekey\", \"cafe_geometry\")\ncafes_geo.createOrReplaceTempView(\"cafes_geo\")\n</pre> cafes_geo = cafes_latest.withColumn(     \"cafe_geometry\",     f.expr(         \"ST_Point(CAST(longitude AS Decimal(24,20)), CAST(latitude AS Decimal(24,20)))\"     ), ).select(\"placekey\", \"cafe_geometry\") cafes_geo.createOrReplaceTempView(\"cafes_geo\") In\u00a0[151]: Copied! <pre># perform a spatial join\nq = \"\"\"\nSELECT cafes_geo.placekey, WA_neighbs.S_HOOD as neighborhood, WA_neighbs.geometry\nFROM WA_neighbs, cafes_geo\nWHERE ST_Intersects(WA_neighbs.geometry, cafes_geo.cafe_geometry)\n\"\"\"\n\ncafe_neighb_join = spark.sql(q)\n</pre> # perform a spatial join q = \"\"\" SELECT cafes_geo.placekey, WA_neighbs.S_HOOD as neighborhood, WA_neighbs.geometry FROM WA_neighbs, cafes_geo WHERE ST_Intersects(WA_neighbs.geometry, cafes_geo.cafe_geometry) \"\"\"  cafe_neighb_join = spark.sql(q) In\u00a0[152]: Copied! <pre>cafe_neighb_join.limit(10).toPandas().head()\n</pre> cafe_neighb_join.limit(10).toPandas().head() <pre>2022-05-22 14:03:39,421 WARN spatialOperator.JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n                                                                                \r</pre> Out[152]: placekey neighborhood geometry 0 222-222@5x4-4fr-mrk Adams POLYGON ((-122.3763365647225 47.67591769896643... 1 zzw-222@5x4-49x-jqf South Lake Union POLYGON ((-122.3296783220331 47.63972718752359... 2 223-222@5x4-487-3t9 Maple Leaf POLYGON ((-122.3300076901854 47.70863525163715... 3 zzw-224@5x4-4vs-jsq International District POLYGON ((-122.3202379357072 47.59582723132589... 4 223-222@5x4-4ft-3bk Sunset Hill POLYGON ((-122.402107196151 47.69768191273809,... In\u00a0[153]: Copied! <pre># add the visit and visitor counts and aggregate up to the neighborhood\nneighborhood_agg = (\n    cafe_neighb_join.join(total_visits, \"placekey\")\n    .groupBy(\"neighborhood\", f.col(\"geometry\").cast(\"string\").alias(\"geometry\"))\n    .agg(\n        f.sum(\"total_visits\").alias(\"total_visits\"),\n        f.sum(\"total_visitors\").alias(\"total_visitors\"),\n    )\n)\n</pre> # add the visit and visitor counts and aggregate up to the neighborhood neighborhood_agg = (     cafe_neighb_join.join(total_visits, \"placekey\")     .groupBy(\"neighborhood\", f.col(\"geometry\").cast(\"string\").alias(\"geometry\"))     .agg(         f.sum(\"total_visits\").alias(\"total_visits\"),         f.sum(\"total_visitors\").alias(\"total_visitors\"),     ) ) In\u00a0[154]: Copied! <pre>neighbs_gdf = neighborhood_agg.toPandas()\nneighbs_gdf = gpd.GeoDataFrame(\n    neighbs_gdf,\n    geometry=gpd.GeoSeries.from_wkt(neighbs_gdf[\"geometry\"]),\n    crs=\"EPSG:4326\",\n)\n</pre> neighbs_gdf = neighborhood_agg.toPandas() neighbs_gdf = gpd.GeoDataFrame(     neighbs_gdf,     geometry=gpd.GeoSeries.from_wkt(neighbs_gdf[\"geometry\"]),     crs=\"EPSG:4326\", ) <pre>2022-05-22 14:03:48,317 WARN spatialOperator.JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n                                                                                \r</pre> In\u00a0[155]: Copied! <pre>def map_neighbs(gdf):\n\n    # map bounds\n    sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]\n    ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]\n    folium_bounds = [sw, ne]\n\n    # map\n    x = gdf.centroid.x[0]\n    y = gdf.centroid.y[0]\n\n    map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")\n\n    gdf[\"percentile\"] = pd.qcut(gdf[\"total_visits\"], 100, labels=False) / 100\n\n    folium.GeoJson(\n        gdf[\n            [\"neighborhood\", \"total_visits\", \"total_visitors\", \"percentile\", \"geometry\"]\n        ],\n        style_function=lambda x: {\n            \"weight\": 0,\n            \"color\": \"blue\",\n            \"fillOpacity\": x[\"properties\"][\"percentile\"],\n        },\n        tooltip=folium.features.GeoJsonTooltip(\n            fields=[\"neighborhood\", \"total_visits\", \"total_visitors\", \"percentile\"]\n        ),\n    ).add_to(map_)\n\n    map_.fit_bounds(folium_bounds)\n\n    return map_\n</pre> def map_neighbs(gdf):      # map bounds     sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]     ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]     folium_bounds = [sw, ne]      # map     x = gdf.centroid.x[0]     y = gdf.centroid.y[0]      map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")      gdf[\"percentile\"] = pd.qcut(gdf[\"total_visits\"], 100, labels=False) / 100      folium.GeoJson(         gdf[             [\"neighborhood\", \"total_visits\", \"total_visitors\", \"percentile\", \"geometry\"]         ],         style_function=lambda x: {             \"weight\": 0,             \"color\": \"blue\",             \"fillOpacity\": x[\"properties\"][\"percentile\"],         },         tooltip=folium.features.GeoJsonTooltip(             fields=[\"neighborhood\", \"total_visits\", \"total_visitors\", \"percentile\"]         ),     ).add_to(map_)      map_.fit_bounds(folium_bounds)      return map_ In\u00a0[156]: Copied! <pre>map_neighbs(neighbs_gdf)\n</pre> map_neighbs(neighbs_gdf) <pre>/tmp/ipykernel_2685102/3760141471.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  x = gdf.centroid.x[0]\n/tmp/ipykernel_2685102/3760141471.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  y = gdf.centroid.y[0]\n</pre> Out[156]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[157]: Copied! <pre>home_loc_most_cafe_visitors = (\n    sample.select(f.explode(\"visitor_home_cbgs\"))\n    .withColumnRenamed(\"key\", \"cbg\")\n    .withColumnRenamed(\"value\", \"visitors\")\n    .groupBy(\"cbg\")\n    .agg(f.sum(\"visitors\").alias(\"visitors\"))\n    .orderBy(\"visitors\", ascending=False)\n)\n</pre> home_loc_most_cafe_visitors = (     sample.select(f.explode(\"visitor_home_cbgs\"))     .withColumnRenamed(\"key\", \"cbg\")     .withColumnRenamed(\"value\", \"visitors\")     .groupBy(\"cbg\")     .agg(f.sum(\"visitors\").alias(\"visitors\"))     .orderBy(\"visitors\", ascending=False) ) In\u00a0[158]: Copied! <pre># 40k visitors to Seattle coffee shops since Jan 2018 originated from CBG `530330082001`, just under twice as many as the next CBG.\nhome_loc_most_cafe_visitors.limit(10).toPandas().head()\n</pre> # 40k visitors to Seattle coffee shops since Jan 2018 originated from CBG `530330082001`, just under twice as many as the next CBG. home_loc_most_cafe_visitors.limit(10).toPandas().head() <pre>                                                                                \r</pre> Out[158]: cbg visitors 0 530330082001 40286 1 530330326023 20529 2 530330072001 19432 3 530530731251 18486 4 530530702031 18131 In\u00a0[159]: Copied! <pre>from shapely import wkt\n</pre> from shapely import wkt In\u00a0[160]: Copied! <pre># Map of the top 1000 CBGs in terms of visitors' origins\nhome_loc_gdf = (\n    home_loc_most_cafe_visitors.limit(1000)\n    .join(WA_cbgs.select(\"cbg\", \"cbg_polygon_geometry\"), \"cbg\")\n    .toPandas()\n)\nhome_loc_gdf = gpd.GeoDataFrame(\n    home_loc_gdf,\n    geometry=gpd.GeoSeries.from_wkt(home_loc_gdf[\"cbg_polygon_geometry\"]),\n    crs=\"EPSG:4326\",\n)\n</pre> # Map of the top 1000 CBGs in terms of visitors' origins home_loc_gdf = (     home_loc_most_cafe_visitors.limit(1000)     .join(WA_cbgs.select(\"cbg\", \"cbg_polygon_geometry\"), \"cbg\")     .toPandas() ) home_loc_gdf = gpd.GeoDataFrame(     home_loc_gdf,     geometry=gpd.GeoSeries.from_wkt(home_loc_gdf[\"cbg_polygon_geometry\"]),     crs=\"EPSG:4326\", ) <pre>                                                                                \r</pre> In\u00a0[161]: Copied! <pre>def map_cbgs(gdf):\n\n    # map bounds\n    sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]\n    ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]\n    folium_bounds = [sw, ne]\n\n    # map\n    x = gdf.centroid.x[0]\n    y = gdf.centroid.y[0]\n\n    map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")\n\n    gdf[\"quantile\"] = pd.qcut(gdf[\"visitors\"], 100, labels=False) / 100\n\n    folium.GeoJson(\n        gdf[[\"cbg\", \"visitors\", \"geometry\", \"quantile\"]],\n        style_function=lambda x: {\n            \"weight\": 0,\n            \"color\": \"blue\",\n            \"fillOpacity\": x[\"properties\"][\"quantile\"],\n        },\n        tooltip=folium.features.GeoJsonTooltip(fields=[\"cbg\", \"visitors\", \"quantile\"]),\n    ).add_to(map_)\n\n    map_.fit_bounds(folium_bounds)\n\n    return map_\n</pre> def map_cbgs(gdf):      # map bounds     sw = [gdf.unary_union.bounds[1], gdf.unary_union.bounds[0]]     ne = [gdf.unary_union.bounds[3], gdf.unary_union.bounds[2]]     folium_bounds = [sw, ne]      # map     x = gdf.centroid.x[0]     y = gdf.centroid.y[0]      map_ = folium.Map(location=[y, x], tiles=\"OpenStreetMap\")      gdf[\"quantile\"] = pd.qcut(gdf[\"visitors\"], 100, labels=False) / 100      folium.GeoJson(         gdf[[\"cbg\", \"visitors\", \"geometry\", \"quantile\"]],         style_function=lambda x: {             \"weight\": 0,             \"color\": \"blue\",             \"fillOpacity\": x[\"properties\"][\"quantile\"],         },         tooltip=folium.features.GeoJsonTooltip(fields=[\"cbg\", \"visitors\", \"quantile\"]),     ).add_to(map_)      map_.fit_bounds(folium_bounds)      return map_ In\u00a0[162]: Copied! <pre># top 1000 home_cbgs by number of people who visited coffee shops in Seattle\nmap_cbgs(home_loc_gdf)\n</pre> # top 1000 home_cbgs by number of people who visited coffee shops in Seattle map_cbgs(home_loc_gdf) <pre>/tmp/ipykernel_2685102/4182001321.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  x = gdf.centroid.x[0]\n/tmp/ipykernel_2685102/4182001321.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  y = gdf.centroid.y[0]\n</pre> Out[162]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usecases/contrib/foot-traffic/#understand-consumer-behavior-with-verified-foot-traffic-data","title":"Understand Consumer Behavior With Verified Foot Traffic Data\u00b6","text":"<p>Background Visitor and demographic aggregation data provide essential context on population behavior. How often a place of interest is visited, how long do visitors stay, where did they come from, and where are they going? The answers are invaluable in numerous industries. Building financial indicators, city and urban planning, public health indicators, or identifying your primary business competitors all require accurate, high quality population and POI data.</p> <p>Objective Our workshop\u2019s objective is to provide professionals, researchers, and practitioners interested in deriving human movement patterns from location data. We use a sample of our Weekly and Monthly Patterns and Core Places products to perform market research on a potential new coffee shop location. We\u2019ll address these concerns and more in building a market analysis proposal in real time.</p> <p>Questions to Answer</p> <ul> <li>How far are customers willing to travel for coffee?</li> <li>What location will receive the most visibility?</li> <li>Where do most of the coffee customers come from?</li> </ul>"},{"location":"usecases/contrib/foot-traffic/#notebook-setup","title":"Notebook Setup\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#load-in-safegraph-sample-data-from-s3","title":"Load in SafeGraph sample data from s3\u00b6","text":"<p>The covers all coffee shops (<code>category_tag.contains(\"Coffee Shop\")</code>) in Seattle (<code>county_FIPS.isin([\"53033\",\"53053\",\"53061\"]</code>), with multiple rows per POI corresponding to monthly foot traffic since the beginning of 2018 (1 month per row per POI).</p> <p>Columns are SafeGraph Core, Geo, and Patterns pre-joined together with <code>placekey</code> as the join key.</p>"},{"location":"usecases/contrib/foot-traffic/#exploratory-data-analysis-and-visualization","title":"Exploratory Data Analysis and Visualization\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#visualize-the-coffee-shops","title":"Visualize the coffee shops\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#analysis","title":"Analysis\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#how-far-are-people-willing-to-travel-for-coffee","title":"How far are people willing to travel for coffee?\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#what-location-will-receive-the-most-visibility","title":"What location will receive the most visibility?\u00b6","text":"<p>In other words, in what neighborhood do coffee shops get the most visits generally?</p>"},{"location":"usecases/contrib/foot-traffic/#what-home-location-has-the-most-coffee-shop-goers","title":"What home location has the most coffee-shop-goers?\u00b6","text":""},{"location":"usecases/contrib/foot-traffic/#conclusion","title":"Conclusion\u00b6","text":"<p>Apache Sedona provides a familiar and straight-forward spatial SQL API for performing distributed spatial queries.</p>"}]}