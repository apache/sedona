# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

name: Example project build
on:
  push:
    branches:
      - master
    paths:
      - 'examples/**'
      - '.github/workflows/example.yml'
  pull_request:
    branches:
      - '*'
    paths:
      - 'examples/**'
      - '.github/workflows/example.yml'

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}

jobs:
  build:
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        include:
          - spark: 4.0.1
            spark-compat: '4.0'
            sedona: 1.8.0
            hadoop: 3.4.2
          - spark: 3.5.0
            spark-compat: '3.5'
            sedona: 1.8.0
            hadoop: 3.3.4
          - spark: 3.4.2
            spark-compat: '3.4'
            sedona: 1.8.0
            hadoop: 3.3.4
    env:
      JAVA_TOOL_OPTIONS: >-
        -XX:+IgnoreUnrecognizedVMOptions
        --add-opens=java.base/java.lang=ALL-UNNAMED
        --add-opens=java.base/java.lang.invoke=ALL-UNNAMED
        --add-opens=java.base/java.lang.reflect=ALL-UNNAMED
        --add-opens=java.base/java.io=ALL-UNNAMED
        --add-opens=java.base/java.net=ALL-UNNAMED
        --add-opens=java.base/java.nio=ALL-UNNAMED
        --add-opens=java.base/java.util=ALL-UNNAMED
        --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
        --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED
        --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
        --add-opens=java.base/sun.nio.cs=ALL-UNNAMED
        --add-opens=java.base/sun.security.action=ALL-UNNAMED
        --add-opens=java.base/sun.util.calendar=ALL-UNNAMED
        -Djdk.reflect.useDirectMethodHandle=false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '17'
      - run: sudo apt-get remove scala-library scala
      - run: sudo wget www.scala-lang.org/files/archive/scala-2.12.11.deb
      - run: sudo dpkg -i scala-2.12.11.deb
      - run: scala -version
      - run: echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
      - run: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823
      - run: sudo apt-get update
      - run: sudo apt-get install sbt
      - name: Cache SBT
        uses: actions/cache@v4
        with:
          path: |
            ~/.ivy2/cache
            ~/.sbt
          key: ${{ runner.os }}-sbt-${{ hashFiles('**/build.sbt') }}
      - name: Cache Maven packages
        uses: actions/cache@v4
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2
      - env:
          SPARK_VERSION: ${{ matrix.spark }}
          SPARK_LOCAL_IP: 127.0.0.1
          SPARK_COMPAT_VERSION: ${{ matrix.spark-compat }}
          SEDONA_VERSION: ${{ matrix.sedona }}
          HADOOP_VERSION: ${{ matrix.hadoop }}
        run: |
          cd examples/spark-sql
          mvn versions:set -DnewVersion=${SEDONA_VERSION} -DgenerateBackupPoms=false
          mvn clean install \
            -Dspark.version=${SPARK_VERSION} \
            -Dspark.compat.version=${SPARK_COMPAT_VERSION} \
            -Dsedona.version=${SEDONA_VERSION} \
            -Dhadoop.version=${HADOOP_VERSION}
          java -jar target/sedona-spark-example-${SEDONA_VERSION}.jar
      - env:
          SEDONA_VERSION: ${{ matrix.sedona }}
        run: |
          cd examples/flink-sql
          mvn versions:set -DnewVersion=${SEDONA_VERSION} -DgenerateBackupPoms=false
          mvn clean install
          java -jar target/sedona-flink-example-${SEDONA_VERSION}.jar
